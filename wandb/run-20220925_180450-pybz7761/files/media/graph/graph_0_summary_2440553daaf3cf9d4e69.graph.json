{"format": "torch", "nodes": [{"name": "conv1", "id": 140542746894192, "class_name": "Conv2d(\n  self.stride=2, self.padding=(3, 3), self.weight=Parameter containing:\n  tensor([[[[-7.3529e-02,  5.8655e-02, -5.0740e-02,  ..., -8.1409e-02,\n             -2.6291e-02,  5.6172e-02],\n            [-7.0438e-02,  5.6207e-02, -4.3231e-02,  ...,  6.4760e-02,\n              1.6695e-02,  2.8689e-03],\n            [-7.7792e-02,  1.4351e-02, -5.9378e-02,  ..., -4.5321e-02,\n              4.6240e-02, -3.6768e-02],\n            ...,\n            [ 7.2065e-02, -1.2564e-02,  3.5008e-02,  ...,  4.8493e-02,\n             -4.7818e-02, -2.2586e-02],\n            [-1.8165e-02, -7.7114e-02,  4.8951e-02,  ..., -2.6002e-02,\n              5.0758e-02,  7.9708e-02],\n            [ 3.1287e-02, -1.6132e-02,  5.2782e-02,  ...,  1.8424e-03,\n              3.6161e-02, -7.3218e-02]],\n  \n           [[-5.2790e-02,  2.8783e-02, -7.7034e-02,  ..., -3.9660e-02,\n             -1.7024e-02, -7.8027e-02],\n            [-7.0383e-02, -1.2533e-02, -3.3575e-02,  ...,  4.5806e-02,\n             -6.1430e-02, -7.4624e-02],\n            [-4.0315e-02,  7.3910e-02, -2.0100e-02,  ..., -4.3380e-03,\n             -6.1844e-02, -4.0992e-02],\n            ...,\n            [ 7.9214e-02,  1.7894e-02,  7.9251e-02,  ..., -6.2155e-02,\n             -3.7272e-02,  8.2476e-02],\n            [-2.9307e-02,  1.9678e-02, -5.4932e-02,  ..., -7.9634e-04,\n              7.8517e-02,  3.6849e-02],\n            [ 3.1642e-02, -8.0052e-02, -7.4636e-02,  ..., -2.9098e-02,\n             -5.4972e-02, -4.4660e-02]],\n  \n           [[-7.2972e-02, -8.1353e-02, -6.7501e-02,  ...,  9.1160e-03,\n              2.3527e-02,  1.1965e-02],\n            [-3.8923e-02, -2.4739e-02, -2.7492e-02,  ...,  1.9328e-02,\n             -2.7227e-02, -3.1983e-03],\n            [ 3.0348e-02, -2.3690e-02,  3.3395e-02,  ...,  1.8958e-02,\n              1.9144e-03,  4.2858e-02],\n            ...,\n            [ 5.3267e-02,  2.6863e-02, -4.7820e-02,  ..., -2.3664e-02,\n              2.9751e-02,  7.4921e-02],\n            [-3.3841e-02,  6.5361e-02,  1.1825e-03,  ...,  2.2211e-02,\n             -8.1378e-02, -7.1938e-02],\n            [-4.5367e-02,  8.1425e-02, -2.9270e-02,  ..., -4.1883e-02,\n              4.4185e-02,  7.8057e-02]]],\n  \n  \n          [[[-3.9239e-02, -7.8628e-02,  6.4461e-02,  ..., -4.1265e-02,\n              2.7649e-04, -3.6388e-02],\n            [ 4.5442e-02,  3.4928e-02,  7.0907e-02,  ..., -6.1606e-02,\n             -6.3991e-03, -3.9908e-02],\n            [-7.8386e-02, -1.3402e-02, -3.5276e-02,  ..., -5.2406e-02,\n              2.8150e-02, -7.5995e-02],\n            ...,\n            [-1.2228e-02,  1.1270e-02,  3.9613e-03,  ..., -7.7388e-02,\n             -4.9856e-03, -7.4484e-02],\n            [-7.8631e-02, -2.8078e-02, -2.5701e-03,  ...,  8.0140e-02,\n             -1.7166e-02, -8.1644e-02],\n            [-3.1498e-02, -6.5141e-04, -2.2108e-02,  ...,  3.1883e-02,\n             -6.5288e-02, -6.7799e-02]],\n  \n           [[-7.1755e-02,  6.1093e-02,  5.3100e-02,  ..., -2.7189e-02,\n              6.5377e-02, -3.3785e-02],\n            [-4.5860e-02, -1.9720e-02, -3.6124e-02,  ..., -1.7590e-02,\n             -5.2338e-02, -7.6129e-02],\n            [ 5.4321e-02,  3.3917e-02, -5.2312e-02,  ...,  2.4942e-02,\n              1.5278e-02, -3.0463e-02],\n            ...,\n            [-5.7522e-03,  7.3455e-02,  4.1810e-02,  ...,  8.0075e-02,\n             -2.9005e-02,  5.7950e-02],\n            [-6.7752e-02,  5.2719e-02, -7.2178e-02,  ..., -3.3476e-02,\n             -1.6258e-02, -1.1891e-02],\n            [-5.8141e-02, -5.2231e-02, -2.1556e-02,  ..., -2.3583e-02,\n             -3.3568e-02,  5.5175e-02]],\n  \n           [[-4.7708e-03,  1.9804e-03,  6.7805e-02,  ..., -1.0498e-02,\n              4.7493e-02,  6.0498e-02],\n            [ 9.7740e-03,  3.8072e-02,  3.8535e-02,  ...,  2.7219e-02,\n              4.9046e-02,  3.0779e-02],\n            [ 2.9537e-02, -4.5909e-02, -4.3306e-02,  ..., -3.1355e-02,\n              5.7495e-02,  1.1626e-02],\n            ...,\n            [-6.0905e-02, -6.3124e-03,  5.3830e-02,  ..., -1.9342e-02,\n             -1.9088e-02,  1.7223e-02],\n            [-3.8035e-02,  6.0071e-02, -5.0341e-02,  ..., -2.0520e-02,\n             -7.7338e-02,  2.4714e-02],\n            [ 2.3067e-02,  1.9217e-02, -8.0472e-02,  ..., -7.9248e-02,\n              1.1838e-02, -7.1099e-02]]],\n  \n  \n          [[[-3.9845e-02,  6.8351e-03,  2.2102e-02,  ..., -2.0315e-02,\n             -8.1223e-04, -7.7169e-03],\n            [ 2.4427e-02,  7.6803e-02, -4.7814e-02,  ...,  2.1946e-02,\n             -2.2510e-02, -4.8026e-02],\n            [-7.2659e-02,  6.9618e-02, -7.8117e-02,  ..., -5.6384e-02,\n             -3.4492e-02,  3.9697e-02],\n            ...,\n            [-6.0632e-02, -2.5075e-03, -4.8349e-02,  ...,  6.1573e-02,\n              1.9271e-02,  3.8304e-02],\n            [-1.2256e-02,  5.7030e-02, -1.2389e-02,  ..., -5.5763e-02,\n              7.1187e-02, -1.2326e-02],\n            [-2.1349e-02,  3.0977e-02,  4.4046e-02,  ...,  2.0825e-02,\n              4.1985e-03, -3.1886e-02]],\n  \n           [[ 5.5835e-02,  3.0193e-02,  5.7887e-02,  ...,  3.5212e-02,\n             -4.2723e-02,  4.6681e-02],\n            [ 2.6523e-02,  4.9732e-02, -7.7275e-02,  ...,  9.8140e-03,\n             -7.4266e-02, -2.6441e-02],\n            [-1.4826e-02, -1.2191e-02, -3.4124e-03,  ..., -3.5034e-02,\n              2.0214e-02, -1.5767e-03],\n            ...,\n            [-7.9116e-02, -6.3946e-02, -4.7932e-03,  ..., -1.5941e-02,\n             -8.0358e-02, -8.3079e-03],\n            [ 8.1179e-02, -4.7295e-02,  7.7908e-02,  ...,  2.6822e-02,\n              2.4900e-02,  5.5315e-02],\n            [ 4.3364e-02,  1.2929e-02,  1.1767e-02,  ..., -1.8281e-02,\n              2.8092e-03, -6.6732e-02]],\n  \n           [[ 7.2626e-02, -7.5962e-03, -4.6649e-02,  ..., -1.7291e-02,\n              3.0932e-02,  6.4219e-02],\n            [ 1.5199e-02,  2.7546e-02,  1.6425e-03,  ..., -6.3426e-02,\n              4.8991e-02, -2.2466e-02],\n            [-3.7132e-02,  6.8075e-02, -2.6642e-02,  ...,  2.9591e-02,\n              1.7570e-03,  7.4187e-02],\n            ...,\n            [ 4.6865e-02, -4.8113e-02, -2.2644e-03,  ...,  4.1863e-02,\n              7.2421e-02, -4.0479e-02],\n            [-4.8506e-02,  4.7013e-02, -7.7992e-02,  ..., -2.0358e-02,\n              2.1265e-02, -7.6862e-02],\n            [-6.6289e-02, -1.7855e-02,  6.0865e-03,  ...,  5.6748e-02,\n              1.8317e-05, -3.5414e-02]]],\n  \n  \n          ...,\n  \n  \n          [[[ 4.6322e-02, -7.5996e-02,  6.0013e-02,  ...,  2.1265e-02,\n              2.1892e-02,  2.9449e-02],\n            [ 7.7261e-02, -5.0971e-02, -3.9722e-02,  ...,  8.0020e-02,\n              3.7253e-02, -3.2729e-02],\n            [ 7.3240e-02, -5.7355e-02,  3.0171e-02,  ..., -2.7031e-02,\n              7.7865e-02,  7.8940e-02],\n            ...,\n            [ 5.9073e-02, -7.2154e-02,  3.5092e-02,  ...,  3.6828e-02,\n              7.4580e-02,  2.4495e-02],\n            [ 1.7500e-02,  4.2760e-02,  5.7321e-03,  ...,  5.0464e-02,\n             -1.6016e-02, -1.4714e-02],\n            [-1.0940e-02, -1.0644e-02,  4.3692e-02,  ...,  1.4755e-02,\n              1.0623e-02, -7.5444e-02]],\n  \n           [[ 1.7345e-02, -7.2845e-02,  4.7526e-02,  ..., -5.9763e-02,\n              8.1748e-02, -8.0755e-02],\n            [-4.2853e-02,  2.8194e-02, -4.6218e-02,  ..., -1.9780e-03,\n             -4.2579e-02, -2.2408e-02],\n            [-4.7047e-02, -1.8556e-02, -6.6435e-02,  ...,  6.4723e-02,\n             -5.6157e-02, -8.0836e-02],\n            ...,\n            [-8.7345e-03,  4.0503e-02,  6.7820e-02,  ...,  1.8841e-02,\n             -6.9789e-02, -4.0402e-02],\n            [ 3.6067e-02, -5.9329e-02, -3.1167e-02,  ..., -4.2051e-02,\n              4.0174e-02, -5.1958e-02],\n            [-9.3212e-03,  7.5415e-02,  7.5685e-02,  ...,  1.2836e-02,\n              1.9177e-03,  8.1470e-02]],\n  \n           [[-2.9221e-02,  8.4204e-03,  6.2090e-02,  ...,  6.7233e-02,\n             -6.0404e-02,  7.1496e-02],\n            [ 7.3562e-02,  2.5436e-02, -4.7223e-02,  ...,  8.2407e-02,\n              7.5885e-02,  1.5265e-02],\n            [ 3.8989e-03,  8.1135e-02,  2.4990e-02,  ..., -1.1533e-02,\n              7.2848e-02,  8.0422e-02],\n            ...,\n            [-3.0036e-02, -4.8353e-02,  4.1100e-02,  ...,  2.8986e-02,\n             -3.8346e-03, -1.7689e-02],\n            [-6.4173e-02, -1.4865e-03, -1.9262e-02,  ...,  2.4688e-03,\n              1.8047e-02, -2.5359e-02],\n            [ 6.4813e-02, -5.7376e-02,  7.2490e-02,  ..., -4.1259e-02,\n              4.9824e-02,  4.9745e-02]]],\n  \n  \n          [[[-9.6774e-03, -3.1167e-02, -2.9369e-02,  ...,  6.9630e-02,\n              7.7311e-02, -1.7736e-03],\n            [-8.1471e-02, -3.6943e-02,  6.0900e-02,  ..., -3.2889e-02,\n             -1.7838e-02,  7.9932e-02],\n            [-8.0337e-02,  6.4789e-02,  5.3608e-02,  ..., -6.9032e-02,\n             -3.2963e-02, -4.9948e-02],\n            ...,\n            [-6.2631e-02, -6.0163e-03, -2.3077e-03,  ...,  6.3200e-02,\n              6.1951e-02,  6.8393e-02],\n            [-1.1424e-02,  5.1675e-02,  6.0782e-03,  ..., -7.1098e-02,\n             -3.7711e-02, -5.7841e-02],\n            [ 8.4408e-04, -3.6643e-03, -4.6079e-02,  ...,  4.1148e-02,\n             -1.0505e-02,  3.3329e-03]],\n  \n           [[ 4.8349e-02,  2.2057e-02, -7.4719e-02,  ..., -7.4314e-02,\n             -6.6321e-02,  4.9299e-02],\n            [ 4.0466e-02, -5.4869e-02, -7.6810e-02,  ...,  6.4292e-02,\n             -9.1574e-03, -2.0008e-02],\n            [-5.0995e-02,  5.8854e-02,  5.4068e-02,  ...,  1.4186e-02,\n              3.5931e-02,  3.1572e-02],\n            ...,\n            [-4.6813e-02, -1.9144e-02,  1.1497e-03,  ...,  2.4488e-02,\n              4.1857e-02, -3.0187e-02],\n            [ 5.1009e-02,  2.2718e-02,  5.8008e-02,  ..., -7.9528e-02,\n              3.8533e-02,  2.3727e-02],\n            [ 5.6089e-02, -5.4413e-02,  7.0458e-02,  ...,  5.6616e-02,\n              3.8404e-02,  3.6890e-02]],\n  \n           [[ 7.7615e-04, -6.9068e-02, -4.0060e-02,  ..., -3.5101e-02,\n              5.4067e-04,  7.1146e-02],\n            [ 8.0234e-02,  5.8845e-02,  7.5113e-02,  ..., -7.5864e-03,\n             -7.7192e-02, -5.4089e-02],\n            [-5.8960e-02, -2.8629e-02,  6.7786e-02,  ...,  3.9138e-02,\n             -4.2331e-02, -7.7581e-04],\n            ...,\n            [ 6.7116e-02, -5.9686e-03,  6.0617e-02,  ...,  6.8425e-02,\n              8.0552e-02,  7.6768e-02],\n            [-3.0353e-02, -4.8583e-02,  2.8673e-03,  ..., -5.8127e-02,\n             -8.2273e-02,  8.1263e-02],\n            [ 2.5662e-02, -4.2401e-02,  6.4866e-02,  ..., -7.7653e-02,\n             -2.9515e-02,  5.9720e-02]]],\n  \n  \n          [[[ 1.5431e-02,  4.3721e-02,  6.0112e-02,  ...,  1.3211e-02,\n              5.7752e-02,  6.3691e-02],\n            [-5.9683e-02,  7.9153e-02, -5.5717e-02,  ...,  6.2882e-02,\n              7.0677e-02, -7.1719e-02],\n            [-8.0590e-02,  3.4036e-02, -6.3418e-02,  ...,  7.7951e-02,\n             -4.7922e-02, -6.1463e-02],\n            ...,\n            [ 3.6019e-02, -5.1479e-02,  4.2024e-02,  ...,  7.3245e-02,\n              4.8531e-02,  5.6512e-02],\n            [ 6.5231e-02, -2.7555e-02,  4.9962e-02,  ...,  2.9864e-02,\n              7.1925e-02,  5.0524e-02],\n            [-5.7682e-02, -3.8342e-02,  6.1170e-02,  ...,  4.0467e-02,\n              7.5106e-02, -2.7220e-02]],\n  \n           [[-7.2112e-03,  4.3382e-02,  2.7614e-02,  ...,  7.3892e-02,\n             -8.7881e-03, -3.9477e-02],\n            [-3.9627e-02,  7.7085e-02, -1.0750e-02,  ..., -5.5556e-02,\n             -8.8009e-03,  6.3480e-02],\n            [ 5.6345e-03, -7.9510e-03,  9.3189e-03,  ...,  4.2809e-02,\n             -1.9401e-02, -6.7664e-02],\n            ...,\n            [ 4.5436e-02,  8.1781e-02,  8.8091e-03,  ...,  6.4748e-02,\n             -6.9688e-03, -2.7663e-02],\n            [ 5.6390e-02, -1.9163e-02,  4.5077e-02,  ...,  5.0892e-02,\n             -7.5516e-02, -3.2398e-02],\n            [ 5.7342e-02,  2.4154e-02,  4.3212e-02,  ..., -6.2164e-02,\n             -7.1499e-02, -5.1716e-02]],\n  \n           [[-7.1289e-02, -4.4947e-02, -7.6235e-02,  ...,  6.3936e-02,\n             -1.0383e-02,  1.0842e-02],\n            [-4.6772e-02,  1.4888e-02,  4.7335e-02,  ...,  6.3537e-02,\n             -5.8266e-02, -1.3852e-02],\n            [-7.0253e-02,  1.5063e-02,  2.9814e-02,  ...,  2.0887e-02,\n             -4.5730e-02,  7.3794e-02],\n            ...,\n            [ 4.2663e-02,  8.1215e-02, -3.8864e-02,  ...,  1.7886e-02,\n             -3.0285e-02, -6.7441e-02],\n            [ 5.1390e-02, -4.3807e-02,  2.2318e-02,  ...,  7.3095e-03,\n              5.1309e-02, -1.3087e-02],\n            [-4.5030e-02,  1.2644e-02,  3.1425e-03,  ...,  2.1652e-02,\n              2.8163e-02, -4.5664e-02]]]], requires_grad=True)\n)", "parameters": [["weight", [64, 3, 7, 7]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [9408]}, {"name": "bn1", "id": 140542746894000, "class_name": "BatchNorm2d(\n  self.momentum=0.1, self.weight=Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 2.3582e-03,  2.4677e-03, -2.9421e-04, -2.3270e-03,  1.1699e-03,\n          -1.2948e-03, -1.3704e-03,  9.7066e-04, -1.7293e-03,  1.2688e-03,\n          -1.3337e-03, -3.3487e-04, -1.5891e-03,  1.2395e-03, -4.3563e-04,\n           1.0389e-04,  1.6233e-03, -1.1954e-03,  1.6574e-03,  2.6404e-03,\n          -2.5197e-03,  8.8664e-04,  5.3089e-04,  2.0262e-03, -4.8220e-04,\n          -1.4319e-03, -8.5548e-04, -2.7525e-03, -7.5104e-04,  1.2840e-03,\n           3.1993e-03,  5.2388e-04, -1.1091e-03, -3.7811e-03,  2.3182e-03,\n          -3.3885e-03,  2.9513e-03, -2.6606e-06,  2.1112e-03, -3.9997e-04,\n          -1.7323e-03,  9.7441e-04,  1.3110e-03, -1.9137e-03,  1.3212e-04,\n          -1.1717e-03, -2.3448e-03,  3.5908e-04,  5.9936e-04,  1.6443e-03,\n           2.5052e-03,  1.0566e-03, -1.5111e-03, -2.0725e-03, -1.8424e-03,\n           8.0121e-04,  4.8616e-04, -1.7354e-03, -3.8259e-03, -8.7397e-04,\n          -2.1749e-03, -9.2181e-04, -1.0749e-03, -1.2819e-03],\n         grad_fn=<AddBackward0>), self.running_var=tensor([0.9562, 1.0700, 0.9086, 1.0700, 0.9433, 0.9279, 0.9572, 0.9355, 0.9454,\n          0.9427, 0.9971, 0.9420, 1.0615, 0.9430, 0.9193, 0.9342, 0.9280, 0.9264,\n          1.0028, 0.9366, 1.0425, 0.9237, 0.9686, 1.0173, 0.9251, 0.9532, 0.9356,\n          0.9317, 0.9205, 0.9288, 1.0838, 0.9184, 0.9257, 1.0068, 0.9178, 1.0287,\n          0.9622, 0.9406, 0.9847, 0.9093, 1.1554, 0.9400, 0.9201, 0.9661, 0.9364,\n          0.9345, 0.9772, 0.9229, 0.9278, 0.9158, 0.9815, 0.9228, 0.9257, 0.9673,\n          0.9579, 0.9300, 0.9196, 0.9356, 0.9523, 0.9312, 0.9409, 0.9682, 0.9172,\n          1.0001], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n)", "parameters": [["weight", [64]], ["bias", [64]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [64, 64]}, {"name": "relu", "id": 140542746894048, "class_name": "ReLU()", "parameters": [], "output_shape": [[512, 64, 16, 16]], "num_parameters": []}, {"name": "pool", "id": 140542746893952, "class_name": "MaxPool2d(self.kernel_size=(3, 3), self.stride=(3, 3), self.padding=1)", "parameters": [], "output_shape": [[512, 64, 6, 6]], "num_parameters": []}, {"name": "layer1", "id": 140542746892752, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 0.0015,  0.0299, -0.0343],\n                [ 0.0265,  0.0361,  0.0294],\n                [-0.0177, -0.0053,  0.0097]],\n      \n               [[ 0.0071, -0.0355, -0.0037],\n                [-0.0308, -0.0235, -0.0360],\n                [-0.0292, -0.0315, -0.0156]],\n      \n               [[ 0.0175,  0.0282, -0.0414],\n                [-0.0356,  0.0383,  0.0072],\n                [-0.0405,  0.0341,  0.0160]],\n      \n               ...,\n      \n               [[ 0.0016, -0.0366,  0.0180],\n                [ 0.0166, -0.0327, -0.0030],\n                [-0.0210,  0.0156,  0.0068]],\n      \n               [[-0.0241,  0.0044,  0.0364],\n                [-0.0283, -0.0131,  0.0189],\n                [ 0.0002,  0.0336, -0.0132]],\n      \n               [[ 0.0225,  0.0128, -0.0155],\n                [ 0.0034,  0.0269, -0.0149],\n                [-0.0339, -0.0152,  0.0186]]],\n      \n      \n              [[[-0.0246,  0.0275, -0.0277],\n                [-0.0040,  0.0404,  0.0078],\n                [ 0.0332, -0.0108,  0.0333]],\n      \n               [[ 0.0049, -0.0168, -0.0260],\n                [-0.0141,  0.0128, -0.0158],\n                [ 0.0278, -0.0262,  0.0044]],\n      \n               [[-0.0328, -0.0012,  0.0406],\n                [-0.0120, -0.0159,  0.0196],\n                [-0.0321,  0.0086, -0.0345]],\n      \n               ...,\n      \n               [[-0.0305, -0.0069,  0.0155],\n                [-0.0107, -0.0136,  0.0013],\n                [ 0.0279, -0.0284,  0.0054]],\n      \n               [[ 0.0318,  0.0137,  0.0344],\n                [-0.0160,  0.0195, -0.0342],\n                [-0.0076, -0.0128, -0.0260]],\n      \n               [[ 0.0112, -0.0235,  0.0316],\n                [-0.0391,  0.0416, -0.0034],\n                [-0.0011, -0.0112, -0.0202]]],\n      \n      \n              [[[-0.0228, -0.0161, -0.0033],\n                [ 0.0182,  0.0381,  0.0365],\n                [-0.0133,  0.0009,  0.0261]],\n      \n               [[ 0.0311, -0.0014, -0.0125],\n                [-0.0198,  0.0250,  0.0140],\n                [-0.0256, -0.0173,  0.0361]],\n      \n               [[-0.0300, -0.0291, -0.0066],\n                [ 0.0321, -0.0321, -0.0415],\n                [ 0.0078, -0.0184, -0.0225]],\n      \n               ...,\n      \n               [[ 0.0376,  0.0023,  0.0400],\n                [ 0.0034,  0.0208, -0.0365],\n                [ 0.0062,  0.0241, -0.0071]],\n      \n               [[-0.0367,  0.0341, -0.0189],\n                [-0.0375,  0.0083, -0.0366],\n                [ 0.0184, -0.0151, -0.0328]],\n      \n               [[-0.0199, -0.0245, -0.0265],\n                [ 0.0067, -0.0174, -0.0358],\n                [ 0.0154,  0.0143,  0.0288]]],\n      \n      \n              ...,\n      \n      \n              [[[ 0.0012, -0.0103, -0.0159],\n                [-0.0260, -0.0303,  0.0256],\n                [ 0.0393,  0.0072,  0.0248]],\n      \n               [[-0.0360, -0.0385,  0.0321],\n                [-0.0156,  0.0393, -0.0398],\n                [-0.0301, -0.0113, -0.0269]],\n      \n               [[-0.0126, -0.0036, -0.0158],\n                [-0.0216,  0.0292, -0.0409],\n                [ 0.0165,  0.0052, -0.0115]],\n      \n               ...,\n      \n               [[-0.0343, -0.0228,  0.0399],\n                [-0.0161, -0.0113,  0.0120],\n                [ 0.0354,  0.0049, -0.0365]],\n      \n               [[-0.0042, -0.0278,  0.0330],\n                [ 0.0144,  0.0216,  0.0114],\n                [ 0.0308,  0.0266, -0.0263]],\n      \n               [[-0.0030,  0.0181, -0.0291],\n                [ 0.0236, -0.0365,  0.0343],\n                [ 0.0144,  0.0382,  0.0163]]],\n      \n      \n              [[[-0.0107, -0.0134, -0.0290],\n                [ 0.0239,  0.0244,  0.0111],\n                [ 0.0353, -0.0408,  0.0200]],\n      \n               [[ 0.0414,  0.0146, -0.0073],\n                [ 0.0084,  0.0416, -0.0298],\n                [ 0.0247,  0.0230,  0.0398]],\n      \n               [[ 0.0257,  0.0239,  0.0075],\n                [-0.0378,  0.0018, -0.0336],\n                [ 0.0183,  0.0222, -0.0089]],\n      \n               ...,\n      \n               [[-0.0397,  0.0261, -0.0275],\n                [-0.0014, -0.0238, -0.0234],\n                [ 0.0313, -0.0299, -0.0411]],\n      \n               [[-0.0253, -0.0226, -0.0413],\n                [-0.0093,  0.0141, -0.0075],\n                [ 0.0416,  0.0244,  0.0048]],\n      \n               [[ 0.0210, -0.0205,  0.0095],\n                [ 0.0255,  0.0357,  0.0318],\n                [-0.0294, -0.0285, -0.0177]]],\n      \n      \n              [[[-0.0128,  0.0085, -0.0090],\n                [ 0.0277,  0.0314, -0.0241],\n                [ 0.0269, -0.0374,  0.0273]],\n      \n               [[-0.0380, -0.0336,  0.0101],\n                [ 0.0410, -0.0252, -0.0049],\n                [ 0.0019,  0.0240, -0.0053]],\n      \n               [[-0.0395, -0.0263, -0.0027],\n                [-0.0101,  0.0061, -0.0028],\n                [-0.0011,  0.0398,  0.0258]],\n      \n               ...,\n      \n               [[ 0.0095, -0.0295,  0.0414],\n                [ 0.0167, -0.0079, -0.0110],\n                [ 0.0158,  0.0143,  0.0216]],\n      \n               [[ 0.0272,  0.0253,  0.0023],\n                [ 0.0039, -0.0332,  0.0349],\n                [ 0.0285,  0.0209,  0.0105]],\n      \n               [[ 0.0276, -0.0317, -0.0216],\n                [-0.0188, -0.0255,  0.0387],\n                [-0.0348,  0.0314,  0.0240]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-1.9539e-03, -1.8830e-03, -5.1599e-02,  3.1993e-02,  5.4130e-02,\n               6.0196e-02, -1.1039e-02, -4.3128e-02,  3.2614e-02,  5.8988e-03,\n               1.1644e-01, -3.3316e-02, -5.0275e-02,  2.5081e-02,  1.1454e-02,\n               1.0050e-02,  2.4385e-02, -2.6312e-02, -4.7281e-02, -6.6418e-02,\n              -5.1294e-02,  4.6522e-02,  3.8439e-02, -4.9751e-02, -1.2224e-02,\n               5.7571e-02,  5.9006e-02, -9.1753e-04,  3.1526e-02, -1.2997e-02,\n              -5.8139e-02, -2.8276e-02,  2.2992e-02,  5.7186e-02, -2.5050e-02,\n               9.1188e-02,  1.4976e-02,  4.3822e-02,  6.4685e-02, -7.4368e-02,\n               5.7532e-02, -1.2067e-02, -2.6159e-02,  9.4757e-05, -7.8184e-02,\n               6.4075e-03,  4.4700e-03, -5.2765e-04, -5.7305e-03,  2.4519e-02,\n               3.8399e-02,  4.9790e-02, -1.4923e-04, -3.2638e-02, -7.4526e-02,\n               2.7748e-02,  7.0681e-02,  2.1808e-02, -1.3950e-02,  5.5000e-02,\n              -1.3700e-02, -8.7059e-02, -6.6804e-03, -2.9221e-02],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9155, 0.9299, 0.9210, 0.9284, 0.9118, 0.9217, 0.9114, 0.9234, 0.9098,\n              0.9200, 0.9438, 0.9267, 0.9180, 0.9179, 0.9210, 0.9219, 0.9156, 0.9113,\n              0.9292, 0.9194, 0.9245, 0.9182, 0.9173, 0.9129, 0.9158, 0.9191, 0.9194,\n              0.9112, 0.9138, 0.9138, 0.9149, 0.9406, 0.9105, 0.9278, 0.9149, 0.9279,\n              0.9088, 0.9202, 0.9252, 0.9234, 0.9251, 0.9200, 0.9197, 0.9180, 0.9295,\n              0.9117, 0.9321, 0.9144, 0.9148, 0.9082, 0.9195, 0.9172, 0.9114, 0.9233,\n              0.9252, 0.9205, 0.9228, 0.9229, 0.9103, 0.9168, 0.9152, 0.9335, 0.9182,\n              0.9220], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-3.2160e-02,  1.9988e-03,  1.3413e-02],\n                [ 3.0812e-02,  3.8994e-02, -9.3152e-03],\n                [-3.8887e-02, -2.1774e-02, -3.1950e-02]],\n      \n               [[ 3.4804e-03,  1.4030e-04,  3.7905e-02],\n                [-1.7303e-02,  3.2779e-03, -1.9226e-02],\n                [-2.7795e-02, -3.2762e-02, -4.1253e-02]],\n      \n               [[ 3.3506e-02, -7.1769e-03, -2.2292e-02],\n                [-2.4511e-02,  2.7747e-02, -2.0072e-02],\n                [ 2.7214e-02, -2.3989e-02, -7.4794e-03]],\n      \n               ...,\n      \n               [[-2.5727e-02,  1.1504e-02,  1.3038e-02],\n                [ 7.6316e-04, -2.9746e-02,  2.5023e-02],\n                [-3.2609e-02, -2.2564e-02,  1.0870e-02]],\n      \n               [[-1.5856e-02, -4.0498e-02, -5.7089e-03],\n                [-2.5323e-02, -3.4030e-02,  2.2887e-04],\n                [-6.4563e-03,  1.8715e-02,  3.1048e-02]],\n      \n               [[ 2.6337e-02,  1.7412e-02, -7.8304e-03],\n                [ 1.0804e-02, -5.4200e-03,  3.2132e-02],\n                [-8.4544e-03, -3.1677e-02, -1.1447e-02]]],\n      \n      \n              [[[-7.9247e-03, -2.1780e-02, -3.2189e-02],\n                [-3.9702e-02, -4.0125e-02,  9.8532e-03],\n                [-1.6422e-02,  4.1370e-02,  2.3968e-02]],\n      \n               [[-4.1152e-02,  2.4577e-02, -2.4963e-02],\n                [-2.8253e-02,  2.4039e-02,  2.0428e-02],\n                [ 3.6979e-02, -2.6246e-02, -2.0573e-02]],\n      \n               [[-2.6227e-02, -3.5694e-03,  3.1061e-02],\n                [-1.4962e-02, -4.2493e-03,  2.8702e-02],\n                [ 3.6465e-02, -3.4656e-02,  1.9332e-03]],\n      \n               ...,\n      \n               [[-9.5156e-03,  3.4934e-02,  8.5265e-03],\n                [-3.0088e-02,  5.6772e-03, -2.3027e-02],\n                [ 3.3597e-02, -3.6664e-02, -1.1690e-02]],\n      \n               [[-1.2130e-02,  4.2731e-03, -1.5270e-02],\n                [ 9.3228e-03,  1.7408e-02, -2.6489e-02],\n                [-4.4572e-03,  3.5885e-02,  1.7134e-02]],\n      \n               [[ 2.7229e-02,  1.2340e-02,  1.4539e-02],\n                [ 1.1996e-02, -3.3332e-02,  2.1888e-02],\n                [ 2.5703e-02,  3.4500e-03,  1.4800e-02]]],\n      \n      \n              [[[-1.6425e-02, -1.9252e-02,  3.7417e-02],\n                [ 2.2231e-02,  8.1770e-03,  1.5472e-02],\n                [-1.3198e-02, -3.2483e-02, -2.4470e-02]],\n      \n               [[ 6.6141e-03,  3.4577e-02,  2.1062e-02],\n                [ 2.5319e-02,  3.2535e-02,  2.4243e-02],\n                [ 3.5487e-02, -8.7315e-03,  3.0108e-02]],\n      \n               [[ 2.7811e-02,  2.9729e-04, -3.1484e-02],\n                [-1.1021e-03, -8.7578e-03, -3.9680e-02],\n                [-4.1461e-02, -9.6659e-03, -1.0283e-02]],\n      \n               ...,\n      \n               [[-3.5264e-02,  1.2200e-02,  3.0361e-02],\n                [ 4.1218e-02, -1.6280e-02,  2.4842e-02],\n                [ 1.9557e-02,  3.3870e-02, -7.9648e-03]],\n      \n               [[-2.9135e-02, -2.3988e-02, -4.5473e-03],\n                [-2.6659e-02,  4.5182e-03,  3.3494e-03],\n                [-3.6715e-02,  4.8880e-04, -3.7020e-02]],\n      \n               [[-2.0418e-02,  4.0055e-02,  3.5620e-02],\n                [ 1.7520e-02,  1.2280e-02, -1.1366e-03],\n                [ 1.1755e-02, -2.9739e-02,  2.7969e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[-1.1218e-02, -4.9319e-03, -1.4799e-02],\n                [-7.1906e-03, -4.0451e-02,  1.8699e-02],\n                [ 5.8707e-03, -2.0628e-03, -3.5648e-03]],\n      \n               [[-1.1247e-02, -1.5916e-02,  2.0244e-02],\n                [ 3.2532e-03,  3.1661e-02,  4.0477e-02],\n                [ 2.5959e-02,  1.1087e-02, -4.4867e-03]],\n      \n               [[ 2.8990e-03, -6.7922e-03, -5.7533e-03],\n                [-6.9868e-03,  3.5190e-03, -2.3264e-02],\n                [ 2.1694e-02, -4.1334e-02, -3.0819e-03]],\n      \n               ...,\n      \n               [[ 1.8296e-02, -3.8805e-02,  3.8491e-02],\n                [-3.4594e-02,  3.1933e-02, -2.0133e-02],\n                [ 3.3298e-02, -1.8478e-03,  5.2376e-03]],\n      \n               [[ 1.1736e-02,  7.3878e-03, -9.0548e-03],\n                [ 3.6853e-02,  3.0597e-02,  3.6282e-02],\n                [-2.0905e-02,  1.7928e-02, -9.4197e-03]],\n      \n               [[-3.5481e-02, -3.2837e-02, -1.3431e-02],\n                [-3.2975e-02, -1.3236e-02, -7.2774e-03],\n                [-3.0065e-03,  6.3051e-03,  9.1896e-03]]],\n      \n      \n              [[[-2.7291e-02,  1.0564e-03, -2.0444e-02],\n                [-5.9719e-03,  1.7792e-02,  2.7106e-02],\n                [-3.8401e-02,  8.9525e-03,  3.0181e-03]],\n      \n               [[ 2.3438e-02,  4.0725e-02, -2.0729e-02],\n                [ 5.5905e-03,  3.4744e-02, -2.4355e-02],\n                [-7.2675e-03,  1.4131e-02, -1.3830e-03]],\n      \n               [[ 3.3177e-02, -8.6122e-03, -2.2569e-02],\n                [-3.0897e-02, -7.0785e-05,  3.7650e-02],\n                [ 2.2885e-02,  2.0122e-04,  2.3991e-02]],\n      \n               ...,\n      \n               [[ 2.6115e-02,  1.2363e-02, -9.2359e-03],\n                [-2.3989e-02,  6.5537e-03,  1.0277e-02],\n                [-6.4855e-03,  4.4730e-03,  3.9814e-02]],\n      \n               [[-1.1394e-02, -8.4912e-05, -3.9173e-02],\n                [ 3.9366e-02, -1.5939e-03, -3.9179e-02],\n                [-4.6783e-03, -1.2882e-02, -2.0245e-02]],\n      \n               [[ 9.8604e-03,  1.8095e-02, -3.1314e-02],\n                [-3.0543e-02,  5.6958e-03, -3.8401e-03],\n                [ 3.1023e-02,  1.2250e-02,  3.5914e-02]]],\n      \n      \n              [[[ 3.1079e-02,  2.5080e-02,  2.5514e-02],\n                [ 3.8942e-02, -1.5952e-02, -1.5592e-02],\n                [ 9.8608e-03, -1.9931e-02,  2.8453e-02]],\n      \n               [[-4.0243e-03, -3.7328e-02,  4.1040e-02],\n                [ 2.7333e-02, -2.3855e-02,  2.9182e-02],\n                [ 1.6966e-02, -2.2749e-02, -1.8787e-02]],\n      \n               [[-1.3491e-02, -2.0107e-02, -4.1104e-02],\n                [-3.7651e-03, -1.9245e-02, -1.9439e-02],\n                [ 1.1891e-02, -2.2456e-02,  1.1230e-02]],\n      \n               ...,\n      \n               [[-3.3945e-02, -6.1512e-03,  6.7916e-03],\n                [ 5.8191e-03,  1.3860e-02,  7.8669e-03],\n                [ 3.5844e-02,  3.7493e-02,  3.3436e-02]],\n      \n               [[ 2.2421e-02,  6.9070e-03, -2.5647e-02],\n                [ 7.6647e-03,  1.5369e-02,  6.7550e-03],\n                [ 2.9753e-02,  6.9842e-03, -1.0998e-03]],\n      \n               [[ 1.7259e-03, -3.3084e-02,  2.7716e-02],\n                [-3.1164e-02,  2.8177e-02,  3.8097e-03],\n                [ 3.2821e-03,  2.4523e-02,  3.6340e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-0.0004, -0.0084,  0.0068, -0.0009,  0.0104, -0.0041, -0.0186, -0.0012,\n              -0.0147,  0.0047,  0.0136, -0.0309,  0.0142, -0.0039,  0.0188,  0.0024,\n               0.0135, -0.0100,  0.0554, -0.0268, -0.0067, -0.0133,  0.0060,  0.0055,\n              -0.0119,  0.0347,  0.0009, -0.0147,  0.0183, -0.0350, -0.0008, -0.0240,\n               0.0184, -0.0136, -0.0111,  0.0024,  0.0159, -0.0205, -0.0126, -0.0256,\n               0.0194, -0.0345,  0.0037, -0.0078,  0.0091, -0.0083, -0.0249,  0.0048,\n               0.0179, -0.0100,  0.0101, -0.0071, -0.0345,  0.0129, -0.0133,  0.0278,\n              -0.0286, -0.0017, -0.0308, -0.0136,  0.0088, -0.0114,  0.0127,  0.0012],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9127, 0.9091, 0.9064, 0.9067, 0.9116, 0.9096, 0.9097, 0.9099, 0.9122,\n              0.9079, 0.9097, 0.9082, 0.9108, 0.9077, 0.9136, 0.9100, 0.9134, 0.9064,\n              0.9099, 0.9094, 0.9090, 0.9059, 0.9099, 0.9118, 0.9140, 0.9097, 0.9096,\n              0.9092, 0.9089, 0.9153, 0.9082, 0.9083, 0.9110, 0.9071, 0.9079, 0.9088,\n              0.9111, 0.9088, 0.9101, 0.9101, 0.9063, 0.9082, 0.9124, 0.9083, 0.9103,\n              0.9084, 0.9073, 0.9084, 0.9109, 0.9112, 0.9063, 0.9078, 0.9091, 0.9092,\n              0.9088, 0.9120, 0.9074, 0.9079, 0.9100, 0.9082, 0.9084, 0.9099, 0.9081,\n              0.9118], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n  )\n)", "parameters": [["0.conv1.weight", [64, 64, 3, 3]], ["0.bn1.weight", [64]], ["0.bn1.bias", [64]], ["0.conv2.weight", [64, 64, 3, 3]], ["0.bn2.weight", [64]], ["0.bn2.bias", [64]]], "output_shape": [[512, 64, 6, 6]], "num_parameters": [36864, 64, 64, 36864, 64, 64]}, {"name": "layer2", "id": 140542746891888, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 1.3853e-02, -2.4893e-02,  3.6407e-02],\n                [ 2.7906e-02, -9.1770e-03, -3.2502e-02],\n                [ 2.6410e-02,  2.3107e-03,  2.7847e-02]],\n      \n               [[-4.1560e-02,  3.9961e-02, -3.1011e-02],\n                [ 3.8196e-02, -5.3359e-03, -1.9776e-03],\n                [ 1.9078e-02,  1.5307e-02, -3.5503e-02]],\n      \n               [[-1.3949e-02, -1.5592e-02,  2.8214e-02],\n                [-3.0446e-02,  3.5370e-02,  1.3716e-02],\n                [-1.5864e-02, -3.3574e-02, -2.5661e-02]],\n      \n               ...,\n      \n               [[ 3.8979e-02, -3.9103e-03,  2.6117e-02],\n                [ 8.5423e-04,  2.8032e-03, -1.5782e-02],\n                [-1.8667e-02,  2.4356e-02, -6.8238e-03]],\n      \n               [[-5.1250e-03,  9.5520e-04,  9.4869e-03],\n                [ 1.9756e-04, -3.9119e-02, -3.5901e-02],\n                [-3.2362e-02, -2.4519e-02,  1.2889e-02]],\n      \n               [[-1.2981e-03, -2.5097e-02, -1.2142e-02],\n                [ 1.5292e-02, -1.2962e-02,  6.5448e-04],\n                [ 3.4872e-02, -1.1035e-02,  3.8409e-02]]],\n      \n      \n              [[[ 1.0564e-02,  1.2995e-02,  2.4824e-02],\n                [ 2.1739e-02, -2.7398e-02,  1.1781e-02],\n                [ 2.0003e-02,  2.3517e-04, -3.3006e-02]],\n      \n               [[ 2.1499e-02, -3.1081e-02,  2.4436e-02],\n                [-4.6711e-03,  3.1303e-02,  3.3317e-02],\n                [ 3.6635e-02,  7.0655e-03,  2.6441e-02]],\n      \n               [[-2.5241e-02, -1.5406e-02, -1.9434e-02],\n                [ 9.1666e-03, -4.0435e-02,  3.6390e-02],\n                [-3.7478e-02,  3.3432e-02,  5.6820e-03]],\n      \n               ...,\n      \n               [[-4.5029e-03,  3.6302e-02,  3.0106e-02],\n                [-3.7069e-02,  2.8009e-02, -3.2452e-02],\n                [-6.3287e-03, -3.0491e-02,  6.2914e-03]],\n      \n               [[ 1.9465e-02,  2.5317e-02,  1.6597e-02],\n                [ 2.0841e-03, -3.5051e-02,  1.6785e-02],\n                [ 2.4565e-02, -1.8876e-02, -8.2802e-03]],\n      \n               [[ 7.4157e-03, -8.7893e-03,  3.9307e-02],\n                [ 4.0769e-02, -1.1005e-02,  3.4684e-02],\n                [-1.9167e-02, -3.3678e-02,  1.2689e-02]]],\n      \n      \n              [[[ 2.8245e-03, -2.0160e-02,  1.3083e-03],\n                [-2.6898e-02, -6.5236e-03, -3.5281e-02],\n                [-1.8255e-02,  2.6250e-02,  2.6752e-02]],\n      \n               [[-3.5037e-03,  1.7014e-03, -3.8252e-02],\n                [ 1.9853e-02, -2.4056e-02, -3.8668e-02],\n                [-8.4477e-03, -2.9709e-02,  2.5823e-02]],\n      \n               [[-2.1237e-02,  1.9874e-02,  4.0948e-02],\n                [-3.5581e-03,  4.0433e-02, -1.2275e-02],\n                [-5.4001e-03,  2.7820e-02,  3.6319e-02]],\n      \n               ...,\n      \n               [[-2.3644e-02, -5.2505e-03,  2.4679e-04],\n                [-3.3549e-02, -2.2539e-02,  3.5382e-02],\n                [ 1.0636e-02, -1.6010e-02,  3.9727e-02]],\n      \n               [[-3.3107e-02, -1.3949e-02, -2.4043e-02],\n                [-2.5633e-02, -1.2856e-02, -3.9597e-02],\n                [-2.4028e-02,  3.0913e-03,  4.2028e-03]],\n      \n               [[-4.8219e-03, -3.7025e-02, -3.3526e-03],\n                [-4.0818e-03,  3.4585e-02,  2.7685e-02],\n                [ 3.0071e-02, -1.2073e-02,  2.8328e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[-1.0360e-02,  2.9299e-02,  3.0082e-02],\n                [ 1.6444e-02, -2.5394e-02, -3.9585e-02],\n                [ 1.4897e-02, -1.7660e-03, -3.3487e-02]],\n      \n               [[ 1.1008e-02,  1.2072e-02, -2.8282e-02],\n                [ 1.8883e-02, -2.3454e-02,  3.4891e-02],\n                [ 3.3380e-03,  2.6987e-02, -3.7937e-02]],\n      \n               [[-4.4981e-03, -4.7429e-03,  1.6109e-03],\n                [-1.4486e-02, -2.8461e-02, -1.3688e-02],\n                [-3.5107e-02,  1.3748e-02,  1.7497e-03]],\n      \n               ...,\n      \n               [[-2.8784e-02,  1.9618e-02, -1.0691e-02],\n                [-5.1113e-03, -1.6675e-02, -2.5099e-02],\n                [ 3.6220e-02, -1.4449e-02,  2.2301e-02]],\n      \n               [[-2.9982e-02,  3.4736e-02,  4.0798e-02],\n                [ 1.8859e-02, -1.3167e-03,  5.3537e-04],\n                [ 1.4981e-02,  2.7729e-04, -1.1392e-02]],\n      \n               [[ 3.0313e-02, -2.8185e-03, -4.1247e-02],\n                [-3.7981e-02, -1.8709e-02,  2.6418e-02],\n                [-5.5809e-03,  2.0430e-02,  6.3803e-03]]],\n      \n      \n              [[[-2.9943e-02,  4.5359e-03, -1.5365e-02],\n                [ 3.8540e-03,  1.9910e-02, -3.7814e-02],\n                [ 4.9589e-03,  1.6098e-02,  1.9372e-02]],\n      \n               [[ 2.1929e-02, -1.3717e-02, -3.2831e-02],\n                [ 3.7923e-02,  4.4825e-03,  1.4582e-04],\n                [ 1.7508e-02, -3.8361e-03, -8.4748e-03]],\n      \n               [[ 3.9385e-02, -7.5476e-03,  7.9384e-04],\n                [-3.4306e-02, -3.7198e-02,  2.6099e-02],\n                [-5.1948e-03, -3.0284e-02, -1.3833e-02]],\n      \n               ...,\n      \n               [[-1.0352e-02,  1.5549e-02,  1.1484e-02],\n                [-3.0751e-02,  3.7227e-02, -3.6409e-03],\n                [-4.1190e-02, -3.5591e-02, -3.1744e-02]],\n      \n               [[-4.1229e-02, -3.1737e-02,  1.4357e-02],\n                [-3.0340e-02, -4.1573e-02, -2.7014e-02],\n                [ 4.6440e-03, -1.6169e-02,  1.6961e-03]],\n      \n               [[-1.0021e-02, -3.4721e-02, -1.7503e-02],\n                [ 2.5360e-02, -1.2842e-02, -3.9802e-02],\n                [-2.0730e-02,  1.4935e-02, -3.2759e-02]]],\n      \n      \n              [[[-1.5029e-02, -1.9807e-02,  6.4213e-03],\n                [-2.2148e-02, -2.5672e-03, -3.7926e-02],\n                [ 2.4538e-02, -1.6301e-02, -3.6105e-02]],\n      \n               [[ 1.9331e-02, -3.3317e-03, -9.2406e-03],\n                [-7.1834e-03, -3.4247e-02,  1.5395e-02],\n                [ 1.8873e-02,  3.7093e-02,  2.7892e-02]],\n      \n               [[ 7.8122e-03,  1.5227e-02,  3.6483e-02],\n                [ 2.8482e-02,  3.6279e-02,  2.0583e-02],\n                [ 2.2348e-02, -3.8692e-02, -1.4660e-02]],\n      \n               ...,\n      \n               [[-3.6587e-02, -3.3608e-02, -1.0105e-02],\n                [-3.4144e-03, -1.3832e-02, -2.1609e-02],\n                [-2.9431e-02, -2.1546e-02, -1.5815e-02]],\n      \n               [[ 2.1092e-02, -2.1177e-02, -5.4589e-03],\n                [-3.2144e-02, -1.1072e-02,  7.8907e-05],\n                [ 4.1578e-02,  2.1921e-02,  1.9482e-02]],\n      \n               [[ 4.5237e-03, -3.3803e-03, -3.6914e-02],\n                [-3.5244e-02, -1.1444e-03,  3.0591e-02],\n                [ 3.3067e-02, -3.9585e-02, -2.5291e-02]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-0.0431, -0.0287, -0.0313, -0.0970, -0.0213, -0.0156,  0.0215,  0.0366,\n               0.0106, -0.0415,  0.0526,  0.0541, -0.0284, -0.0298,  0.0426, -0.0010,\n              -0.0179,  0.0763, -0.0507, -0.0307, -0.0319, -0.0530, -0.0429,  0.0130,\n               0.0520, -0.0727,  0.0794, -0.1186, -0.0807,  0.0396,  0.0113, -0.0924,\n               0.0381,  0.0534,  0.0304, -0.0074,  0.0124, -0.0014,  0.0285, -0.1396,\n              -0.0101, -0.0203, -0.0404, -0.0133, -0.0733, -0.0524,  0.0051, -0.0568,\n              -0.1053, -0.0167, -0.0534, -0.0708,  0.0668, -0.0965, -0.0400, -0.0380,\n              -0.0253,  0.0502,  0.0378, -0.0227, -0.0282,  0.0838,  0.0046, -0.0817,\n               0.0213,  0.0204, -0.0253,  0.0260,  0.0396,  0.0381, -0.0069,  0.0309,\n               0.0959, -0.0886, -0.0909, -0.0189,  0.0027, -0.0210,  0.0388, -0.0570,\n              -0.0295,  0.0218,  0.0694,  0.0211,  0.0082,  0.0467, -0.0295,  0.0102,\n              -0.0367,  0.0942, -0.0689, -0.0107, -0.0318, -0.0255, -0.1103, -0.0245,\n               0.0024, -0.1247,  0.0078, -0.0324,  0.0483,  0.0167,  0.0679, -0.0321,\n               0.0299,  0.0267,  0.0456,  0.0235, -0.0175,  0.0403,  0.0050, -0.0784,\n              -0.0139, -0.0596,  0.0279, -0.0555, -0.0110,  0.0678, -0.0448, -0.0336,\n               0.0499, -0.0394, -0.0308,  0.0120, -0.0124,  0.0205, -0.0112, -0.0345],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9453, 0.9209, 0.9424, 0.9332, 0.9272, 0.9369, 0.9274, 0.9240, 0.9202,\n              0.9347, 0.9416, 0.9407, 0.9259, 0.9333, 0.9234, 0.9324, 0.9299, 0.9243,\n              0.9418, 0.9263, 0.9335, 0.9218, 0.9279, 0.9290, 0.9343, 0.9442, 0.9343,\n              0.9484, 0.9390, 0.9276, 0.9241, 0.9313, 0.9169, 0.9297, 0.9205, 0.9275,\n              0.9717, 0.9366, 0.9421, 0.9431, 0.9360, 0.9323, 0.9292, 0.9336, 0.9335,\n              0.9268, 0.9321, 0.9384, 0.9451, 0.9390, 0.9212, 0.9302, 0.9375, 0.9396,\n              0.9324, 0.9256, 0.9250, 0.9227, 0.9245, 0.9230, 0.9200, 0.9290, 0.9288,\n              0.9322, 0.9321, 0.9295, 0.9296, 0.9227, 0.9321, 0.9183, 0.9294, 0.9357,\n              0.9540, 0.9461, 0.9418, 0.9340, 0.9424, 0.9191, 0.9349, 0.9251, 0.9341,\n              0.9401, 0.9502, 0.9264, 0.9229, 0.9269, 0.9227, 0.9290, 0.9315, 0.9583,\n              0.9342, 0.9237, 0.9309, 0.9404, 0.9383, 0.9323, 0.9364, 0.9659, 0.9325,\n              0.9256, 0.9233, 0.9449, 0.9348, 0.9301, 0.9269, 0.9226, 0.9276, 0.9399,\n              0.9370, 0.9295, 0.9408, 0.9361, 0.9339, 0.9394, 0.9282, 0.9400, 0.9199,\n              0.9420, 0.9352, 0.9338, 0.9433, 0.9408, 0.9315, 0.9192, 0.9271, 0.9219,\n              0.9249, 0.9224], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-2.8296e-02,  5.6412e-03, -2.4821e-02],\n                [ 2.7147e-02,  2.1256e-02,  2.2394e-02],\n                [ 2.9154e-02, -2.7680e-02,  2.4130e-02]],\n      \n               [[ 6.8499e-03,  1.1048e-02,  1.0174e-02],\n                [ 1.2116e-03,  2.7109e-02, -1.7108e-02],\n                [ 1.0728e-02, -1.7606e-02, -3.7869e-03]],\n      \n               [[ 9.6974e-04,  9.4302e-03, -2.4288e-03],\n                [-2.6426e-03, -1.9593e-02, -2.4756e-03],\n                [ 2.2295e-02, -1.5266e-02, -5.9892e-03]],\n      \n               ...,\n      \n               [[ 2.4519e-02,  1.2732e-02, -2.0286e-02],\n                [-2.1579e-02, -1.4416e-02, -7.1621e-03],\n                [-1.0387e-02, -7.9722e-03, -1.9556e-02]],\n      \n               [[-2.0408e-02, -7.0852e-03,  1.1629e-02],\n                [-6.3990e-03, -2.0738e-02, -1.6013e-02],\n                [ 6.0533e-03,  1.4103e-02,  6.0046e-03]],\n      \n               [[-1.1663e-02,  4.5921e-03,  5.7434e-03],\n                [ 2.2288e-02, -2.5199e-02,  2.9485e-04],\n                [-1.1849e-02,  1.1386e-02,  4.1010e-03]]],\n      \n      \n              [[[ 1.9846e-02,  2.5724e-02, -2.3307e-05],\n                [-2.7848e-02,  1.0612e-02,  7.7760e-03],\n                [-2.0692e-02, -2.7857e-02,  1.5023e-02]],\n      \n               [[-1.0122e-02, -2.9009e-02,  2.9159e-02],\n                [-2.1674e-03,  8.1355e-03, -1.6578e-02],\n                [-2.0422e-02,  1.6519e-02, -2.5498e-02]],\n      \n               [[ 2.7569e-03,  7.7402e-03, -2.2361e-02],\n                [-2.2824e-02, -2.0626e-02, -7.6651e-03],\n                [-5.7007e-03, -9.6075e-03, -7.3637e-03]],\n      \n               ...,\n      \n               [[-2.5275e-02,  3.0515e-04,  1.4554e-02],\n                [ 1.6797e-02, -3.8282e-03, -1.9376e-02],\n                [ 7.6289e-03, -7.9673e-03, -2.2885e-02]],\n      \n               [[ 2.7352e-03,  1.9298e-02, -1.7115e-02],\n                [ 2.7407e-02,  7.3029e-03, -3.7782e-03],\n                [-2.7617e-02,  4.7947e-03, -2.4025e-02]],\n      \n               [[-2.6241e-02, -1.1176e-03, -2.4403e-02],\n                [-2.3660e-02,  3.2645e-03,  2.5304e-02],\n                [-3.3578e-03, -2.3123e-02, -2.7929e-03]]],\n      \n      \n              [[[-1.7105e-03,  2.5669e-02,  1.8438e-03],\n                [ 8.8345e-03,  2.1477e-02,  2.8883e-02],\n                [ 1.3699e-02,  1.7352e-02, -2.1835e-02]],\n      \n               [[-1.6133e-02,  1.7359e-03,  2.4447e-02],\n                [-1.0770e-02,  3.4837e-03, -2.6628e-02],\n                [-1.2465e-02,  2.4615e-02,  7.6709e-04]],\n      \n               [[-1.0122e-02,  2.9306e-02, -1.4831e-02],\n                [-6.3404e-03,  2.2712e-02,  6.4985e-03],\n                [ 2.7869e-02,  1.4375e-02,  3.2268e-03]],\n      \n               ...,\n      \n               [[-1.5539e-02, -4.2997e-03,  3.7786e-04],\n                [ 7.1863e-03, -2.5087e-02,  3.5054e-04],\n                [ 2.3524e-02, -9.7252e-03,  1.3378e-02]],\n      \n               [[-2.3018e-02, -7.5219e-03, -1.6177e-02],\n                [-2.4867e-03, -7.0423e-03, -1.8016e-03],\n                [ 8.2582e-03,  2.6460e-02, -1.2186e-03]],\n      \n               [[ 2.3186e-02, -6.9691e-03, -2.8555e-02],\n                [ 1.8392e-02, -1.8002e-02,  1.0781e-02],\n                [ 1.4437e-03, -1.4655e-02, -2.9212e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[-1.9439e-02,  2.1113e-02, -8.9264e-03],\n                [ 1.7572e-02, -2.2997e-03,  1.1751e-02],\n                [ 1.2539e-02, -8.1848e-04,  2.2362e-02]],\n      \n               [[-1.2438e-03,  1.7518e-02, -1.0499e-02],\n                [-1.1515e-02, -3.5711e-03, -1.1619e-02],\n                [ 1.2061e-02, -2.8457e-02, -1.3713e-02]],\n      \n               [[-1.8491e-02, -2.5026e-02, -2.8698e-02],\n                [ 4.0549e-03, -1.1715e-02, -2.0612e-02],\n                [ 5.0613e-03,  1.6581e-02, -2.3770e-02]],\n      \n               ...,\n      \n               [[ 1.9621e-02,  2.9087e-02, -1.9398e-04],\n                [-4.1295e-03,  1.2768e-02,  2.8130e-02],\n                [ 1.9815e-02, -6.4704e-03, -3.5633e-03]],\n      \n               [[ 1.5089e-03,  1.7688e-02,  2.4948e-02],\n                [-9.8529e-03, -2.0550e-03, -2.5427e-02],\n                [ 1.7008e-02,  1.9734e-02,  2.4861e-02]],\n      \n               [[ 1.6881e-02,  1.9989e-02, -1.0712e-02],\n                [-9.7891e-03, -1.5212e-02,  6.3124e-03],\n                [-7.7310e-03,  1.5925e-02, -1.9178e-02]]],\n      \n      \n              [[[-1.3589e-02,  1.5122e-03,  2.1903e-03],\n                [-2.9308e-02,  2.5935e-02, -5.3596e-03],\n                [-2.2116e-02, -1.1160e-02,  1.0338e-02]],\n      \n               [[-1.9833e-02, -6.5809e-04,  2.1715e-02],\n                [-9.5883e-03, -8.4261e-03, -2.9370e-02],\n                [ 3.3238e-03,  5.8712e-03, -7.9032e-03]],\n      \n               [[-2.0536e-02, -6.3417e-03, -1.8131e-02],\n                [ 1.1595e-02,  1.8304e-02,  2.0140e-02],\n                [-5.0051e-04,  2.3922e-02,  2.3853e-02]],\n      \n               ...,\n      \n               [[ 4.3211e-03,  2.8324e-02, -1.4103e-02],\n                [-1.8817e-02,  1.2511e-02, -2.6175e-02],\n                [ 1.7659e-02, -2.3729e-02, -1.1282e-02]],\n      \n               [[ 2.1166e-02, -2.7977e-02, -8.5741e-03],\n                [-1.2491e-02,  2.8400e-02, -1.3112e-02],\n                [ 2.8264e-02,  1.6708e-02, -1.4390e-02]],\n      \n               [[-4.7786e-03, -1.9104e-02,  2.5217e-02],\n                [-2.6757e-02, -5.6511e-03,  1.4896e-02],\n                [ 1.7590e-02, -1.4212e-02,  2.8848e-02]]],\n      \n      \n              [[[ 1.5332e-02,  1.1825e-02, -1.8863e-03],\n                [ 2.4330e-02,  3.7196e-03, -9.0881e-03],\n                [-1.4167e-02,  2.4675e-02,  9.8441e-03]],\n      \n               [[ 1.3439e-02, -1.8767e-02, -6.4862e-03],\n                [ 2.6613e-02, -2.4410e-02,  1.0967e-02],\n                [-2.8673e-03, -4.7657e-04,  2.4142e-02]],\n      \n               [[-1.1287e-02, -1.7089e-03, -1.8597e-02],\n                [ 1.6313e-02, -1.4053e-04,  2.1225e-02],\n                [-2.0483e-02,  1.4160e-02, -2.1900e-02]],\n      \n               ...,\n      \n               [[ 5.5748e-03,  1.3896e-02, -2.2416e-02],\n                [-2.7347e-02, -2.2451e-02,  8.2430e-04],\n                [ 1.9726e-02,  2.1372e-03, -9.3862e-03]],\n      \n               [[-1.7828e-02,  2.7608e-02, -1.9528e-02],\n                [ 1.3087e-02,  6.9135e-03, -5.6959e-03],\n                [-2.0525e-02, -1.2357e-02, -1.3494e-02]],\n      \n               [[ 2.1526e-02,  1.1617e-02, -6.1563e-04],\n                [ 1.6365e-03,  1.8751e-02, -2.5980e-02],\n                [ 2.1846e-02,  2.0310e-02,  1.5563e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-6.0745e-03, -3.1497e-02, -1.7907e-03, -1.3455e-02, -3.5250e-02,\n               6.8203e-03,  6.9595e-03, -2.3774e-02,  1.6598e-02,  9.0691e-03,\n               5.1123e-03, -4.1301e-03, -3.9635e-03, -1.0419e-03,  1.3493e-02,\n               8.3452e-03,  3.3566e-03,  7.1018e-05,  1.0386e-02,  4.0295e-03,\n               4.2767e-03, -1.4933e-02,  1.9421e-03,  2.4899e-02,  1.3245e-02,\n               1.9550e-02,  7.8575e-04,  1.6266e-02,  1.3360e-02,  1.4988e-02,\n              -2.9683e-02, -1.9930e-03,  1.9809e-02, -1.1385e-02,  1.1021e-02,\n               1.7281e-02,  9.7980e-03, -6.1173e-03, -2.0573e-03, -8.1226e-03,\n               1.0849e-02, -8.9207e-03, -6.2021e-03,  1.5621e-02, -1.7660e-02,\n              -3.3841e-02,  5.1690e-03,  1.5936e-02, -1.7153e-02,  2.1853e-02,\n               1.1507e-02, -1.0970e-02,  1.5484e-02, -6.7720e-03,  6.1436e-03,\n              -5.1671e-03, -2.2942e-03,  6.3717e-04, -4.4897e-04,  9.4658e-03,\n               1.7335e-02, -6.5505e-03,  1.1046e-02,  1.7252e-02, -1.0073e-02,\n               1.3008e-02, -2.3913e-04,  5.4817e-04, -4.2651e-03,  9.4781e-03,\n               2.0837e-02,  1.0522e-02,  1.2116e-03, -5.0479e-03,  1.9091e-02,\n               6.6555e-04,  5.2759e-04,  2.4564e-02,  1.9622e-02, -1.3717e-02,\n              -7.6704e-04,  1.7422e-02, -3.2691e-03, -8.5002e-03,  2.5482e-03,\n              -1.3197e-02,  8.0656e-03,  1.2092e-03, -5.7613e-03, -3.2335e-02,\n              -1.3918e-02, -1.0767e-02,  1.8487e-02, -3.6696e-02, -2.2735e-02,\n              -2.5207e-02,  9.7303e-03, -4.6410e-04, -1.3042e-02, -1.4528e-03,\n              -3.2014e-03, -3.3616e-02,  1.4565e-02,  7.2136e-03, -4.4404e-04,\n               9.6748e-03,  2.9024e-02,  7.9881e-04, -1.8293e-03, -1.3199e-02,\n               2.5967e-04,  2.0243e-03,  4.5057e-03, -9.4793e-03, -1.4109e-02,\n              -1.1178e-02, -1.3421e-02,  4.7035e-03,  1.3202e-02,  8.1498e-03,\n              -3.0718e-02, -1.9436e-03, -8.9612e-03, -3.6856e-02,  7.7239e-03,\n              -2.6476e-03, -1.5674e-02, -1.8733e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9074, 0.9072, 0.9082, 0.9095, 0.9070, 0.9078, 0.9076, 0.9104, 0.9082,\n              0.9078, 0.9063, 0.9085, 0.9087, 0.9077, 0.9077, 0.9088, 0.9106, 0.9065,\n              0.9082, 0.9067, 0.9168, 0.9074, 0.9080, 0.9093, 0.9083, 0.9064, 0.9123,\n              0.9091, 0.9077, 0.9090, 0.9101, 0.9091, 0.9063, 0.9086, 0.9055, 0.9116,\n              0.9077, 0.9077, 0.9107, 0.9066, 0.9091, 0.9057, 0.9094, 0.9102, 0.9084,\n              0.9117, 0.9079, 0.9119, 0.9085, 0.9079, 0.9069, 0.9077, 0.9061, 0.9079,\n              0.9068, 0.9071, 0.9097, 0.9053, 0.9076, 0.9075, 0.9084, 0.9102, 0.9087,\n              0.9078, 0.9077, 0.9154, 0.9097, 0.9076, 0.9068, 0.9087, 0.9087, 0.9070,\n              0.9063, 0.9093, 0.9067, 0.9056, 0.9061, 0.9103, 0.9128, 0.9055, 0.9071,\n              0.9090, 0.9095, 0.9084, 0.9104, 0.9104, 0.9085, 0.9100, 0.9091, 0.9121,\n              0.9065, 0.9082, 0.9079, 0.9110, 0.9076, 0.9058, 0.9063, 0.9076, 0.9063,\n              0.9067, 0.9127, 0.9110, 0.9056, 0.9073, 0.9062, 0.9081, 0.9100, 0.9062,\n              0.9154, 0.9062, 0.9087, 0.9073, 0.9075, 0.9073, 0.9056, 0.9061, 0.9074,\n              0.9073, 0.9089, 0.9086, 0.9054, 0.9072, 0.9081, 0.9073, 0.9106, 0.9053,\n              0.9101, 0.9091], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[-0.0220]],\n        \n                 [[-0.1124]],\n        \n                 [[ 0.0823]],\n        \n                 ...,\n        \n                 [[ 0.0186]],\n        \n                 [[-0.0058]],\n        \n                 [[-0.0779]]],\n        \n        \n                [[[ 0.0401]],\n        \n                 [[ 0.0228]],\n        \n                 [[-0.0207]],\n        \n                 ...,\n        \n                 [[ 0.0029]],\n        \n                 [[-0.0002]],\n        \n                 [[-0.0649]]],\n        \n        \n                [[[-0.0232]],\n        \n                 [[-0.0188]],\n        \n                 [[-0.1183]],\n        \n                 ...,\n        \n                 [[ 0.0241]],\n        \n                 [[-0.0082]],\n        \n                 [[-0.0105]]],\n        \n        \n                ...,\n        \n        \n                [[[-0.0294]],\n        \n                 [[ 0.0607]],\n        \n                 [[ 0.1001]],\n        \n                 ...,\n        \n                 [[-0.0631]],\n        \n                 [[-0.0642]],\n        \n                 [[ 0.0568]]],\n        \n        \n                [[[-0.1072]],\n        \n                 [[-0.0348]],\n        \n                 [[-0.0099]],\n        \n                 ...,\n        \n                 [[ 0.0485]],\n        \n                 [[-0.0791]],\n        \n                 [[ 0.0881]]],\n        \n        \n                [[[-0.0185]],\n        \n                 [[ 0.0177]],\n        \n                 [[ 0.0396]],\n        \n                 ...,\n        \n                 [[ 0.0905]],\n        \n                 [[-0.1079]],\n        \n                 [[ 0.0182]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-0.0630,  0.0460, -0.0122, -0.0311,  0.0117,  0.0590, -0.0157, -0.0222,\n                 0.0198,  0.1454, -0.1006, -0.0234, -0.0062, -0.0037, -0.0453, -0.1044,\n                -0.0447,  0.0461,  0.0406,  0.0129, -0.0456, -0.0606, -0.0485,  0.0444,\n                 0.0310, -0.0200,  0.1021, -0.0004,  0.0498, -0.0189,  0.0435,  0.0374,\n                 0.0712,  0.0351,  0.1031,  0.0864, -0.0509,  0.0103,  0.0683,  0.0070,\n                 0.0317,  0.0536, -0.0024, -0.0470,  0.0513, -0.0516, -0.1447,  0.0521,\n                -0.0544,  0.0370,  0.0561,  0.1107, -0.0329, -0.0542, -0.0665,  0.0279,\n                -0.0143,  0.0476, -0.0185,  0.0066, -0.0318,  0.0079, -0.0857, -0.0613,\n                 0.0296,  0.0225, -0.0511,  0.0072,  0.0315,  0.0501,  0.0309,  0.0330,\n                 0.0185, -0.0629,  0.0043, -0.0292,  0.0587,  0.0603,  0.0331, -0.0153,\n                 0.0464, -0.0396, -0.0326,  0.0834, -0.0627, -0.1204, -0.0926,  0.0107,\n                -0.0020,  0.0030, -0.0583,  0.1016,  0.0302, -0.0458,  0.0435, -0.1061,\n                -0.1044,  0.0733,  0.0135,  0.0459,  0.0342,  0.0169,  0.0460,  0.0766,\n                 0.0536, -0.0407, -0.1662,  0.0503,  0.0992, -0.1001, -0.0766, -0.0285,\n                -0.1964, -0.0380, -0.0406, -0.0556,  0.0584,  0.0522, -0.0068, -0.0507,\n                -0.0386, -0.1286, -0.0496, -0.0365, -0.0664, -0.0175,  0.0502, -0.0910],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9534, 0.9375, 0.9271, 0.9220, 0.9312, 0.9294, 0.9193, 0.9352, 0.9311,\n                0.9464, 0.9475, 0.9395, 0.9543, 0.9232, 0.9343, 0.9526, 0.9429, 0.9384,\n                0.9358, 0.9336, 0.9385, 0.9234, 0.9247, 0.9376, 0.9455, 0.9317, 0.9510,\n                0.9169, 0.9339, 0.9407, 0.9298, 0.9625, 0.9308, 0.9243, 0.9374, 0.9376,\n                0.9358, 0.9266, 0.9293, 0.9202, 0.9234, 0.9306, 0.9192, 0.9484, 0.9250,\n                0.9419, 0.9651, 0.9249, 0.9317, 0.9319, 0.9491, 0.9497, 0.9194, 0.9255,\n                0.9244, 0.9213, 0.9249, 0.9516, 0.9209, 0.9236, 0.9422, 0.9295, 0.9286,\n                0.9358, 0.9178, 0.9690, 0.9684, 0.9366, 0.9361, 0.9273, 0.9388, 0.9243,\n                0.9137, 0.9334, 0.9224, 0.9392, 0.9504, 0.9410, 0.9264, 0.9266, 0.9311,\n                0.9295, 0.9366, 0.9225, 0.9294, 0.9376, 0.9293, 0.9359, 0.9290, 0.9821,\n                0.9309, 0.9278, 0.9240, 0.9471, 0.9284, 0.9280, 0.9372, 0.9362, 0.9490,\n                0.9476, 0.9215, 0.9428, 0.9555, 0.9377, 0.9498, 0.9373, 0.9826, 0.9432,\n                0.9622, 0.9726, 0.9251, 0.9231, 0.9815, 0.9362, 0.9190, 0.9581, 0.9238,\n                0.9258, 0.9291, 0.9252, 0.9260, 0.9565, 0.9218, 0.9440, 0.9424, 0.9295,\n                0.9381, 0.9654], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [128, 64, 3, 3]], ["0.bn1.weight", [128]], ["0.bn1.bias", [128]], ["0.conv2.weight", [128, 128, 3, 3]], ["0.bn2.weight", [128]], ["0.bn2.bias", [128]], ["0.downsample.0.weight", [128, 64, 1, 1]], ["0.downsample.1.weight", [128]], ["0.downsample.1.bias", [128]]], "output_shape": [[512, 128, 3, 3]], "num_parameters": [73728, 128, 128, 147456, 128, 128, 8192, 128, 128]}, {"name": "layer3", "id": 140542746892512, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 2.5942e-03, -9.6526e-03, -7.4046e-03],\n                [-1.8083e-02,  5.1928e-03,  2.5361e-02],\n                [ 2.3137e-02, -1.9420e-03,  1.1743e-03]],\n      \n               [[ 1.2486e-02,  4.7737e-03, -5.8364e-03],\n                [ 2.4664e-02, -2.5313e-02, -1.2050e-02],\n                [-1.1606e-03,  1.6656e-02,  1.5317e-02]],\n      \n               [[-1.2124e-02,  1.5363e-02, -4.3264e-04],\n                [ 1.2339e-03, -2.8855e-02, -1.9516e-02],\n                [-2.9236e-02,  6.8466e-03,  1.4171e-02]],\n      \n               ...,\n      \n               [[-2.3841e-02,  6.4864e-03, -3.4530e-03],\n                [-6.4156e-03,  1.2564e-02, -1.1702e-02],\n                [ 1.1035e-02,  1.1303e-02,  4.3859e-03]],\n      \n               [[ 1.2405e-02,  3.6961e-03, -1.6195e-05],\n                [ 2.1557e-02,  1.5396e-02, -2.2782e-02],\n                [-1.5722e-02,  1.7377e-02,  2.0965e-02]],\n      \n               [[-2.2205e-02, -2.2502e-03,  1.4356e-02],\n                [-1.4756e-02,  1.6084e-02,  8.3861e-03],\n                [ 6.7576e-03, -2.9190e-02, -1.7371e-02]]],\n      \n      \n              [[[ 7.3044e-03, -2.6192e-02,  1.5925e-02],\n                [-1.2151e-02,  2.6627e-02,  6.6365e-03],\n                [ 7.6851e-03,  5.0672e-03,  2.9312e-02]],\n      \n               [[ 5.0611e-04,  1.6273e-02, -2.7033e-02],\n                [-1.1404e-02, -2.4044e-02,  2.5186e-02],\n                [-2.6366e-02,  2.0695e-02, -1.9711e-02]],\n      \n               [[-1.2109e-02,  2.9737e-03, -2.4816e-03],\n                [-2.1044e-02, -8.7301e-03, -1.1399e-02],\n                [-1.6481e-02, -1.6243e-02,  2.2177e-02]],\n      \n               ...,\n      \n               [[ 2.4169e-02,  2.9183e-05,  6.7242e-04],\n                [-2.1293e-02,  7.6682e-03,  1.6851e-02],\n                [ 2.2326e-02,  2.5077e-02, -2.6965e-02]],\n      \n               [[-1.6766e-02,  9.3191e-03, -1.7106e-02],\n                [ 2.0264e-02, -2.5893e-02, -2.6460e-02],\n                [-5.7035e-03, -9.3224e-03,  5.8428e-03]],\n      \n               [[-1.4134e-02, -1.1414e-02, -1.4253e-02],\n                [-2.0900e-02, -2.8003e-02,  9.2147e-03],\n                [ 1.3017e-02,  2.6410e-02,  8.6753e-03]]],\n      \n      \n              [[[-1.1317e-02,  3.2017e-03, -2.2016e-02],\n                [-2.5238e-02, -1.8630e-02,  2.4172e-02],\n                [-1.6923e-02, -4.9088e-03,  9.1367e-04]],\n      \n               [[-2.0753e-03,  7.1660e-03, -1.0550e-03],\n                [-2.6116e-02, -6.6258e-03,  2.5316e-02],\n                [-1.6376e-02, -1.5568e-02,  2.0607e-02]],\n      \n               [[-2.5001e-02,  2.1851e-03, -2.7429e-02],\n                [-2.8628e-03, -2.4398e-02, -1.4561e-04],\n                [-4.5098e-03, -5.7678e-03,  1.0489e-03]],\n      \n               ...,\n      \n               [[-9.8293e-03, -1.7386e-02, -8.7963e-03],\n                [-1.3837e-02,  2.4635e-03,  1.5559e-02],\n                [-2.6133e-02, -6.8640e-03, -7.2734e-04]],\n      \n               [[ 8.0148e-03, -5.4001e-03,  1.3075e-02],\n                [-2.7486e-02,  9.5277e-03, -3.9022e-04],\n                [ 1.1148e-02,  9.1140e-03,  1.9382e-02]],\n      \n               [[ 3.2360e-03, -1.7753e-02,  4.4926e-03],\n                [-2.2219e-02,  2.2975e-02, -8.1269e-03],\n                [ 2.5302e-02, -1.1876e-02, -1.2533e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[-2.7945e-02,  7.7786e-03,  1.4146e-02],\n                [-1.2945e-02,  6.1707e-03,  2.3094e-02],\n                [-9.7408e-03, -2.5932e-02,  2.5798e-02]],\n      \n               [[ 2.5775e-03,  6.6268e-03,  8.8629e-03],\n                [-1.2607e-02, -2.5297e-02, -1.2560e-02],\n                [ 4.7951e-03,  1.2245e-02, -2.1894e-02]],\n      \n               [[-2.1868e-02,  1.5952e-02,  2.8446e-02],\n                [-4.5619e-03, -1.6404e-02, -8.9168e-03],\n                [ 2.6441e-03, -1.1856e-02, -2.8005e-02]],\n      \n               ...,\n      \n               [[ 1.1535e-02,  2.4509e-02,  2.7696e-02],\n                [-2.0106e-02, -8.7966e-04, -1.5761e-02],\n                [-1.7488e-02,  6.9213e-03, -1.4345e-02]],\n      \n               [[ 2.3308e-02, -1.7980e-02,  2.6318e-02],\n                [-3.9741e-04, -1.6479e-02,  1.4059e-02],\n                [ 2.5430e-02,  1.0627e-02,  1.5943e-02]],\n      \n               [[-1.5787e-02,  1.0502e-02,  5.3011e-03],\n                [ 2.7513e-02,  1.6759e-02, -1.4797e-02],\n                [ 2.5038e-02,  8.7993e-03, -2.3394e-02]]],\n      \n      \n              [[[-2.2051e-02, -1.4258e-03, -1.8664e-02],\n                [ 1.1907e-02,  1.0790e-02, -1.7940e-02],\n                [-2.8922e-02,  1.4221e-02,  1.1098e-03]],\n      \n               [[ 1.2379e-02,  2.2485e-02,  7.7868e-03],\n                [ 2.6734e-02,  2.7042e-02,  2.4768e-03],\n                [-2.5641e-02, -5.0110e-04, -2.5884e-02]],\n      \n               [[-2.6602e-02,  1.3932e-02, -2.2214e-02],\n                [ 1.6202e-02,  2.0711e-02, -2.8637e-02],\n                [ 2.3808e-02, -2.5262e-02,  2.0955e-02]],\n      \n               ...,\n      \n               [[-9.5200e-03, -9.6906e-03, -2.4977e-02],\n                [-2.1125e-02, -2.6490e-02, -1.7305e-02],\n                [ 2.7594e-02,  1.1133e-02, -2.6417e-02]],\n      \n               [[ 2.6642e-02, -4.4887e-03,  4.7445e-03],\n                [ 3.2566e-03,  2.3710e-02, -2.0536e-02],\n                [ 1.1120e-02,  3.8449e-03, -2.7407e-02]],\n      \n               [[ 2.3672e-02,  7.1335e-03,  1.9475e-02],\n                [-7.0409e-03, -1.7174e-02,  3.4955e-03],\n                [ 1.3845e-02, -2.4125e-02,  2.4632e-02]]],\n      \n      \n              [[[-1.7437e-02, -5.0018e-03,  2.1364e-02],\n                [ 1.7895e-02, -2.9213e-02, -2.5844e-02],\n                [ 1.3943e-02,  5.9689e-03,  1.9143e-02]],\n      \n               [[-1.7063e-02,  1.6231e-02,  2.0370e-02],\n                [-2.8537e-02, -6.5578e-03, -1.3541e-02],\n                [ 2.3631e-02, -1.0978e-02,  1.5750e-02]],\n      \n               [[ 1.9825e-03, -1.8241e-03,  1.4235e-02],\n                [ 2.8001e-02, -7.5411e-03,  2.6047e-03],\n                [ 2.6838e-02,  2.6475e-02,  2.0153e-02]],\n      \n               ...,\n      \n               [[-1.3262e-02,  2.2098e-02, -8.3818e-03],\n                [ 1.0442e-02, -4.5971e-04, -1.1518e-02],\n                [ 1.8571e-02, -7.5017e-03,  2.0137e-02]],\n      \n               [[ 6.5562e-03, -3.0930e-03, -2.6194e-02],\n                [-7.7747e-03,  1.5769e-02,  3.2302e-04],\n                [-4.6171e-03, -2.5610e-02,  2.2753e-02]],\n      \n               [[ 2.1918e-02,  7.6628e-03,  2.6991e-02],\n                [-1.8689e-02, -9.1157e-03,  1.7636e-02],\n                [ 2.4782e-02,  1.9324e-02,  2.6570e-02]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-2.8777e-03, -5.0246e-03,  1.4061e-03,  1.4194e-02,  4.5802e-03,\n              -3.3754e-02,  1.1896e-02, -6.2773e-03,  2.9091e-02,  2.4899e-03,\n              -5.5168e-03, -4.9249e-03,  9.6649e-03,  1.1315e-02, -5.1588e-03,\n               1.5372e-02,  4.8789e-03,  2.4990e-02, -9.9512e-03,  9.6482e-03,\n               1.3565e-02,  2.0496e-02, -4.1075e-02, -2.9012e-03, -7.6942e-03,\n              -2.0466e-02, -4.6741e-03,  1.9240e-02, -1.1316e-02,  1.8571e-02,\n               2.6811e-02, -6.0724e-03, -2.2733e-02,  9.3095e-04, -1.0525e-02,\n               1.8619e-02,  2.0522e-02,  1.6695e-02,  4.0040e-04, -1.2205e-02,\n              -3.3123e-02,  7.7733e-04,  2.2331e-04,  2.9393e-03, -3.7416e-03,\n              -5.2803e-03,  3.5609e-03,  8.1255e-03, -9.8333e-03, -2.2311e-02,\n              -1.4203e-02,  2.0695e-02,  2.0184e-03, -1.8124e-03,  1.1075e-02,\n              -3.1232e-03,  3.4219e-02,  2.2510e-02, -2.9986e-03,  9.7604e-03,\n               1.7249e-02, -3.1322e-03, -1.5257e-02,  2.8597e-02,  1.0629e-02,\n               8.5037e-03, -3.8947e-03, -3.1317e-02,  4.2819e-02, -1.4181e-02,\n              -1.6212e-02, -7.8781e-03, -1.4593e-03, -7.2347e-03, -1.5244e-02,\n               9.7922e-03, -5.6867e-03, -8.1921e-03, -9.5339e-03,  4.8246e-02,\n              -1.0797e-02, -1.9155e-03, -1.0027e-02,  6.9430e-03, -2.8703e-02,\n              -2.4806e-02, -1.2390e-02,  2.9999e-02,  1.7227e-02, -1.3395e-02,\n               9.1590e-03,  3.4187e-02, -1.0820e-02, -2.5449e-02, -1.7925e-03,\n               5.8656e-03, -5.1507e-03,  2.7200e-03,  7.0597e-02, -1.3048e-02,\n              -3.3960e-03, -1.3232e-03, -2.4545e-02, -5.0250e-03,  3.7314e-05,\n              -1.5946e-02, -1.1756e-03, -1.0413e-02,  3.7672e-03,  4.3587e-03,\n              -2.6276e-03,  1.2768e-02, -1.2865e-02,  9.4181e-03,  1.7139e-03,\n              -1.6683e-02, -6.8002e-03, -2.6169e-02,  7.2398e-03, -1.2236e-02,\n              -1.4254e-02,  1.0237e-02, -4.9021e-03,  1.7844e-02,  1.6094e-02,\n               2.9639e-02, -1.6518e-02, -1.0550e-02, -1.0332e-02, -1.1220e-02,\n               3.2847e-02, -2.4308e-03,  2.5760e-03, -3.0548e-03,  7.7004e-03,\n               1.8699e-03, -1.6700e-02,  9.5002e-03,  9.2503e-04, -2.4556e-02,\n               6.4839e-03, -9.4540e-03,  7.5581e-03,  3.3581e-02,  2.2409e-02,\n               2.1079e-02,  1.8632e-02, -4.4847e-03, -1.6314e-03,  5.7250e-03,\n              -1.6160e-02,  9.0018e-03,  1.2371e-02,  2.0885e-02,  2.8456e-02,\n               1.9042e-02, -3.4143e-03, -2.2873e-03,  5.6279e-03,  1.9039e-02,\n              -3.8879e-02,  2.2239e-02, -1.2293e-02,  3.0017e-02,  2.4322e-02,\n               4.2611e-03,  1.7610e-02,  8.1840e-03,  9.0678e-03,  1.0882e-03,\n              -1.6289e-02,  2.0775e-02,  3.8682e-03, -9.6497e-03, -8.9257e-03,\n              -1.2485e-02,  4.3452e-02, -9.9286e-04,  2.7627e-02,  1.7067e-02,\n               2.9115e-02,  9.1247e-04,  3.5747e-02, -7.6783e-03,  2.1366e-02,\n               3.6942e-02, -3.9538e-03,  3.0124e-03,  2.3359e-02, -3.2519e-03,\n              -1.4909e-03,  2.5690e-02,  1.5306e-02, -1.7887e-02, -1.5830e-02,\n              -8.0900e-04, -5.2610e-03,  2.0976e-03, -2.7147e-03,  2.5318e-02,\n               1.7726e-02,  1.6740e-02, -1.9035e-02,  8.6356e-04, -1.4252e-02,\n              -2.7437e-03,  8.4755e-03, -1.9149e-03,  2.0026e-02,  8.9008e-03,\n              -6.5353e-04, -2.3833e-02, -9.9486e-03,  5.1496e-03, -1.6595e-02,\n              -5.8684e-03, -3.9352e-03,  1.0238e-02, -4.7634e-03, -1.2891e-02,\n              -8.5515e-04, -1.7736e-03,  8.6097e-03, -1.4424e-02, -1.2243e-02,\n              -4.2058e-03, -7.2362e-03,  1.1529e-02, -2.1378e-02, -7.2141e-03,\n               1.6096e-02, -1.8457e-02,  2.7245e-02, -1.9078e-02,  2.6895e-02,\n               1.1012e-02, -7.7106e-04,  1.4537e-02,  1.0974e-02,  3.1723e-02,\n              -1.5614e-02, -5.2786e-03, -1.8277e-02, -3.1969e-03,  1.6412e-02,\n               2.3876e-03, -1.1014e-02, -1.0528e-02,  1.6390e-02,  1.8618e-02,\n              -2.9950e-03,  3.2243e-02,  5.7861e-02,  2.9392e-02,  3.9684e-02,\n               2.8541e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9178, 0.9135, 0.9097, 0.9107, 0.9149, 0.9105, 0.9134, 0.9093, 0.9134,\n              0.9114, 0.9124, 0.9098, 0.9113, 0.9135, 0.9175, 0.9120, 0.9128, 0.9078,\n              0.9118, 0.9095, 0.9301, 0.9129, 0.9087, 0.9157, 0.9114, 0.9149, 0.9108,\n              0.9121, 0.9142, 0.9109, 0.9126, 0.9118, 0.9131, 0.9118, 0.9104, 0.9104,\n              0.9100, 0.9110, 0.9115, 0.9136, 0.9145, 0.9125, 0.9096, 0.9123, 0.9136,\n              0.9125, 0.9122, 0.9140, 0.9118, 0.9115, 0.9135, 0.9090, 0.9121, 0.9128,\n              0.9139, 0.9100, 0.9152, 0.9150, 0.9161, 0.9110, 0.9127, 0.9088, 0.9100,\n              0.9148, 0.9188, 0.9165, 0.9123, 0.9152, 0.9103, 0.9153, 0.9117, 0.9136,\n              0.9103, 0.9134, 0.9094, 0.9098, 0.9127, 0.9151, 0.9121, 0.9139, 0.9108,\n              0.9215, 0.9091, 0.9138, 0.9196, 0.9144, 0.9099, 0.9099, 0.9124, 0.9156,\n              0.9123, 0.9114, 0.9137, 0.9104, 0.9127, 0.9162, 0.9083, 0.9096, 0.9186,\n              0.9136, 0.9094, 0.9240, 0.9155, 0.9114, 0.9157, 0.9186, 0.9180, 0.9080,\n              0.9100, 0.9127, 0.9095, 0.9113, 0.9133, 0.9124, 0.9137, 0.9094, 0.9176,\n              0.9178, 0.9107, 0.9125, 0.9141, 0.9151, 0.9161, 0.9168, 0.9163, 0.9148,\n              0.9192, 0.9237, 0.9116, 0.9108, 0.9140, 0.9224, 0.9136, 0.9104, 0.9076,\n              0.9147, 0.9150, 0.9102, 0.9093, 0.9142, 0.9201, 0.9142, 0.9212, 0.9144,\n              0.9096, 0.9096, 0.9123, 0.9157, 0.9141, 0.9129, 0.9135, 0.9153, 0.9147,\n              0.9130, 0.9099, 0.9112, 0.9083, 0.9102, 0.9116, 0.9117, 0.9124, 0.9127,\n              0.9121, 0.9142, 0.9098, 0.9103, 0.9146, 0.9106, 0.9113, 0.9105, 0.9162,\n              0.9092, 0.9102, 0.9169, 0.9115, 0.9159, 0.9155, 0.9113, 0.9151, 0.9109,\n              0.9185, 0.9143, 0.9113, 0.9098, 0.9094, 0.9124, 0.9109, 0.9199, 0.9196,\n              0.9156, 0.9091, 0.9105, 0.9148, 0.9195, 0.9153, 0.9163, 0.9098, 0.9098,\n              0.9179, 0.9148, 0.9122, 0.9123, 0.9151, 0.9183, 0.9126, 0.9110, 0.9092,\n              0.9141, 0.9147, 0.9090, 0.9107, 0.9131, 0.9097, 0.9122, 0.9128, 0.9104,\n              0.9139, 0.9228, 0.9138, 0.9135, 0.9116, 0.9112, 0.9211, 0.9134, 0.9133,\n              0.9082, 0.9147, 0.9124, 0.9094, 0.9103, 0.9166, 0.9145, 0.9089, 0.9104,\n              0.9151, 0.9241, 0.9128, 0.9146, 0.9165, 0.9096, 0.9093, 0.9093, 0.9147,\n              0.9091, 0.9139, 0.9127, 0.9216, 0.9111, 0.9119, 0.9093, 0.9109, 0.9085,\n              0.9174, 0.9154, 0.9185, 0.9114], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 1.1271e-02, -1.6379e-02, -5.2267e-03],\n                [ 2.0807e-02,  1.9025e-03, -3.1688e-03],\n                [ 9.9512e-03, -2.0369e-02,  1.9133e-02]],\n      \n               [[ 1.0391e-02,  1.3100e-02,  1.0803e-02],\n                [ 7.2871e-03, -7.5236e-03,  1.7972e-02],\n                [ 1.3799e-02,  3.7427e-03,  8.9191e-03]],\n      \n               [[-1.1040e-02, -1.0920e-02,  1.8515e-02],\n                [ 9.9195e-03,  1.4607e-02,  1.5091e-02],\n                [ 1.4752e-02,  1.6154e-02, -1.4202e-02]],\n      \n               ...,\n      \n               [[-1.9381e-02, -1.1905e-02,  1.6489e-02],\n                [-8.1392e-03, -1.9224e-03, -2.0543e-02],\n                [-4.0884e-03, -2.2802e-03, -1.1020e-02]],\n      \n               [[ 1.2945e-02, -1.5383e-02,  1.3599e-02],\n                [-9.2119e-03,  1.9238e-02, -1.8382e-03],\n                [-1.3614e-02,  1.8982e-02,  1.6914e-02]],\n      \n               [[-5.5841e-03,  1.3061e-02, -1.9252e-02],\n                [ 6.7440e-03, -7.6936e-03,  1.3308e-02],\n                [ 5.3871e-03, -9.0266e-03,  1.2386e-02]]],\n      \n      \n              [[[ 2.3798e-03, -3.5069e-03,  9.6275e-03],\n                [ 1.9595e-02, -6.7089e-03, -1.2463e-02],\n                [-1.4570e-02, -1.5175e-02,  8.4168e-03]],\n      \n               [[-1.4716e-02,  1.5416e-02, -3.8402e-03],\n                [ 1.2066e-02, -1.4369e-02,  8.2297e-03],\n                [-2.0331e-02, -1.9488e-02,  3.1624e-04]],\n      \n               [[-2.0329e-02, -1.7454e-03, -9.8070e-03],\n                [-7.6497e-04,  1.4503e-02,  1.0537e-02],\n                [-1.6068e-02, -1.2298e-02, -1.4776e-02]],\n      \n               ...,\n      \n               [[-1.0360e-02, -3.3821e-03,  9.7001e-03],\n                [ 2.6974e-03,  1.9729e-02,  1.2220e-02],\n                [-9.7709e-03, -8.0019e-04, -6.2322e-03]],\n      \n               [[-4.1608e-03, -8.6491e-03,  1.0455e-02],\n                [ 1.5654e-02, -1.9592e-02, -1.7826e-02],\n                [ 1.0463e-02,  1.2619e-02, -6.8295e-03]],\n      \n               [[ 7.3429e-03, -1.2129e-02,  7.8154e-03],\n                [-6.3035e-03, -4.2817e-03, -5.9520e-03],\n                [ 1.1167e-02,  1.0010e-02,  1.0064e-02]]],\n      \n      \n              [[[ 4.9812e-03,  3.3924e-03, -4.5387e-03],\n                [ 7.0478e-03,  1.5841e-02,  2.0505e-03],\n                [ 1.0900e-02,  6.0272e-03, -5.7347e-03]],\n      \n               [[-1.7381e-02, -1.6932e-02,  1.5443e-02],\n                [-5.7619e-03, -1.9548e-02,  1.8599e-02],\n                [ 5.6951e-03, -8.9063e-03,  2.3230e-03]],\n      \n               [[-1.6705e-02, -1.6716e-02,  3.6361e-03],\n                [-1.6252e-02,  1.4823e-02, -2.0631e-02],\n                [-7.8624e-03,  7.9129e-03, -4.3153e-03]],\n      \n               ...,\n      \n               [[-7.2091e-03,  1.7987e-02,  1.0383e-02],\n                [-1.2378e-02,  1.4336e-02, -8.8943e-03],\n                [-4.8377e-03, -3.1310e-03,  1.0406e-02]],\n      \n               [[-9.1232e-03, -1.7770e-02, -1.9611e-02],\n                [ 1.5749e-02, -1.6267e-02, -8.4234e-03],\n                [-1.3064e-02, -1.6796e-02, -1.3591e-02]],\n      \n               [[-1.4814e-02,  1.3263e-03, -2.0722e-02],\n                [ 1.9979e-02, -1.8513e-02, -1.5640e-02],\n                [ 3.1487e-04, -2.4883e-03,  4.2182e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[-7.2526e-03,  1.9126e-02,  6.7359e-03],\n                [-5.2145e-04, -8.1697e-03, -7.7282e-03],\n                [ 8.7827e-03,  3.1527e-04,  9.6280e-03]],\n      \n               [[-1.0348e-02, -1.5391e-02, -1.7247e-02],\n                [-1.2153e-02, -6.5143e-03,  3.7847e-03],\n                [-8.2758e-03, -1.5137e-02,  1.7059e-02]],\n      \n               [[-1.3076e-02, -2.5776e-03,  1.8083e-02],\n                [-2.0171e-05,  1.4744e-02,  1.7316e-02],\n                [-3.9449e-04,  1.2536e-02,  5.0930e-04]],\n      \n               ...,\n      \n               [[-1.4792e-02, -1.7998e-02, -3.7981e-03],\n                [-8.4203e-03,  5.1342e-03,  1.3059e-02],\n                [ 3.6889e-03,  1.4221e-02,  4.3599e-03]],\n      \n               [[-3.3614e-03, -5.7319e-03, -6.4825e-03],\n                [-4.4420e-03, -1.9808e-02,  9.7536e-03],\n                [ 4.4880e-03,  1.2383e-02, -1.3645e-02]],\n      \n               [[-1.2345e-02, -1.6252e-02, -4.2853e-03],\n                [ 1.5557e-02,  7.5026e-03, -1.3222e-02],\n                [ 1.5796e-02,  1.3472e-02,  1.7703e-02]]],\n      \n      \n              [[[ 9.2593e-03, -2.5494e-03,  5.6741e-03],\n                [ 1.4672e-02, -1.0649e-02,  2.0219e-02],\n                [ 1.8491e-02, -1.6710e-02, -1.3741e-02]],\n      \n               [[-6.0494e-03, -4.8719e-03, -1.6048e-02],\n                [ 1.1836e-02,  5.0387e-03, -4.7032e-03],\n                [-2.0553e-03, -1.5415e-02, -9.2818e-03]],\n      \n               [[ 1.7408e-02, -1.8872e-02,  1.5017e-02],\n                [-2.0228e-02,  1.7542e-02,  1.2950e-02],\n                [ 2.0612e-02,  7.1278e-03, -2.7786e-03]],\n      \n               ...,\n      \n               [[ 1.3651e-02,  2.8672e-03,  2.0461e-02],\n                [-6.6320e-03,  1.1510e-02, -3.4409e-03],\n                [-3.0646e-03,  6.0877e-03, -2.0106e-02]],\n      \n               [[ 1.4078e-02, -1.7094e-02,  5.4326e-03],\n                [-9.4198e-03, -1.5817e-02,  1.0880e-02],\n                [-1.1792e-02,  1.2490e-02,  1.9634e-02]],\n      \n               [[ 1.6507e-02, -3.2975e-03,  1.2178e-02],\n                [-1.1994e-02,  1.3273e-02, -8.7332e-03],\n                [ 2.0393e-02,  1.5298e-02,  2.5994e-03]]],\n      \n      \n              [[[ 5.0196e-03,  5.1807e-03,  1.0801e-02],\n                [ 1.5606e-02, -8.8967e-03,  2.1617e-03],\n                [-1.0086e-02,  1.8315e-02,  6.9859e-03]],\n      \n               [[-2.0019e-02, -7.2372e-03, -1.9101e-02],\n                [ 1.1513e-02, -1.0494e-02, -4.7214e-03],\n                [-3.2720e-03, -3.6906e-03,  2.0750e-03]],\n      \n               [[ 5.1912e-03, -2.0292e-02, -5.1723e-04],\n                [ 1.7303e-02,  1.7247e-02, -6.2493e-03],\n                [ 1.5479e-02,  3.4994e-04, -1.8239e-02]],\n      \n               ...,\n      \n               [[ 1.1459e-02,  2.1115e-04,  5.1908e-03],\n                [ 8.7702e-03, -3.6110e-03, -7.1834e-03],\n                [ 1.1279e-03, -6.6010e-03, -1.4568e-02]],\n      \n               [[ 5.4152e-03,  2.8916e-03, -2.5093e-03],\n                [ 7.9939e-03,  3.2847e-03, -7.6354e-03],\n                [ 3.6525e-03, -1.0358e-02,  6.8559e-04]],\n      \n               [[-1.1588e-02,  6.1637e-03,  3.1675e-03],\n                [-1.1550e-02, -6.8073e-03, -9.3952e-04],\n                [-4.4949e-04,  5.3527e-03, -4.4125e-03]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-0.0074,  0.0111, -0.0043,  0.0038,  0.0023,  0.0043,  0.0002,  0.0025,\n              -0.0083, -0.0106, -0.0123,  0.0027,  0.0103,  0.0065, -0.0127, -0.0096,\n               0.0052,  0.0089, -0.0361,  0.0132, -0.0117, -0.0026,  0.0084, -0.0160,\n              -0.0086, -0.0160,  0.0108, -0.0187, -0.0128, -0.0046,  0.0139,  0.0061,\n              -0.0019, -0.0172,  0.0357, -0.0160,  0.0173, -0.0081,  0.0083,  0.0035,\n              -0.0020, -0.0224, -0.0121, -0.0148, -0.0037, -0.0154,  0.0094, -0.0053,\n              -0.0165,  0.0118,  0.0333, -0.0034, -0.0177, -0.0066,  0.0130,  0.0048,\n               0.0025,  0.0173, -0.0080,  0.0030, -0.0106, -0.0143, -0.0024,  0.0147,\n              -0.0119, -0.0160, -0.0025,  0.0058, -0.0091, -0.0213, -0.0109, -0.0111,\n              -0.0028, -0.0028,  0.0037,  0.0092,  0.0131, -0.0004, -0.0258, -0.0129,\n              -0.0024, -0.0166, -0.0156, -0.0044,  0.0102,  0.0024, -0.0020,  0.0118,\n              -0.0070,  0.0001,  0.0188,  0.0133, -0.0037, -0.0064,  0.0004,  0.0070,\n               0.0022, -0.0211,  0.0174, -0.0025, -0.0057,  0.0095, -0.0051, -0.0021,\n               0.0163, -0.0053, -0.0221, -0.0276,  0.0113, -0.0062,  0.0049,  0.0141,\n               0.0105, -0.0091, -0.0010,  0.0144, -0.0158, -0.0170, -0.0151, -0.0319,\n               0.0032, -0.0101, -0.0110,  0.0139,  0.0236,  0.0096,  0.0135,  0.0135,\n              -0.0085,  0.0078, -0.0035,  0.0094,  0.0222, -0.0205,  0.0257,  0.0022,\n               0.0016, -0.0113, -0.0086,  0.0062, -0.0117, -0.0039, -0.0072,  0.0042,\n              -0.0227, -0.0022,  0.0243,  0.0028, -0.0072, -0.0134, -0.0138, -0.0168,\n               0.0100,  0.0063, -0.0201, -0.0102,  0.0210,  0.0053,  0.0037, -0.0157,\n              -0.0021, -0.0188, -0.0203, -0.0170, -0.0012,  0.0269,  0.0203,  0.0039,\n               0.0041,  0.0017, -0.0049, -0.0037, -0.0107, -0.0037, -0.0078,  0.0009,\n               0.0140, -0.0109,  0.0131, -0.0111,  0.0145, -0.0101,  0.0119, -0.0102,\n              -0.0316, -0.0047,  0.0101, -0.0073,  0.0050,  0.0035, -0.0148, -0.0124,\n              -0.0058, -0.0115, -0.0122,  0.0078, -0.0017,  0.0022,  0.0011,  0.0224,\n               0.0025,  0.0090, -0.0065,  0.0224,  0.0029, -0.0018,  0.0065, -0.0079,\n              -0.0069,  0.0013, -0.0163,  0.0204,  0.0146, -0.0177,  0.0011,  0.0115,\n              -0.0009, -0.0040, -0.0072,  0.0262,  0.0146,  0.0114,  0.0167,  0.0122,\n               0.0075,  0.0135, -0.0121, -0.0197,  0.0030, -0.0095, -0.0015, -0.0008,\n               0.0031,  0.0159,  0.0184, -0.0062, -0.0008, -0.0026, -0.0126, -0.0191,\n               0.0036,  0.0085,  0.0052,  0.0087,  0.0042, -0.0162,  0.0075,  0.0052,\n               0.0095,  0.0175,  0.0018, -0.0101, -0.0142, -0.0113,  0.0069,  0.0106],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9063, 0.9058, 0.9069, 0.9072, 0.9066, 0.9083, 0.9051, 0.9056, 0.9089,\n              0.9070, 0.9085, 0.9061, 0.9053, 0.9046, 0.9069, 0.9036, 0.9051, 0.9041,\n              0.9080, 0.9056, 0.9051, 0.9044, 0.9064, 0.9041, 0.9048, 0.9084, 0.9047,\n              0.9070, 0.9070, 0.9086, 0.9059, 0.9045, 0.9058, 0.9055, 0.9064, 0.9056,\n              0.9044, 0.9056, 0.9039, 0.9056, 0.9058, 0.9058, 0.9062, 0.9069, 0.9058,\n              0.9043, 0.9044, 0.9074, 0.9077, 0.9084, 0.9046, 0.9148, 0.9060, 0.9048,\n              0.9050, 0.9044, 0.9056, 0.9045, 0.9062, 0.9114, 0.9042, 0.9101, 0.9126,\n              0.9046, 0.9064, 0.9058, 0.9057, 0.9084, 0.9052, 0.9055, 0.9046, 0.9060,\n              0.9051, 0.9048, 0.9048, 0.9041, 0.9048, 0.9042, 0.9076, 0.9060, 0.9044,\n              0.9040, 0.9054, 0.9065, 0.9057, 0.9070, 0.9067, 0.9051, 0.9051, 0.9073,\n              0.9061, 0.9042, 0.9040, 0.9058, 0.9055, 0.9067, 0.9049, 0.9064, 0.9046,\n              0.9063, 0.9045, 0.9079, 0.9062, 0.9058, 0.9053, 0.9063, 0.9047, 0.9084,\n              0.9041, 0.9061, 0.9083, 0.9051, 0.9083, 0.9069, 0.9061, 0.9078, 0.9045,\n              0.9056, 0.9042, 0.9100, 0.9056, 0.9040, 0.9054, 0.9047, 0.9071, 0.9058,\n              0.9047, 0.9053, 0.9059, 0.9085, 0.9072, 0.9053, 0.9047, 0.9072, 0.9049,\n              0.9054, 0.9052, 0.9120, 0.9038, 0.9073, 0.9059, 0.9045, 0.9048, 0.9064,\n              0.9062, 0.9049, 0.9046, 0.9043, 0.9070, 0.9071, 0.9072, 0.9059, 0.9040,\n              0.9051, 0.9106, 0.9059, 0.9055, 0.9053, 0.9054, 0.9089, 0.9064, 0.9056,\n              0.9047, 0.9066, 0.9064, 0.9041, 0.9075, 0.9079, 0.9067, 0.9057, 0.9039,\n              0.9054, 0.9056, 0.9042, 0.9060, 0.9047, 0.9049, 0.9092, 0.9089, 0.9064,\n              0.9074, 0.9053, 0.9039, 0.9041, 0.9055, 0.9080, 0.9043, 0.9073, 0.9058,\n              0.9052, 0.9042, 0.9056, 0.9059, 0.9046, 0.9055, 0.9059, 0.9066, 0.9046,\n              0.9046, 0.9061, 0.9049, 0.9047, 0.9047, 0.9065, 0.9060, 0.9081, 0.9045,\n              0.9058, 0.9054, 0.9047, 0.9112, 0.9070, 0.9074, 0.9066, 0.9062, 0.9051,\n              0.9049, 0.9056, 0.9041, 0.9064, 0.9057, 0.9045, 0.9037, 0.9054, 0.9063,\n              0.9063, 0.9048, 0.9053, 0.9055, 0.9083, 0.9070, 0.9057, 0.9059, 0.9050,\n              0.9054, 0.9068, 0.9060, 0.9044, 0.9066, 0.9055, 0.9048, 0.9058, 0.9044,\n              0.9056, 0.9050, 0.9088, 0.9058, 0.9099, 0.9045, 0.9049, 0.9040, 0.9059,\n              0.9074, 0.9065, 0.9045, 0.9049], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[ 0.0058]],\n        \n                 [[ 0.0802]],\n        \n                 [[ 0.0609]],\n        \n                 ...,\n        \n                 [[-0.0649]],\n        \n                 [[-0.0057]],\n        \n                 [[ 0.0498]]],\n        \n        \n                [[[ 0.0046]],\n        \n                 [[-0.0496]],\n        \n                 [[-0.0329]],\n        \n                 ...,\n        \n                 [[ 0.0570]],\n        \n                 [[-0.0137]],\n        \n                 [[-0.0539]]],\n        \n        \n                [[[ 0.0379]],\n        \n                 [[ 0.0297]],\n        \n                 [[-0.0506]],\n        \n                 ...,\n        \n                 [[-0.0561]],\n        \n                 [[-0.0426]],\n        \n                 [[ 0.0743]]],\n        \n        \n                ...,\n        \n        \n                [[[ 0.0176]],\n        \n                 [[-0.0308]],\n        \n                 [[-0.0164]],\n        \n                 ...,\n        \n                 [[-0.0136]],\n        \n                 [[ 0.0664]],\n        \n                 [[-0.0632]]],\n        \n        \n                [[[ 0.0121]],\n        \n                 [[ 0.0267]],\n        \n                 [[ 0.0595]],\n        \n                 ...,\n        \n                 [[-0.0351]],\n        \n                 [[-0.0231]],\n        \n                 [[ 0.0451]]],\n        \n        \n                [[[-0.0425]],\n        \n                 [[ 0.0032]],\n        \n                 [[ 0.0632]],\n        \n                 ...,\n        \n                 [[-0.0484]],\n        \n                 [[ 0.0148]],\n        \n                 [[-0.0559]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n               requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0220,  0.0248, -0.0504,  0.0009,  0.0152,  0.0148, -0.0175, -0.0387,\n                 0.0284,  0.0135,  0.0046, -0.0387,  0.0010,  0.0190,  0.0430,  0.0305,\n                 0.0056, -0.0247, -0.0321, -0.0296,  0.0164,  0.0213, -0.0049, -0.0164,\n                -0.0360,  0.0416,  0.0138, -0.0466, -0.0053,  0.0224, -0.0477, -0.0278,\n                 0.0324,  0.0522, -0.0243,  0.0230, -0.0136,  0.0211, -0.0263,  0.0232,\n                -0.0194, -0.0175,  0.0022, -0.0122, -0.0200,  0.0425, -0.0053,  0.0013,\n                 0.0220,  0.0378,  0.0288,  0.0273, -0.0408, -0.0370, -0.0041, -0.0064,\n                -0.0191, -0.0179, -0.0152,  0.0560, -0.0224, -0.0227,  0.0316, -0.0103,\n                -0.0420,  0.0306,  0.0294,  0.0483, -0.0311,  0.0108,  0.0265,  0.0158,\n                 0.0104, -0.0106, -0.0340,  0.0619, -0.0469, -0.0073,  0.0031, -0.0133,\n                 0.0113,  0.0736, -0.0010, -0.0232,  0.0269, -0.0101, -0.0015,  0.0112,\n                 0.0404,  0.0336, -0.0279,  0.0178, -0.0156,  0.0673, -0.0046, -0.0503,\n                -0.0017,  0.0654, -0.0082, -0.0143, -0.0508, -0.0297,  0.0240,  0.0054,\n                -0.0190,  0.0318, -0.0110, -0.0137, -0.0128,  0.0239,  0.0140,  0.0129,\n                -0.0226, -0.0187, -0.0163, -0.0156,  0.0063,  0.0019,  0.0221,  0.0143,\n                -0.0115, -0.0111,  0.0207, -0.0034, -0.0270, -0.0268,  0.0239, -0.0065,\n                 0.0629,  0.0498, -0.0134,  0.0221,  0.0554, -0.0079,  0.0430,  0.0127,\n                -0.0295,  0.0427, -0.0463,  0.0138, -0.0123,  0.0450,  0.0273,  0.0326,\n                -0.0131, -0.0055,  0.0053, -0.0893,  0.0544, -0.0105, -0.0249, -0.0817,\n                -0.0323, -0.0187,  0.0449,  0.0273, -0.0105,  0.0157, -0.0679, -0.0266,\n                 0.0186, -0.0314, -0.0123,  0.0779,  0.0125, -0.0017, -0.0060,  0.0278,\n                -0.0135,  0.0868, -0.0476,  0.0378, -0.0116, -0.0140, -0.0090,  0.0292,\n                 0.0147, -0.0058,  0.0149,  0.0128, -0.0520,  0.0482, -0.0252,  0.0147,\n                 0.0174,  0.0170,  0.0300, -0.0431, -0.0093, -0.0538, -0.0220, -0.0103,\n                 0.0372,  0.0096, -0.0178, -0.0063,  0.0691, -0.0552,  0.0069, -0.0006,\n                 0.0074,  0.0198, -0.0578,  0.0151,  0.0124,  0.0159, -0.0069, -0.0178,\n                 0.0108,  0.0203, -0.0341,  0.0077, -0.0181, -0.0091,  0.0039, -0.0400,\n                 0.0067,  0.0263,  0.0313, -0.0038, -0.0651, -0.0460, -0.0354,  0.0056,\n                 0.0047,  0.0280,  0.0124, -0.0717, -0.0180,  0.0039, -0.0023, -0.0499,\n                 0.0598, -0.0039,  0.0254,  0.0132, -0.0323, -0.0395, -0.0136, -0.0352,\n                -0.0693, -0.0094,  0.0330,  0.0004,  0.0297,  0.0033, -0.0769,  0.0160,\n                -0.0273, -0.0334, -0.0171,  0.0049, -0.0368,  0.0289, -0.0213,  0.0214],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9155, 0.9155, 0.9201, 0.9148, 0.9200, 0.9262, 0.9182, 0.9145, 0.9156,\n                0.9142, 0.9296, 0.9134, 0.9152, 0.9189, 0.9184, 0.9250, 0.9269, 0.9170,\n                0.9142, 0.9169, 0.9175, 0.9181, 0.9215, 0.9121, 0.9215, 0.9220, 0.9158,\n                0.9186, 0.9117, 0.9163, 0.9181, 0.9133, 0.9163, 0.9197, 0.9219, 0.9149,\n                0.9153, 0.9225, 0.9229, 0.9155, 0.9172, 0.9137, 0.9184, 0.9109, 0.9113,\n                0.9198, 0.9148, 0.9131, 0.9213, 0.9184, 0.9201, 0.9276, 0.9249, 0.9137,\n                0.9165, 0.9143, 0.9150, 0.9137, 0.9172, 0.9222, 0.9176, 0.9230, 0.9144,\n                0.9213, 0.9184, 0.9170, 0.9196, 0.9205, 0.9200, 0.9116, 0.9141, 0.9182,\n                0.9237, 0.9251, 0.9128, 0.9175, 0.9194, 0.9428, 0.9211, 0.9186, 0.9143,\n                0.9215, 0.9201, 0.9161, 0.9155, 0.9196, 0.9178, 0.9170, 0.9150, 0.9141,\n                0.9244, 0.9143, 0.9177, 0.9195, 0.9171, 0.9179, 0.9142, 0.9164, 0.9238,\n                0.9186, 0.9174, 0.9245, 0.9188, 0.9140, 0.9186, 0.9191, 0.9157, 0.9145,\n                0.9156, 0.9232, 0.9195, 0.9184, 0.9139, 0.9205, 0.9148, 0.9206, 0.9195,\n                0.9188, 0.9181, 0.9159, 0.9141, 0.9185, 0.9110, 0.9174, 0.9195, 0.9202,\n                0.9292, 0.9158, 0.9178, 0.9200, 0.9143, 0.9186, 0.9187, 0.9235, 0.9191,\n                0.9141, 0.9135, 0.9171, 0.9128, 0.9132, 0.9142, 0.9201, 0.9264, 0.9213,\n                0.9135, 0.9175, 0.9144, 0.9202, 0.9162, 0.9143, 0.9194, 0.9265, 0.9157,\n                0.9222, 0.9244, 0.9160, 0.9199, 0.9122, 0.9173, 0.9155, 0.9135, 0.9162,\n                0.9162, 0.9221, 0.9259, 0.9169, 0.9151, 0.9309, 0.9207, 0.9166, 0.9157,\n                0.9179, 0.9178, 0.9154, 0.9195, 0.9166, 0.9145, 0.9194, 0.9131, 0.9157,\n                0.9146, 0.9323, 0.9147, 0.9183, 0.9147, 0.9165, 0.9238, 0.9147, 0.9182,\n                0.9195, 0.9196, 0.9257, 0.9181, 0.9180, 0.9129, 0.9204, 0.9227, 0.9220,\n                0.9183, 0.9259, 0.9252, 0.9207, 0.9203, 0.9149, 0.9178, 0.9393, 0.9150,\n                0.9183, 0.9194, 0.9321, 0.9189, 0.9162, 0.9172, 0.9194, 0.9139, 0.9154,\n                0.9143, 0.9167, 0.9263, 0.9213, 0.9205, 0.9252, 0.9171, 0.9161, 0.9129,\n                0.9139, 0.9196, 0.9167, 0.9166, 0.9134, 0.9167, 0.9225, 0.9153, 0.9303,\n                0.9211, 0.9181, 0.9211, 0.9223, 0.9179, 0.9218, 0.9194, 0.9206, 0.9156,\n                0.9168, 0.9148, 0.9201, 0.9209, 0.9168, 0.9173, 0.9195, 0.9189, 0.9194,\n                0.9150, 0.9162, 0.9244, 0.9175], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [256, 128, 3, 3]], ["0.bn1.weight", [256]], ["0.bn1.bias", [256]], ["0.conv2.weight", [256, 256, 3, 3]], ["0.bn2.weight", [256]], ["0.bn2.bias", [256]], ["0.downsample.0.weight", [256, 128, 1, 1]], ["0.downsample.1.weight", [256]], ["0.downsample.1.bias", [256]]], "output_shape": [[512, 256, 2, 2]], "num_parameters": [294912, 256, 256, 589824, 256, 256, 32768, 256, 256]}, {"name": "layer4", "id": 140542746891360, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 1.4364e-03, -1.1273e-02, -1.4461e-02],\n                [ 9.8457e-03,  1.8108e-02, -2.0745e-02],\n                [ 1.0131e-02,  1.3801e-02, -1.0157e-02]],\n      \n               [[-1.4758e-02, -1.1043e-02,  1.4026e-02],\n                [ 4.9217e-03, -2.0061e-02,  5.2970e-04],\n                [ 1.2656e-02, -2.0811e-02,  2.0487e-02]],\n      \n               [[ 7.6323e-03,  2.0120e-02, -4.5203e-03],\n                [ 7.3318e-03,  4.5459e-04,  5.1893e-03],\n                [-1.0452e-02, -2.0368e-02,  1.1118e-02]],\n      \n               ...,\n      \n               [[-1.4941e-02,  1.2246e-02,  8.1161e-03],\n                [ 1.1956e-02,  1.9652e-02, -9.3669e-03],\n                [ 1.5112e-02,  1.3292e-02,  1.9482e-02]],\n      \n               [[-1.1739e-02,  1.6141e-02,  6.2119e-03],\n                [-2.0254e-02,  1.6432e-02, -1.3518e-02],\n                [-8.0758e-03, -7.4589e-03, -1.2873e-02]],\n      \n               [[-6.1119e-03,  3.0557e-04, -1.9329e-02],\n                [ 2.0437e-02, -1.6922e-02,  1.6379e-02],\n                [ 1.0127e-02,  1.8799e-02, -1.4216e-02]]],\n      \n      \n              [[[-1.0150e-02, -2.0820e-02, -2.0150e-02],\n                [-5.4175e-03, -1.8836e-02, -5.8055e-03],\n                [-6.7588e-03,  1.8813e-03, -8.6269e-03]],\n      \n               [[-1.8013e-02,  3.6722e-03,  2.8928e-03],\n                [-5.4610e-03,  1.6957e-02,  1.4046e-02],\n                [ 9.0833e-03,  2.1465e-03, -1.0073e-02]],\n      \n               [[ 4.7707e-03,  2.1885e-03,  1.7981e-02],\n                [-6.3302e-03, -3.0500e-03,  8.0323e-03],\n                [ 8.3492e-03,  1.5316e-02, -2.8346e-03]],\n      \n               ...,\n      \n               [[ 1.3714e-02,  1.4910e-02,  5.1461e-03],\n                [ 7.9140e-03,  3.9453e-03,  1.5803e-02],\n                [-5.9638e-03,  1.8489e-02, -1.8304e-02]],\n      \n               [[-6.8711e-03,  1.0914e-02,  7.6774e-03],\n                [ 5.9178e-03,  1.4725e-02,  4.4296e-03],\n                [ 1.6842e-03, -1.1721e-02,  2.0599e-02]],\n      \n               [[ 1.4615e-02,  5.1966e-03, -8.4140e-03],\n                [ 1.0832e-02,  1.6611e-02, -1.2264e-02],\n                [-1.3136e-02, -3.6822e-03, -6.9413e-03]]],\n      \n      \n              [[[ 1.3019e-02, -1.4946e-02, -3.7333e-03],\n                [ 1.3031e-02, -1.4639e-02,  6.8725e-03],\n                [-8.3598e-03,  5.2264e-03,  1.1853e-02]],\n      \n               [[-1.5325e-03, -1.7870e-03,  9.4288e-03],\n                [ 1.5847e-02,  2.0694e-02, -1.1125e-03],\n                [-1.0981e-02, -1.2751e-02, -1.1125e-03]],\n      \n               [[-1.8338e-02, -6.9852e-03, -1.2749e-02],\n                [ 1.5369e-02,  5.9812e-03, -2.6932e-03],\n                [-5.8368e-03,  2.0647e-02, -7.7952e-03]],\n      \n               ...,\n      \n               [[-8.0139e-03,  1.1673e-02, -2.5918e-03],\n                [-8.4121e-03, -1.7732e-02,  5.8270e-03],\n                [ 1.3095e-02, -3.8051e-03, -1.4996e-02]],\n      \n               [[-4.4612e-04,  8.6211e-03,  2.0138e-02],\n                [ 6.3816e-03, -7.3608e-03, -1.4208e-02],\n                [-2.0687e-03, -1.4633e-02, -1.0530e-03]],\n      \n               [[-3.5490e-03, -4.0415e-03, -1.4769e-02],\n                [ 1.0775e-02,  3.6264e-03, -4.6327e-03],\n                [-9.3646e-04, -1.3732e-02, -1.6483e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[ 1.2508e-02, -4.9853e-03,  3.0460e-03],\n                [ 1.8187e-02, -9.1416e-03, -3.1459e-03],\n                [-1.7764e-02,  1.3957e-02,  1.2907e-02]],\n      \n               [[ 7.0449e-03,  1.7893e-02,  1.4022e-02],\n                [ 3.0022e-03, -1.1375e-02, -1.4078e-02],\n                [-1.5094e-02, -1.4777e-02, -1.6908e-02]],\n      \n               [[ 1.0858e-02,  1.9858e-02,  1.6052e-02],\n                [-1.5733e-02,  5.0259e-03, -1.5139e-02],\n                [-1.8751e-02, -8.0771e-03, -1.3193e-02]],\n      \n               ...,\n      \n               [[-1.4589e-02, -1.2574e-02, -6.6614e-03],\n                [ 3.2278e-03, -6.4622e-03, -1.0048e-02],\n                [ 1.4252e-03,  1.4124e-02, -1.6507e-02]],\n      \n               [[-1.2793e-03,  8.5102e-03, -1.6277e-03],\n                [ 2.9540e-03, -3.4011e-03, -1.1278e-02],\n                [ 1.5406e-02, -1.6735e-02,  4.9964e-03]],\n      \n               [[ 3.6031e-03, -1.3791e-02,  2.0492e-02],\n                [ 1.2695e-02, -2.0069e-03,  5.0439e-03],\n                [-1.1257e-02,  7.1782e-03, -9.6255e-03]]],\n      \n      \n              [[[ 7.4311e-03,  3.5834e-03, -1.3177e-03],\n                [ 1.1082e-02, -1.7533e-02, -6.6173e-03],\n                [-3.4848e-03,  2.2489e-03,  7.1585e-03]],\n      \n               [[-8.5155e-03,  4.0459e-03,  1.9612e-02],\n                [ 9.3788e-03, -1.2075e-02, -4.5112e-03],\n                [-5.3072e-03,  2.0464e-02,  1.6353e-02]],\n      \n               [[-7.4116e-03, -8.4700e-03, -6.3910e-03],\n                [ 1.2767e-02,  2.0523e-03, -9.2231e-03],\n                [ 1.8697e-02,  1.0051e-02,  2.0721e-02]],\n      \n               ...,\n      \n               [[ 5.6000e-03,  9.9377e-03, -1.9896e-03],\n                [ 1.7022e-02,  9.1728e-04, -1.3762e-03],\n                [ 1.6121e-02,  6.5830e-03, -2.7812e-03]],\n      \n               [[-1.1777e-02,  9.3158e-03,  8.6486e-05],\n                [ 1.1729e-02,  5.3535e-03, -1.4199e-02],\n                [ 1.8069e-02, -1.0914e-02,  1.5149e-02]],\n      \n               [[ 1.6785e-03,  1.2724e-02,  1.4822e-02],\n                [ 1.1891e-02, -1.8135e-02, -1.4494e-02],\n                [ 1.6633e-02, -1.2248e-02, -1.8693e-02]]],\n      \n      \n              [[[ 2.0723e-04,  1.9862e-02,  8.8241e-03],\n                [-4.7099e-03, -1.4695e-02, -1.0816e-02],\n                [ 1.8578e-02,  7.1440e-03, -1.1953e-02]],\n      \n               [[ 1.1681e-02, -8.0286e-04,  1.7213e-02],\n                [ 9.6834e-03,  1.6116e-02,  1.6067e-03],\n                [ 7.1301e-03, -1.7100e-02, -6.1133e-04]],\n      \n               [[ 6.9963e-03, -2.3323e-03,  2.1336e-03],\n                [ 4.1726e-03,  1.8958e-02,  1.3549e-02],\n                [ 1.4178e-02,  5.1827e-03, -1.5033e-02]],\n      \n               ...,\n      \n               [[-2.9589e-03, -8.4422e-03,  1.7338e-02],\n                [ 3.7990e-03,  1.1520e-03, -1.8201e-02],\n                [ 5.3905e-03,  1.2231e-02,  9.5592e-03]],\n      \n               [[-1.2481e-02, -2.2896e-04,  2.0219e-02],\n                [ 2.1144e-03, -6.9474e-03, -4.2896e-03],\n                [-4.4456e-03, -1.1712e-02, -9.5182e-03]],\n      \n               [[ 1.6910e-02,  1.7095e-02, -1.0718e-02],\n                [ 2.9823e-03, -5.0395e-03,  1.3057e-02],\n                [-2.0670e-02, -7.2557e-03,  1.6631e-02]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-6.6474e-03, -1.2108e-02, -2.9756e-02,  2.9097e-02, -3.1529e-02,\n              -1.3953e-02,  2.7148e-02,  3.3133e-02,  3.6966e-02, -1.6596e-03,\n               3.4238e-02,  2.8199e-02, -3.3775e-03, -2.3008e-02,  5.9774e-03,\n              -3.0881e-03,  4.6115e-02, -3.3573e-02,  1.5095e-02, -2.3691e-02,\n               2.2265e-02, -3.3547e-02, -1.6048e-02, -2.2253e-02, -6.7839e-03,\n              -3.9896e-02,  1.4463e-02, -3.5873e-03,  1.6532e-02,  8.3153e-03,\n               1.2976e-02,  4.2995e-03, -5.3934e-03,  3.8989e-02, -5.1348e-04,\n              -3.1412e-02,  3.0349e-02,  1.8671e-02,  6.9433e-03, -1.1502e-02,\n               9.1922e-03, -8.7069e-03,  5.4615e-02, -1.5227e-02,  4.5448e-03,\n               4.6576e-03,  2.9524e-02,  1.5810e-02,  1.4355e-02, -5.6143e-03,\n               2.3202e-02,  4.3129e-02, -2.9688e-02, -2.8988e-03,  9.4440e-03,\n               1.2143e-02,  1.6765e-02,  3.6302e-02, -3.2003e-03,  2.3065e-02,\n               1.9492e-02, -9.8716e-03, -3.3925e-02,  4.6028e-02, -3.8195e-02,\n              -2.5670e-02, -2.6086e-02, -1.7895e-02,  4.6513e-02, -2.7204e-02,\n               1.5748e-02, -2.7744e-02, -1.0056e-02,  6.3428e-03,  2.3213e-02,\n               5.9814e-03,  4.1389e-02,  3.6168e-02, -6.3595e-04, -4.3503e-03,\n              -4.3193e-03,  6.8080e-03, -2.0398e-02, -1.6859e-02,  1.5216e-02,\n               8.6185e-03,  2.4005e-02,  1.3680e-02,  1.2512e-02, -3.0866e-02,\n              -1.2766e-02,  3.8366e-02, -5.7784e-02,  2.2280e-02, -1.5709e-03,\n              -1.9732e-02,  5.9731e-02, -2.6913e-02, -3.1826e-02,  9.8569e-03,\n               1.9767e-02,  4.4316e-02,  6.9605e-03,  3.7005e-02, -1.6903e-02,\n               9.2719e-03, -6.6646e-03,  1.0715e-02, -3.7230e-02, -1.5038e-02,\n              -2.3271e-02,  8.4095e-03, -2.2557e-02,  9.6351e-03, -4.5950e-02,\n               1.7872e-02,  2.1144e-02,  2.1013e-02, -1.4335e-02, -2.8645e-03,\n               2.5732e-02,  2.1854e-02,  1.2827e-02,  2.2424e-02,  2.5580e-02,\n               2.1761e-02,  1.5537e-02, -5.5798e-03,  6.2001e-03,  2.2264e-02,\n              -1.5901e-02, -1.3494e-02,  1.1637e-02,  9.1193e-03,  6.3000e-03,\n              -3.9930e-02, -3.9788e-02,  1.3523e-02, -1.2919e-02, -1.5867e-02,\n              -3.3900e-02,  2.7054e-02,  2.1613e-02,  1.2369e-02, -3.1483e-03,\n              -2.6101e-02,  3.5066e-02, -8.2974e-03, -1.2214e-02,  2.5457e-02,\n              -2.0633e-02,  1.9491e-02, -6.5169e-02, -2.1607e-02,  4.2602e-02,\n               1.2031e-02,  2.2592e-02,  2.3844e-03,  6.5872e-02, -3.8769e-02,\n              -2.6944e-02, -1.5540e-02, -5.5567e-03, -3.7466e-03,  2.3611e-03,\n              -1.4129e-02,  4.9844e-02, -1.7862e-02,  2.4728e-02, -2.4018e-02,\n               1.2994e-02, -1.6689e-02, -5.3341e-02, -1.3268e-02,  5.2721e-02,\n              -1.2888e-02,  3.0707e-02,  5.6681e-02, -4.3451e-02,  2.6707e-02,\n              -1.2665e-02, -3.6493e-03, -3.7250e-02, -2.2428e-02,  1.1124e-02,\n              -5.8292e-02, -6.9948e-03, -8.6515e-03,  4.2168e-03, -9.9905e-03,\n              -5.4026e-03, -3.5345e-02, -4.9598e-02, -6.6473e-03,  1.0410e-02,\n               2.9288e-02,  1.9406e-02, -6.0330e-03,  6.1069e-02,  3.6354e-02,\n              -2.9268e-02,  1.1509e-02,  3.3916e-02,  1.9611e-02,  3.7937e-02,\n               1.5902e-03, -3.6773e-02,  7.4317e-03, -5.3430e-02,  5.1425e-03,\n               2.3171e-02, -4.0186e-02, -1.2651e-02, -2.7325e-03,  1.2157e-02,\n               3.4688e-02,  7.0541e-04, -8.4072e-03, -1.7784e-02,  1.9163e-02,\n              -4.0018e-02, -4.4532e-02,  2.5463e-02, -8.3476e-03, -1.4932e-03,\n              -4.7460e-02, -9.3653e-03,  2.8475e-02, -3.3693e-02,  2.6410e-02,\n              -2.3235e-02, -1.2659e-02,  1.2601e-02,  2.4906e-03, -1.0448e-02,\n               6.4801e-03,  6.0771e-03, -1.6102e-02,  2.4828e-02, -3.6026e-02,\n              -1.5841e-02,  3.8649e-02, -9.6957e-03, -1.5413e-03, -1.7758e-02,\n              -1.8755e-02, -3.1313e-02,  3.7010e-02, -1.7087e-02, -7.1400e-03,\n               5.7067e-02,  4.7722e-02, -2.3380e-02, -9.3497e-03,  1.0008e-02,\n              -5.3180e-03, -1.3673e-02, -6.6002e-03,  3.1544e-02, -5.9781e-04,\n              -5.8153e-03,  2.2455e-02,  6.3992e-03,  6.9774e-04,  8.2082e-05,\n               1.8504e-02, -9.8360e-03, -3.9038e-02, -1.8157e-02,  5.0271e-02,\n               2.3246e-02,  1.6224e-02, -4.7889e-02, -4.6472e-04,  5.6141e-03,\n               3.5488e-02,  3.4943e-02, -1.7599e-02,  1.5718e-02, -1.0730e-02,\n              -2.5787e-02,  1.2507e-02, -2.5611e-02,  9.8090e-04,  1.5597e-02,\n               2.1066e-02, -5.8701e-02,  1.4951e-02,  1.4939e-02, -1.5801e-02,\n              -3.3529e-02,  5.1764e-02, -2.5579e-02,  2.2239e-02,  1.9410e-02,\n              -2.8810e-02,  1.4569e-03,  7.8463e-03,  1.7581e-02, -1.0995e-02,\n               2.0694e-02,  1.6880e-02, -7.9367e-03,  3.9971e-02, -1.4660e-02,\n              -1.0195e-02, -3.5068e-02, -1.6629e-02,  3.6444e-02, -1.9681e-02,\n               3.3835e-02,  1.6818e-02,  1.7350e-02,  1.6398e-02,  3.0370e-03,\n               2.6916e-02, -1.3101e-02, -3.1210e-02, -1.0452e-02, -1.8213e-02,\n               1.0332e-02,  7.2292e-03, -2.7184e-02,  9.4790e-03, -7.8473e-03,\n               7.8280e-03,  1.9961e-02, -1.6421e-02,  1.7265e-02, -1.5425e-02,\n              -2.6105e-02, -1.5140e-02,  1.4000e-02,  3.2284e-02,  2.7835e-02,\n               2.9364e-02, -4.5213e-02, -1.3555e-02,  3.6747e-02,  4.9099e-02,\n              -5.3205e-02, -5.3332e-03, -4.5741e-02,  2.7798e-03, -2.4085e-02,\n              -2.5618e-02,  3.1126e-02, -2.5830e-02,  1.3667e-03, -1.1435e-02,\n               8.7079e-03, -5.3543e-02, -1.1848e-02,  1.3861e-02,  5.1424e-02,\n               2.6850e-02, -2.4055e-02, -4.9306e-02, -8.9149e-03, -2.0683e-02,\n               2.1729e-02,  1.8141e-02, -1.5914e-02,  1.5313e-02, -1.2328e-02,\n               3.1587e-02, -2.5932e-02,  3.7312e-02, -8.7743e-03,  2.5809e-04,\n              -3.0345e-02, -1.3988e-02, -1.7408e-02, -1.7157e-02, -3.8940e-03,\n               1.9249e-02,  1.1655e-02,  3.7377e-02, -2.9308e-04, -4.5157e-02,\n               2.7241e-02,  4.4105e-02,  8.1096e-03, -5.0110e-02,  7.7592e-03,\n               6.7597e-03, -6.1666e-03,  1.7310e-02,  2.7581e-02, -1.1206e-02,\n               1.1730e-02, -2.1884e-02,  1.0029e-02, -3.7379e-03, -8.7145e-03,\n               3.7750e-02, -2.5377e-02, -7.6163e-03,  1.7581e-02,  6.3296e-03,\n              -3.2998e-02,  1.8085e-02, -2.3041e-02, -5.5212e-03, -7.7125e-03,\n              -1.3039e-02, -5.3234e-02,  1.7048e-02,  3.4201e-03, -3.8096e-02,\n              -5.8546e-02,  1.3297e-02,  3.3046e-02, -2.7565e-02,  2.3774e-02,\n              -3.9093e-02,  6.6683e-02,  1.4155e-02, -1.4421e-02,  2.6473e-02,\n               4.4971e-03,  3.1175e-03, -1.2275e-02, -5.5294e-02, -1.3322e-02,\n               1.8398e-02,  5.1323e-02, -2.9182e-02, -5.2897e-03,  1.1131e-03,\n              -3.2000e-02,  7.4467e-03, -2.8968e-03,  3.4530e-02,  5.3497e-03,\n               1.3020e-02,  1.3873e-02, -1.3564e-02, -1.2591e-02, -4.4859e-02,\n               4.1874e-03, -2.2454e-02,  8.6985e-03,  1.3733e-02, -1.6256e-03,\n               9.5579e-03,  3.3780e-02, -2.9182e-02, -4.8307e-03, -4.3521e-02,\n               1.0820e-02,  4.8293e-02, -5.2755e-03, -2.2043e-02,  4.7197e-02,\n               5.5859e-03,  3.4046e-02,  5.0963e-03,  2.7943e-03,  4.5630e-03,\n               1.3062e-02,  2.5281e-02,  2.8868e-02,  9.5796e-03,  4.1073e-02,\n              -4.2852e-02,  2.5164e-02,  3.2547e-02, -2.4844e-02,  3.8587e-02,\n               2.5355e-02, -2.6943e-03,  3.0367e-03,  2.9927e-02, -8.8035e-04,\n               3.9242e-03,  1.4440e-02,  1.4105e-02,  4.0747e-02,  9.3109e-03,\n              -4.4210e-03,  1.3897e-02, -2.1230e-02, -3.1753e-02,  1.3281e-02,\n              -6.3266e-03, -2.1863e-02, -1.0040e-04,  2.8363e-02, -2.1581e-04,\n              -2.5659e-03, -2.3594e-02,  2.0128e-02, -2.6553e-02, -1.8828e-02,\n               1.0551e-03, -1.0190e-02, -2.7415e-02, -3.7679e-02,  2.7032e-03,\n              -1.8473e-02, -8.7963e-04, -2.8470e-02,  3.1395e-03, -4.7024e-02,\n               3.3889e-02, -8.2038e-03,  3.6461e-02,  1.1946e-02, -6.1707e-02,\n               5.0849e-03, -3.3104e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9099, 0.9064, 0.9138, 0.9084, 0.9081, 0.9093, 0.9084, 0.9090, 0.9069,\n              0.9073, 0.9104, 0.9081, 0.9075, 0.9067, 0.9113, 0.9070, 0.9088, 0.9077,\n              0.9088, 0.9073, 0.9077, 0.9082, 0.9071, 0.9088, 0.9075, 0.9111, 0.9077,\n              0.9083, 0.9095, 0.9089, 0.9100, 0.9074, 0.9120, 0.9098, 0.9111, 0.9110,\n              0.9100, 0.9096, 0.9103, 0.9077, 0.9080, 0.9078, 0.9093, 0.9071, 0.9080,\n              0.9078, 0.9080, 0.9076, 0.9089, 0.9064, 0.9086, 0.9105, 0.9091, 0.9078,\n              0.9101, 0.9099, 0.9080, 0.9106, 0.9085, 0.9086, 0.9122, 0.9087, 0.9102,\n              0.9096, 0.9088, 0.9093, 0.9092, 0.9066, 0.9082, 0.9086, 0.9088, 0.9105,\n              0.9084, 0.9070, 0.9099, 0.9069, 0.9071, 0.9071, 0.9073, 0.9059, 0.9068,\n              0.9063, 0.9068, 0.9081, 0.9075, 0.9075, 0.9097, 0.9074, 0.9084, 0.9080,\n              0.9076, 0.9080, 0.9096, 0.9109, 0.9083, 0.9074, 0.9121, 0.9091, 0.9069,\n              0.9092, 0.9093, 0.9073, 0.9080, 0.9114, 0.9086, 0.9077, 0.9086, 0.9069,\n              0.9104, 0.9076, 0.9075, 0.9112, 0.9093, 0.9072, 0.9081, 0.9074, 0.9098,\n              0.9082, 0.9088, 0.9072, 0.9089, 0.9082, 0.9078, 0.9082, 0.9072, 0.9087,\n              0.9111, 0.9079, 0.9071, 0.9084, 0.9080, 0.9072, 0.9074, 0.9088, 0.9100,\n              0.9085, 0.9083, 0.9086, 0.9070, 0.9117, 0.9086, 0.9083, 0.9069, 0.9094,\n              0.9091, 0.9093, 0.9083, 0.9123, 0.9099, 0.9099, 0.9067, 0.9080, 0.9103,\n              0.9084, 0.9107, 0.9082, 0.9093, 0.9069, 0.9119, 0.9084, 0.9074, 0.9086,\n              0.9070, 0.9112, 0.9107, 0.9069, 0.9085, 0.9103, 0.9088, 0.9101, 0.9073,\n              0.9071, 0.9108, 0.9102, 0.9095, 0.9099, 0.9090, 0.9091, 0.9089, 0.9094,\n              0.9064, 0.9100, 0.9077, 0.9104, 0.9083, 0.9097, 0.9084, 0.9096, 0.9091,\n              0.9077, 0.9082, 0.9146, 0.9084, 0.9111, 0.9102, 0.9066, 0.9084, 0.9077,\n              0.9063, 0.9084, 0.9078, 0.9068, 0.9112, 0.9075, 0.9090, 0.9075, 0.9068,\n              0.9069, 0.9078, 0.9103, 0.9083, 0.9087, 0.9075, 0.9084, 0.9094, 0.9110,\n              0.9068, 0.9061, 0.9057, 0.9084, 0.9101, 0.9075, 0.9064, 0.9088, 0.9090,\n              0.9084, 0.9076, 0.9077, 0.9085, 0.9067, 0.9082, 0.9080, 0.9062, 0.9078,\n              0.9096, 0.9084, 0.9087, 0.9076, 0.9224, 0.9135, 0.9105, 0.9109, 0.9082,\n              0.9080, 0.9082, 0.9067, 0.9108, 0.9081, 0.9083, 0.9066, 0.9103, 0.9088,\n              0.9076, 0.9098, 0.9072, 0.9076, 0.9096, 0.9086, 0.9076, 0.9068, 0.9088,\n              0.9100, 0.9074, 0.9101, 0.9074, 0.9086, 0.9067, 0.9089, 0.9087, 0.9092,\n              0.9098, 0.9093, 0.9075, 0.9077, 0.9074, 0.9080, 0.9097, 0.9097, 0.9064,\n              0.9085, 0.9082, 0.9084, 0.9084, 0.9074, 0.9065, 0.9067, 0.9090, 0.9100,\n              0.9067, 0.9113, 0.9071, 0.9096, 0.9069, 0.9108, 0.9100, 0.9091, 0.9116,\n              0.9094, 0.9104, 0.9078, 0.9114, 0.9084, 0.9092, 0.9102, 0.9126, 0.9092,\n              0.9082, 0.9091, 0.9074, 0.9069, 0.9092, 0.9094, 0.9074, 0.9081, 0.9094,\n              0.9056, 0.9080, 0.9082, 0.9081, 0.9087, 0.9079, 0.9107, 0.9083, 0.9067,\n              0.9075, 0.9100, 0.9083, 0.9071, 0.9077, 0.9076, 0.9081, 0.9084, 0.9095,\n              0.9099, 0.9129, 0.9095, 0.9070, 0.9080, 0.9073, 0.9084, 0.9145, 0.9072,\n              0.9085, 0.9080, 0.9088, 0.9072, 0.9071, 0.9094, 0.9084, 0.9085, 0.9070,\n              0.9102, 0.9103, 0.9069, 0.9101, 0.9070, 0.9083, 0.9093, 0.9091, 0.9100,\n              0.9061, 0.9071, 0.9075, 0.9089, 0.9083, 0.9098, 0.9089, 0.9079, 0.9086,\n              0.9073, 0.9065, 0.9089, 0.9098, 0.9092, 0.9097, 0.9081, 0.9084, 0.9083,\n              0.9084, 0.9081, 0.9079, 0.9107, 0.9062, 0.9082, 0.9075, 0.9083, 0.9084,\n              0.9064, 0.9076, 0.9077, 0.9102, 0.9079, 0.9074, 0.9086, 0.9138, 0.9124,\n              0.9074, 0.9103, 0.9082, 0.9072, 0.9087, 0.9100, 0.9100, 0.9087, 0.9075,\n              0.9131, 0.9070, 0.9080, 0.9066, 0.9088, 0.9072, 0.9069, 0.9106, 0.9082,\n              0.9104, 0.9068, 0.9088, 0.9074, 0.9079, 0.9078, 0.9071, 0.9092, 0.9086,\n              0.9073, 0.9086, 0.9079, 0.9095, 0.9117, 0.9062, 0.9080, 0.9078, 0.9087,\n              0.9111, 0.9097, 0.9095, 0.9094, 0.9114, 0.9085, 0.9074, 0.9091, 0.9074,\n              0.9090, 0.9076, 0.9088, 0.9083, 0.9099, 0.9101, 0.9076, 0.9087, 0.9115,\n              0.9070, 0.9074, 0.9097, 0.9084, 0.9097, 0.9083, 0.9076, 0.9081, 0.9102,\n              0.9080, 0.9081, 0.9088, 0.9083, 0.9066, 0.9086, 0.9072, 0.9068, 0.9077,\n              0.9058, 0.9076, 0.9107, 0.9074, 0.9091, 0.9068, 0.9074, 0.9062, 0.9090,\n              0.9067, 0.9064, 0.9085, 0.9086, 0.9113, 0.9080, 0.9074, 0.9089, 0.9075,\n              0.9102, 0.9103, 0.9083, 0.9080, 0.9075, 0.9078, 0.9060, 0.9078, 0.9098,\n              0.9095, 0.9079, 0.9080, 0.9107, 0.9088, 0.9068, 0.9081, 0.9082, 0.9074,\n              0.9081, 0.9088, 0.9075, 0.9144, 0.9079, 0.9118, 0.9075, 0.9122],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 1.2995e-02, -5.5322e-03, -4.1589e-03],\n                [-6.5288e-03, -2.2873e-03, -1.9147e-04],\n                [ 1.7405e-03, -7.8575e-03, -7.8816e-03]],\n      \n               [[-7.3274e-04, -1.4705e-02, -1.2585e-02],\n                [-6.4641e-03, -1.2305e-03, -3.4055e-05],\n                [-4.7003e-03,  8.5750e-04, -1.5255e-03]],\n      \n               [[-1.4104e-02,  1.1601e-02, -4.0303e-04],\n                [-4.3493e-03,  4.6276e-03, -2.3761e-03],\n                [ 9.8987e-03, -1.3707e-02, -1.2717e-02]],\n      \n               ...,\n      \n               [[-4.9479e-04,  1.4617e-02,  1.1096e-03],\n                [ 1.2184e-02, -1.1268e-02,  1.0659e-02],\n                [-5.3815e-03, -1.3579e-02,  7.0560e-03]],\n      \n               [[-3.9905e-03,  1.3083e-02,  1.2842e-02],\n                [-9.6670e-03,  1.0824e-02, -1.7010e-03],\n                [ 2.6615e-03,  1.2908e-02, -7.3412e-03]],\n      \n               [[ 9.0647e-03, -6.1318e-03, -9.3177e-03],\n                [ 4.2539e-03, -3.8807e-03,  1.3466e-02],\n                [ 4.2217e-03, -5.2138e-03, -1.3680e-02]]],\n      \n      \n              [[[-1.5648e-03,  9.4856e-03,  2.6803e-03],\n                [-3.0798e-03,  8.4917e-03, -1.2147e-03],\n                [-1.1443e-02, -2.0463e-03, -3.7298e-03]],\n      \n               [[ 1.4698e-02, -7.3523e-03,  6.8491e-03],\n                [ 7.3027e-03, -1.2385e-02, -1.2737e-02],\n                [-3.5827e-03, -9.5792e-03, -9.7751e-03]],\n      \n               [[-1.0676e-02, -8.8713e-03, -9.9928e-03],\n                [-3.2130e-03,  7.8163e-03,  5.9423e-03],\n                [-5.8211e-03, -7.8224e-03,  7.2569e-03]],\n      \n               ...,\n      \n               [[-1.4081e-02,  9.2815e-03,  5.2041e-03],\n                [-7.0052e-03,  1.2567e-02, -2.7938e-03],\n                [ 1.0935e-02, -1.1702e-02,  8.0721e-03]],\n      \n               [[ 1.1054e-02, -7.5428e-03,  1.2514e-03],\n                [ 7.7261e-03,  1.0071e-02, -9.4402e-03],\n                [-1.7445e-03,  9.3569e-03,  6.0777e-03]],\n      \n               [[ 1.1754e-03, -8.2491e-03,  4.9828e-03],\n                [ 1.0565e-02,  2.4310e-05, -8.1282e-03],\n                [-8.4542e-03, -9.8304e-03, -9.0763e-05]]],\n      \n      \n              [[[ 7.6640e-03,  4.5454e-03, -2.9189e-03],\n                [ 1.2021e-02,  3.8474e-04, -9.8953e-03],\n                [ 4.5086e-04,  2.7757e-03, -4.6474e-03]],\n      \n               [[ 9.6743e-03,  1.2591e-02,  5.0479e-03],\n                [-3.0432e-04,  9.3964e-03, -1.1716e-02],\n                [ 6.1852e-03, -3.6836e-03, -2.2616e-03]],\n      \n               [[-8.5551e-03, -1.1510e-02, -1.1210e-02],\n                [-7.0569e-03, -5.6281e-03,  1.2122e-02],\n                [-1.0609e-02,  1.0981e-02, -9.4288e-03]],\n      \n               ...,\n      \n               [[-1.2031e-02,  1.9229e-03,  3.3153e-03],\n                [ 8.1571e-03, -3.0900e-03,  8.0624e-04],\n                [ 2.4583e-03, -8.5248e-04, -4.1831e-03]],\n      \n               [[-3.9765e-03, -1.4011e-02, -2.7771e-03],\n                [-1.1494e-02,  3.8220e-03, -4.6345e-03],\n                [ 1.2019e-02, -5.0083e-03, -5.4695e-03]],\n      \n               [[-4.3508e-03, -5.0018e-03, -1.0161e-02],\n                [-8.1928e-04, -7.4092e-03, -1.2334e-02],\n                [-1.7778e-03,  1.2708e-02,  8.0382e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[-5.2908e-04,  8.6488e-03,  1.2685e-02],\n                [ 6.1137e-03,  5.3142e-03, -7.2494e-03],\n                [ 9.3563e-03,  1.4256e-02, -4.1414e-03]],\n      \n               [[-7.9798e-03, -3.4570e-03,  1.1289e-02],\n                [ 1.1847e-02, -4.0047e-03, -9.5407e-03],\n                [ 1.4376e-02, -1.4039e-02,  1.3491e-02]],\n      \n               [[-2.2021e-03,  1.2640e-02, -1.0812e-02],\n                [-1.1898e-02,  2.3424e-03,  4.9095e-03],\n                [ 5.7437e-04,  6.4398e-03,  6.6720e-03]],\n      \n               ...,\n      \n               [[-1.3002e-02, -8.0209e-03,  1.4634e-02],\n                [ 1.0879e-02,  8.8946e-03,  4.4633e-03],\n                [ 1.0299e-02, -7.3335e-03, -5.3603e-03]],\n      \n               [[-1.1707e-02, -7.5930e-03, -3.9943e-03],\n                [-3.1151e-03, -8.7829e-03,  9.2917e-03],\n                [-3.2346e-03,  1.8765e-03, -9.8202e-03]],\n      \n               [[ 6.1978e-03, -9.9194e-03,  1.2711e-02],\n                [-1.1393e-02, -3.8471e-03,  7.1972e-03],\n                [-7.1932e-03,  1.2344e-02,  8.8490e-04]]],\n      \n      \n              [[[-7.6542e-04,  4.8747e-03, -6.5180e-04],\n                [ 1.2425e-02, -1.3054e-02,  8.5682e-03],\n                [-8.9012e-03, -2.2868e-03, -1.4388e-02]],\n      \n               [[ 3.0277e-03, -7.9776e-03, -8.2029e-03],\n                [ 7.1433e-03, -7.3085e-04, -1.2132e-02],\n                [ 1.4586e-02, -2.9444e-03, -8.6410e-03]],\n      \n               [[ 6.7710e-03,  4.8661e-03,  6.4931e-03],\n                [ 1.3363e-02, -7.4424e-03,  7.3396e-03],\n                [ 3.2324e-03, -2.9843e-03, -5.1823e-03]],\n      \n               ...,\n      \n               [[-8.9653e-03,  1.2560e-02, -4.7324e-03],\n                [-8.5506e-03,  1.5163e-03,  9.5008e-03],\n                [-7.3975e-03,  1.0480e-02, -2.2851e-03]],\n      \n               [[-6.7266e-03,  1.3981e-02, -5.5892e-03],\n                [ 1.0499e-02, -6.2071e-03, -7.3076e-03],\n                [ 6.9708e-03,  7.5622e-03,  4.0399e-03]],\n      \n               [[ 9.2245e-03,  9.8833e-03, -8.8761e-03],\n                [-8.9584e-03,  2.7126e-03, -4.0407e-03],\n                [-1.2470e-02,  7.4288e-03, -1.3473e-02]]],\n      \n      \n              [[[-9.5815e-03, -1.3544e-02, -7.0852e-05],\n                [ 3.4936e-04, -1.4452e-02, -1.3293e-02],\n                [-1.1974e-02, -1.1519e-02,  1.4221e-02]],\n      \n               [[-9.1489e-05, -7.1288e-03,  6.9324e-03],\n                [-1.5051e-03,  2.2256e-03,  9.6348e-03],\n                [-8.1261e-03,  9.3475e-03, -4.5275e-03]],\n      \n               [[-7.9920e-03, -1.1488e-02, -6.2508e-03],\n                [ 2.4130e-03, -1.1587e-02,  8.1508e-03],\n                [ 2.8083e-03, -1.3849e-02,  1.9280e-05]],\n      \n               ...,\n      \n               [[-6.3032e-03, -2.1902e-03, -3.9196e-03],\n                [-7.5923e-04, -5.2011e-03, -8.6373e-03],\n                [-4.3584e-03,  1.3302e-02, -1.2792e-02]],\n      \n               [[ 1.2617e-02, -1.3331e-02,  5.4059e-03],\n                [ 8.6356e-03, -1.4508e-02,  1.0709e-02],\n                [-1.6577e-03, -1.2650e-02,  4.5443e-03]],\n      \n               [[-3.2095e-03, -4.3988e-03,  8.9948e-03],\n                [-9.6500e-03,  4.6797e-03, -7.2535e-04],\n                [-4.3140e-03,  3.3124e-03, -1.1857e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-1.9555e-02,  9.5140e-03,  1.4060e-02, -4.2434e-03,  1.2344e-02,\n               1.3060e-02, -4.8218e-03,  6.1634e-03,  1.2219e-03,  1.6540e-03,\n               1.3605e-02,  3.1466e-03,  1.7853e-03, -7.2073e-03,  7.4409e-03,\n              -1.0410e-02, -7.7347e-03,  5.4739e-03, -1.0875e-02, -6.2272e-03,\n              -4.0827e-03, -1.6449e-02, -7.2777e-04,  3.8574e-04,  2.4956e-03,\n              -4.9886e-04,  2.4042e-03, -6.6733e-05, -8.7788e-03,  3.1097e-03,\n              -6.4865e-03,  2.2657e-03,  1.8516e-03,  5.8235e-03, -1.2028e-02,\n               1.1674e-03, -2.6845e-03,  1.1146e-02,  1.2192e-02,  1.2937e-03,\n               6.5095e-03,  7.5350e-03, -5.7468e-03,  8.6774e-03,  2.3877e-03,\n              -3.5957e-03, -5.3903e-03, -5.2032e-03,  7.6311e-04, -4.9245e-04,\n               2.2557e-03,  2.4269e-04,  7.9678e-03, -1.2427e-03,  1.1319e-02,\n               2.7465e-03, -4.7864e-03, -8.0940e-03, -8.9930e-03, -5.0893e-03,\n              -4.0365e-03,  8.8414e-03,  1.1477e-02, -2.3560e-03,  5.3937e-03,\n              -6.9675e-03,  4.9124e-03,  5.6216e-03,  7.2078e-03, -1.3970e-02,\n              -2.6616e-04,  7.8439e-03, -1.1710e-03,  6.0900e-03,  3.8095e-03,\n               6.3263e-03, -3.7102e-03, -8.5978e-04, -2.7999e-03, -3.0701e-03,\n               7.3603e-03,  1.7685e-03, -5.4230e-03,  1.4413e-02, -1.0395e-02,\n              -1.5630e-03,  1.2826e-02, -2.8195e-04, -7.5715e-03,  2.8938e-03,\n               3.5001e-03, -5.5977e-03,  4.9162e-03, -2.5712e-03, -2.7571e-03,\n               9.7947e-03, -1.1713e-02, -8.9407e-04, -5.6727e-03,  4.0260e-03,\n              -5.5282e-03, -4.1521e-03, -5.4250e-03, -4.6111e-03,  7.2563e-03,\n              -7.1521e-03, -6.9561e-03, -8.5746e-03,  2.2460e-03, -8.8910e-03,\n               3.5133e-05,  5.9170e-03,  3.7771e-03,  8.7945e-03, -1.8470e-02,\n              -3.2764e-03, -5.6256e-03,  8.0044e-03,  7.2985e-03,  9.4090e-04,\n              -9.4681e-04,  1.5970e-02,  2.2936e-03,  4.7869e-03, -6.4478e-03,\n              -4.3163e-03, -2.2380e-02,  3.7047e-03, -3.8752e-03,  6.3647e-03,\n               2.6577e-03, -5.5088e-03, -2.3384e-03, -1.1246e-02, -7.0884e-03,\n               1.1686e-02,  1.0189e-03, -3.5361e-03, -1.3204e-02,  4.1820e-03,\n               7.3621e-03,  1.4182e-02, -1.0236e-02, -3.1947e-04,  1.5727e-02,\n               8.4657e-03,  2.3460e-03, -3.6722e-03,  2.5296e-03, -2.4234e-03,\n              -7.0418e-04, -3.2940e-03,  3.8262e-03,  3.0238e-03, -6.0729e-03,\n               1.7733e-03,  8.4833e-03,  3.8674e-03,  3.8588e-04, -1.7356e-02,\n               4.4652e-03, -6.2550e-03,  1.5806e-02,  9.2198e-03, -5.2881e-03,\n               1.0174e-02,  4.1711e-03,  1.4912e-02,  2.4108e-03,  5.0053e-04,\n              -1.1995e-03, -3.5921e-03, -3.0288e-03, -6.9627e-03, -5.7988e-03,\n              -6.4565e-03, -6.6065e-03,  1.2484e-02, -1.4810e-02,  1.2041e-02,\n              -6.8603e-03, -9.3181e-03, -3.5160e-03,  2.6007e-03, -7.0954e-05,\n              -2.2771e-03, -9.2256e-03, -4.8020e-04,  3.5150e-03, -1.2364e-03,\n              -1.5149e-03, -3.8297e-03,  1.1784e-03, -3.9328e-03, -3.6986e-03,\n              -1.2682e-03, -9.3710e-04, -9.3385e-03,  1.4152e-03, -7.2997e-03,\n              -6.6435e-03,  5.2826e-03,  1.2935e-03,  1.5631e-02, -3.6081e-03,\n              -1.3597e-02, -7.7965e-03, -1.2913e-03,  1.5910e-03,  2.1204e-03,\n               2.3708e-03, -3.2146e-03,  1.7298e-03,  1.4689e-02,  6.2278e-03,\n               2.3646e-03,  7.7470e-03, -1.5963e-02, -1.7928e-02, -6.2626e-03,\n              -1.7316e-03, -1.1318e-02,  3.9949e-03, -1.2936e-02, -8.6654e-03,\n               2.2968e-03,  6.8509e-03, -7.6822e-04,  6.8589e-03,  1.6284e-02,\n               5.8340e-03,  5.6902e-03, -7.8542e-03, -1.3621e-03,  1.1001e-03,\n              -1.1800e-02,  4.2806e-03, -5.5484e-03,  2.4131e-03,  4.8674e-03,\n               1.2936e-02,  2.3744e-04, -7.5628e-03,  2.1988e-03, -4.7332e-04,\n              -1.8751e-03, -5.1744e-05, -2.7381e-03,  7.4405e-04,  6.3430e-03,\n              -6.1091e-03,  5.1487e-03, -8.1208e-03,  9.7824e-03,  4.0702e-03,\n               1.2473e-03, -9.2013e-03,  1.1501e-02, -1.2944e-03, -6.6771e-03,\n              -1.1369e-03, -9.0035e-04, -6.9081e-03,  4.0511e-03,  1.1781e-02,\n              -8.3753e-03, -2.6621e-03, -1.0303e-02,  2.1987e-03,  4.1600e-03,\n               9.0051e-03, -1.6510e-02, -8.7672e-03, -4.1685e-03,  2.2014e-03,\n              -1.1094e-03,  5.2548e-04, -1.0396e-03, -8.5964e-03,  6.4472e-03,\n              -2.8092e-04, -6.2180e-03, -5.1910e-03,  3.1180e-03,  3.4800e-03,\n              -4.2481e-03,  1.2049e-03, -4.2984e-03,  5.2060e-03,  6.1778e-03,\n              -8.0954e-03, -2.6833e-03,  2.5138e-03, -7.4967e-03,  9.3085e-03,\n               7.5402e-03, -2.5089e-03, -5.2558e-03,  2.5495e-03, -4.6824e-03,\n              -7.1215e-03, -5.3423e-04, -8.5651e-03, -2.4796e-03, -2.8090e-03,\n              -2.5654e-03,  2.3165e-03,  1.2684e-02, -5.9943e-04, -4.7333e-03,\n               5.3007e-03,  9.8639e-03, -7.6258e-03,  7.0917e-03, -5.2445e-03,\n              -2.5397e-05, -4.3013e-03,  2.7515e-03, -4.3067e-03,  3.8332e-03,\n              -9.0865e-03,  6.3336e-03, -5.2501e-03,  7.2468e-06,  2.8435e-03,\n               2.6524e-03, -7.1233e-03, -2.7992e-04, -1.0044e-02, -7.7795e-03,\n               1.0071e-02,  5.7622e-03, -3.6454e-03,  2.0622e-03, -4.5723e-03,\n              -4.0759e-03, -4.5732e-03,  2.3596e-02,  6.1042e-03,  1.2398e-02,\n              -1.6246e-03,  8.9973e-03,  3.0569e-03, -6.5615e-03, -5.4251e-03,\n               1.0231e-04, -6.5242e-03,  1.2511e-02,  8.0881e-03,  1.6083e-03,\n               1.3940e-03,  4.8660e-03, -3.7963e-03, -1.9512e-03, -4.7039e-03,\n              -1.2053e-02, -4.2411e-03,  3.2629e-03,  6.5449e-03, -7.0331e-03,\n              -9.7353e-04, -4.7633e-03,  3.2147e-03,  1.1423e-02,  3.5083e-03,\n              -4.9477e-03, -1.0271e-02, -8.6162e-03,  1.1601e-03,  2.5463e-04,\n              -8.4930e-03,  8.6188e-03, -1.6715e-02, -1.2361e-02,  2.2284e-03,\n               4.9618e-03, -2.5573e-04,  3.6987e-03, -7.9217e-03, -5.0069e-03,\n               5.1266e-03, -3.3821e-04,  8.9940e-03,  8.8820e-03,  5.7026e-03,\n              -7.7041e-03, -9.6212e-03, -2.9203e-03, -4.0963e-03, -6.3333e-03,\n               2.5614e-03,  1.0214e-02, -3.1731e-03, -5.8059e-03, -1.1450e-02,\n              -5.5307e-03, -1.9014e-02, -6.5127e-03,  4.7441e-04,  1.4204e-02,\n              -1.6293e-03,  1.4475e-02, -5.0496e-03, -7.4184e-04, -4.3254e-03,\n               5.5840e-03, -7.7894e-03,  1.2711e-02, -3.3162e-03,  8.4809e-03,\n              -9.4950e-03,  7.5318e-04,  3.1080e-03,  1.0182e-02,  3.6798e-03,\n              -1.2113e-02,  3.1113e-03,  1.0321e-03,  2.3748e-04, -3.8096e-03,\n               9.7446e-03, -4.8978e-03,  1.6510e-04, -3.8563e-03,  9.6745e-04,\n               1.7207e-03, -6.1344e-05, -1.6290e-03,  3.6024e-03,  3.1879e-03,\n              -1.6347e-03, -3.9500e-03, -2.8976e-03,  1.1450e-02,  3.8375e-03,\n               9.0655e-03, -2.9642e-03,  1.3644e-02,  1.5586e-03, -5.3392e-03,\n              -1.3030e-02,  9.9834e-03,  6.5653e-03, -6.8618e-03, -9.6382e-04,\n               7.3473e-03,  4.7903e-03,  9.5512e-04,  7.2254e-03,  3.3645e-03,\n              -5.3391e-03,  3.8704e-03, -1.4120e-02,  4.8242e-03, -1.1839e-02,\n               9.0963e-03, -1.1555e-02, -7.0394e-03, -9.5137e-04,  1.6139e-03,\n               1.0382e-02, -1.5663e-03,  5.1533e-03,  1.4672e-03, -1.4869e-02,\n               1.3660e-02,  2.9945e-04, -5.6913e-03,  4.1750e-03, -6.8269e-03,\n              -1.8973e-02,  1.1292e-03,  1.0517e-02, -4.1086e-03,  3.3689e-03,\n               5.4710e-03, -1.5736e-02,  1.6211e-02, -6.5251e-03, -4.0313e-03,\n              -6.5774e-03, -1.3654e-02, -4.4815e-03,  4.8030e-03, -1.4456e-03,\n               4.3095e-03,  1.6684e-02,  1.4556e-02,  1.7832e-02,  3.2993e-03,\n               7.8251e-03,  3.9322e-03, -7.9063e-03, -1.3585e-02, -6.4331e-04,\n               1.8252e-03, -1.3129e-02, -1.8056e-03,  1.1522e-02,  8.4341e-03,\n              -4.6554e-03, -1.6562e-03,  2.3727e-02, -2.7227e-03,  3.9545e-03,\n              -1.5101e-03, -8.9222e-03,  8.5481e-03, -1.7576e-02,  4.8458e-03,\n              -7.2421e-03,  7.2484e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9016, 0.9010, 0.9011, 0.9011, 0.9013, 0.9015, 0.9017, 0.9011, 0.9013,\n              0.9012, 0.9015, 0.9013, 0.9014, 0.9010, 0.9010, 0.9012, 0.9011, 0.9014,\n              0.9014, 0.9011, 0.9013, 0.9016, 0.9021, 0.9009, 0.9013, 0.9013, 0.9015,\n              0.9013, 0.9012, 0.9011, 0.9016, 0.9012, 0.9011, 0.9012, 0.9017, 0.9014,\n              0.9015, 0.9014, 0.9013, 0.9012, 0.9014, 0.9013, 0.9011, 0.9015, 0.9016,\n              0.9013, 0.9010, 0.9012, 0.9011, 0.9012, 0.9012, 0.9012, 0.9013, 0.9014,\n              0.9012, 0.9010, 0.9012, 0.9016, 0.9012, 0.9012, 0.9012, 0.9014, 0.9011,\n              0.9017, 0.9013, 0.9010, 0.9012, 0.9011, 0.9013, 0.9013, 0.9012, 0.9015,\n              0.9011, 0.9012, 0.9014, 0.9010, 0.9014, 0.9010, 0.9010, 0.9010, 0.9011,\n              0.9012, 0.9015, 0.9017, 0.9010, 0.9017, 0.9012, 0.9011, 0.9010, 0.9014,\n              0.9013, 0.9013, 0.9013, 0.9014, 0.9014, 0.9015, 0.9014, 0.9011, 0.9016,\n              0.9012, 0.9013, 0.9014, 0.9010, 0.9012, 0.9015, 0.9013, 0.9018, 0.9010,\n              0.9014, 0.9012, 0.9012, 0.9014, 0.9012, 0.9012, 0.9015, 0.9010, 0.9015,\n              0.9013, 0.9011, 0.9012, 0.9011, 0.9013, 0.9012, 0.9018, 0.9014, 0.9013,\n              0.9020, 0.9009, 0.9010, 0.9013, 0.9013, 0.9011, 0.9016, 0.9013, 0.9012,\n              0.9012, 0.9010, 0.9018, 0.9010, 0.9012, 0.9010, 0.9014, 0.9015, 0.9016,\n              0.9016, 0.9012, 0.9011, 0.9014, 0.9013, 0.9012, 0.9012, 0.9011, 0.9011,\n              0.9011, 0.9010, 0.9012, 0.9012, 0.9010, 0.9014, 0.9011, 0.9013, 0.9015,\n              0.9013, 0.9011, 0.9012, 0.9013, 0.9012, 0.9015, 0.9017, 0.9015, 0.9012,\n              0.9014, 0.9015, 0.9014, 0.9011, 0.9012, 0.9010, 0.9013, 0.9011, 0.9013,\n              0.9013, 0.9014, 0.9017, 0.9015, 0.9010, 0.9012, 0.9013, 0.9013, 0.9011,\n              0.9012, 0.9014, 0.9011, 0.9013, 0.9012, 0.9010, 0.9013, 0.9011, 0.9012,\n              0.9011, 0.9016, 0.9021, 0.9010, 0.9009, 0.9011, 0.9013, 0.9015, 0.9014,\n              0.9014, 0.9013, 0.9012, 0.9012, 0.9010, 0.9014, 0.9014, 0.9012, 0.9014,\n              0.9016, 0.9012, 0.9014, 0.9016, 0.9016, 0.9012, 0.9015, 0.9013, 0.9011,\n              0.9011, 0.9015, 0.9012, 0.9014, 0.9012, 0.9012, 0.9012, 0.9013, 0.9017,\n              0.9012, 0.9014, 0.9011, 0.9012, 0.9009, 0.9011, 0.9013, 0.9011, 0.9012,\n              0.9012, 0.9012, 0.9016, 0.9013, 0.9010, 0.9012, 0.9012, 0.9011, 0.9009,\n              0.9024, 0.9014, 0.9015, 0.9014, 0.9012, 0.9017, 0.9012, 0.9013, 0.9010,\n              0.9010, 0.9012, 0.9015, 0.9012, 0.9013, 0.9013, 0.9014, 0.9014, 0.9013,\n              0.9017, 0.9017, 0.9014, 0.9011, 0.9012, 0.9015, 0.9010, 0.9010, 0.9012,\n              0.9011, 0.9013, 0.9014, 0.9015, 0.9012, 0.9019, 0.9011, 0.9011, 0.9010,\n              0.9011, 0.9014, 0.9010, 0.9011, 0.9011, 0.9012, 0.9011, 0.9012, 0.9011,\n              0.9016, 0.9011, 0.9011, 0.9013, 0.9011, 0.9013, 0.9012, 0.9017, 0.9012,\n              0.9013, 0.9013, 0.9012, 0.9011, 0.9012, 0.9013, 0.9013, 0.9014, 0.9013,\n              0.9011, 0.9011, 0.9012, 0.9013, 0.9013, 0.9015, 0.9012, 0.9013, 0.9013,\n              0.9011, 0.9017, 0.9014, 0.9012, 0.9011, 0.9012, 0.9013, 0.9012, 0.9013,\n              0.9012, 0.9012, 0.9016, 0.9010, 0.9016, 0.9013, 0.9013, 0.9013, 0.9010,\n              0.9011, 0.9012, 0.9009, 0.9012, 0.9014, 0.9014, 0.9010, 0.9013, 0.9012,\n              0.9014, 0.9013, 0.9014, 0.9011, 0.9012, 0.9012, 0.9016, 0.9011, 0.9012,\n              0.9013, 0.9010, 0.9012, 0.9011, 0.9015, 0.9016, 0.9013, 0.9011, 0.9011,\n              0.9013, 0.9012, 0.9016, 0.9011, 0.9012, 0.9014, 0.9014, 0.9012, 0.9011,\n              0.9016, 0.9012, 0.9013, 0.9015, 0.9012, 0.9011, 0.9012, 0.9011, 0.9015,\n              0.9013, 0.9011, 0.9014, 0.9014, 0.9015, 0.9013, 0.9012, 0.9015, 0.9011,\n              0.9018, 0.9016, 0.9012, 0.9013, 0.9012, 0.9011, 0.9015, 0.9012, 0.9012,\n              0.9013, 0.9014, 0.9011, 0.9012, 0.9011, 0.9016, 0.9014, 0.9012, 0.9013,\n              0.9011, 0.9010, 0.9013, 0.9014, 0.9011, 0.9011, 0.9017, 0.9012, 0.9013,\n              0.9011, 0.9012, 0.9013, 0.9012, 0.9011, 0.9013, 0.9014, 0.9013, 0.9012,\n              0.9013, 0.9012, 0.9014, 0.9014, 0.9012, 0.9014, 0.9013, 0.9015, 0.9011,\n              0.9012, 0.9013, 0.9014, 0.9012, 0.9020, 0.9012, 0.9012, 0.9010, 0.9012,\n              0.9011, 0.9013, 0.9012, 0.9015, 0.9015, 0.9012, 0.9012, 0.9011, 0.9014,\n              0.9012, 0.9013, 0.9012, 0.9012, 0.9011, 0.9013, 0.9012, 0.9017, 0.9017,\n              0.9014, 0.9015, 0.9011, 0.9014, 0.9014, 0.9012, 0.9013, 0.9012, 0.9012,\n              0.9015, 0.9014, 0.9014, 0.9013, 0.9013, 0.9014, 0.9010, 0.9012, 0.9016,\n              0.9014, 0.9015, 0.9014, 0.9012, 0.9010, 0.9014, 0.9014, 0.9011, 0.9010,\n              0.9016, 0.9020, 0.9017, 0.9010, 0.9012, 0.9017, 0.9012, 0.9020, 0.9012,\n              0.9014, 0.9016, 0.9014, 0.9010, 0.9014, 0.9011, 0.9011, 0.9016],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[ 0.0343]],\n        \n                 [[ 0.0564]],\n        \n                 [[-0.0442]],\n        \n                 ...,\n        \n                 [[-0.0016]],\n        \n                 [[ 0.0463]],\n        \n                 [[-0.0150]]],\n        \n        \n                [[[-0.0592]],\n        \n                 [[-0.0325]],\n        \n                 [[-0.0393]],\n        \n                 ...,\n        \n                 [[ 0.0213]],\n        \n                 [[ 0.0310]],\n        \n                 [[-0.0503]]],\n        \n        \n                [[[ 0.0286]],\n        \n                 [[-0.0512]],\n        \n                 [[-0.0260]],\n        \n                 ...,\n        \n                 [[ 0.0499]],\n        \n                 [[ 0.0109]],\n        \n                 [[-0.0402]]],\n        \n        \n                ...,\n        \n        \n                [[[ 0.0017]],\n        \n                 [[-0.0380]],\n        \n                 [[ 0.0370]],\n        \n                 ...,\n        \n                 [[ 0.0483]],\n        \n                 [[-0.0194]],\n        \n                 [[-0.0236]]],\n        \n        \n                [[[ 0.0443]],\n        \n                 [[ 0.0433]],\n        \n                 [[ 0.0546]],\n        \n                 ...,\n        \n                 [[ 0.0511]],\n        \n                 [[ 0.0059]],\n        \n                 [[ 0.0488]]],\n        \n        \n                [[[ 0.0438]],\n        \n                 [[ 0.0621]],\n        \n                 [[-0.0437]],\n        \n                 ...,\n        \n                 [[-0.0429]],\n        \n                 [[-0.0319]],\n        \n                 [[ 0.0277]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0441, -0.0008, -0.0706,  0.0108,  0.0606, -0.0580,  0.0147, -0.0541,\n                 0.0073,  0.0119,  0.0385, -0.0239,  0.0225,  0.0067, -0.0041, -0.0082,\n                -0.0636, -0.0018,  0.0538, -0.0644, -0.0273,  0.0372,  0.0160, -0.0368,\n                 0.0047, -0.0146,  0.0127,  0.0173, -0.0444, -0.0123,  0.0159, -0.0151,\n                -0.0113,  0.0335, -0.0508,  0.0060, -0.0408, -0.0022, -0.0147,  0.0503,\n                -0.0320, -0.0302, -0.0062,  0.0379,  0.0209, -0.0365,  0.0064,  0.0042,\n                -0.0450,  0.0187, -0.0277,  0.0222, -0.0127,  0.0301,  0.0319,  0.0137,\n                -0.0166, -0.0233, -0.0377,  0.0173,  0.0240, -0.0377,  0.0031, -0.0147,\n                -0.0205, -0.0211,  0.0269, -0.0024,  0.0303, -0.0238, -0.0083,  0.0182,\n                 0.0236, -0.0824,  0.0008,  0.0272,  0.0540, -0.0025, -0.0154,  0.0002,\n                -0.0321, -0.0083,  0.0238, -0.0360,  0.0262, -0.0023,  0.0028, -0.0380,\n                 0.0135,  0.0112, -0.0016,  0.0302, -0.0226, -0.0171,  0.0252, -0.0276,\n                -0.0518, -0.0284,  0.0351,  0.0090, -0.0673, -0.0206, -0.0150,  0.0780,\n                -0.0121, -0.0180, -0.0029,  0.0282,  0.0077,  0.0108,  0.0209,  0.0419,\n                 0.0025,  0.0240,  0.0052,  0.0197, -0.0258, -0.0317, -0.0387,  0.0261,\n                -0.0943, -0.0044,  0.0354,  0.0107, -0.0424,  0.0070,  0.0328,  0.0426,\n                 0.0276, -0.0247, -0.0213, -0.0210,  0.0218,  0.0359,  0.0117, -0.0288,\n                -0.0281, -0.0321,  0.0498, -0.0141, -0.0267,  0.0184,  0.0295, -0.0121,\n                 0.0030, -0.0372,  0.0224,  0.0480, -0.0514,  0.0208,  0.0173,  0.0370,\n                -0.0283,  0.0326, -0.0466,  0.0554, -0.0094, -0.0129, -0.0217, -0.0140,\n                -0.0322, -0.0191,  0.0224,  0.0011, -0.0533,  0.0605,  0.0399,  0.0358,\n                -0.0064,  0.0528, -0.0652, -0.0283,  0.0238, -0.0257, -0.0049, -0.0497,\n                -0.0237,  0.0327, -0.0341,  0.0073, -0.0069,  0.0022,  0.0423, -0.0351,\n                -0.0011,  0.0237, -0.0051,  0.0161, -0.0159, -0.0428,  0.0257, -0.0078,\n                 0.0039, -0.0109,  0.0315, -0.0420, -0.0526, -0.0247,  0.0359, -0.0277,\n                 0.0384, -0.0142, -0.0185, -0.0063, -0.0298,  0.0206, -0.0255,  0.0099,\n                -0.0097,  0.0018,  0.0258,  0.0027, -0.0155,  0.0104, -0.0124,  0.0119,\n                -0.0159,  0.0323,  0.0062,  0.0005,  0.0382,  0.0388, -0.0064, -0.0191,\n                -0.0186,  0.0141, -0.0112,  0.0129, -0.0150, -0.0569, -0.0084,  0.0126,\n                -0.0169,  0.0098,  0.0232,  0.0338,  0.0209,  0.0156, -0.0054,  0.0044,\n                -0.0468,  0.0013,  0.0155,  0.0098, -0.0029, -0.0194,  0.0081,  0.0263,\n                 0.0650, -0.0423, -0.0148,  0.0503, -0.0126,  0.0094,  0.0594, -0.0145,\n                -0.0103,  0.0324,  0.0039,  0.0085,  0.0333, -0.0232, -0.0758,  0.0139,\n                -0.0177, -0.0227, -0.0395,  0.0244,  0.0289,  0.0069,  0.0135,  0.0097,\n                -0.0104,  0.0384,  0.0196,  0.0350, -0.0310,  0.0149, -0.0117,  0.0085,\n                 0.0374, -0.0128,  0.0609, -0.0069,  0.0226,  0.0337, -0.0393,  0.0155,\n                 0.0278,  0.0333, -0.0219,  0.0227,  0.0095,  0.0236,  0.0002, -0.0136,\n                -0.0162, -0.0053,  0.0345,  0.0123,  0.0100,  0.0016, -0.0030, -0.0248,\n                -0.0444,  0.0142, -0.0516, -0.0023,  0.0165,  0.0133, -0.0190, -0.0133,\n                -0.0477,  0.0527, -0.1009, -0.0345, -0.0004, -0.0115,  0.0219, -0.0384,\n                 0.0585, -0.0040,  0.0301, -0.0415,  0.0313,  0.0169, -0.0369,  0.1037,\n                 0.0115,  0.0350,  0.0109, -0.0364, -0.0055, -0.0503,  0.0902, -0.0332,\n                -0.0130, -0.0142,  0.0290, -0.0129,  0.0539, -0.0165,  0.0141,  0.0518,\n                 0.0322,  0.0629, -0.0253, -0.0149,  0.0044,  0.0552, -0.0157, -0.0038,\n                 0.0278,  0.0642,  0.0129, -0.0142,  0.0070, -0.0140, -0.0383, -0.0024,\n                -0.0332,  0.0706, -0.0541, -0.0181,  0.0379, -0.0029, -0.0192,  0.0146,\n                -0.0425, -0.0285,  0.0023,  0.0117,  0.0194, -0.0217,  0.0116,  0.0064,\n                -0.0011, -0.0222,  0.0707, -0.0188,  0.0045,  0.0487,  0.0388, -0.0728,\n                -0.0681,  0.0087, -0.0545, -0.0167, -0.0902,  0.0432, -0.0201,  0.0072,\n                -0.0053,  0.0266,  0.0322,  0.0122,  0.0246,  0.0512, -0.0186, -0.0475,\n                -0.0175, -0.0108, -0.0272, -0.0306,  0.0011, -0.0229,  0.0545, -0.0233,\n                 0.0130, -0.0038,  0.0126, -0.0044, -0.0264, -0.0163, -0.0354,  0.0263,\n                -0.0405,  0.0036,  0.0108, -0.0016, -0.0493, -0.0106,  0.0637,  0.0271,\n                -0.0269,  0.0286,  0.0542,  0.0815, -0.0362, -0.0136,  0.0545, -0.0217,\n                 0.0191,  0.0360, -0.0007,  0.0087, -0.0038, -0.0170, -0.0177, -0.0403,\n                 0.0035,  0.0882,  0.0555, -0.0039, -0.0312,  0.0028, -0.0208,  0.0065,\n                -0.0143, -0.0283,  0.0111,  0.0110,  0.0367,  0.0372,  0.0566,  0.0414,\n                 0.0051,  0.0297, -0.0367,  0.0178, -0.0339, -0.0192, -0.0219, -0.0164,\n                -0.0364, -0.0631,  0.0336, -0.0183,  0.0004, -0.0081,  0.0030, -0.0107,\n                -0.0128,  0.0374, -0.0025, -0.0150, -0.0018,  0.0003,  0.0090,  0.0470,\n                 0.0428,  0.0122,  0.0667,  0.0046,  0.0303, -0.0751,  0.0119, -0.0443,\n                 0.0253,  0.0058, -0.0331, -0.0223, -0.0239, -0.0103,  0.0174,  0.0263,\n                 0.0228, -0.0106,  0.0246, -0.0040,  0.0160,  0.0067, -0.0222, -0.0795,\n                 0.0088,  0.0104, -0.0420, -0.0337, -0.0173, -0.0299,  0.0228,  0.0391],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9199, 0.9163, 0.9155, 0.9129, 0.9125, 0.9134, 0.9171, 0.9243, 0.9151,\n                0.9181, 0.9125, 0.9161, 0.9135, 0.9113, 0.9129, 0.9138, 0.9149, 0.9143,\n                0.9192, 0.9157, 0.9165, 0.9134, 0.9134, 0.9117, 0.9130, 0.9179, 0.9148,\n                0.9145, 0.9240, 0.9202, 0.9141, 0.9144, 0.9184, 0.9125, 0.9165, 0.9129,\n                0.9202, 0.9161, 0.9129, 0.9123, 0.9152, 0.9168, 0.9302, 0.9127, 0.9139,\n                0.9188, 0.9143, 0.9123, 0.9143, 0.9142, 0.9179, 0.9147, 0.9120, 0.9141,\n                0.9135, 0.9144, 0.9110, 0.9178, 0.9209, 0.9140, 0.9282, 0.9141, 0.9148,\n                0.9181, 0.9111, 0.9153, 0.9167, 0.9175, 0.9145, 0.9127, 0.9150, 0.9146,\n                0.9114, 0.9165, 0.9146, 0.9177, 0.9210, 0.9129, 0.9179, 0.9168, 0.9141,\n                0.9189, 0.9150, 0.9146, 0.9180, 0.9139, 0.9140, 0.9124, 0.9125, 0.9139,\n                0.9152, 0.9156, 0.9132, 0.9176, 0.9164, 0.9114, 0.9254, 0.9173, 0.9129,\n                0.9141, 0.9203, 0.9158, 0.9119, 0.9161, 0.9187, 0.9260, 0.9142, 0.9127,\n                0.9113, 0.9190, 0.9160, 0.9118, 0.9162, 0.9148, 0.9147, 0.9133, 0.9127,\n                0.9123, 0.9152, 0.9141, 0.9205, 0.9093, 0.9157, 0.9134, 0.9131, 0.9166,\n                0.9208, 0.9160, 0.9172, 0.9155, 0.9156, 0.9290, 0.9134, 0.9126, 0.9120,\n                0.9135, 0.9131, 0.9110, 0.9162, 0.9123, 0.9140, 0.9141, 0.9146, 0.9168,\n                0.9164, 0.9145, 0.9148, 0.9143, 0.9177, 0.9137, 0.9131, 0.9153, 0.9105,\n                0.9153, 0.9182, 0.9134, 0.9137, 0.9124, 0.9126, 0.9132, 0.9194, 0.9149,\n                0.9173, 0.9146, 0.9135, 0.9130, 0.9155, 0.9182, 0.9142, 0.9218, 0.9144,\n                0.9147, 0.9130, 0.9251, 0.9138, 0.9140, 0.9155, 0.9182, 0.9188, 0.9160,\n                0.9189, 0.9156, 0.9244, 0.9157, 0.9150, 0.9130, 0.9149, 0.9218, 0.9140,\n                0.9134, 0.9139, 0.9142, 0.9142, 0.9188, 0.9172, 0.9149, 0.9165, 0.9121,\n                0.9173, 0.9157, 0.9112, 0.9144, 0.9148, 0.9129, 0.9160, 0.9146, 0.9154,\n                0.9142, 0.9131, 0.9128, 0.9124, 0.9180, 0.9125, 0.9163, 0.9201, 0.9115,\n                0.9164, 0.9208, 0.9149, 0.9173, 0.9132, 0.9140, 0.9142, 0.9217, 0.9130,\n                0.9126, 0.9218, 0.9113, 0.9141, 0.9159, 0.9130, 0.9162, 0.9129, 0.9197,\n                0.9102, 0.9149, 0.9183, 0.9186, 0.9208, 0.9137, 0.9119, 0.9143, 0.9194,\n                0.9154, 0.9146, 0.9137, 0.9221, 0.9111, 0.9136, 0.9136, 0.9116, 0.9144,\n                0.9217, 0.9139, 0.9101, 0.9131, 0.9228, 0.9141, 0.9126, 0.9173, 0.9163,\n                0.9124, 0.9166, 0.9128, 0.9122, 0.9131, 0.9122, 0.9125, 0.9194, 0.9123,\n                0.9126, 0.9145, 0.9136, 0.9155, 0.9232, 0.9144, 0.9163, 0.9137, 0.9111,\n                0.9122, 0.9104, 0.9168, 0.9150, 0.9204, 0.9184, 0.9131, 0.9127, 0.9129,\n                0.9147, 0.9145, 0.9169, 0.9208, 0.9186, 0.9145, 0.9169, 0.9184, 0.9168,\n                0.9142, 0.9122, 0.9189, 0.9133, 0.9144, 0.9189, 0.9167, 0.9134, 0.9116,\n                0.9158, 0.9132, 0.9152, 0.9204, 0.9184, 0.9157, 0.9142, 0.9182, 0.9207,\n                0.9137, 0.9147, 0.9148, 0.9176, 0.9139, 0.9183, 0.9153, 0.9127, 0.9227,\n                0.9153, 0.9154, 0.9136, 0.9147, 0.9125, 0.9160, 0.9147, 0.9174, 0.9144,\n                0.9140, 0.9222, 0.9152, 0.9164, 0.9215, 0.9141, 0.9152, 0.9241, 0.9170,\n                0.9196, 0.9160, 0.9156, 0.9133, 0.9181, 0.9117, 0.9170, 0.9131, 0.9179,\n                0.9108, 0.9144, 0.9148, 0.9172, 0.9140, 0.9155, 0.9180, 0.9146, 0.9125,\n                0.9154, 0.9157, 0.9195, 0.9154, 0.9188, 0.9115, 0.9135, 0.9139, 0.9176,\n                0.9167, 0.9155, 0.9256, 0.9163, 0.9123, 0.9144, 0.9140, 0.9161, 0.9124,\n                0.9146, 0.9229, 0.9144, 0.9182, 0.9158, 0.9140, 0.9216, 0.9182, 0.9206,\n                0.9130, 0.9156, 0.9160, 0.9138, 0.9149, 0.9154, 0.9151, 0.9218, 0.9150,\n                0.9148, 0.9149, 0.9116, 0.9127, 0.9138, 0.9109, 0.9160, 0.9163, 0.9140,\n                0.9193, 0.9166, 0.9157, 0.9119, 0.9155, 0.9170, 0.9129, 0.9109, 0.9159,\n                0.9210, 0.9254, 0.9124, 0.9134, 0.9142, 0.9148, 0.9203, 0.9130, 0.9163,\n                0.9159, 0.9177, 0.9120, 0.9178, 0.9138, 0.9123, 0.9194, 0.9141, 0.9141,\n                0.9196, 0.9129, 0.9142, 0.9174, 0.9138, 0.9106, 0.9145, 0.9139, 0.9119,\n                0.9201, 0.9151, 0.9131, 0.9135, 0.9119, 0.9254, 0.9144, 0.9113, 0.9210,\n                0.9153, 0.9148, 0.9140, 0.9183, 0.9148, 0.9167, 0.9142, 0.9138, 0.9222,\n                0.9135, 0.9108, 0.9207, 0.9214, 0.9202, 0.9147, 0.9216, 0.9154, 0.9165,\n                0.9120, 0.9144, 0.9129, 0.9133, 0.9141, 0.9133, 0.9214, 0.9128, 0.9127,\n                0.9142, 0.9153, 0.9117, 0.9179, 0.9163, 0.9155, 0.9139, 0.9159, 0.9172,\n                0.9118, 0.9163, 0.9107, 0.9118, 0.9194, 0.9167, 0.9125, 0.9139, 0.9158,\n                0.9264, 0.9146, 0.9120, 0.9118, 0.9166, 0.9148, 0.9286, 0.9118, 0.9221,\n                0.9135, 0.9135, 0.9157, 0.9137, 0.9163, 0.9178, 0.9140, 0.9132],\n               grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [512, 256, 3, 3]], ["0.bn1.weight", [512]], ["0.bn1.bias", [512]], ["0.conv2.weight", [512, 512, 3, 3]], ["0.bn2.weight", [512]], ["0.bn2.bias", [512]], ["0.downsample.0.weight", [512, 256, 1, 1]], ["0.downsample.1.weight", [512]], ["0.downsample.1.bias", [512]]], "output_shape": [[512, 512, 1, 1]], "num_parameters": [1179648, 512, 512, 2359296, 512, 512, 131072, 512, 512]}, {"name": "avgpool", "id": 140542746893904, "class_name": "AveragePool()", "parameters": [], "output_shape": [[512, 512]], "num_parameters": []}, {"name": "fc", "id": 140542746893808, "class_name": "Linear(in_features=512, out_features=10, bias=True)", "parameters": [["weight", [10, 512]], ["bias", [10]]], "output_shape": [[512, 10]], "num_parameters": [5120, 10]}], "edges": []}