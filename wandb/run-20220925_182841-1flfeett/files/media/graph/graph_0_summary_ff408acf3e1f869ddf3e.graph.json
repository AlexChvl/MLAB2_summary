{"format": "torch", "nodes": [{"name": "conv1", "id": 140608652877680, "class_name": "Conv2d(\n  self.stride=2, self.padding=(3, 3), self.weight=Parameter containing:\n  tensor([[[[-3.9383e-02,  6.0194e-02, -6.9658e-02,  ...,  1.7203e-02,\n             -8.1303e-02,  7.6832e-02],\n            [ 4.8625e-02, -5.5590e-02,  5.7403e-02,  ..., -3.7302e-02,\n             -3.5579e-02, -4.4801e-02],\n            [ 3.6267e-02, -1.5606e-02, -3.4572e-02,  ...,  6.8919e-02,\n             -5.3377e-02,  6.5395e-02],\n            ...,\n            [-5.3388e-02,  4.2051e-03, -1.6075e-02,  ..., -4.6749e-02,\n              8.1847e-02, -5.8579e-02],\n            [-6.3730e-02,  4.7471e-02,  4.9068e-02,  ...,  5.1095e-02,\n              5.5817e-02,  7.4338e-02],\n            [ 8.2093e-02,  4.3803e-03, -3.9094e-02,  ...,  4.3404e-03,\n             -7.1513e-02, -1.5433e-02]],\n  \n           [[-6.3789e-02,  1.3239e-02,  4.0807e-02,  ...,  1.4715e-02,\n             -4.9916e-02,  5.8346e-02],\n            [ 2.0018e-02,  7.2259e-02, -2.3215e-02,  ...,  7.4834e-02,\n             -4.4783e-03, -6.5851e-02],\n            [ 9.7055e-03, -1.4107e-02, -1.6209e-03,  ...,  4.2025e-02,\n              1.5504e-02,  1.2700e-03],\n            ...,\n            [ 4.5421e-02, -7.6783e-02, -7.6552e-02,  ...,  2.9584e-02,\n              2.0248e-02,  2.2056e-02],\n            [ 6.0837e-02, -3.5816e-02,  4.7938e-02,  ...,  4.6829e-02,\n              3.3866e-02,  6.9660e-02],\n            [-7.4680e-02, -2.6994e-02,  1.4353e-02,  ...,  4.0683e-02,\n             -5.7970e-02, -9.9576e-03]],\n  \n           [[-5.1489e-02,  3.7327e-03,  4.3040e-02,  ..., -3.1493e-03,\n              8.2265e-02,  2.9172e-02],\n            [-6.2963e-02,  3.3139e-02,  1.7391e-02,  ...,  1.5841e-03,\n             -3.6239e-02, -2.1644e-02],\n            [-7.4545e-02, -4.9661e-02, -8.7418e-05,  ..., -7.8969e-02,\n             -5.2675e-02,  1.6473e-02],\n            ...,\n            [-7.1127e-02,  5.7344e-02, -8.0417e-02,  ...,  5.7878e-02,\n             -5.0858e-02,  2.9037e-02],\n            [-5.1283e-02,  4.9624e-02,  2.4409e-02,  ..., -5.1349e-02,\n             -7.8276e-02, -7.1786e-02],\n            [ 3.7835e-02,  4.9491e-02,  5.5482e-02,  ...,  5.0524e-02,\n              3.5544e-02, -1.8402e-02]]],\n  \n  \n          [[[ 5.5140e-02, -6.6046e-02,  5.9962e-02,  ..., -7.4707e-02,\n             -6.0514e-02, -3.4038e-02],\n            [ 9.2525e-03, -7.8873e-02, -4.9392e-02,  ..., -6.2389e-02,\n              6.7295e-02, -3.1279e-02],\n            [-1.8733e-02, -5.7210e-03, -5.2228e-02,  ..., -6.9436e-03,\n              6.9239e-02, -2.3030e-02],\n            ...,\n            [-5.0516e-03, -4.4417e-02,  2.7449e-03,  ..., -4.1594e-02,\n             -4.9585e-02, -7.7680e-02],\n            [ 7.3443e-03, -4.2000e-02,  1.4367e-02,  ...,  3.9977e-03,\n              6.9175e-03,  6.0139e-02],\n            [ 1.7210e-03,  4.3503e-02, -1.6132e-02,  ..., -5.2426e-02,\n             -6.2092e-02, -4.5069e-02]],\n  \n           [[-3.0338e-02, -5.2898e-02, -4.0221e-02,  ...,  6.4619e-02,\n              8.3022e-03,  5.7145e-02],\n            [-3.0893e-02, -2.4691e-02,  6.3845e-02,  ...,  5.0499e-02,\n             -8.2172e-02, -7.1667e-02],\n            [ 5.6040e-02,  6.4423e-02, -5.1645e-02,  ..., -4.6215e-02,\n              7.4923e-02,  2.4332e-03],\n            ...,\n            [ 6.2580e-02,  5.8361e-02, -2.5269e-03,  ..., -5.3717e-02,\n              5.7246e-02,  1.2251e-02],\n            [ 5.1223e-02,  1.8236e-02, -6.9487e-02,  ...,  5.2097e-02,\n             -2.1907e-02, -5.6921e-03],\n            [-4.4923e-02, -6.7427e-02, -5.5274e-03,  ..., -4.3151e-02,\n              4.7913e-02, -8.2297e-02]],\n  \n           [[-7.5762e-02, -4.0601e-02,  1.6002e-02,  ...,  1.5476e-02,\n              7.0265e-02,  4.7579e-02],\n            [ 2.3857e-03,  5.9119e-02, -9.9918e-03,  ...,  2.9785e-02,\n              7.9536e-02,  5.4593e-02],\n            [ 3.0053e-02, -2.5237e-02, -6.0527e-02,  ..., -5.2991e-02,\n              2.3657e-02, -5.0093e-02],\n            ...,\n            [-4.7916e-03, -3.6831e-02,  6.2195e-02,  ...,  2.4536e-03,\n              6.3232e-03, -6.3070e-02],\n            [ 3.0874e-02,  7.9634e-02,  5.7056e-02,  ..., -3.3948e-02,\n             -5.5516e-02, -6.8242e-02],\n            [ 4.0648e-02, -2.7732e-02, -7.1524e-02,  ...,  4.5707e-02,\n              3.1261e-02,  3.5960e-02]]],\n  \n  \n          [[[ 3.6655e-02,  7.6169e-02,  5.1036e-02,  ...,  2.7942e-02,\n             -2.6333e-02, -6.3914e-02],\n            [-1.0563e-02, -2.5105e-02,  5.4996e-02,  ..., -1.5504e-02,\n              3.0257e-02,  2.2486e-02],\n            [ 3.8293e-02,  7.3864e-02, -6.5607e-02,  ...,  2.3034e-02,\n              7.6120e-02,  3.7461e-03],\n            ...,\n            [ 2.7812e-02, -6.6131e-02, -7.3156e-02,  ..., -4.8138e-02,\n              3.1955e-02, -8.4140e-04],\n            [-2.3430e-03, -2.9187e-02, -8.5346e-03,  ..., -2.5035e-02,\n              6.3342e-03, -8.1732e-02],\n            [ 3.4469e-02, -5.1253e-02,  8.0034e-02,  ..., -4.7588e-02,\n              6.0245e-02, -3.7968e-02]],\n  \n           [[-5.9297e-02, -1.6154e-02, -3.9502e-02,  ...,  6.4337e-02,\n             -6.5025e-02,  7.0364e-02],\n            [-7.6721e-03, -8.2288e-02, -8.4173e-03,  ..., -3.3305e-02,\n             -4.1643e-02,  1.0713e-02],\n            [-7.7512e-02, -4.4260e-02,  3.3290e-03,  ...,  5.0297e-02,\n             -3.7849e-02, -4.2536e-02],\n            ...,\n            [-4.9886e-02,  4.4543e-02, -2.9432e-02,  ...,  8.0900e-02,\n             -2.3750e-02,  7.3915e-02],\n            [ 2.6789e-02,  6.3552e-03,  4.5854e-02,  ..., -4.7548e-02,\n              3.9618e-02,  4.0790e-02],\n            [ 3.1780e-02, -4.1783e-02,  6.1600e-02,  ...,  3.5162e-02,\n             -6.8350e-02,  2.3400e-02]],\n  \n           [[ 7.4146e-03,  7.7907e-02, -7.8219e-02,  ..., -4.3347e-02,\n              8.9734e-03, -2.2204e-02],\n            [-6.1681e-02, -3.3154e-02,  4.1849e-02,  ..., -3.8403e-03,\n              6.4441e-02, -2.3331e-02],\n            [-5.0157e-02,  5.3112e-02, -3.9902e-02,  ...,  3.9775e-02,\n              7.5671e-02,  5.7261e-02],\n            ...,\n            [ 2.8821e-02, -2.4014e-02,  2.4524e-02,  ..., -6.0284e-02,\n              5.4155e-02, -2.9362e-02],\n            [ 1.0350e-02, -1.6924e-02,  1.7553e-02,  ...,  6.8434e-02,\n              3.7675e-02,  1.0827e-02],\n            [-2.3568e-03, -4.3532e-02, -6.4175e-02,  ..., -5.3166e-02,\n              6.4482e-02,  3.6757e-02]]],\n  \n  \n          ...,\n  \n  \n          [[[ 4.2926e-02, -4.2321e-02,  2.6299e-02,  ...,  5.1766e-02,\n             -6.6445e-02, -8.0489e-02],\n            [ 5.8849e-02,  4.1989e-02,  5.5526e-02,  ...,  9.8335e-03,\n             -6.5748e-02, -7.0490e-02],\n            [ 2.4306e-02, -7.2318e-02,  7.4165e-02,  ...,  6.9327e-02,\n             -4.7239e-02,  5.1344e-03],\n            ...,\n            [ 3.8139e-03, -3.5190e-02, -3.3148e-02,  ...,  1.6962e-02,\n             -2.1136e-02, -6.8991e-02],\n            [-3.4337e-02,  7.0106e-02, -3.2008e-02,  ...,  2.4148e-02,\n             -4.5585e-02,  5.8768e-02],\n            [-1.7816e-02,  1.4555e-02,  3.6533e-02,  ...,  5.7944e-02,\n              6.7620e-02, -4.3197e-02]],\n  \n           [[ 4.4827e-02, -4.5568e-03,  4.6350e-02,  ..., -6.8939e-02,\n              3.2979e-02, -5.4939e-02],\n            [-2.6659e-02, -6.7420e-02, -5.9712e-02,  ..., -5.5531e-02,\n             -4.1722e-02, -6.0234e-02],\n            [ 8.0279e-02, -6.3369e-02,  4.0261e-02,  ...,  5.2131e-02,\n              1.7429e-02, -4.0209e-02],\n            ...,\n            [-6.2482e-02, -4.6525e-02, -7.6340e-02,  ...,  4.3542e-02,\n              2.2325e-02,  4.5584e-02],\n            [ 6.7824e-02,  6.1630e-03, -4.7666e-02,  ..., -6.5673e-02,\n              1.6434e-02,  2.2745e-02],\n            [ 3.1763e-02,  2.3619e-02,  2.3542e-02,  ..., -3.5429e-02,\n             -4.7007e-03, -5.3151e-02]],\n  \n           [[ 7.2893e-02,  8.9732e-03,  5.2495e-02,  ...,  3.5709e-02,\n              2.8189e-02, -6.6409e-02],\n            [ 4.3618e-02,  3.7971e-02, -6.9528e-02,  ..., -2.8227e-02,\n              5.2427e-02, -6.7132e-02],\n            [ 5.7024e-02, -5.1609e-02,  6.5927e-03,  ...,  3.8317e-03,\n              7.1618e-02, -7.1581e-02],\n            ...,\n            [-7.1104e-03, -8.1947e-02, -4.1548e-02,  ..., -4.2197e-02,\n              1.6117e-02,  3.5701e-02],\n            [ 5.5452e-03, -3.7601e-03, -2.8355e-03,  ..., -1.6176e-03,\n              6.9443e-02,  9.4604e-03],\n            [-4.3477e-02,  4.6371e-02,  2.3807e-02,  ...,  8.0683e-04,\n             -6.2717e-02, -3.0689e-02]]],\n  \n  \n          [[[ 2.7014e-02,  3.2033e-02,  4.7551e-02,  ..., -5.3259e-02,\n             -5.7893e-02, -4.4330e-02],\n            [ 3.7291e-03,  7.9491e-02,  4.1033e-02,  ...,  3.1892e-02,\n             -4.4614e-02,  8.6071e-03],\n            [ 1.9260e-02, -5.3861e-02,  4.3086e-02,  ..., -6.2785e-02,\n             -5.0858e-02,  6.2314e-02],\n            ...,\n            [ 3.4315e-02,  5.8667e-02,  3.6778e-02,  ..., -1.7046e-02,\n             -9.0148e-04,  3.0060e-02],\n            [ 3.5060e-02,  6.9967e-02, -2.9349e-02,  ..., -8.9636e-03,\n             -9.1222e-03,  4.1102e-02],\n            [ 4.3711e-03,  4.1051e-02, -7.8136e-02,  ..., -4.8194e-02,\n             -8.0044e-02,  4.3632e-02]],\n  \n           [[-2.0164e-02, -3.1905e-02, -1.7893e-02,  ...,  4.4683e-02,\n              1.3471e-02, -3.5066e-03],\n            [ 8.1410e-02,  5.2899e-02, -3.1037e-02,  ..., -3.0208e-02,\n              6.8709e-02, -3.1723e-02],\n            [ 3.8909e-02, -3.8746e-02,  3.6672e-02,  ..., -5.2711e-02,\n             -2.4600e-02, -5.9090e-02],\n            ...,\n            [-8.6534e-03,  2.4119e-03,  7.8834e-02,  ..., -8.0800e-02,\n             -7.9477e-02, -4.1090e-02],\n            [ 1.4293e-02, -6.5162e-02,  5.9310e-02,  ...,  3.7775e-03,\n              5.7875e-03,  4.0013e-02],\n            [-6.3544e-02,  4.7460e-03, -1.1194e-02,  ..., -6.4133e-02,\n              4.8205e-02, -5.4181e-02]],\n  \n           [[ 5.5911e-02, -8.4349e-03,  8.1303e-02,  ...,  1.2424e-02,\n             -5.5735e-02,  2.8164e-02],\n            [-3.9198e-02,  3.0304e-02, -2.8068e-02,  ...,  1.1040e-03,\n              6.0690e-02, -6.0397e-02],\n            [-5.4948e-02,  8.1102e-02,  7.0992e-02,  ..., -7.7316e-02,\n             -6.0993e-02,  4.4126e-02],\n            ...,\n            [ 2.1525e-02, -8.3998e-03,  6.9850e-02,  ...,  5.9193e-02,\n             -3.6359e-02, -6.0472e-03],\n            [-4.2032e-02,  3.2275e-02, -2.3358e-03,  ..., -3.9110e-02,\n             -3.2520e-02, -5.5325e-02],\n            [-6.2509e-02, -3.7118e-02, -4.6894e-02,  ...,  4.9526e-02,\n              5.6545e-02,  4.7783e-02]]],\n  \n  \n          [[[ 2.8031e-02, -4.8603e-02, -6.1952e-02,  ..., -7.5367e-02,\n             -7.6458e-02, -4.6346e-02],\n            [-9.0919e-03,  7.9056e-02,  3.9145e-02,  ...,  4.3952e-02,\n             -1.7508e-02, -6.6768e-03],\n            [-3.1815e-03,  1.0217e-02, -4.2954e-02,  ..., -7.6983e-03,\n             -4.9640e-02, -6.0462e-02],\n            ...,\n            [ 1.7382e-02, -6.2952e-02,  5.2323e-02,  ...,  2.3920e-02,\n              1.9025e-02,  5.2508e-02],\n            [-8.0220e-03, -6.7232e-02,  7.8928e-02,  ..., -7.8624e-02,\n             -2.2029e-02,  2.5948e-04],\n            [ 1.8168e-02, -4.0806e-02,  7.4368e-02,  ...,  2.4233e-02,\n              8.4375e-03,  7.7469e-02]],\n  \n           [[ 2.8999e-02, -5.1870e-02,  1.0022e-02,  ...,  1.2729e-03,\n              1.6060e-02,  6.9661e-02],\n            [ 6.5389e-02, -8.0584e-02,  2.4961e-02,  ...,  1.9686e-02,\n              6.4743e-02, -3.5769e-02],\n            [-2.3255e-02,  2.7186e-02, -1.4113e-02,  ...,  2.7866e-02,\n             -2.4918e-03, -7.6638e-03],\n            ...,\n            [-4.0029e-02, -4.5514e-02, -2.8922e-02,  ..., -6.2550e-02,\n             -2.2344e-02, -8.2253e-02],\n            [ 3.1125e-02,  5.8917e-03, -6.8612e-02,  ..., -3.2969e-02,\n             -7.6118e-02,  2.5691e-02],\n            [-5.1472e-02, -2.2466e-02,  5.3071e-02,  ...,  1.3933e-02,\n              2.1670e-02, -8.1295e-03]],\n  \n           [[-4.5416e-02, -7.9235e-03, -7.4890e-02,  ..., -6.3378e-02,\n             -4.0477e-02, -6.5841e-02],\n            [-4.5442e-02,  1.2963e-02,  4.6789e-02,  ..., -2.0675e-02,\n             -8.6283e-03,  1.4760e-02],\n            [-2.0946e-03,  6.4276e-03,  4.4530e-02,  ..., -1.9723e-02,\n              1.4108e-02,  6.9566e-02],\n            ...,\n            [ 8.3005e-03,  7.1973e-02, -4.7260e-02,  ...,  3.7063e-02,\n              4.8534e-02, -1.0500e-02],\n            [ 2.1036e-02,  5.9862e-02,  1.0388e-02,  ...,  2.1408e-02,\n             -6.4346e-03,  7.8030e-02],\n            [ 5.2678e-02, -7.8306e-03, -8.0588e-02,  ..., -2.7986e-02,\n             -8.0166e-03,  7.0205e-02]]]], requires_grad=True)\n)", "parameters": [["weight", [64, 3, 7, 7]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [9408]}, {"name": "bn1", "id": 140608652877488, "class_name": "BatchNorm2d(\n  self.momentum=0.1, self.weight=Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 5.5159e-05,  3.5582e-03,  9.2969e-05, -1.3322e-03, -1.1131e-04,\n          -1.3677e-03, -2.6723e-03,  1.5692e-03,  9.4110e-06, -5.0015e-04,\n           1.7436e-03, -1.3522e-03, -2.5844e-03,  1.0186e-03, -1.7105e-03,\n          -3.0463e-03,  3.7564e-04, -7.1582e-04, -7.0226e-04,  2.3895e-03,\n           8.3583e-04,  1.0087e-03, -2.4137e-03,  9.7717e-04, -7.0156e-04,\n          -4.8605e-05,  3.6550e-04, -1.0366e-03,  3.3734e-04, -1.1950e-03,\n           2.2662e-03,  3.9449e-04,  1.6125e-03,  1.1115e-03,  1.9415e-03,\n          -1.3225e-03, -6.5121e-04, -4.1336e-03, -7.8725e-04, -6.8543e-04,\n           1.8678e-04, -4.9361e-04,  9.2706e-04,  5.1763e-04,  5.3999e-04,\n          -2.5563e-03, -8.4890e-04, -1.5331e-03,  3.0277e-04, -1.3705e-03,\n          -6.5574e-04,  9.7091e-04, -9.0410e-04,  2.5665e-03,  7.6021e-04,\n          -7.7062e-04,  2.1694e-03,  1.2124e-04,  1.4040e-03, -5.7435e-04,\n           2.6207e-04,  6.2638e-04,  7.2877e-04,  1.9419e-04],\n         grad_fn=<AddBackward0>), self.running_var=tensor([0.9321, 1.0000, 0.9247, 0.9295, 0.9125, 0.9187, 0.9431, 0.9547, 0.9194,\n          0.9333, 0.9363, 0.9842, 1.0562, 0.9166, 0.9475, 0.9404, 0.9352, 0.9150,\n          0.9292, 0.9716, 0.9207, 0.9231, 0.9339, 1.0090, 0.9190, 0.9233, 0.9274,\n          0.9407, 0.9477, 0.9136, 0.9234, 0.9492, 0.9374, 0.9695, 0.9405, 0.9859,\n          1.0023, 0.9933, 0.9395, 0.9451, 0.9172, 0.9261, 0.9563, 0.9312, 0.9668,\n          1.0107, 0.9494, 0.9432, 1.0108, 0.9404, 1.0191, 0.9580, 0.9405, 0.9769,\n          0.9159, 0.9216, 0.9948, 0.9345, 0.9231, 0.9216, 0.9617, 0.9361, 0.9192,\n          0.9119], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n)", "parameters": [["weight", [64]], ["bias", [64]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [64, 64]}, {"name": "relu", "id": 140608652877536, "class_name": "ReLU()", "parameters": [], "output_shape": [[512, 64, 16, 16]], "num_parameters": []}, {"name": "pool", "id": 140608652877440, "class_name": "MaxPool2d(self.kernel_size=(3, 3), self.stride=(3, 3), self.padding=1)", "parameters": [], "output_shape": [[512, 64, 6, 6]], "num_parameters": []}, {"name": "layer1", "id": 140608652876240, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-4.0730e-02, -2.0952e-02,  1.8586e-03],\n                [ 2.6267e-02, -2.3762e-02, -1.9602e-02],\n                [ 4.1408e-03,  2.5155e-02,  3.6697e-02]],\n      \n               [[ 2.5011e-02, -1.9668e-02,  4.0170e-02],\n                [ 4.7170e-03,  3.8073e-02, -1.7961e-02],\n                [ 4.4478e-03,  3.4067e-02,  2.7567e-02]],\n      \n               [[-2.4759e-03, -1.2928e-02, -2.5800e-02],\n                [ 2.8641e-02, -5.9006e-03, -1.3313e-02],\n                [ 3.4675e-02, -9.6428e-04,  2.4486e-02]],\n      \n               ...,\n      \n               [[-3.7581e-02,  1.6675e-02, -2.6040e-02],\n                [ 1.3883e-02,  2.1216e-03,  2.0260e-02],\n                [ 1.9166e-02, -3.7979e-02, -4.0564e-02]],\n      \n               [[ 1.7036e-02,  1.3356e-02,  3.4758e-02],\n                [ 1.5688e-03,  6.5195e-03,  3.5095e-02],\n                [-2.4081e-02, -1.8654e-02, -2.4437e-02]],\n      \n               [[ 1.7552e-02,  3.8601e-02, -1.6851e-02],\n                [ 2.8858e-02,  6.9439e-03, -3.9783e-03],\n                [ 5.9609e-03, -7.4817e-03,  9.0780e-03]]],\n      \n      \n              [[[ 6.3455e-04,  3.4179e-02,  2.6666e-02],\n                [ 2.3954e-02, -1.5123e-02, -1.3198e-03],\n                [-8.3827e-03, -3.6118e-02,  2.5865e-02]],\n      \n               [[-1.5323e-02,  3.5159e-02, -3.5522e-02],\n                [ 3.8316e-02,  3.0644e-03,  2.4826e-02],\n                [-2.5164e-02, -3.7329e-02,  6.3727e-03]],\n      \n               [[-2.3447e-02,  6.0016e-03,  1.7154e-02],\n                [-3.3827e-02, -2.5588e-02,  5.6105e-03],\n                [-3.4387e-02,  3.6010e-03, -4.1585e-02]],\n      \n               ...,\n      \n               [[ 2.1122e-02, -2.7778e-02, -8.6965e-03],\n                [ 2.9038e-02, -3.6138e-02,  1.6888e-03],\n                [ 5.4905e-03, -1.5545e-02,  1.3860e-02]],\n      \n               [[-2.4544e-02,  2.1443e-03,  2.8690e-02],\n                [-3.0617e-02, -3.4953e-02,  3.6543e-02],\n                [ 3.0760e-02,  1.3689e-02,  2.2129e-03]],\n      \n               [[ 9.6618e-03, -2.4640e-02,  4.0154e-02],\n                [-2.2269e-02, -2.0725e-03,  3.1243e-02],\n                [-1.5162e-02,  1.1785e-02, -1.1395e-02]]],\n      \n      \n              [[[-2.0678e-03,  9.0555e-03, -2.1851e-02],\n                [ 2.0966e-02,  3.2413e-02, -8.7825e-03],\n                [-2.2295e-02, -2.3118e-02,  8.3706e-03]],\n      \n               [[ 3.1780e-02,  1.9457e-02,  3.6444e-03],\n                [ 9.2739e-03,  2.8055e-02, -4.7021e-03],\n                [-1.6046e-02,  2.4607e-03,  1.4045e-02]],\n      \n               [[-3.5064e-03, -4.0258e-02,  1.9254e-02],\n                [ 3.9134e-02,  1.1607e-03,  1.9005e-02],\n                [ 2.8056e-02,  8.9357e-04,  2.7376e-02]],\n      \n               ...,\n      \n               [[ 2.7784e-02, -3.7618e-02, -1.0947e-04],\n                [ 2.7053e-02,  3.3194e-02, -2.2539e-02],\n                [ 2.6493e-02, -3.5500e-02, -3.7913e-02]],\n      \n               [[ 9.7384e-03, -1.9534e-02, -2.1720e-02],\n                [ 8.4375e-05, -1.9796e-02,  2.1466e-02],\n                [ 2.8468e-02,  1.5786e-03, -2.4118e-02]],\n      \n               [[ 2.8352e-02, -7.9127e-03,  1.0708e-02],\n                [-3.1087e-02,  1.3340e-02, -1.0826e-02],\n                [ 2.3987e-02, -3.0211e-02, -3.3668e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[-2.8846e-02,  3.5421e-02,  3.7337e-03],\n                [-3.3839e-03,  3.0564e-02, -3.7277e-02],\n                [ 2.5794e-02,  2.4911e-02,  1.3592e-02]],\n      \n               [[ 2.0115e-02, -2.6239e-02, -2.6384e-02],\n                [ 3.2121e-02, -3.5997e-02,  3.8420e-02],\n                [ 3.8807e-02, -2.0203e-02,  6.4001e-03]],\n      \n               [[ 1.8291e-02, -2.2992e-02, -3.3437e-02],\n                [ 3.0514e-02,  7.4368e-03,  6.2428e-03],\n                [ 2.7378e-03,  1.7828e-02,  1.1410e-03]],\n      \n               ...,\n      \n               [[ 3.6781e-02, -3.3420e-02,  1.0623e-02],\n                [ 1.7369e-02, -1.0439e-02, -4.3407e-03],\n                [ 4.0377e-02, -8.4023e-03, -3.7529e-02]],\n      \n               [[-2.6735e-02,  1.3675e-03,  3.0063e-02],\n                [ 3.2166e-02,  3.9478e-03,  3.7337e-02],\n                [ 2.7396e-02,  1.5650e-02, -1.2700e-02]],\n      \n               [[-2.7983e-02, -1.8671e-02,  4.8500e-03],\n                [ 1.9059e-02,  1.3503e-02,  2.7391e-02],\n                [-1.9647e-02,  1.5446e-02,  3.0534e-02]]],\n      \n      \n              [[[ 3.7781e-02, -1.7412e-02,  3.7273e-02],\n                [-1.1694e-03,  2.5953e-02, -1.3466e-03],\n                [ 1.2305e-02,  3.2636e-02, -2.8104e-02]],\n      \n               [[ 1.0030e-02,  3.9625e-02,  2.8076e-02],\n                [-2.9585e-02, -2.1947e-02,  6.6880e-03],\n                [-1.3696e-02, -3.2665e-02, -7.3881e-03]],\n      \n               [[-2.2712e-02, -3.4677e-02,  2.5610e-02],\n                [ 3.6100e-02,  2.0092e-02, -2.2162e-02],\n                [ 3.8572e-02,  3.1646e-02, -1.0745e-02]],\n      \n               ...,\n      \n               [[ 5.0017e-03, -3.7634e-02, -3.5957e-02],\n                [-2.6282e-03,  3.8459e-02, -2.3967e-02],\n                [-2.8106e-02, -4.0055e-02,  3.9066e-03]],\n      \n               [[-2.0798e-02, -1.8642e-02,  4.5950e-05],\n                [-2.2012e-02, -4.8381e-03, -3.9431e-02],\n                [-1.0036e-02, -2.1383e-02,  3.2411e-03]],\n      \n               [[-2.1650e-02, -3.2314e-02,  3.7335e-02],\n                [ 3.3881e-03,  4.0535e-02, -3.7724e-02],\n                [-3.5681e-02,  1.2531e-02,  3.2335e-02]]],\n      \n      \n              [[[ 4.0727e-03,  7.7741e-03, -3.4153e-02],\n                [ 1.6643e-02,  2.8252e-02, -4.1067e-03],\n                [ 3.5014e-02,  2.3978e-02, -1.9828e-02]],\n      \n               [[ 2.6038e-02,  2.8609e-02,  3.3802e-02],\n                [-3.9770e-02,  2.4855e-02, -3.6435e-02],\n                [ 2.2724e-02, -3.9819e-02,  2.8541e-02]],\n      \n               [[ 2.9449e-02,  9.4456e-03, -2.4298e-02],\n                [ 3.6644e-02,  3.9878e-02, -2.8900e-02],\n                [ 2.8710e-02,  1.4826e-02,  4.6540e-03]],\n      \n               ...,\n      \n               [[-4.0448e-03,  2.7223e-02, -1.3540e-02],\n                [-3.7215e-02, -2.7303e-02,  1.2776e-02],\n                [-5.0814e-03, -1.9525e-02,  1.5466e-02]],\n      \n               [[ 2.8923e-02,  3.9273e-03, -2.7269e-02],\n                [ 3.5954e-02, -2.3295e-03, -2.0649e-02],\n                [ 2.5313e-02,  3.2111e-02, -8.7043e-04]],\n      \n               [[-2.0008e-02, -6.4848e-03,  1.0905e-02],\n                [ 1.3529e-02,  7.8420e-03, -1.1449e-02],\n                [-3.4222e-03,  4.1180e-02, -1.0655e-02]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0104, -0.0454,  0.0259,  0.0555,  0.0204, -0.0302, -0.0065, -0.0249,\n              -0.0144,  0.0464, -0.0791,  0.0216, -0.0379,  0.0058,  0.0302, -0.0793,\n              -0.0101, -0.0896, -0.0302,  0.0477, -0.0320,  0.0531,  0.0306,  0.0276,\n               0.0293,  0.0862,  0.0621,  0.0290,  0.0454, -0.0042, -0.0199, -0.0581,\n              -0.0230,  0.0410,  0.0692, -0.0546, -0.0031,  0.0331,  0.0332, -0.0555,\n               0.0412,  0.0050, -0.0093,  0.0194, -0.0028,  0.0545,  0.0086, -0.0156,\n               0.0143,  0.0251, -0.0039,  0.0175,  0.0131, -0.0048, -0.0007,  0.0168,\n               0.0360,  0.0254,  0.0109, -0.0186, -0.0234,  0.0110, -0.0243,  0.0723],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9193, 0.9155, 0.9146, 0.9184, 0.9168, 0.9114, 0.9344, 0.9175, 0.9172,\n              0.9309, 0.9300, 0.9108, 0.9271, 0.9127, 0.9132, 0.9346, 0.9211, 0.9376,\n              0.9123, 0.9191, 0.9221, 0.9198, 0.9236, 0.9162, 0.9132, 0.9335, 0.9367,\n              0.9172, 0.9263, 0.9290, 0.9145, 0.9212, 0.9214, 0.9134, 0.9458, 0.9141,\n              0.9100, 0.9114, 0.9134, 0.9181, 0.9150, 0.9086, 0.9082, 0.9126, 0.9197,\n              0.9253, 0.9201, 0.9256, 0.9214, 0.9164, 0.9115, 0.9196, 0.9111, 0.9227,\n              0.9159, 0.9171, 0.9275, 0.9104, 0.9182, 0.9290, 0.9172, 0.9123, 0.9213,\n              0.9289], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 3.3825e-02,  3.7061e-02, -2.5459e-02],\n                [-3.3493e-03, -5.2040e-03, -1.4800e-02],\n                [-3.9768e-02,  1.4320e-03,  1.0908e-02]],\n      \n               [[ 3.2568e-02, -3.0363e-02, -2.5023e-03],\n                [-2.3781e-02,  3.9058e-02, -1.8654e-02],\n                [ 1.8049e-03,  4.0030e-02,  2.8362e-02]],\n      \n               [[-2.9918e-02,  5.7553e-03,  5.6786e-03],\n                [-3.6475e-02,  3.0049e-02, -2.7288e-02],\n                [ 2.6723e-02,  2.3016e-02, -1.2066e-02]],\n      \n               ...,\n      \n               [[-3.4094e-02,  3.4774e-02,  1.5847e-02],\n                [ 6.2983e-04, -7.9798e-03, -7.7936e-03],\n                [ 3.4588e-02,  1.5061e-02, -3.9972e-03]],\n      \n               [[ 3.5484e-02,  5.7662e-03,  1.4664e-02],\n                [ 3.0037e-02,  1.6392e-02,  3.0238e-02],\n                [ 1.1141e-02,  3.1033e-03, -1.8311e-02]],\n      \n               [[-3.0603e-02,  1.7696e-02,  1.7906e-02],\n                [ 2.9398e-03, -1.2313e-02,  3.8168e-02],\n                [-6.5756e-03, -4.1608e-02, -2.3003e-02]]],\n      \n      \n              [[[ 2.6446e-02, -3.6418e-02, -8.8244e-04],\n                [-9.2858e-03, -3.6293e-02,  3.9291e-02],\n                [ 1.3534e-02,  6.2860e-03, -2.0243e-02]],\n      \n               [[-9.6319e-03, -3.4926e-02,  2.6987e-02],\n                [ 2.7726e-02,  2.5492e-02, -3.2528e-02],\n                [-3.0693e-02,  4.0249e-03, -7.9227e-03]],\n      \n               [[ 1.3018e-02,  5.4758e-03, -1.9540e-02],\n                [-4.1115e-02, -8.5668e-03, -2.4594e-02],\n                [-4.0712e-02,  3.2157e-03,  1.7195e-02]],\n      \n               ...,\n      \n               [[-1.0324e-02,  3.2480e-02, -9.8460e-03],\n                [ 9.6260e-03,  1.7159e-02,  9.2654e-03],\n                [ 3.3963e-02, -1.4018e-02,  2.7710e-02]],\n      \n               [[ 2.1253e-02, -2.7064e-02,  3.7252e-02],\n                [ 9.6434e-03, -1.1869e-02, -3.9625e-02],\n                [ 2.1341e-02, -2.9657e-02, -5.1318e-03]],\n      \n               [[-3.2565e-02,  1.2471e-02, -1.3107e-02],\n                [ 1.0882e-04, -1.6977e-02,  5.5907e-03],\n                [-3.3344e-02,  3.4361e-02,  2.8726e-02]]],\n      \n      \n              [[[ 8.8519e-03, -4.0723e-02,  3.4424e-02],\n                [-3.4835e-02,  2.8568e-02,  8.7772e-03],\n                [-3.8426e-02, -2.5628e-02,  1.7987e-02]],\n      \n               [[-4.2606e-03, -1.2892e-02,  9.7456e-03],\n                [ 2.8382e-02,  1.5636e-02, -3.7134e-02],\n                [-3.0926e-02,  3.6290e-02, -3.1280e-02]],\n      \n               [[ 8.9328e-03, -2.5289e-03,  2.2459e-02],\n                [ 1.7998e-02, -4.0370e-02, -7.3241e-03],\n                [-3.6446e-02,  1.6763e-02, -4.1496e-02]],\n      \n               ...,\n      \n               [[ 3.1609e-03,  3.2650e-02, -3.4888e-02],\n                [-8.7560e-03, -3.2169e-02, -1.2265e-02],\n                [ 2.0957e-02, -3.2414e-02, -2.5751e-02]],\n      \n               [[ 4.0436e-02, -3.7480e-02,  1.4606e-02],\n                [ 3.0703e-02, -2.7391e-02, -1.4311e-02],\n                [ 4.0906e-02, -1.7975e-02, -2.2717e-02]],\n      \n               [[ 1.4920e-03, -3.9124e-02, -2.3210e-02],\n                [ 1.1323e-02,  4.1604e-02,  3.0323e-02],\n                [-1.1203e-02, -2.9871e-02,  1.6348e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[ 2.0233e-02,  3.4895e-02,  3.8171e-02],\n                [-3.6445e-02, -2.5939e-02, -2.0311e-02],\n                [ 2.9997e-02,  5.7415e-03,  1.8208e-02]],\n      \n               [[ 4.2589e-03, -2.8218e-05,  2.3583e-02],\n                [ 2.1269e-02,  3.5456e-02, -3.3340e-02],\n                [-2.9149e-02, -1.3999e-02,  1.5672e-02]],\n      \n               [[-8.5624e-03, -3.8953e-02, -1.1788e-02],\n                [ 2.3469e-02,  9.3723e-03,  6.5815e-03],\n                [-4.4552e-03, -3.7071e-03,  2.4586e-02]],\n      \n               ...,\n      \n               [[ 1.3509e-02,  2.3408e-02,  1.6137e-02],\n                [-2.6988e-02, -1.5200e-02,  2.6956e-02],\n                [-1.6813e-02, -2.1818e-02,  3.8681e-02]],\n      \n               [[ 3.5717e-02,  4.0155e-02, -1.7291e-02],\n                [ 3.8765e-02, -3.9225e-02,  4.0551e-02],\n                [-1.8481e-02, -1.5800e-02, -1.8139e-02]],\n      \n               [[ 9.1659e-03, -6.6996e-03,  4.1543e-02],\n                [-7.2388e-03,  2.4521e-02,  2.3866e-02],\n                [-5.3346e-05, -3.2453e-03, -1.2011e-02]]],\n      \n      \n              [[[-9.9113e-03, -1.3205e-03,  1.2659e-03],\n                [ 3.8370e-02,  3.6768e-02, -3.9784e-02],\n                [ 1.3547e-02, -3.8955e-02, -9.3231e-03]],\n      \n               [[ 1.6065e-02,  5.2328e-03, -3.9035e-02],\n                [-1.0833e-02,  2.5751e-02,  2.3495e-02],\n                [ 6.5288e-03, -1.3018e-02, -3.9638e-02]],\n      \n               [[-2.3914e-02, -3.0968e-02, -3.5489e-02],\n                [-6.3906e-03,  1.7481e-02,  9.3479e-04],\n                [ 6.5637e-04,  2.5984e-02,  1.8593e-02]],\n      \n               ...,\n      \n               [[ 4.1347e-02, -3.4137e-02,  6.9409e-03],\n                [-7.5284e-03,  2.1497e-02,  7.7391e-03],\n                [ 1.6259e-02,  2.6094e-02, -3.5544e-03]],\n      \n               [[-1.3418e-02,  9.0742e-04,  3.6799e-02],\n                [ 1.6463e-02,  4.0221e-02, -1.0539e-02],\n                [-3.1715e-02, -2.8077e-02, -3.4020e-02]],\n      \n               [[-6.0589e-03,  3.3767e-02,  3.3227e-02],\n                [-3.6714e-02,  3.1314e-02,  4.1474e-03],\n                [-1.2581e-02,  2.2392e-02,  1.3774e-02]]],\n      \n      \n              [[[ 2.8349e-02,  1.8103e-02,  3.0236e-02],\n                [ 2.1014e-02, -8.8845e-03,  3.4856e-02],\n                [-8.2659e-03, -3.2105e-02,  2.7439e-02]],\n      \n               [[ 2.4644e-02,  3.8145e-03, -8.7705e-03],\n                [-2.6393e-02,  1.9328e-02,  1.3485e-02],\n                [ 1.1486e-02,  1.5933e-02, -2.5982e-02]],\n      \n               [[ 3.5808e-02,  1.1643e-02, -3.3720e-02],\n                [ 1.6876e-02,  2.8870e-02,  3.9047e-02],\n                [-2.5201e-02, -9.2473e-03,  1.8474e-04]],\n      \n               ...,\n      \n               [[-2.9670e-02,  3.0136e-02,  3.6320e-02],\n                [ 1.9057e-02,  3.7670e-02, -8.2189e-03],\n                [-8.0094e-03,  2.5554e-03, -5.4412e-03]],\n      \n               [[ 2.8651e-02, -2.1412e-02, -3.5253e-02],\n                [-1.8042e-02, -1.9857e-03,  3.3594e-02],\n                [-3.0997e-02,  1.6047e-02,  2.6101e-02]],\n      \n               [[ 3.6085e-04, -2.4076e-02,  3.2457e-02],\n                [ 3.9091e-02,  2.4147e-02, -3.2937e-02],\n                [-1.3601e-02,  3.4462e-02, -1.6730e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-6.2242e-03,  4.6574e-04, -1.1038e-02, -2.5945e-02, -8.2224e-03,\n              -9.3099e-03,  3.2799e-02,  6.4537e-03,  8.3604e-03,  3.3365e-02,\n               4.6138e-03, -1.3169e-02,  3.9489e-02, -2.9270e-03,  9.4587e-03,\n              -2.9869e-03, -1.1199e-02,  2.8382e-02, -1.8575e-02,  2.4105e-02,\n              -4.3698e-03,  1.1267e-02,  1.4143e-02, -2.0455e-02,  5.3940e-03,\n               4.3386e-03, -1.6651e-02, -2.1941e-02,  1.3559e-02,  2.9346e-02,\n               2.2866e-02, -5.5932e-02,  1.5439e-02, -4.1517e-03, -1.6973e-02,\n              -1.7592e-02,  2.0932e-03,  2.3129e-04,  2.4825e-03,  1.8017e-02,\n              -1.0312e-02, -2.3330e-03, -7.6303e-03,  1.3008e-02, -2.2403e-02,\n               1.5325e-02,  4.8242e-03,  8.8030e-04,  2.8076e-02, -1.6155e-02,\n               2.4358e-03,  4.2602e-05,  2.4179e-02, -1.3659e-02,  1.1813e-02,\n               5.4767e-04,  8.6959e-03,  3.4822e-03, -1.5825e-02,  4.6878e-03,\n              -4.2230e-02, -2.3971e-02,  2.1956e-02, -7.1330e-03],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9101, 0.9102, 0.9101, 0.9076, 0.9104, 0.9092, 0.9090, 0.9076, 0.9106,\n              0.9109, 0.9104, 0.9079, 0.9142, 0.9158, 0.9097, 0.9116, 0.9112, 0.9180,\n              0.9098, 0.9102, 0.9066, 0.9110, 0.9086, 0.9132, 0.9086, 0.9079, 0.9093,\n              0.9134, 0.9079, 0.9141, 0.9114, 0.9158, 0.9096, 0.9117, 0.9217, 0.9083,\n              0.9096, 0.9083, 0.9075, 0.9095, 0.9093, 0.9098, 0.9073, 0.9095, 0.9083,\n              0.9075, 0.9173, 0.9122, 0.9163, 0.9136, 0.9086, 0.9110, 0.9149, 0.9100,\n              0.9125, 0.9142, 0.9078, 0.9122, 0.9081, 0.9115, 0.9103, 0.9122, 0.9138,\n              0.9057], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n  )\n)", "parameters": [["0.conv1.weight", [64, 64, 3, 3]], ["0.bn1.weight", [64]], ["0.bn1.bias", [64]], ["0.conv2.weight", [64, 64, 3, 3]], ["0.bn2.weight", [64]], ["0.bn2.bias", [64]]], "output_shape": [[512, 64, 6, 6]], "num_parameters": [36864, 64, 64, 36864, 64, 64]}, {"name": "layer2", "id": 140608652875376, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-0.0217,  0.0048,  0.0085],\n                [ 0.0034,  0.0159,  0.0070],\n                [-0.0035,  0.0387, -0.0290]],\n      \n               [[-0.0211,  0.0016,  0.0011],\n                [ 0.0327, -0.0379, -0.0295],\n                [-0.0226,  0.0153,  0.0300]],\n      \n               [[ 0.0335,  0.0371, -0.0121],\n                [ 0.0013, -0.0212, -0.0224],\n                [-0.0259,  0.0187,  0.0067]],\n      \n               ...,\n      \n               [[-0.0136,  0.0101,  0.0011],\n                [-0.0248, -0.0196, -0.0003],\n                [ 0.0229,  0.0357,  0.0381]],\n      \n               [[-0.0093,  0.0385, -0.0180],\n                [ 0.0355, -0.0225, -0.0388],\n                [-0.0247, -0.0050,  0.0183]],\n      \n               [[-0.0048,  0.0206,  0.0148],\n                [-0.0150, -0.0132,  0.0333],\n                [ 0.0391,  0.0186, -0.0354]]],\n      \n      \n              [[[ 0.0198,  0.0255,  0.0306],\n                [-0.0177, -0.0200,  0.0068],\n                [ 0.0083, -0.0389, -0.0292]],\n      \n               [[ 0.0066,  0.0185,  0.0254],\n                [-0.0066,  0.0009, -0.0379],\n                [ 0.0413, -0.0249, -0.0110]],\n      \n               [[ 0.0290,  0.0126,  0.0324],\n                [-0.0003,  0.0161, -0.0267],\n                [ 0.0117,  0.0231, -0.0123]],\n      \n               ...,\n      \n               [[ 0.0171, -0.0251, -0.0383],\n                [-0.0290, -0.0256,  0.0154],\n                [-0.0265,  0.0196,  0.0061]],\n      \n               [[-0.0131,  0.0088,  0.0158],\n                [ 0.0090, -0.0268, -0.0187],\n                [ 0.0113, -0.0211,  0.0005]],\n      \n               [[ 0.0058, -0.0096,  0.0129],\n                [ 0.0106, -0.0233, -0.0170],\n                [ 0.0279, -0.0284,  0.0382]]],\n      \n      \n              [[[-0.0292, -0.0071, -0.0197],\n                [ 0.0168, -0.0169,  0.0184],\n                [-0.0146, -0.0014,  0.0188]],\n      \n               [[ 0.0316,  0.0064,  0.0051],\n                [ 0.0224,  0.0181, -0.0018],\n                [ 0.0139, -0.0016, -0.0404]],\n      \n               [[-0.0105,  0.0325, -0.0388],\n                [ 0.0156, -0.0249, -0.0367],\n                [-0.0027, -0.0378,  0.0152]],\n      \n               ...,\n      \n               [[-0.0044, -0.0090,  0.0162],\n                [-0.0130,  0.0137,  0.0219],\n                [ 0.0391, -0.0177, -0.0272]],\n      \n               [[-0.0266,  0.0076, -0.0162],\n                [-0.0125,  0.0297,  0.0219],\n                [ 0.0283,  0.0367, -0.0403]],\n      \n               [[ 0.0008,  0.0243,  0.0382],\n                [-0.0062, -0.0028, -0.0348],\n                [ 0.0258, -0.0003,  0.0002]]],\n      \n      \n              ...,\n      \n      \n              [[[-0.0204,  0.0413,  0.0232],\n                [ 0.0220, -0.0340,  0.0347],\n                [-0.0096, -0.0315, -0.0408]],\n      \n               [[-0.0306, -0.0119, -0.0080],\n                [-0.0353,  0.0396, -0.0362],\n                [ 0.0205, -0.0192,  0.0091]],\n      \n               [[-0.0100, -0.0402, -0.0104],\n                [-0.0029, -0.0197,  0.0380],\n                [-0.0415,  0.0358,  0.0377]],\n      \n               ...,\n      \n               [[-0.0055,  0.0143,  0.0227],\n                [-0.0051,  0.0212, -0.0056],\n                [ 0.0099,  0.0330,  0.0408]],\n      \n               [[ 0.0026,  0.0100,  0.0166],\n                [ 0.0135,  0.0320, -0.0032],\n                [ 0.0365,  0.0267,  0.0339]],\n      \n               [[ 0.0314, -0.0229, -0.0097],\n                [-0.0372, -0.0020,  0.0232],\n                [-0.0267, -0.0356, -0.0178]]],\n      \n      \n              [[[ 0.0317, -0.0286,  0.0054],\n                [-0.0275,  0.0239, -0.0049],\n                [-0.0254,  0.0178, -0.0095]],\n      \n               [[-0.0214,  0.0274, -0.0124],\n                [-0.0078, -0.0304, -0.0310],\n                [ 0.0061, -0.0102, -0.0156]],\n      \n               [[-0.0382,  0.0377, -0.0220],\n                [-0.0241,  0.0146, -0.0070],\n                [-0.0257,  0.0144,  0.0314]],\n      \n               ...,\n      \n               [[-0.0129,  0.0280, -0.0387],\n                [-0.0346, -0.0007, -0.0300],\n                [-0.0413,  0.0165, -0.0371]],\n      \n               [[ 0.0177, -0.0152, -0.0181],\n                [-0.0369,  0.0164, -0.0030],\n                [-0.0308, -0.0092, -0.0256]],\n      \n               [[ 0.0310, -0.0308,  0.0081],\n                [-0.0409, -0.0280,  0.0199],\n                [ 0.0375,  0.0139, -0.0200]]],\n      \n      \n              [[[-0.0267,  0.0085,  0.0252],\n                [-0.0094, -0.0192,  0.0192],\n                [ 0.0252,  0.0104,  0.0039]],\n      \n               [[ 0.0212, -0.0373,  0.0033],\n                [ 0.0391, -0.0033,  0.0040],\n                [ 0.0171,  0.0116, -0.0262]],\n      \n               [[-0.0358, -0.0219,  0.0105],\n                [-0.0409,  0.0282, -0.0379],\n                [-0.0135, -0.0366,  0.0398]],\n      \n               ...,\n      \n               [[ 0.0283,  0.0185,  0.0225],\n                [ 0.0297,  0.0050,  0.0366],\n                [ 0.0100, -0.0245,  0.0070]],\n      \n               [[ 0.0255,  0.0363, -0.0022],\n                [ 0.0302,  0.0302, -0.0072],\n                [-0.0406, -0.0343, -0.0032]],\n      \n               [[ 0.0078, -0.0398, -0.0201],\n                [-0.0153, -0.0086, -0.0342],\n                [-0.0357,  0.0196, -0.0181]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0721, -0.0675,  0.0549, -0.0511,  0.0316, -0.0266,  0.0108,  0.0392,\n              -0.0337,  0.0565, -0.0051, -0.0465,  0.0571,  0.0119, -0.0063,  0.0108,\n               0.0077,  0.0843, -0.0209, -0.0218, -0.0187,  0.0083,  0.0061,  0.0748,\n               0.0546, -0.0140,  0.0277, -0.0058, -0.0236,  0.0126,  0.0327, -0.0090,\n               0.0856, -0.0335,  0.0430,  0.0060, -0.0041,  0.0247,  0.0406, -0.0431,\n               0.1043, -0.0209,  0.0297, -0.0310,  0.0971, -0.0003,  0.0242,  0.0698,\n              -0.0800,  0.0007, -0.0612,  0.1076, -0.0098,  0.0787,  0.0964,  0.0489,\n               0.0954,  0.0474, -0.0230, -0.0261, -0.0346, -0.0543, -0.0365, -0.0248,\n               0.0230, -0.0247,  0.0426, -0.0339,  0.0100,  0.0746,  0.0165,  0.0671,\n               0.0881, -0.0556, -0.0629, -0.1077,  0.0697, -0.0054, -0.0126, -0.0338,\n               0.0975,  0.0072,  0.0266,  0.0459, -0.0715, -0.0669,  0.0533,  0.0233,\n               0.0094, -0.0564,  0.0079, -0.0004, -0.0507,  0.0261,  0.0763,  0.0461,\n               0.0563,  0.0435, -0.0120, -0.0782,  0.0261,  0.0696,  0.0348,  0.0625,\n               0.0540, -0.0352,  0.0436,  0.0332,  0.0378, -0.0059, -0.0470,  0.1340,\n              -0.0183, -0.0604,  0.0361,  0.0548,  0.0195,  0.0651,  0.0158,  0.0211,\n              -0.0279, -0.0703, -0.0095,  0.0089, -0.0358,  0.0186,  0.0196,  0.0352],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9282, 0.9254, 0.9496, 0.9234, 0.9216, 0.9393, 0.9449, 0.9326, 0.9415,\n              0.9530, 0.9441, 0.9583, 0.9289, 0.9296, 0.9405, 0.9249, 0.9489, 0.9404,\n              0.9244, 0.9263, 0.9480, 0.9254, 0.9366, 0.9502, 0.9426, 0.9346, 0.9336,\n              0.9280, 0.9274, 0.9243, 0.9234, 0.9227, 0.9386, 0.9300, 0.9337, 0.9323,\n              0.9286, 0.9243, 0.9250, 0.9286, 0.9497, 0.9456, 0.9385, 0.9222, 0.9304,\n              0.9350, 0.9358, 0.9615, 0.9304, 0.9226, 0.9277, 0.9422, 0.9357, 0.9529,\n              0.9304, 0.9580, 0.9378, 0.9330, 0.9276, 0.9575, 0.9541, 0.9361, 0.9480,\n              0.9225, 0.9598, 0.9256, 0.9356, 0.9334, 0.9318, 0.9369, 0.9256, 0.9225,\n              0.9694, 0.9448, 0.9401, 0.9356, 0.9237, 0.9302, 0.9270, 0.9284, 0.9268,\n              0.9288, 0.9227, 0.9364, 0.9491, 0.9278, 0.9294, 0.9400, 0.9397, 0.9345,\n              0.9278, 0.9219, 0.9423, 0.9312, 0.9399, 0.9540, 0.9214, 0.9303, 0.9455,\n              0.9496, 0.9268, 0.9246, 0.9277, 0.9487, 0.9269, 0.9417, 0.9318, 0.9273,\n              0.9382, 0.9455, 0.9257, 0.9658, 0.9683, 0.9273, 0.9357, 0.9607, 0.9323,\n              0.9357, 0.9337, 0.9269, 0.9232, 0.9382, 0.9668, 0.9288, 0.9272, 0.9358,\n              0.9347, 0.9346], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 2.5703e-02, -1.0761e-02, -1.4252e-02],\n                [-2.0688e-02,  1.1633e-02, -1.1067e-02],\n                [-1.2904e-02, -2.9107e-02, -1.3166e-03]],\n      \n               [[-5.9216e-03, -1.7679e-02, -2.4366e-02],\n                [-2.3422e-02,  1.2592e-02,  2.7629e-02],\n                [ 3.8500e-03,  1.5195e-02,  2.2480e-02]],\n      \n               [[-5.1941e-04, -8.7559e-03, -6.2448e-04],\n                [-3.3427e-03,  1.5640e-02,  1.7709e-02],\n                [-1.7039e-04, -1.0563e-02, -1.0992e-02]],\n      \n               ...,\n      \n               [[-2.0020e-02,  2.0942e-02, -5.4496e-03],\n                [ 7.6094e-03, -1.0627e-02, -3.7061e-05],\n                [ 2.2935e-02,  1.1481e-02,  2.7847e-02]],\n      \n               [[-1.9252e-02, -2.3669e-02, -1.9731e-02],\n                [-1.6209e-02, -2.7751e-02,  1.0611e-02],\n                [-7.5248e-03,  2.3255e-02, -1.8709e-02]],\n      \n               [[-1.6281e-02,  1.4471e-02,  2.2965e-02],\n                [-2.7591e-04,  2.1567e-02, -2.3599e-02],\n                [ 1.9814e-02, -2.3906e-02,  5.2287e-04]]],\n      \n      \n              [[[-1.1128e-02,  5.0460e-04, -1.6582e-02],\n                [-6.5032e-03,  7.8675e-03,  2.7371e-02],\n                [ 2.3388e-02, -1.0331e-02,  2.2079e-02]],\n      \n               [[ 2.2636e-02, -1.1867e-03, -1.7344e-02],\n                [ 1.4806e-02, -1.1656e-02, -5.7210e-03],\n                [-1.0878e-02, -1.7279e-02,  2.1642e-02]],\n      \n               [[ 7.4785e-03, -1.5726e-03, -2.4500e-02],\n                [ 6.8418e-03, -2.9268e-02,  1.6796e-02],\n                [ 2.0223e-02,  1.3006e-02,  2.5521e-02]],\n      \n               ...,\n      \n               [[-1.9805e-02,  2.6801e-02, -1.1174e-03],\n                [-7.9472e-03, -2.1108e-02, -1.4289e-03],\n                [-1.3463e-02,  2.7372e-02, -2.2777e-02]],\n      \n               [[ 2.3268e-02,  7.8450e-03,  2.4540e-02],\n                [ 1.2622e-03, -1.9637e-02,  7.2436e-03],\n                [ 1.7937e-02, -2.5867e-02,  1.4690e-02]],\n      \n               [[-1.0993e-02,  1.1430e-02,  1.0859e-02],\n                [-1.9110e-02, -2.4838e-02,  1.6921e-02],\n                [-2.0330e-02, -1.5926e-02, -7.4794e-03]]],\n      \n      \n              [[[-1.2319e-02, -7.7192e-03, -7.3685e-03],\n                [ 2.1917e-02, -2.1445e-02, -7.5482e-03],\n                [ 5.1393e-03,  5.3793e-03,  9.8122e-03]],\n      \n               [[-2.9226e-02,  1.6767e-02,  1.6418e-02],\n                [-4.2640e-03, -5.7184e-04,  3.9086e-03],\n                [-1.0884e-02,  1.9939e-02,  1.1795e-02]],\n      \n               [[-2.1894e-02, -1.7348e-02,  2.1367e-02],\n                [ 9.8741e-03, -2.7036e-02,  1.0882e-02],\n                [-1.4477e-02, -1.7560e-02, -5.5383e-03]],\n      \n               ...,\n      \n               [[ 1.2633e-02,  2.5941e-02, -6.4494e-03],\n                [-2.1607e-02,  2.3335e-02,  1.7916e-03],\n                [ 5.1005e-03,  1.0425e-04, -5.9829e-03]],\n      \n               [[ 1.3667e-02, -2.9197e-02,  1.0909e-02],\n                [-1.2827e-02,  2.5722e-02, -1.3174e-02],\n                [-5.1224e-03, -6.6347e-03, -1.0090e-02]],\n      \n               [[ 1.4588e-02,  2.6859e-02, -2.2639e-02],\n                [-2.0143e-02, -2.3744e-02, -3.5587e-03],\n                [-1.9022e-02,  1.9952e-02,  2.2711e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[ 1.8047e-02, -2.8585e-02,  2.4582e-02],\n                [-1.7053e-02, -1.1932e-02,  2.9049e-02],\n                [ 2.7192e-02,  2.2662e-02, -1.6669e-02]],\n      \n               [[-1.2243e-02,  2.6864e-02,  6.5812e-03],\n                [ 2.5600e-02,  1.5472e-02, -1.2265e-02],\n                [ 2.8955e-02, -1.3749e-02,  3.6375e-03]],\n      \n               [[-5.9877e-04, -1.8339e-02,  2.5802e-02],\n                [ 1.3215e-02,  2.8805e-02, -4.9059e-03],\n                [-1.4695e-02, -3.4574e-03, -3.6159e-03]],\n      \n               ...,\n      \n               [[-1.0983e-02, -2.5045e-02,  5.3861e-03],\n                [ 8.5367e-03,  9.5433e-03, -5.4166e-03],\n                [ 2.8380e-02,  2.5231e-02, -2.9458e-02]],\n      \n               [[-9.6518e-03,  1.5751e-03, -2.8270e-02],\n                [-3.0022e-03, -1.6835e-03, -5.2565e-03],\n                [-2.5839e-02, -1.0770e-02,  3.3505e-03]],\n      \n               [[-1.4597e-02, -7.0857e-03, -1.1069e-02],\n                [-1.0709e-02,  2.4927e-02, -3.8813e-03],\n                [ 9.4174e-03,  1.6364e-02, -1.2120e-02]]],\n      \n      \n              [[[ 5.7911e-03, -1.1734e-02,  7.7512e-03],\n                [-1.4853e-02,  3.7636e-03,  1.2930e-03],\n                [ 1.6891e-02, -1.4202e-02, -7.0189e-05]],\n      \n               [[ 8.3385e-04,  3.6900e-03,  6.7084e-03],\n                [-1.1197e-02,  2.7702e-02, -3.3960e-03],\n                [ 2.8206e-02, -4.4275e-03,  7.4797e-03]],\n      \n               [[ 2.5735e-02,  2.5819e-02,  1.2068e-02],\n                [-1.8991e-02,  6.7159e-03,  1.6183e-02],\n                [ 2.8116e-02,  2.0796e-02,  1.5041e-03]],\n      \n               ...,\n      \n               [[ 1.4858e-02, -3.7604e-03,  5.8612e-05],\n                [-8.9218e-03,  2.5940e-02,  7.6352e-03],\n                [ 4.3206e-03, -2.2631e-02,  2.6860e-02]],\n      \n               [[ 3.5945e-03, -1.3551e-02,  7.5391e-03],\n                [ 1.6137e-02,  2.7480e-04,  6.4283e-03],\n                [ 9.0742e-03, -4.3372e-03, -1.4783e-02]],\n      \n               [[-1.2730e-02, -2.4071e-02,  2.0418e-03],\n                [-1.5480e-02, -8.0307e-03, -2.8144e-02],\n                [-2.1596e-02,  1.5253e-03,  1.0755e-02]]],\n      \n      \n              [[[-6.7260e-03,  6.2490e-03, -1.3600e-02],\n                [ 2.7254e-02, -2.4706e-03, -2.3198e-02],\n                [ 2.1195e-02,  8.8615e-03, -2.5011e-03]],\n      \n               [[ 2.6512e-02, -2.2641e-02, -8.7187e-03],\n                [ 2.1406e-02, -1.7909e-02,  2.2830e-02],\n                [ 2.6747e-02, -2.6746e-02, -2.4331e-02]],\n      \n               [[-2.8031e-03,  5.9619e-03, -8.3533e-04],\n                [-5.8183e-03, -2.3059e-02,  1.4730e-02],\n                [ 2.5671e-02,  1.6383e-02, -1.1942e-02]],\n      \n               ...,\n      \n               [[-2.4964e-02, -5.9346e-04,  2.3123e-02],\n                [-5.0438e-03,  1.7186e-02,  9.1268e-03],\n                [ 9.8209e-03,  2.8834e-02, -2.0957e-02]],\n      \n               [[ 2.7529e-02, -2.0377e-02, -9.2177e-03],\n                [-1.9215e-02,  2.4285e-02, -1.2348e-03],\n                [ 4.6449e-03, -2.4713e-02, -1.5324e-02]],\n      \n               [[ 6.0576e-03, -2.8471e-02, -1.9508e-02],\n                [ 1.7574e-02, -3.5415e-04,  2.6131e-02],\n                [ 1.7912e-02, -1.5673e-02,  5.4149e-03]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 1.4046e-02, -3.6953e-03,  4.8353e-03,  1.5765e-02, -2.6673e-02,\n               1.3630e-02, -6.4974e-03, -6.6353e-03, -1.8258e-02,  5.7875e-05,\n              -2.2549e-03,  6.3844e-03,  4.8324e-03, -1.4539e-02, -1.1680e-02,\n              -9.1495e-03,  1.3745e-02, -3.1708e-02, -4.0904e-03, -3.1635e-04,\n               2.5170e-02,  2.1006e-02,  2.6425e-02,  3.6783e-03, -7.1274e-03,\n              -1.7274e-02, -1.2132e-02,  1.8015e-02, -4.1341e-02,  6.0815e-04,\n               5.7899e-03, -1.5383e-02,  2.2775e-02,  1.4850e-02,  5.1170e-03,\n              -1.3495e-02, -3.4561e-02,  2.8073e-02, -7.7002e-03, -1.8653e-02,\n              -4.5605e-03,  1.1842e-02, -2.0779e-02, -1.5330e-02,  2.5848e-03,\n              -5.6632e-03,  1.1536e-03,  3.1207e-03, -8.0207e-03,  1.4802e-02,\n              -2.0147e-02,  1.6133e-02,  1.7184e-02, -1.3813e-02, -1.0139e-02,\n               5.3556e-03, -1.8562e-02, -2.0575e-02,  3.9200e-03, -2.5157e-03,\n               5.2072e-03,  1.9382e-02,  1.9556e-02, -2.3866e-02,  7.6487e-03,\n              -9.2835e-03, -2.0803e-02, -1.4313e-02,  6.7202e-03,  2.8563e-02,\n               2.2379e-02, -7.7792e-03, -1.7472e-02,  2.9911e-02, -3.4692e-02,\n              -3.8760e-02,  2.3624e-02, -1.3859e-02,  1.1197e-02,  1.4674e-02,\n              -1.7411e-02, -4.1042e-03,  3.0382e-02,  2.5547e-02,  3.6414e-02,\n              -4.0607e-03,  7.4807e-04, -2.1588e-02, -3.3200e-02, -1.0869e-02,\n               6.2789e-04,  1.8200e-03,  8.9744e-03, -2.0129e-02, -4.2457e-03,\n              -1.1124e-02,  1.0588e-02, -1.6504e-02, -1.0580e-02, -7.6116e-03,\n              -7.8953e-03,  1.9244e-02, -2.5515e-02, -2.9939e-03,  9.8297e-03,\n              -2.5589e-02,  5.1698e-03,  2.5453e-02,  7.5714e-03,  2.3843e-02,\n               1.1705e-02, -1.5977e-02, -1.4650e-02,  1.5820e-02,  6.7734e-03,\n               7.9591e-03, -3.7746e-03, -5.5854e-03,  9.6954e-03, -2.5155e-02,\n              -9.0019e-03, -7.5220e-03, -1.1340e-02,  6.5460e-04,  1.5900e-02,\n               1.8127e-02,  5.8153e-03, -7.7637e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9102, 0.9081, 0.9091, 0.9077, 0.9082, 0.9092, 0.9079, 0.9098, 0.9094,\n              0.9075, 0.9056, 0.9099, 0.9088, 0.9064, 0.9094, 0.9080, 0.9077, 0.9104,\n              0.9083, 0.9080, 0.9102, 0.9082, 0.9068, 0.9057, 0.9090, 0.9062, 0.9073,\n              0.9135, 0.9142, 0.9097, 0.9072, 0.9138, 0.9104, 0.9074, 0.9095, 0.9094,\n              0.9087, 0.9121, 0.9095, 0.9079, 0.9087, 0.9095, 0.9076, 0.9078, 0.9051,\n              0.9060, 0.9060, 0.9100, 0.9080, 0.9111, 0.9119, 0.9080, 0.9066, 0.9083,\n              0.9062, 0.9093, 0.9064, 0.9083, 0.9078, 0.9067, 0.9073, 0.9070, 0.9090,\n              0.9079, 0.9061, 0.9099, 0.9085, 0.9120, 0.9069, 0.9065, 0.9074, 0.9095,\n              0.9109, 0.9147, 0.9093, 0.9109, 0.9092, 0.9070, 0.9071, 0.9089, 0.9066,\n              0.9110, 0.9110, 0.9087, 0.9103, 0.9114, 0.9071, 0.9121, 0.9093, 0.9132,\n              0.9115, 0.9087, 0.9105, 0.9078, 0.9068, 0.9093, 0.9100, 0.9070, 0.9088,\n              0.9110, 0.9074, 0.9093, 0.9072, 0.9079, 0.9093, 0.9093, 0.9071, 0.9084,\n              0.9101, 0.9076, 0.9075, 0.9126, 0.9060, 0.9085, 0.9114, 0.9082, 0.9090,\n              0.9081, 0.9134, 0.9080, 0.9077, 0.9089, 0.9094, 0.9073, 0.9109, 0.9096,\n              0.9097, 0.9073], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[-0.1030]],\n        \n                 [[-0.0141]],\n        \n                 [[-0.0156]],\n        \n                 ...,\n        \n                 [[ 0.0984]],\n        \n                 [[-0.0959]],\n        \n                 [[ 0.0657]]],\n        \n        \n                [[[ 0.0881]],\n        \n                 [[ 0.1015]],\n        \n                 [[-0.0359]],\n        \n                 ...,\n        \n                 [[-0.0314]],\n        \n                 [[-0.1026]],\n        \n                 [[-0.0112]]],\n        \n        \n                [[[ 0.0586]],\n        \n                 [[-0.0698]],\n        \n                 [[-0.0535]],\n        \n                 ...,\n        \n                 [[-0.0330]],\n        \n                 [[ 0.0379]],\n        \n                 [[ 0.1075]]],\n        \n        \n                ...,\n        \n        \n                [[[-0.0367]],\n        \n                 [[ 0.0389]],\n        \n                 [[-0.1013]],\n        \n                 ...,\n        \n                 [[ 0.0251]],\n        \n                 [[-0.1022]],\n        \n                 [[ 0.0945]]],\n        \n        \n                [[[-0.0947]],\n        \n                 [[-0.0999]],\n        \n                 [[-0.0099]],\n        \n                 ...,\n        \n                 [[ 0.0762]],\n        \n                 [[ 0.0355]],\n        \n                 [[-0.0190]]],\n        \n        \n                [[[ 0.0728]],\n        \n                 [[-0.1170]],\n        \n                 [[-0.0103]],\n        \n                 ...,\n        \n                 [[ 0.0576]],\n        \n                 [[ 0.0159]],\n        \n                 [[ 0.0080]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-3.4861e-02, -4.6994e-02,  8.1667e-03, -3.0492e-02, -6.0856e-02,\n                -1.0641e-04, -3.9934e-02, -3.1303e-02, -8.6915e-02,  7.0056e-02,\n                -3.8079e-02, -8.0902e-02,  7.4441e-02,  1.4890e-02,  8.7889e-03,\n                 7.8068e-02,  8.7461e-02,  6.4867e-02, -6.6586e-02, -2.4492e-02,\n                -5.3994e-02, -1.4165e-02,  6.0843e-02,  8.4339e-02, -1.4810e-03,\n                -4.3319e-02, -1.3337e-02, -7.7724e-02,  1.6040e-02,  1.5456e-02,\n                 1.9489e-02,  1.0296e-01,  7.7459e-02, -1.0434e-01, -6.4901e-02,\n                 1.4408e-01, -3.3489e-02,  2.8896e-02,  3.0110e-03,  6.5436e-02,\n                 2.5324e-02, -4.7766e-02, -2.2248e-02, -1.4941e-02,  3.4042e-02,\n                -1.0863e-02,  6.2539e-02,  8.4107e-02, -2.4632e-02,  4.2173e-02,\n                -1.6267e-02,  9.1861e-02,  2.8078e-02, -3.3939e-02,  2.6329e-02,\n                 1.6538e-02, -1.8993e-02, -3.4844e-02, -3.4133e-03,  1.3558e-02,\n                -5.8650e-02, -2.1928e-02,  1.9656e-02, -7.7820e-02,  3.8371e-03,\n                -2.6674e-02, -7.6894e-02, -3.6390e-02,  1.3651e-01, -6.2922e-03,\n                 7.4325e-02,  6.6351e-02, -2.1902e-02,  9.2678e-02,  4.8008e-02,\n                -4.4155e-02,  5.4181e-02,  1.7049e-02,  5.5519e-02, -2.7850e-02,\n                 8.7034e-02, -1.2663e-02,  3.0708e-02,  6.8017e-02,  4.2972e-02,\n                 1.2670e-01, -4.9561e-02, -1.0650e-01,  1.7816e-02,  1.3576e-02,\n                 1.0447e-01, -5.7463e-02, -5.2337e-02,  8.4442e-03,  7.0776e-03,\n                -4.0867e-02,  3.2365e-02, -8.8406e-02,  4.1811e-02,  4.0449e-02,\n                -1.2429e-01,  6.9720e-02, -4.4093e-02,  4.8042e-03,  5.0665e-02,\n                -3.1035e-02,  4.1228e-02, -9.0335e-02, -3.7687e-02, -1.1636e-01,\n                -1.9549e-02,  6.0620e-02,  2.2620e-02,  4.7058e-02,  3.2695e-03,\n                 9.2030e-02, -6.3722e-02, -1.0456e-01,  2.4504e-02, -5.7922e-02,\n                -1.1822e-01, -1.1662e-02,  1.2132e-01, -2.4567e-03, -6.8912e-02,\n                -3.5407e-02, -1.2564e-01,  1.0030e-01], grad_fn=<AddBackward0>), self.running_var=tensor([0.9328, 0.9444, 0.9352, 0.9500, 0.9320, 0.9375, 0.9362, 0.9567, 0.9352,\n                0.9263, 0.9573, 0.9698, 0.9354, 0.9518, 0.9305, 0.9405, 0.9261, 0.9302,\n                0.9449, 0.9274, 0.9377, 0.9665, 0.9230, 0.9342, 0.9534, 0.9315, 0.9323,\n                0.9296, 0.9407, 0.9323, 0.9226, 0.9294, 0.9257, 0.9392, 0.9209, 0.9414,\n                0.9199, 0.9559, 0.9229, 0.9364, 0.9308, 0.9405, 0.9329, 0.9295, 0.9299,\n                0.9196, 0.9494, 0.9474, 0.9304, 0.9484, 0.9293, 0.9495, 0.9241, 0.9235,\n                0.9325, 0.9284, 0.9255, 0.9287, 0.9260, 0.9254, 0.9273, 0.9219, 0.9410,\n                0.9338, 0.9325, 0.9283, 0.9306, 0.9369, 0.9527, 0.9545, 0.9287, 0.9413,\n                0.9229, 0.9380, 0.9275, 0.9407, 0.9244, 0.9304, 0.9412, 0.9746, 0.9508,\n                0.9227, 0.9333, 0.9323, 0.9366, 0.9526, 0.9406, 0.9784, 0.9414, 0.9174,\n                0.9474, 0.9321, 0.9430, 0.9184, 0.9308, 0.9410, 0.9580, 0.9268, 0.9336,\n                0.9212, 0.9740, 0.9322, 0.9314, 0.9257, 0.9374, 0.9165, 0.9189, 0.9206,\n                0.9258, 0.9461, 0.9974, 0.9658, 0.9331, 0.9420, 0.9377, 0.9258, 0.9327,\n                0.9338, 0.9305, 0.9393, 0.9768, 0.9171, 0.9696, 0.9350, 0.9488, 0.9407,\n                0.9426, 0.9320], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [128, 64, 3, 3]], ["0.bn1.weight", [128]], ["0.bn1.bias", [128]], ["0.conv2.weight", [128, 128, 3, 3]], ["0.bn2.weight", [128]], ["0.bn2.bias", [128]], ["0.downsample.0.weight", [128, 64, 1, 1]], ["0.downsample.1.weight", [128]], ["0.downsample.1.bias", [128]]], "output_shape": [[512, 128, 3, 3]], "num_parameters": [73728, 128, 128, 147456, 128, 128, 8192, 128, 128]}, {"name": "layer3", "id": 140608652876000, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-1.4014e-02, -2.6524e-03, -2.8867e-02],\n                [-2.9956e-03,  8.8654e-03, -1.4565e-02],\n                [-1.1419e-02,  1.0709e-02,  9.4620e-03]],\n      \n               [[ 2.8750e-02, -3.2093e-03,  2.8174e-02],\n                [-2.0258e-02, -1.7357e-02, -5.4491e-03],\n                [-5.4425e-03,  2.7788e-02, -1.2059e-02]],\n      \n               [[-9.4001e-03,  1.5097e-02,  1.8441e-02],\n                [-9.6487e-03,  2.3329e-03,  1.1826e-02],\n                [-2.6014e-02, -1.2752e-02, -1.4155e-02]],\n      \n               ...,\n      \n               [[ 2.0985e-02, -1.7590e-02, -6.2976e-03],\n                [-1.9797e-02,  1.1693e-02, -9.2709e-03],\n                [ 1.7212e-02, -4.7274e-03, -1.9534e-02]],\n      \n               [[ 1.9486e-02,  2.5952e-02,  1.2451e-02],\n                [-2.2893e-02, -2.8925e-02,  1.2605e-02],\n                [ 1.9122e-02, -2.1040e-03, -2.0125e-02]],\n      \n               [[-2.1488e-02,  1.6713e-02,  8.3227e-03],\n                [-1.4931e-02,  2.6572e-02, -9.1275e-03],\n                [-1.1080e-02,  1.7481e-02,  3.0084e-03]]],\n      \n      \n              [[[ 1.2401e-02, -2.7838e-02,  2.4503e-02],\n                [ 1.3265e-02, -1.4763e-02, -2.3079e-02],\n                [-1.6173e-02, -1.0450e-02, -1.5231e-02]],\n      \n               [[ 1.9417e-02, -1.1193e-02,  2.7345e-02],\n                [ 3.6575e-03,  1.0022e-02,  2.7910e-02],\n                [-2.7831e-04, -1.6775e-02,  2.9164e-03]],\n      \n               [[ 2.2866e-02, -2.8551e-02,  1.0846e-02],\n                [ 4.3128e-03, -2.8529e-02, -1.0411e-02],\n                [ 1.8032e-02,  7.1082e-04, -2.0568e-02]],\n      \n               ...,\n      \n               [[ 7.8554e-03, -2.5148e-02, -2.3431e-02],\n                [ 1.7139e-02, -2.5777e-03, -9.7144e-03],\n                [ 2.1225e-02, -1.3979e-06, -2.3898e-02]],\n      \n               [[ 1.8001e-03,  2.7697e-02,  2.7881e-02],\n                [ 5.5144e-03,  1.9160e-02, -2.0213e-03],\n                [-2.7379e-02,  6.7992e-03, -1.1018e-02]],\n      \n               [[ 3.5356e-03,  2.2261e-02,  8.4136e-03],\n                [ 1.2103e-02, -2.2972e-02, -2.7260e-02],\n                [-1.8877e-02, -1.2104e-02,  1.7053e-02]]],\n      \n      \n              [[[-1.4452e-02,  2.1460e-02,  1.9890e-02],\n                [ 2.0624e-02,  2.2584e-02, -2.8647e-02],\n                [-2.9131e-02, -1.2474e-02,  2.8716e-02]],\n      \n               [[-7.9784e-03, -2.6616e-02,  2.2696e-02],\n                [-8.5665e-04,  8.2480e-04,  2.0058e-02],\n                [-1.4385e-02,  1.1133e-02,  5.4004e-03]],\n      \n               [[ 1.8466e-02, -1.2925e-02,  5.0838e-03],\n                [ 7.3391e-03,  2.2246e-02, -8.4597e-03],\n                [ 1.4093e-02,  2.8668e-02, -1.7874e-02]],\n      \n               ...,\n      \n               [[-1.1856e-02,  1.8453e-02,  1.9658e-02],\n                [-2.7647e-02, -3.6667e-03, -8.1281e-03],\n                [ 9.3488e-03,  2.6911e-02, -2.9223e-03]],\n      \n               [[ 7.5648e-03,  1.5603e-02,  8.4492e-03],\n                [-2.1000e-02, -9.2911e-03,  2.0279e-02],\n                [ 1.0343e-02,  1.5758e-03, -6.0820e-04]],\n      \n               [[-2.5336e-02, -1.6812e-03, -9.4485e-03],\n                [-2.8096e-02,  1.5084e-03, -6.6084e-03],\n                [-4.5757e-04,  2.7374e-02, -5.3844e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[-1.0841e-02, -5.9404e-04,  2.0785e-02],\n                [ 7.7300e-03, -4.5526e-03,  2.5959e-02],\n                [ 4.0430e-03, -2.8882e-02,  2.1951e-02]],\n      \n               [[-7.4355e-03,  1.9306e-03, -5.2278e-03],\n                [ 4.0908e-04,  1.3223e-02, -3.5028e-03],\n                [ 2.9378e-02,  1.3397e-02, -3.6627e-03]],\n      \n               [[-8.9210e-03,  2.5022e-02, -1.5623e-02],\n                [ 7.9989e-04, -5.5593e-03,  2.1766e-02],\n                [-2.8422e-02,  1.8808e-02,  5.6237e-03]],\n      \n               ...,\n      \n               [[-1.4293e-02, -1.6414e-02, -3.4218e-04],\n                [-2.7817e-02,  2.7641e-02,  1.6629e-02],\n                [-8.5955e-03,  7.0598e-03, -1.1661e-02]],\n      \n               [[-1.8577e-02, -2.3798e-02,  2.4354e-02],\n                [ 4.9568e-03, -1.2162e-02, -2.3109e-03],\n                [ 1.0367e-02,  2.9762e-03,  3.0569e-03]],\n      \n               [[-1.6131e-02, -4.4449e-03, -2.7671e-02],\n                [ 1.7955e-02, -7.9298e-03,  1.6964e-02],\n                [-1.4468e-02,  2.4140e-02,  2.5779e-02]]],\n      \n      \n              [[[-2.2318e-02,  1.6712e-02,  7.1651e-04],\n                [ 8.1410e-05, -2.4035e-02, -1.8638e-02],\n                [-1.6560e-02, -2.2763e-02,  1.5331e-02]],\n      \n               [[ 1.6120e-02, -7.8089e-03, -6.8348e-03],\n                [-1.4639e-02, -1.8873e-02,  8.5465e-03],\n                [ 7.9402e-03, -1.5536e-02,  1.6318e-02]],\n      \n               [[ 9.1864e-03, -2.1419e-02,  5.3658e-03],\n                [-9.0488e-03,  6.1737e-03,  8.5121e-03],\n                [ 1.4747e-02,  1.1306e-02, -2.0691e-02]],\n      \n               ...,\n      \n               [[ 2.5152e-02,  3.5438e-03, -6.4218e-03],\n                [ 2.5938e-02, -9.9393e-03, -1.9437e-02],\n                [-1.9855e-02,  1.6883e-02, -1.3603e-02]],\n      \n               [[-2.7060e-02, -4.1553e-03,  2.9438e-02],\n                [-1.5294e-02,  2.5862e-02, -2.1913e-02],\n                [-2.8806e-02,  2.9348e-02, -1.5021e-02]],\n      \n               [[ 4.4520e-03, -1.8724e-02, -2.9342e-02],\n                [ 1.4522e-02,  2.9406e-02, -4.8856e-03],\n                [-2.8208e-02, -6.1265e-03, -1.1726e-02]]],\n      \n      \n              [[[ 2.4966e-02, -1.0699e-02,  1.6451e-02],\n                [ 4.8301e-03,  5.8503e-03,  8.1802e-03],\n                [ 1.3157e-02, -1.1208e-02, -1.3821e-02]],\n      \n               [[ 7.0322e-03,  2.9378e-02, -2.1434e-02],\n                [ 2.7744e-02,  2.2934e-02, -1.2113e-02],\n                [-2.9110e-02, -2.4434e-02, -2.3179e-02]],\n      \n               [[ 2.8204e-02,  2.6259e-02, -2.0281e-02],\n                [-1.0350e-02,  1.8025e-02,  6.8317e-03],\n                [ 5.9796e-03,  2.5827e-02, -2.1565e-02]],\n      \n               ...,\n      \n               [[-1.3843e-02,  1.7736e-02,  1.1564e-02],\n                [-2.0218e-02,  2.6433e-02, -2.6807e-02],\n                [-7.7859e-03, -2.6185e-02,  9.1459e-03]],\n      \n               [[ 2.4251e-02,  1.8077e-02,  2.1433e-02],\n                [ 1.4518e-02,  1.0585e-02, -1.9315e-02],\n                [-1.5564e-02,  1.3102e-02, -1.6854e-02]],\n      \n               [[ 2.7038e-02, -3.7726e-03, -2.1733e-03],\n                [ 5.6751e-03,  1.9789e-02, -2.3875e-02],\n                [ 2.8103e-02, -2.8430e-02,  2.9405e-02]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0046,  0.0152,  0.0024,  0.0152,  0.0073, -0.0204, -0.0205,  0.0115,\n               0.0067,  0.0081, -0.0188,  0.0045,  0.0275,  0.0075,  0.0229,  0.0012,\n               0.0093, -0.0211, -0.0093,  0.0114,  0.0057,  0.0051,  0.0012, -0.0066,\n              -0.0022,  0.0220, -0.0105,  0.0044,  0.0071,  0.0070, -0.0118,  0.0111,\n              -0.0151, -0.0250, -0.0011, -0.0209, -0.0189, -0.0127,  0.0291,  0.0150,\n               0.0111, -0.0182,  0.0161,  0.0082,  0.0087, -0.0086,  0.0175,  0.0060,\n               0.0116, -0.0122, -0.0200, -0.0441,  0.0123, -0.0053,  0.0143, -0.0037,\n              -0.0150, -0.0233, -0.0150,  0.0018, -0.0209,  0.0006, -0.0110,  0.0156,\n               0.0229, -0.0417,  0.0469,  0.0276, -0.0036,  0.0085, -0.0136,  0.0124,\n              -0.0046, -0.0137, -0.0386,  0.0372, -0.0049, -0.0156, -0.0211,  0.0019,\n               0.0048,  0.0176,  0.0014, -0.0225,  0.0196,  0.0157,  0.0286,  0.0069,\n               0.0130,  0.0090, -0.0104, -0.0099,  0.0159,  0.0227,  0.0076, -0.0083,\n               0.0083,  0.0159, -0.0087,  0.0034, -0.0046, -0.0049, -0.0020,  0.0148,\n              -0.0065, -0.0172, -0.0042, -0.0083, -0.0131, -0.0262, -0.0063, -0.0098,\n              -0.0026,  0.0091, -0.0181,  0.0116,  0.0029, -0.0029, -0.0234,  0.0144,\n               0.0044, -0.0209,  0.0240,  0.0315, -0.0126, -0.0120,  0.0040,  0.0127,\n               0.0200,  0.0288,  0.0320, -0.0237,  0.0235, -0.0444,  0.0191,  0.0011,\n              -0.0031,  0.0036, -0.0065,  0.0088,  0.0256, -0.0017, -0.0066, -0.0195,\n              -0.0017,  0.0242, -0.0050, -0.0188, -0.0024, -0.0473,  0.0097,  0.0135,\n              -0.0250,  0.0254, -0.0050,  0.0104, -0.0092,  0.0141,  0.0024,  0.0079,\n              -0.0316,  0.0076, -0.0116, -0.0037, -0.0072, -0.0066,  0.0053,  0.0304,\n              -0.0110,  0.0136, -0.0041,  0.0441, -0.0024, -0.0072,  0.0048,  0.0002,\n              -0.0105, -0.0034,  0.0122, -0.0055,  0.0048, -0.0096,  0.0237, -0.0052,\n               0.0046,  0.0042,  0.0098,  0.0135, -0.0018,  0.0230,  0.0040,  0.0086,\n               0.0133,  0.0083,  0.0030,  0.0224,  0.0018, -0.0176,  0.0196,  0.0035,\n              -0.0297,  0.0001, -0.0127, -0.0143, -0.0026, -0.0045,  0.0075, -0.0036,\n              -0.0221,  0.0061,  0.0206, -0.0295, -0.0045,  0.0329,  0.0083, -0.0188,\n               0.0184, -0.0030, -0.0176,  0.0279,  0.0069,  0.0017, -0.0161, -0.0112,\n               0.0342,  0.0185, -0.0060, -0.0284,  0.0143, -0.0038,  0.0011, -0.0107,\n              -0.0140,  0.0065, -0.0489, -0.0353, -0.0195, -0.0097,  0.0137,  0.0360,\n              -0.0191,  0.0148, -0.0168, -0.0116,  0.0027,  0.0122,  0.0396,  0.0071,\n              -0.0108,  0.0086, -0.0084, -0.0206,  0.0141,  0.0142, -0.0366,  0.0245],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9112, 0.9120, 0.9098, 0.9133, 0.9109, 0.9118, 0.9114, 0.9211, 0.9116,\n              0.9087, 0.9164, 0.9109, 0.9129, 0.9094, 0.9133, 0.9111, 0.9143, 0.9106,\n              0.9170, 0.9183, 0.9175, 0.9119, 0.9147, 0.9155, 0.9092, 0.9125, 0.9128,\n              0.9123, 0.9107, 0.9154, 0.9119, 0.9099, 0.9143, 0.9117, 0.9138, 0.9151,\n              0.9188, 0.9149, 0.9177, 0.9116, 0.9131, 0.9099, 0.9119, 0.9093, 0.9178,\n              0.9128, 0.9125, 0.9167, 0.9125, 0.9112, 0.9106, 0.9113, 0.9107, 0.9157,\n              0.9172, 0.9104, 0.9117, 0.9178, 0.9130, 0.9113, 0.9132, 0.9104, 0.9170,\n              0.9183, 0.9113, 0.9114, 0.9149, 0.9102, 0.9095, 0.9178, 0.9123, 0.9107,\n              0.9104, 0.9084, 0.9135, 0.9127, 0.9135, 0.9093, 0.9166, 0.9157, 0.9102,\n              0.9095, 0.9103, 0.9090, 0.9109, 0.9109, 0.9084, 0.9117, 0.9101, 0.9121,\n              0.9111, 0.9128, 0.9121, 0.9165, 0.9148, 0.9146, 0.9108, 0.9224, 0.9098,\n              0.9195, 0.9117, 0.9101, 0.9170, 0.9156, 0.9178, 0.9104, 0.9108, 0.9126,\n              0.9106, 0.9205, 0.9098, 0.9102, 0.9108, 0.9097, 0.9107, 0.9101, 0.9109,\n              0.9115, 0.9126, 0.9172, 0.9095, 0.9189, 0.9113, 0.9157, 0.9129, 0.9104,\n              0.9153, 0.9127, 0.9136, 0.9127, 0.9119, 0.9181, 0.9133, 0.9140, 0.9127,\n              0.9089, 0.9108, 0.9121, 0.9166, 0.9113, 0.9122, 0.9114, 0.9078, 0.9186,\n              0.9107, 0.9131, 0.9377, 0.9135, 0.9137, 0.9107, 0.9112, 0.9184, 0.9112,\n              0.9173, 0.9103, 0.9119, 0.9121, 0.9176, 0.9161, 0.9154, 0.9112, 0.9105,\n              0.9096, 0.9125, 0.9118, 0.9130, 0.9133, 0.9191, 0.9135, 0.9115, 0.9129,\n              0.9167, 0.9107, 0.9127, 0.9145, 0.9121, 0.9137, 0.9113, 0.9166, 0.9132,\n              0.9163, 0.9101, 0.9119, 0.9120, 0.9110, 0.9141, 0.9097, 0.9132, 0.9124,\n              0.9121, 0.9112, 0.9167, 0.9139, 0.9222, 0.9092, 0.9121, 0.9116, 0.9173,\n              0.9140, 0.9158, 0.9329, 0.9093, 0.9114, 0.9145, 0.9106, 0.9129, 0.9091,\n              0.9100, 0.9163, 0.9125, 0.9123, 0.9092, 0.9114, 0.9136, 0.9107, 0.9146,\n              0.9191, 0.9118, 0.9141, 0.9115, 0.9107, 0.9109, 0.9129, 0.9183, 0.9096,\n              0.9095, 0.9129, 0.9232, 0.9110, 0.9156, 0.9123, 0.9145, 0.9135, 0.9136,\n              0.9160, 0.9203, 0.9121, 0.9125, 0.9096, 0.9140, 0.9108, 0.9106, 0.9116,\n              0.9181, 0.9135, 0.9233, 0.9093, 0.9151, 0.9154, 0.9101, 0.9127, 0.9092,\n              0.9118, 0.9150, 0.9099, 0.9117], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 6.2228e-04, -8.3396e-03, -1.3426e-02],\n                [ 1.5780e-02, -1.6113e-02, -3.9022e-03],\n                [ 1.8054e-02, -1.4752e-02, -1.9625e-02]],\n      \n               [[ 1.6504e-03,  1.4930e-02, -1.6988e-02],\n                [ 4.3278e-03,  1.8574e-02, -1.7948e-02],\n                [ 6.9688e-03,  1.5304e-02, -1.0641e-02]],\n      \n               [[ 1.0681e-02,  1.2331e-02,  1.4222e-02],\n                [-8.2341e-03,  1.5041e-02,  1.1415e-02],\n                [ 7.7368e-03,  4.9003e-03, -3.5839e-03]],\n      \n               ...,\n      \n               [[-5.8996e-03, -7.7363e-03,  5.7008e-03],\n                [ 1.9379e-02, -3.0427e-03, -1.1519e-02],\n                [-1.7326e-02, -3.5647e-03, -2.3725e-03]],\n      \n               [[ 2.0636e-02, -9.7804e-03,  4.3405e-03],\n                [-1.1011e-02,  1.2492e-02, -1.9769e-02],\n                [-1.7665e-03,  1.8546e-02,  1.2054e-02]],\n      \n               [[ 7.9049e-03, -4.2042e-03,  1.2590e-02],\n                [-7.2396e-03,  4.0139e-03,  7.6615e-03],\n                [ 6.7178e-03,  1.2194e-02, -1.4032e-02]]],\n      \n      \n              [[[-6.1276e-03,  1.8659e-02, -2.0831e-02],\n                [ 9.4903e-03, -1.4194e-02, -2.5059e-03],\n                [-1.3381e-03,  8.5762e-03, -1.2507e-02]],\n      \n               [[-1.8026e-02,  6.5729e-04, -6.7887e-04],\n                [ 1.5981e-02, -1.9042e-02,  1.5966e-02],\n                [ 1.2625e-02, -1.0281e-03,  1.9618e-04]],\n      \n               [[ 1.0428e-03, -1.8460e-02,  9.9846e-03],\n                [ 1.6313e-02, -1.3255e-02, -1.5701e-02],\n                [ 1.3618e-02, -1.3225e-02, -1.6396e-02]],\n      \n               ...,\n      \n               [[-1.7048e-02,  1.9612e-02, -4.0907e-03],\n                [ 1.6856e-02, -9.0334e-03,  3.5754e-03],\n                [ 5.1069e-04, -6.7636e-03, -6.4048e-03]],\n      \n               [[-1.6565e-02,  1.1084e-02,  1.3972e-02],\n                [ 1.2267e-02,  1.0058e-02,  1.8717e-03],\n                [-1.2060e-02,  1.0390e-02,  6.7576e-03]],\n      \n               [[-1.3776e-02,  1.3634e-02,  1.2409e-02],\n                [ 1.7060e-02,  2.0331e-02,  6.7952e-03],\n                [ 2.7314e-03,  1.1752e-02, -1.9917e-02]]],\n      \n      \n              [[[ 3.9592e-03, -1.0650e-02, -4.0536e-03],\n                [ 1.1424e-02, -6.4376e-03, -2.0540e-02],\n                [-1.6329e-02, -1.2727e-02, -1.7203e-02]],\n      \n               [[-1.5138e-03, -1.7942e-02, -1.3730e-02],\n                [ 1.7713e-02, -1.8618e-02,  2.0114e-02],\n                [-6.1389e-03, -8.6347e-03, -7.7795e-03]],\n      \n               [[-1.3007e-02,  1.3752e-02,  8.5185e-03],\n                [ 1.1609e-02, -1.9308e-02,  7.1686e-03],\n                [-9.5332e-03,  1.3363e-02, -2.4415e-04]],\n      \n               ...,\n      \n               [[-7.9696e-03, -4.4544e-04, -9.9579e-04],\n                [ 4.0769e-03,  6.4274e-03,  1.6729e-02],\n                [-1.0839e-02,  1.6849e-02, -1.8891e-02]],\n      \n               [[ 6.5391e-03, -1.7637e-02,  1.7568e-02],\n                [ 5.6613e-03, -7.5024e-03,  6.9658e-03],\n                [-9.1000e-03, -1.9919e-02, -1.6938e-02]],\n      \n               [[ 1.1533e-02, -1.0433e-02, -5.6903e-03],\n                [ 6.6587e-04,  6.3123e-03,  1.6867e-02],\n                [ 1.9380e-02, -8.4022e-03, -1.5012e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[-7.7673e-03,  1.7563e-02,  1.5111e-02],\n                [-1.4031e-02,  1.0327e-02,  1.7539e-02],\n                [ 8.3274e-03, -5.6335e-03, -2.0744e-02]],\n      \n               [[ 1.2342e-02, -1.9809e-02, -8.6773e-03],\n                [-1.1514e-02, -1.0360e-02,  6.0256e-03],\n                [ 4.9265e-03,  8.1694e-03,  1.0382e-02]],\n      \n               [[ 7.1747e-03,  1.6476e-02, -2.4542e-03],\n                [ 5.7143e-03,  1.5777e-02, -2.0220e-02],\n                [-6.4979e-03,  1.6164e-02,  6.1483e-03]],\n      \n               ...,\n      \n               [[-1.7413e-02,  1.1660e-02, -1.9110e-02],\n                [-7.0233e-03,  1.7710e-02, -1.7966e-02],\n                [-5.7369e-03,  2.0302e-02,  9.5510e-03]],\n      \n               [[-1.7664e-02,  9.1167e-03, -9.7475e-03],\n                [ 1.1103e-02, -1.6563e-02, -5.7386e-03],\n                [ 2.0362e-02,  8.4963e-03, -1.3059e-02]],\n      \n               [[-8.9455e-04, -1.5886e-02, -3.4667e-03],\n                [-6.2100e-03, -1.4685e-02,  1.6179e-02],\n                [-1.6239e-03, -1.9063e-02,  5.9505e-03]]],\n      \n      \n              [[[-5.7867e-03, -1.9959e-02, -3.3778e-03],\n                [-7.9911e-03, -1.6444e-02, -6.5381e-03],\n                [ 6.6424e-03,  1.7542e-02,  1.3005e-02]],\n      \n               [[-6.1492e-04, -6.8057e-03,  6.3059e-03],\n                [-6.7352e-03, -1.4667e-02,  8.7107e-03],\n                [ 1.8399e-02, -1.7721e-02,  5.3462e-04]],\n      \n               [[ 4.7147e-03, -1.9328e-02,  7.6161e-03],\n                [ 1.9162e-02,  1.7937e-02, -6.6133e-03],\n                [ 5.2080e-03, -1.7994e-02,  7.5189e-03]],\n      \n               ...,\n      \n               [[ 6.7939e-03, -1.2344e-02, -8.1138e-03],\n                [-1.0075e-02, -9.7004e-03,  1.2886e-02],\n                [ 1.4685e-03, -1.8440e-02,  7.0094e-03]],\n      \n               [[-2.6047e-03,  1.8621e-02,  4.9002e-03],\n                [-3.9535e-03,  1.2568e-03,  1.3657e-02],\n                [ 1.1567e-02,  6.3102e-03, -9.5630e-03]],\n      \n               [[ 2.0228e-02,  1.5334e-02,  1.6943e-02],\n                [ 1.0227e-02,  2.5601e-03, -5.7599e-03],\n                [-1.5734e-02, -1.7619e-02, -1.7064e-02]]],\n      \n      \n              [[[-1.5544e-02, -1.9603e-02, -1.5178e-02],\n                [-1.8017e-02, -2.0790e-02, -8.2332e-03],\n                [ 1.6478e-02,  1.4137e-02, -2.1124e-03]],\n      \n               [[ 1.5327e-02,  1.2500e-03,  9.2900e-03],\n                [ 1.2905e-02,  1.7520e-02,  7.9215e-03],\n                [-6.0466e-03,  5.4929e-03, -2.4224e-03]],\n      \n               [[-1.7504e-02,  1.9784e-02, -1.2690e-02],\n                [ 3.2348e-03, -6.4453e-04, -6.4784e-03],\n                [ 1.9093e-02,  1.8268e-02,  6.5116e-03]],\n      \n               ...,\n      \n               [[ 1.2539e-02,  5.4990e-05,  3.4583e-03],\n                [-2.0124e-02, -1.4816e-02, -6.8864e-03],\n                [ 1.4513e-02, -1.2983e-02,  1.7928e-02]],\n      \n               [[ 1.3336e-02,  4.5148e-03, -4.4219e-03],\n                [ 1.0580e-02, -1.8258e-02, -3.8496e-03],\n                [ 1.5852e-02, -2.6735e-03,  9.6596e-03]],\n      \n               [[ 4.5856e-03,  3.8294e-03,  1.0069e-02],\n                [ 2.8854e-04,  2.6520e-03,  2.0590e-02],\n                [-1.8156e-02,  7.3529e-03,  4.7351e-03]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 8.3107e-03,  1.0569e-02,  2.6325e-03, -1.1808e-02,  5.8060e-03,\n              -2.1235e-02,  3.1228e-03,  5.9707e-03,  1.4476e-02, -2.0414e-02,\n               1.1463e-02, -8.3462e-03, -2.2994e-02, -5.6048e-03, -3.0706e-04,\n               4.1708e-04,  4.2544e-03,  1.7163e-02,  1.0867e-02, -1.3439e-02,\n               1.6049e-02, -2.0584e-02,  1.9528e-02, -9.9220e-03,  3.8420e-03,\n              -4.2221e-03,  7.4937e-03, -2.5869e-03,  1.0476e-02, -9.3971e-04,\n               1.5235e-02,  8.6316e-04, -9.3228e-03, -9.7332e-03,  7.8413e-03,\n               1.7903e-02,  1.2051e-02,  4.7670e-03,  2.7802e-03, -5.3328e-03,\n              -6.8771e-03, -5.9048e-03, -3.5297e-03, -5.1897e-03,  7.2273e-03,\n               1.3929e-02, -2.3350e-04,  2.2141e-03,  1.2117e-03, -1.0133e-02,\n              -2.6115e-03, -2.2041e-02, -1.6181e-02,  1.0513e-02, -1.1434e-02,\n              -8.2448e-03, -4.4370e-03, -2.9347e-03, -5.5058e-03,  2.1582e-03,\n              -1.1682e-02,  1.8451e-02,  6.7487e-03, -1.3498e-02, -1.5655e-02,\n               7.6908e-03,  2.0366e-03,  2.6402e-02,  7.7936e-03,  4.3330e-03,\n              -1.3207e-02, -2.1862e-02,  5.1678e-03, -4.9793e-03, -3.4614e-03,\n               1.1316e-02, -3.1776e-03, -6.7688e-03,  2.3278e-03,  7.4247e-03,\n              -6.1559e-03,  2.6248e-03, -2.6505e-03,  3.8282e-03,  1.7791e-02,\n               1.1759e-02,  1.5535e-03, -6.0524e-05, -1.5639e-02,  4.8206e-04,\n              -1.5515e-02, -5.2551e-03,  5.0115e-03,  3.7675e-03,  4.3526e-03,\n               2.3302e-03,  9.1993e-03,  2.8788e-03, -5.6829e-03, -9.2595e-03,\n              -1.5397e-02,  1.0174e-02,  1.3827e-02,  1.7982e-02, -1.3187e-02,\n              -3.2704e-03, -9.7927e-03, -1.4690e-02,  1.2948e-02,  7.5588e-03,\n               4.5097e-04,  2.4071e-02,  1.0920e-02, -9.8007e-03,  1.8716e-02,\n               2.1382e-02,  1.7723e-02,  1.0194e-02,  2.3758e-03,  6.2731e-03,\n              -1.6133e-02,  4.6825e-03,  6.9857e-03,  4.6040e-03,  1.7525e-02,\n              -5.8807e-03, -1.6080e-04, -1.0662e-02, -1.9660e-02, -2.2246e-02,\n               7.1489e-03,  2.7669e-02,  1.0054e-02,  1.3505e-03,  1.8415e-04,\n               1.6652e-02,  1.3944e-02, -9.1798e-03, -1.2643e-02,  7.9248e-03,\n              -3.3628e-03,  7.3355e-03, -1.5472e-02,  6.7025e-03, -7.0777e-03,\n              -1.3739e-02,  1.5750e-03,  1.6802e-02, -1.1210e-02, -6.7174e-03,\n               8.5547e-03,  8.6604e-03,  6.9008e-03,  1.9835e-05,  2.3341e-03,\n              -7.1578e-03, -4.1727e-03,  2.1112e-02,  1.5071e-02,  1.0886e-02,\n              -2.1248e-04, -8.8838e-03,  1.3322e-02, -4.2937e-03,  5.3709e-03,\n               2.1171e-02, -2.0875e-03, -5.5486e-03, -9.2376e-03, -5.7042e-03,\n              -2.5262e-03, -6.9826e-03,  1.6098e-03, -2.2235e-02,  1.1798e-02,\n              -5.6978e-03, -1.2216e-02,  9.9766e-04, -2.7564e-03,  4.4751e-03,\n               2.7703e-02,  5.9071e-03, -4.8722e-04,  3.5678e-03, -5.8110e-03,\n              -1.3250e-02,  1.2931e-02,  9.6017e-04, -9.2924e-03,  1.3631e-02,\n              -6.7952e-03,  2.4158e-02, -6.2729e-03,  1.0663e-02, -9.1622e-03,\n               3.2014e-04,  1.7936e-02,  3.8657e-03, -1.1893e-02,  6.4919e-03,\n              -1.9009e-03, -5.6548e-03, -9.2131e-03, -6.0402e-03, -1.9109e-02,\n              -9.9341e-03,  1.5141e-02, -1.2129e-03,  5.5225e-03,  9.4406e-03,\n               7.0771e-04,  6.2162e-03, -3.4060e-03, -2.6389e-03,  2.6381e-03,\n               4.7455e-03,  9.4985e-03, -1.9247e-02, -8.6944e-03, -8.8022e-03,\n               1.3430e-02, -5.7159e-03, -7.9285e-03, -5.0134e-03,  1.6263e-02,\n               1.6053e-02,  2.6619e-02,  1.4512e-02,  1.5654e-02, -2.2048e-02,\n               2.1648e-02,  3.6330e-03,  7.3127e-03,  1.5320e-02, -1.0065e-03,\n               2.1151e-03,  3.2680e-03, -1.5600e-03,  7.8830e-03,  3.5791e-03,\n               5.7863e-03,  1.0198e-02,  9.6178e-03, -2.1042e-02, -6.0493e-03,\n              -2.9951e-03,  1.0936e-02,  4.5914e-03, -1.5289e-02,  1.6680e-02,\n              -8.8624e-03,  1.0028e-03,  1.3744e-02,  4.4281e-03, -1.6187e-02,\n              -1.2020e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9042, 0.9083, 0.9057, 0.9067, 0.9052, 0.9109, 0.9044, 0.9075, 0.9061,\n              0.9066, 0.9107, 0.9047, 0.9053, 0.9050, 0.9047, 0.9055, 0.9053, 0.9088,\n              0.9082, 0.9086, 0.9040, 0.9055, 0.9056, 0.9077, 0.9047, 0.9047, 0.9053,\n              0.9077, 0.9039, 0.9047, 0.9048, 0.9048, 0.9047, 0.9064, 0.9048, 0.9057,\n              0.9129, 0.9075, 0.9042, 0.9044, 0.9057, 0.9074, 0.9063, 0.9062, 0.9074,\n              0.9060, 0.9058, 0.9050, 0.9045, 0.9071, 0.9041, 0.9058, 0.9046, 0.9052,\n              0.9039, 0.9056, 0.9075, 0.9042, 0.9046, 0.9047, 0.9045, 0.9072, 0.9048,\n              0.9063, 0.9085, 0.9053, 0.9097, 0.9075, 0.9059, 0.9095, 0.9065, 0.9050,\n              0.9044, 0.9068, 0.9037, 0.9051, 0.9057, 0.9063, 0.9087, 0.9061, 0.9053,\n              0.9047, 0.9049, 0.9042, 0.9052, 0.9060, 0.9067, 0.9040, 0.9045, 0.9061,\n              0.9076, 0.9044, 0.9062, 0.9073, 0.9045, 0.9072, 0.9056, 0.9054, 0.9059,\n              0.9056, 0.9076, 0.9059, 0.9051, 0.9080, 0.9039, 0.9061, 0.9083, 0.9060,\n              0.9081, 0.9045, 0.9051, 0.9063, 0.9042, 0.9054, 0.9082, 0.9068, 0.9053,\n              0.9066, 0.9049, 0.9066, 0.9051, 0.9082, 0.9082, 0.9065, 0.9042, 0.9046,\n              0.9064, 0.9048, 0.9049, 0.9054, 0.9047, 0.9049, 0.9041, 0.9083, 0.9067,\n              0.9066, 0.9079, 0.9056, 0.9048, 0.9050, 0.9037, 0.9073, 0.9058, 0.9063,\n              0.9074, 0.9070, 0.9052, 0.9078, 0.9090, 0.9047, 0.9040, 0.9079, 0.9103,\n              0.9041, 0.9080, 0.9044, 0.9096, 0.9057, 0.9056, 0.9046, 0.9050, 0.9077,\n              0.9075, 0.9039, 0.9069, 0.9053, 0.9054, 0.9081, 0.9059, 0.9041, 0.9047,\n              0.9073, 0.9057, 0.9040, 0.9055, 0.9057, 0.9047, 0.9078, 0.9078, 0.9064,\n              0.9053, 0.9056, 0.9050, 0.9052, 0.9047, 0.9069, 0.9064, 0.9041, 0.9037,\n              0.9060, 0.9062, 0.9060, 0.9052, 0.9070, 0.9047, 0.9085, 0.9099, 0.9071,\n              0.9043, 0.9054, 0.9038, 0.9089, 0.9056, 0.9074, 0.9087, 0.9051, 0.9046,\n              0.9067, 0.9061, 0.9042, 0.9052, 0.9054, 0.9077, 0.9056, 0.9050, 0.9074,\n              0.9062, 0.9041, 0.9062, 0.9054, 0.9090, 0.9059, 0.9046, 0.9071, 0.9053,\n              0.9047, 0.9060, 0.9066, 0.9075, 0.9048, 0.9056, 0.9083, 0.9087, 0.9058,\n              0.9069, 0.9042, 0.9068, 0.9057, 0.9064, 0.9061, 0.9045, 0.9050, 0.9060,\n              0.9100, 0.9050, 0.9082, 0.9052, 0.9066, 0.9062, 0.9077, 0.9062, 0.9051,\n              0.9056, 0.9061, 0.9048, 0.9070], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[ 0.0027]],\n        \n                 [[-0.0036]],\n        \n                 [[-0.0670]],\n        \n                 ...,\n        \n                 [[-0.0117]],\n        \n                 [[-0.0276]],\n        \n                 [[-0.0356]]],\n        \n        \n                [[[ 0.0723]],\n        \n                 [[ 0.0416]],\n        \n                 [[-0.0589]],\n        \n                 ...,\n        \n                 [[ 0.0451]],\n        \n                 [[-0.0826]],\n        \n                 [[ 0.0529]]],\n        \n        \n                [[[-0.0699]],\n        \n                 [[-0.0295]],\n        \n                 [[ 0.0309]],\n        \n                 ...,\n        \n                 [[-0.0880]],\n        \n                 [[ 0.0113]],\n        \n                 [[ 0.0561]]],\n        \n        \n                ...,\n        \n        \n                [[[-0.0355]],\n        \n                 [[ 0.0103]],\n        \n                 [[ 0.0877]],\n        \n                 ...,\n        \n                 [[-0.0038]],\n        \n                 [[-0.0254]],\n        \n                 [[ 0.0357]]],\n        \n        \n                [[[ 0.0125]],\n        \n                 [[-0.0404]],\n        \n                 [[ 0.0221]],\n        \n                 ...,\n        \n                 [[ 0.0030]],\n        \n                 [[ 0.0497]],\n        \n                 [[ 0.0364]]],\n        \n        \n                [[[-0.0456]],\n        \n                 [[-0.0343]],\n        \n                 [[ 0.0459]],\n        \n                 ...,\n        \n                 [[-0.0046]],\n        \n                 [[-0.0835]],\n        \n                 [[-0.0774]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n               requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 1.4856e-02, -7.6803e-03, -6.8650e-03, -1.3906e-02, -1.8712e-02,\n                -3.0382e-02, -1.0714e-02, -3.1259e-02,  3.7687e-02,  1.6849e-02,\n                -4.6185e-02, -4.0096e-03,  1.6023e-02, -3.3187e-02,  3.7040e-02,\n                 1.6396e-02, -5.0095e-04, -1.6503e-02, -1.0914e-02, -2.7566e-02,\n                -3.1574e-02,  7.6690e-02,  6.7551e-03, -2.1945e-03,  5.3553e-03,\n                 2.7424e-02, -1.3304e-02, -6.1748e-02,  4.3375e-02, -1.6950e-02,\n                 5.6808e-02,  9.3894e-03, -3.2178e-02,  7.8484e-03, -3.2857e-03,\n                -7.7728e-03,  6.1674e-02,  2.9564e-02, -3.4976e-02, -1.0593e-02,\n                 6.5000e-03,  9.6293e-03, -1.0622e-02, -4.7432e-02,  1.3573e-03,\n                -1.3363e-02, -2.5992e-02,  3.1653e-02, -2.8860e-02,  3.7688e-02,\n                -7.7512e-03,  9.1037e-03, -2.4381e-02,  7.9254e-03,  1.9157e-02,\n                -1.6352e-02,  3.8435e-02,  5.9365e-02,  2.6555e-02,  5.2303e-02,\n                -3.7764e-02, -4.2346e-02, -2.9918e-02,  6.9121e-02, -8.3728e-02,\n                 3.1485e-03,  2.4827e-03, -1.5956e-02,  2.6218e-02,  3.1897e-02,\n                -5.9727e-03, -6.1917e-03,  6.6217e-03, -1.4255e-02, -6.8259e-03,\n                -2.6612e-02, -2.0028e-02, -2.5102e-03, -3.6397e-02,  1.2958e-02,\n                 6.0733e-03,  2.3088e-02, -5.6520e-03,  6.5173e-02,  2.8840e-02,\n                -1.3995e-02, -1.2652e-02, -8.9079e-04, -2.9229e-02,  4.4145e-03,\n                -2.2511e-03,  1.1936e-02, -2.6790e-02, -2.2128e-04,  2.4637e-02,\n                -4.6020e-02, -3.1213e-02,  8.1301e-03, -4.0516e-02, -2.9172e-02,\n                -8.6599e-03,  2.6754e-02, -7.1313e-03,  2.9445e-02,  5.3642e-02,\n                 5.4059e-02, -6.3253e-05,  1.1012e-02, -4.5221e-02, -6.7124e-05,\n                 1.4268e-02,  2.5339e-02,  1.5793e-02, -4.5670e-02,  5.5040e-02,\n                -3.8280e-02,  2.9857e-02, -2.1729e-02,  1.9190e-02,  1.8718e-02,\n                -5.4055e-02, -7.0026e-03, -3.6856e-03, -4.3850e-02,  8.3780e-03,\n                -1.5885e-02, -1.5212e-02,  3.9870e-03,  9.5086e-03,  2.5190e-02,\n                -6.5446e-02,  2.7007e-02,  5.4662e-02,  4.6147e-03, -4.5968e-02,\n                -7.9452e-04, -4.5590e-02,  1.8138e-02,  4.1866e-02, -2.2742e-02,\n                 5.2535e-03, -2.8677e-02,  2.4794e-02,  5.9382e-03,  2.9355e-02,\n                -2.5094e-02, -4.4774e-02, -1.1645e-02,  1.8707e-02,  5.1141e-02,\n                 1.1080e-03,  1.4949e-02, -2.7627e-02,  3.4044e-02,  1.6484e-02,\n                 4.7363e-02,  1.0209e-03,  2.5951e-02,  9.6028e-03, -1.2238e-02,\n                 1.2124e-03,  2.8260e-05, -7.9999e-03, -8.7073e-03, -3.6466e-03,\n                -2.9631e-02,  1.9768e-02,  7.5420e-04,  3.7003e-03,  5.3143e-02,\n                -2.5486e-03, -1.2591e-02, -3.1096e-02, -8.5421e-03, -3.6897e-03,\n                -9.5562e-03,  1.3805e-02, -8.0634e-03,  9.8275e-03,  5.2887e-02,\n                 4.1059e-02,  2.2240e-03, -4.9121e-02,  7.7395e-03,  2.7641e-02,\n                -3.2507e-02, -1.4555e-02, -3.0550e-02,  1.9133e-02,  8.4336e-03,\n                 6.2036e-02, -2.5738e-02, -4.4744e-02, -3.5223e-02, -3.5117e-02,\n                -2.7212e-02, -3.1969e-02,  8.0891e-03,  2.1925e-02,  1.6835e-02,\n                -2.4531e-02,  9.8265e-03, -1.4199e-02, -9.3629e-03,  4.8293e-02,\n                -3.0598e-02,  3.3146e-03,  2.3519e-02,  5.9714e-03, -8.4658e-03,\n                -3.2617e-02,  2.2030e-02, -3.7804e-02,  9.4658e-04,  1.2030e-02,\n                -2.9703e-03,  2.5639e-02,  1.2100e-02,  3.6852e-03, -3.8632e-02,\n                 2.5226e-02,  6.9691e-03,  4.2702e-03,  1.1744e-03, -9.3132e-03,\n                -1.9313e-02, -1.4670e-02, -9.3944e-04, -2.6197e-02,  1.1821e-02,\n                -3.1387e-02, -7.4187e-02,  5.3170e-02,  5.2001e-02, -8.2414e-03,\n                -3.2540e-03, -1.5440e-03,  5.2217e-02, -4.2772e-02, -2.4889e-02,\n                 5.9218e-03, -1.4225e-02,  4.6312e-02,  3.1855e-02, -2.3996e-02,\n                 4.7987e-02,  5.0405e-02,  3.4835e-02,  2.5059e-02, -3.5695e-02,\n                -1.8990e-03, -8.4991e-02,  3.5509e-02, -1.8988e-03,  6.8869e-04,\n                -6.1836e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9195, 0.9172, 0.9190, 0.9167, 0.9157, 0.9172, 0.9158, 0.9201, 0.9198,\n                0.9186, 0.9209, 0.9172, 0.9155, 0.9165, 0.9179, 0.9151, 0.9262, 0.9174,\n                0.9130, 0.9130, 0.9169, 0.9191, 0.9212, 0.9147, 0.9301, 0.9160, 0.9144,\n                0.9274, 0.9131, 0.9246, 0.9206, 0.9238, 0.9377, 0.9161, 0.9159, 0.9181,\n                0.9206, 0.9183, 0.9199, 0.9195, 0.9270, 0.9135, 0.9157, 0.9200, 0.9145,\n                0.9151, 0.9188, 0.9190, 0.9163, 0.9200, 0.9133, 0.9158, 0.9154, 0.9193,\n                0.9219, 0.9270, 0.9214, 0.9187, 0.9162, 0.9238, 0.9202, 0.9181, 0.9169,\n                0.9154, 0.9182, 0.9246, 0.9149, 0.9215, 0.9243, 0.9158, 0.9242, 0.9222,\n                0.9174, 0.9191, 0.9247, 0.9227, 0.9297, 0.9399, 0.9196, 0.9133, 0.9149,\n                0.9189, 0.9253, 0.9132, 0.9215, 0.9300, 0.9157, 0.9157, 0.9175, 0.9181,\n                0.9165, 0.9136, 0.9169, 0.9236, 0.9228, 0.9149, 0.9109, 0.9191, 0.9147,\n                0.9208, 0.9249, 0.9252, 0.9219, 0.9323, 0.9189, 0.9161, 0.9247, 0.9175,\n                0.9194, 0.9347, 0.9195, 0.9162, 0.9196, 0.9180, 0.9182, 0.9161, 0.9127,\n                0.9180, 0.9305, 0.9236, 0.9125, 0.9238, 0.9234, 0.9157, 0.9198, 0.9169,\n                0.9217, 0.9223, 0.9139, 0.9150, 0.9192, 0.9155, 0.9227, 0.9192, 0.9180,\n                0.9215, 0.9284, 0.9103, 0.9254, 0.9201, 0.9176, 0.9170, 0.9241, 0.9136,\n                0.9265, 0.9173, 0.9168, 0.9177, 0.9243, 0.9155, 0.9169, 0.9195, 0.9174,\n                0.9142, 0.9159, 0.9177, 0.9196, 0.9239, 0.9209, 0.9276, 0.9209, 0.9214,\n                0.9122, 0.9177, 0.9189, 0.9183, 0.9200, 0.9207, 0.9175, 0.9178, 0.9128,\n                0.9148, 0.9193, 0.9207, 0.9218, 0.9159, 0.9175, 0.9144, 0.9212, 0.9124,\n                0.9248, 0.9201, 0.9128, 0.9135, 0.9460, 0.9127, 0.9195, 0.9128, 0.9270,\n                0.9201, 0.9174, 0.9208, 0.9179, 0.9143, 0.9220, 0.9189, 0.9204, 0.9271,\n                0.9196, 0.9197, 0.9295, 0.9156, 0.9148, 0.9209, 0.9114, 0.9196, 0.9148,\n                0.9218, 0.9171, 0.9319, 0.9299, 0.9179, 0.9140, 0.9219, 0.9225, 0.9164,\n                0.9261, 0.9192, 0.9190, 0.9200, 0.9223, 0.9217, 0.9147, 0.9309, 0.9141,\n                0.9249, 0.9231, 0.9166, 0.9153, 0.9223, 0.9291, 0.9121, 0.9152, 0.9174,\n                0.9308, 0.9218, 0.9131, 0.9271, 0.9241, 0.9171, 0.9171, 0.9135, 0.9166,\n                0.9202, 0.9191, 0.9232, 0.9361, 0.9235, 0.9371, 0.9248, 0.9150, 0.9130,\n                0.9181, 0.9240, 0.9196, 0.9177], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [256, 128, 3, 3]], ["0.bn1.weight", [256]], ["0.bn1.bias", [256]], ["0.conv2.weight", [256, 256, 3, 3]], ["0.bn2.weight", [256]], ["0.bn2.bias", [256]], ["0.downsample.0.weight", [256, 128, 1, 1]], ["0.downsample.1.weight", [256]], ["0.downsample.1.bias", [256]]], "output_shape": [[512, 256, 2, 2]], "num_parameters": [294912, 256, 256, 589824, 256, 256, 32768, 256, 256]}, {"name": "layer4", "id": 140608652874848, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 2.0653e-02,  8.2720e-03,  4.1467e-03],\n                [ 2.8721e-03, -8.1781e-03,  5.4384e-03],\n                [-1.1363e-02,  5.8303e-04, -7.3082e-03]],\n      \n               [[ 1.7753e-03,  3.3799e-03,  1.8749e-02],\n                [ 5.4572e-03, -1.5539e-02, -1.7819e-03],\n                [-5.0239e-03,  1.4582e-02, -8.2150e-04]],\n      \n               [[ 1.7973e-02,  7.0909e-03,  5.2326e-04],\n                [-1.6633e-02,  1.9171e-02, -1.1564e-02],\n                [ 1.6228e-03,  1.9967e-02,  2.0008e-02]],\n      \n               ...,\n      \n               [[ 8.8120e-03, -9.4185e-03,  1.5465e-02],\n                [-5.4130e-03, -1.2654e-02, -8.9880e-03],\n                [ 7.1707e-03,  3.3044e-03,  7.6814e-03]],\n      \n               [[ 6.5538e-03,  1.0992e-02, -2.0065e-02],\n                [ 1.1569e-02, -9.9693e-03,  8.4008e-03],\n                [-5.7475e-03,  1.9260e-02,  5.6012e-04]],\n      \n               [[-1.7984e-02, -1.2255e-02,  8.7272e-03],\n                [ 1.4544e-02,  1.7778e-02, -3.1849e-03],\n                [-1.7417e-02, -1.2895e-02, -1.6277e-02]]],\n      \n      \n              [[[-1.6248e-02,  9.7767e-03,  1.3469e-02],\n                [ 3.4630e-03,  2.9298e-03,  4.0733e-03],\n                [ 8.2640e-03, -1.6521e-02, -9.4791e-03]],\n      \n               [[ 1.1935e-02,  1.2004e-02, -1.3245e-02],\n                [-8.8794e-03,  1.1511e-02, -1.5029e-02],\n                [ 4.0456e-03, -1.4366e-02,  1.7598e-02]],\n      \n               [[-1.2530e-02, -3.3509e-03,  7.1421e-03],\n                [ 2.0810e-02, -3.0151e-03, -2.0020e-02],\n                [-1.3464e-02,  4.1544e-03,  1.4076e-02]],\n      \n               ...,\n      \n               [[ 3.1968e-03,  3.8363e-03, -4.4037e-03],\n                [ 1.2281e-02, -2.0310e-02, -7.4887e-03],\n                [ 1.2376e-02,  9.5524e-03,  8.3504e-03]],\n      \n               [[-1.8163e-02,  4.2525e-03, -9.3868e-03],\n                [-8.9386e-03, -1.2589e-02, -6.6715e-04],\n                [-2.2345e-04,  2.6770e-03, -8.5243e-03]],\n      \n               [[-2.3790e-03, -3.7716e-03,  1.9045e-03],\n                [-1.4791e-03, -1.2587e-02, -1.4575e-02],\n                [-6.5046e-03, -2.0237e-02, -6.7642e-03]]],\n      \n      \n              [[[ 1.1637e-03, -1.6054e-02,  5.8320e-03],\n                [-8.8455e-03, -1.6743e-02, -1.2320e-02],\n                [-1.6745e-02, -8.8253e-03,  1.3863e-02]],\n      \n               [[ 1.3761e-03, -1.0985e-02, -1.8836e-02],\n                [-1.6329e-02,  1.1781e-03,  1.3262e-02],\n                [ 9.6924e-03, -8.2389e-03, -4.5577e-03]],\n      \n               [[ 2.0322e-02, -1.4938e-02, -9.3363e-03],\n                [-4.7948e-03,  1.6911e-02,  7.4817e-03],\n                [-1.2536e-03,  1.1491e-02, -1.3768e-02]],\n      \n               ...,\n      \n               [[-1.6024e-02, -8.3447e-03, -1.9233e-02],\n                [ 1.8272e-02,  1.5738e-02, -1.5012e-02],\n                [-1.2227e-02, -1.2152e-02, -1.7680e-03]],\n      \n               [[-2.4179e-03, -2.9268e-03, -1.1733e-02],\n                [-1.7580e-02,  1.1729e-02,  1.1488e-02],\n                [ 2.5536e-03, -1.8482e-02,  4.3427e-03]],\n      \n               [[-1.0575e-02,  2.1276e-04, -1.5357e-02],\n                [-1.4205e-02,  1.1575e-02,  7.3123e-03],\n                [ 1.5055e-02,  1.4552e-02, -1.9744e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[ 1.8185e-02,  3.4584e-04,  8.7842e-03],\n                [ 4.4784e-03, -1.0558e-02,  1.3505e-02],\n                [ 1.0875e-02, -1.5152e-02,  6.3368e-04]],\n      \n               [[ 2.0774e-02, -1.9148e-02,  1.6988e-02],\n                [-8.8286e-03, -3.3762e-03,  6.1912e-03],\n                [ 2.0182e-02, -2.2128e-03, -3.3660e-03]],\n      \n               [[ 1.4781e-02,  8.9250e-04,  1.9582e-02],\n                [-1.9727e-02,  1.5084e-02,  1.5648e-03],\n                [-1.1069e-05,  1.4316e-02, -2.2278e-03]],\n      \n               ...,\n      \n               [[ 1.8717e-02, -2.3863e-03,  1.3207e-02],\n                [ 1.0863e-02,  1.4227e-02,  1.8274e-02],\n                [-6.1320e-03,  7.2598e-05,  3.0881e-03]],\n      \n               [[-9.0769e-03, -1.7447e-03,  1.1018e-03],\n                [-7.9683e-03, -8.5211e-03,  1.1705e-02],\n                [ 1.1665e-02,  1.4625e-02, -1.9184e-02]],\n      \n               [[ 1.9217e-02,  1.5097e-02, -1.6498e-02],\n                [-1.2496e-03, -1.6569e-02, -1.7454e-02],\n                [ 2.0027e-02,  1.8900e-03,  3.9196e-03]]],\n      \n      \n              [[[-9.0907e-03, -1.4174e-03,  1.9597e-02],\n                [-3.3159e-03, -3.3273e-03, -1.5292e-02],\n                [-7.7829e-03, -4.0466e-03, -8.9161e-03]],\n      \n               [[ 8.8289e-03, -6.9680e-03,  4.0549e-03],\n                [ 2.0586e-02, -2.1899e-03,  1.9606e-02],\n                [-6.9722e-03,  1.7084e-02, -6.0030e-03]],\n      \n               [[-1.3551e-02,  1.9860e-02, -7.3912e-03],\n                [-9.7554e-03,  1.9723e-02,  1.9673e-02],\n                [-2.6305e-03, -1.1326e-03, -7.4962e-03]],\n      \n               ...,\n      \n               [[-4.8109e-03,  3.5353e-03,  5.9913e-03],\n                [ 5.9498e-03,  8.4831e-03, -1.0062e-03],\n                [-1.3174e-02, -1.2666e-02, -9.6952e-04]],\n      \n               [[-1.8591e-03, -4.7924e-04,  1.9549e-02],\n                [ 1.6692e-04, -1.7519e-02,  1.2943e-02],\n                [-2.0178e-02, -1.7111e-02, -7.9370e-03]],\n      \n               [[-6.2162e-03, -6.5692e-03,  1.0125e-02],\n                [ 1.5015e-02,  5.9357e-03, -1.2207e-03],\n                [-1.3098e-02, -1.3064e-02,  1.2144e-02]]],\n      \n      \n              [[[-1.6708e-02,  1.6484e-02, -5.9341e-03],\n                [ 1.4972e-02, -8.3240e-03, -6.5971e-03],\n                [-7.3252e-03,  1.8432e-02,  1.8174e-02]],\n      \n               [[ 5.5903e-03, -1.7559e-02, -9.1810e-03],\n                [ 1.8953e-02,  1.5439e-02,  1.2881e-02],\n                [ 5.4122e-03, -1.8352e-03, -1.8570e-02]],\n      \n               [[ 1.2645e-02, -1.6772e-02, -6.7327e-04],\n                [ 5.4515e-03, -1.8647e-02, -1.4292e-02],\n                [ 9.8039e-03, -1.9583e-02,  1.3023e-02]],\n      \n               ...,\n      \n               [[ 1.7031e-02,  2.0291e-02,  1.9091e-02],\n                [ 1.5716e-02, -6.4228e-03, -1.2901e-02],\n                [ 9.1023e-03,  5.0037e-03,  1.8587e-02]],\n      \n               [[-9.5129e-03, -1.9300e-02,  1.0863e-02],\n                [-2.5020e-03, -1.9863e-03, -7.0871e-03],\n                [ 1.9322e-02, -8.4727e-03,  1.1996e-02]],\n      \n               [[ 1.8185e-02, -1.3005e-02,  1.8257e-02],\n                [ 9.0578e-03,  1.1191e-02, -1.2059e-02],\n                [ 6.2287e-03,  1.4726e-02,  8.5161e-03]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0240, -0.0173,  0.0292,  0.0484, -0.0329,  0.0066,  0.0016,  0.0442,\n              -0.0056,  0.0297, -0.0300, -0.0263, -0.0187,  0.0020, -0.0511,  0.0019,\n              -0.0297,  0.0407, -0.0413,  0.0048, -0.0019, -0.0067, -0.0201, -0.0201,\n              -0.0253, -0.0361, -0.0043, -0.0353, -0.0197,  0.0315, -0.0247,  0.0271,\n              -0.0273,  0.0021,  0.0210, -0.0105,  0.0408,  0.0049, -0.0041, -0.0345,\n               0.0195, -0.0325, -0.0387,  0.0026, -0.0071, -0.0007, -0.0005, -0.0210,\n              -0.0241, -0.0414, -0.0149,  0.0025, -0.0013, -0.0037,  0.0316,  0.0084,\n              -0.0286,  0.0228,  0.0211,  0.0033, -0.0169, -0.0042,  0.0314, -0.0328,\n               0.0006,  0.0217,  0.0223,  0.0567, -0.0006, -0.0232, -0.0109, -0.0071,\n              -0.0096, -0.0015,  0.0049, -0.0624, -0.0356, -0.0181, -0.0115,  0.0022,\n              -0.0056,  0.0094, -0.0107, -0.0098, -0.0136,  0.0104, -0.0267, -0.0310,\n              -0.0080, -0.0384, -0.0220, -0.0391, -0.0281,  0.0049, -0.0183,  0.0149,\n               0.0133,  0.0024, -0.0018, -0.0084, -0.0125,  0.0053, -0.0193, -0.0153,\n              -0.0022,  0.0078, -0.0290,  0.0057, -0.0324, -0.0018,  0.0237, -0.0051,\n               0.0149, -0.0218,  0.0564,  0.0338,  0.0239, -0.0099, -0.0098, -0.0395,\n               0.0224, -0.0644, -0.0073,  0.0593,  0.0095, -0.0269,  0.0172, -0.0358,\n              -0.0145,  0.0180,  0.0204,  0.0041,  0.0154,  0.0184, -0.0278,  0.0249,\n              -0.0526,  0.0102, -0.0128, -0.0542, -0.0065, -0.0347,  0.0323,  0.0006,\n               0.0096,  0.0299, -0.0337, -0.0445, -0.0214,  0.0285, -0.0004, -0.0228,\n               0.0114,  0.0001, -0.0265,  0.0335, -0.0168,  0.0443, -0.0219, -0.0065,\n               0.0075, -0.0191, -0.0063, -0.0003, -0.0239, -0.0204, -0.0100, -0.0006,\n               0.0076,  0.0015,  0.0267,  0.0342,  0.0069, -0.0053, -0.0151,  0.0084,\n              -0.0307,  0.0355,  0.0221, -0.0031, -0.0184, -0.0272, -0.0217, -0.0321,\n              -0.0212,  0.0158,  0.0163, -0.0388,  0.0196, -0.0002, -0.0417, -0.0637,\n              -0.0165, -0.0261, -0.0246,  0.0191,  0.0116, -0.0220,  0.0002,  0.0427,\n              -0.0080, -0.0014,  0.0077, -0.0045, -0.0067, -0.0542,  0.0397,  0.0148,\n              -0.0095,  0.0231,  0.0282, -0.0009,  0.0150,  0.0066,  0.0299,  0.0414,\n              -0.0280,  0.0179,  0.0124, -0.0329, -0.0569,  0.0091, -0.0028, -0.0178,\n              -0.0146,  0.0194,  0.0029,  0.0323,  0.0163, -0.0033, -0.0029,  0.0139,\n               0.0223,  0.0205,  0.0376,  0.0244,  0.0229,  0.0162,  0.0022,  0.0228,\n               0.0107,  0.0182,  0.0208, -0.0170, -0.0346, -0.0064,  0.0192, -0.0045,\n               0.0489, -0.0268, -0.0125,  0.0126,  0.0168,  0.0265, -0.0177,  0.0047,\n               0.0303,  0.0186, -0.0337, -0.0008,  0.0068,  0.0070,  0.0004, -0.0147,\n              -0.0244, -0.0069,  0.0072,  0.0593, -0.0046, -0.0040, -0.0070,  0.0135,\n               0.0119, -0.0064, -0.0169, -0.0025, -0.0240,  0.0188,  0.0215,  0.0248,\n               0.0013,  0.0132, -0.0342,  0.0323, -0.0455,  0.0035, -0.0750, -0.0127,\n               0.0205, -0.0489, -0.0133, -0.0389, -0.0233, -0.0214,  0.0484, -0.0131,\n               0.0250,  0.0047, -0.0168, -0.0066,  0.0411,  0.0361,  0.0787,  0.0113,\n               0.0018,  0.0174,  0.0121,  0.0391,  0.0104,  0.0190,  0.0392, -0.0318,\n              -0.0188, -0.0079, -0.0095, -0.0214,  0.0229, -0.0399, -0.0228, -0.0218,\n               0.0163,  0.0521,  0.0437, -0.0263,  0.0140,  0.0181, -0.0438, -0.0374,\n              -0.0157, -0.0076,  0.0013, -0.0011, -0.0294, -0.0071, -0.0182,  0.0137,\n              -0.0303,  0.0011,  0.0177,  0.0058,  0.0205, -0.0055,  0.0083, -0.0200,\n              -0.0113,  0.0140, -0.0117, -0.0114, -0.0240, -0.0110,  0.0200, -0.0062,\n              -0.0167,  0.0501,  0.0304,  0.0006,  0.0433,  0.0264, -0.0006, -0.0308,\n               0.0058,  0.0282, -0.0077,  0.0148,  0.0071, -0.0005, -0.0154, -0.0625,\n              -0.0176, -0.0232, -0.0314, -0.0110, -0.0012, -0.0217,  0.0159,  0.0394,\n               0.0168,  0.0870,  0.0404, -0.0019,  0.0144,  0.0047, -0.0311,  0.0359,\n               0.0507, -0.0220, -0.0075, -0.0300,  0.0054,  0.0543,  0.0397,  0.0144,\n               0.0109,  0.0120,  0.0198, -0.0204, -0.0019,  0.0155, -0.0133,  0.0135,\n              -0.0180, -0.0545, -0.0091,  0.0087,  0.0306, -0.0123,  0.0063, -0.0073,\n              -0.0221,  0.0653, -0.0220,  0.0394, -0.0093, -0.0239, -0.0005,  0.0111,\n               0.0260, -0.0007, -0.0101,  0.0213,  0.0400,  0.0181,  0.0035,  0.0055,\n              -0.0364, -0.0265, -0.0331,  0.0052,  0.0099, -0.0048, -0.0462,  0.0067,\n              -0.0049,  0.0240, -0.0324,  0.0430, -0.0069,  0.0073, -0.0128, -0.0352,\n               0.0342,  0.0045, -0.0176,  0.0031, -0.0218, -0.0270, -0.0180,  0.0075,\n              -0.0151, -0.0432, -0.0221,  0.0296,  0.0376, -0.0068,  0.0404,  0.0070,\n               0.0259, -0.0156, -0.0043,  0.0056, -0.0009, -0.0083, -0.0066,  0.0155,\n              -0.0144,  0.0179, -0.0167,  0.0158,  0.0067,  0.0129, -0.0051,  0.0236,\n               0.0420, -0.0198, -0.0172, -0.0579, -0.0043,  0.0116, -0.0249,  0.0064,\n               0.0289,  0.0108,  0.0003, -0.0051, -0.0402, -0.0241,  0.0012, -0.0188,\n               0.0013, -0.0193, -0.0253,  0.0506,  0.0210, -0.0048,  0.0119, -0.0120,\n              -0.0035, -0.0403, -0.0093,  0.0057,  0.0035,  0.0144,  0.0517,  0.0021,\n               0.0216,  0.0078,  0.0437, -0.0411,  0.0471, -0.0229, -0.0486,  0.0100],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9089, 0.9121, 0.9071, 0.9070, 0.9061, 0.9077, 0.9069, 0.9089, 0.9072,\n              0.9085, 0.9117, 0.9096, 0.9099, 0.9119, 0.9144, 0.9092, 0.9077, 0.9084,\n              0.9105, 0.9063, 0.9064, 0.9090, 0.9095, 0.9069, 0.9064, 0.9108, 0.9078,\n              0.9086, 0.9112, 0.9072, 0.9106, 0.9071, 0.9079, 0.9060, 0.9082, 0.9070,\n              0.9075, 0.9065, 0.9072, 0.9096, 0.9066, 0.9096, 0.9077, 0.9079, 0.9104,\n              0.9068, 0.9085, 0.9079, 0.9071, 0.9107, 0.9071, 0.9066, 0.9066, 0.9079,\n              0.9135, 0.9081, 0.9102, 0.9084, 0.9074, 0.9076, 0.9067, 0.9052, 0.9114,\n              0.9106, 0.9080, 0.9083, 0.9079, 0.9119, 0.9075, 0.9074, 0.9079, 0.9069,\n              0.9070, 0.9084, 0.9073, 0.9143, 0.9130, 0.9075, 0.9075, 0.9070, 0.9080,\n              0.9086, 0.9085, 0.9077, 0.9081, 0.9085, 0.9055, 0.9118, 0.9101, 0.9129,\n              0.9077, 0.9094, 0.9078, 0.9068, 0.9077, 0.9068, 0.9074, 0.9096, 0.9090,\n              0.9070, 0.9071, 0.9076, 0.9068, 0.9086, 0.9075, 0.9098, 0.9077, 0.9109,\n              0.9070, 0.9070, 0.9083, 0.9076, 0.9075, 0.9095, 0.9124, 0.9073, 0.9197,\n              0.9086, 0.9097, 0.9084, 0.9113, 0.9099, 0.9076, 0.9071, 0.9087, 0.9069,\n              0.9073, 0.9079, 0.9073, 0.9066, 0.9079, 0.9087, 0.9063, 0.9079, 0.9093,\n              0.9062, 0.9097, 0.9075, 0.9071, 0.9094, 0.9093, 0.9079, 0.9077, 0.9076,\n              0.9074, 0.9073, 0.9077, 0.9064, 0.9095, 0.9114, 0.9098, 0.9092, 0.9065,\n              0.9100, 0.9090, 0.9081, 0.9079, 0.9056, 0.9077, 0.9105, 0.9076, 0.9082,\n              0.9069, 0.9081, 0.9090, 0.9077, 0.9087, 0.9073, 0.9077, 0.9118, 0.9091,\n              0.9089, 0.9080, 0.9082, 0.9083, 0.9077, 0.9119, 0.9087, 0.9122, 0.9085,\n              0.9076, 0.9105, 0.9140, 0.9086, 0.9079, 0.9082, 0.9073, 0.9085, 0.9068,\n              0.9066, 0.9093, 0.9116, 0.9095, 0.9074, 0.9085, 0.9094, 0.9067, 0.9107,\n              0.9083, 0.9076, 0.9063, 0.9115, 0.9086, 0.9093, 0.9076, 0.9085, 0.9086,\n              0.9060, 0.9081, 0.9076, 0.9096, 0.9068, 0.9069, 0.9084, 0.9108, 0.9122,\n              0.9096, 0.9075, 0.9075, 0.9106, 0.9099, 0.9088, 0.9066, 0.9092, 0.9102,\n              0.9063, 0.9082, 0.9098, 0.9081, 0.9094, 0.9077, 0.9087, 0.9074, 0.9070,\n              0.9101, 0.9108, 0.9089, 0.9091, 0.9069, 0.9099, 0.9070, 0.9068, 0.9096,\n              0.9091, 0.9069, 0.9094, 0.9078, 0.9078, 0.9086, 0.9106, 0.9083, 0.9084,\n              0.9079, 0.9078, 0.9096, 0.9077, 0.9092, 0.9087, 0.9090, 0.9126, 0.9076,\n              0.9063, 0.9138, 0.9095, 0.9084, 0.9075, 0.9111, 0.9098, 0.9076, 0.9086,\n              0.9065, 0.9077, 0.9141, 0.9072, 0.9095, 0.9080, 0.9089, 0.9091, 0.9055,\n              0.9069, 0.9088, 0.9093, 0.9066, 0.9098, 0.9108, 0.9073, 0.9081, 0.9083,\n              0.9074, 0.9114, 0.9103, 0.9061, 0.9072, 0.9101, 0.9127, 0.9062, 0.9081,\n              0.9106, 0.9067, 0.9071, 0.9118, 0.9108, 0.9120, 0.9066, 0.9081, 0.9104,\n              0.9064, 0.9114, 0.9083, 0.9069, 0.9096, 0.9108, 0.9065, 0.9067, 0.9070,\n              0.9106, 0.9065, 0.9086, 0.9153, 0.9082, 0.9061, 0.9095, 0.9087, 0.9089,\n              0.9115, 0.9080, 0.9073, 0.9096, 0.9087, 0.9081, 0.9066, 0.9077, 0.9081,\n              0.9076, 0.9084, 0.9091, 0.9093, 0.9081, 0.9068, 0.9073, 0.9090, 0.9072,\n              0.9078, 0.9081, 0.9097, 0.9152, 0.9071, 0.9080, 0.9117, 0.9073, 0.9108,\n              0.9077, 0.9094, 0.9086, 0.9067, 0.9088, 0.9085, 0.9084, 0.9067, 0.9075,\n              0.9068, 0.9082, 0.9065, 0.9069, 0.9071, 0.9086, 0.9086, 0.9080, 0.9072,\n              0.9103, 0.9096, 0.9071, 0.9060, 0.9084, 0.9093, 0.9102, 0.9066, 0.9150,\n              0.9086, 0.9071, 0.9076, 0.9074, 0.9095, 0.9103, 0.9137, 0.9080, 0.9101,\n              0.9074, 0.9081, 0.9114, 0.9078, 0.9075, 0.9076, 0.9090, 0.9065, 0.9072,\n              0.9074, 0.9071, 0.9097, 0.9077, 0.9118, 0.9106, 0.9105, 0.9085, 0.9085,\n              0.9091, 0.9095, 0.9085, 0.9086, 0.9128, 0.9082, 0.9088, 0.9059, 0.9115,\n              0.9088, 0.9066, 0.9111, 0.9079, 0.9100, 0.9114, 0.9102, 0.9076, 0.9086,\n              0.9076, 0.9078, 0.9071, 0.9099, 0.9072, 0.9056, 0.9086, 0.9079, 0.9065,\n              0.9071, 0.9079, 0.9079, 0.9079, 0.9076, 0.9094, 0.9080, 0.9106, 0.9073,\n              0.9094, 0.9132, 0.9072, 0.9081, 0.9081, 0.9083, 0.9073, 0.9083, 0.9098,\n              0.9070, 0.9103, 0.9103, 0.9089, 0.9101, 0.9087, 0.9077, 0.9078, 0.9072,\n              0.9075, 0.9074, 0.9089, 0.9061, 0.9074, 0.9075, 0.9100, 0.9076, 0.9077,\n              0.9074, 0.9082, 0.9091, 0.9101, 0.9107, 0.9082, 0.9079, 0.9100, 0.9065,\n              0.9083, 0.9090, 0.9068, 0.9084, 0.9082, 0.9086, 0.9066, 0.9089, 0.9079,\n              0.9073, 0.9094, 0.9079, 0.9124, 0.9092, 0.9093, 0.9097, 0.9102, 0.9085,\n              0.9065, 0.9062, 0.9126, 0.9082, 0.9068, 0.9060, 0.9064, 0.9078, 0.9108,\n              0.9065, 0.9089, 0.9114, 0.9063, 0.9073, 0.9109, 0.9099, 0.9058],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 6.0226e-05,  1.0374e-02,  1.4402e-02],\n                [ 5.1018e-04, -1.2045e-02,  1.4477e-02],\n                [-1.1514e-02,  1.2953e-02,  1.0956e-02]],\n      \n               [[-8.2308e-03,  3.8798e-03, -9.7609e-03],\n                [-1.0825e-02,  1.2181e-02, -1.3725e-02],\n                [ 1.2365e-02, -1.0413e-02,  8.1149e-03]],\n      \n               [[ 5.0028e-03, -6.9393e-05,  1.2301e-02],\n                [ 1.4104e-02, -7.3703e-03,  1.0485e-02],\n                [ 1.0462e-03, -6.0684e-03, -1.3189e-02]],\n      \n               ...,\n      \n               [[-6.9378e-03, -3.0796e-03,  1.4078e-02],\n                [ 4.6840e-04,  2.4691e-03,  8.6551e-03],\n                [ 9.4631e-03,  7.1761e-03,  1.4543e-02]],\n      \n               [[-6.0904e-03,  5.8227e-03,  5.1257e-04],\n                [ 5.6529e-03,  1.3884e-02, -1.4092e-02],\n                [ 4.6146e-03, -6.7399e-03,  2.4059e-03]],\n      \n               [[-8.7611e-03, -4.6529e-03,  1.7449e-03],\n                [ 5.7339e-03, -1.2755e-03,  1.1272e-02],\n                [-1.0439e-02, -9.7332e-03,  9.3872e-03]]],\n      \n      \n              [[[ 7.2074e-03,  4.4068e-03, -8.8573e-04],\n                [-2.9278e-03, -9.1630e-03, -1.0172e-02],\n                [-9.5972e-03, -1.2798e-02, -7.4789e-03]],\n      \n               [[-5.3844e-03,  2.9877e-03,  7.4719e-03],\n                [-1.1970e-02, -2.2525e-03,  1.1397e-02],\n                [ 1.4125e-02, -1.2876e-02,  1.1620e-02]],\n      \n               [[-1.0946e-02, -5.4643e-03,  1.0336e-02],\n                [-7.8302e-03,  4.0481e-03,  7.9984e-03],\n                [ 8.3414e-03,  9.3550e-04, -5.0484e-03]],\n      \n               ...,\n      \n               [[ 3.8349e-03,  3.0608e-03,  5.7821e-03],\n                [-1.3393e-02, -2.5222e-04, -1.1163e-02],\n                [ 1.2803e-02,  1.4139e-02,  9.1109e-03]],\n      \n               [[ 8.7158e-04,  7.8553e-03, -4.9316e-03],\n                [-1.4524e-02,  1.4448e-02, -6.2784e-03],\n                [-1.4622e-02,  7.4128e-03,  7.7260e-03]],\n      \n               [[ 2.0436e-03, -1.4141e-02,  1.9922e-03],\n                [ 1.8880e-03, -1.5547e-03, -1.3171e-02],\n                [-1.8469e-03,  8.2271e-03,  6.8078e-03]]],\n      \n      \n              [[[-1.2225e-03, -7.1505e-03,  5.6563e-03],\n                [-7.2884e-03,  5.7453e-03,  1.1089e-02],\n                [ 9.0523e-03, -1.3562e-02, -9.7478e-03]],\n      \n               [[-7.0347e-03,  1.0926e-02, -9.5624e-03],\n                [ 1.5830e-03, -8.8162e-03,  1.3742e-02],\n                [ 1.2610e-02, -5.4636e-04, -8.3543e-03]],\n      \n               [[ 1.2205e-02,  4.7181e-03,  1.5907e-03],\n                [-3.9265e-03, -1.4245e-02, -1.0903e-02],\n                [-1.7674e-03,  1.4416e-02,  1.2405e-02]],\n      \n               ...,\n      \n               [[-8.2741e-03, -3.0408e-03, -6.2857e-03],\n                [-1.2773e-02, -1.0232e-02, -2.6605e-03],\n                [ 1.2152e-02, -5.5930e-03,  6.6461e-04]],\n      \n               [[-9.9109e-03,  7.5485e-03,  4.1226e-03],\n                [ 1.4510e-02,  5.3757e-03, -1.3929e-02],\n                [-1.1096e-02,  7.0697e-03,  5.9833e-03]],\n      \n               [[ 1.2375e-02,  1.1360e-02,  6.8589e-03],\n                [-1.1473e-02, -3.8843e-04, -8.2300e-03],\n                [-9.6626e-04, -1.3754e-02,  1.9368e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[ 1.4955e-03,  4.2249e-03,  4.7487e-04],\n                [-6.1461e-03,  1.4731e-02, -1.2923e-02],\n                [-1.0124e-02, -9.4271e-03,  3.5243e-03]],\n      \n               [[-8.2527e-03,  7.8247e-03, -2.5793e-03],\n                [-3.4379e-03,  1.1239e-03,  7.6617e-03],\n                [ 9.7746e-03, -2.6912e-04,  1.0866e-02]],\n      \n               [[-2.5306e-03,  6.2666e-03,  4.4189e-04],\n                [-1.4117e-02,  4.6636e-03, -7.5312e-03],\n                [-7.1191e-04, -1.0361e-02,  7.7296e-03]],\n      \n               ...,\n      \n               [[ 5.3793e-03,  1.0344e-02, -1.1024e-02],\n                [-1.0084e-02, -9.5807e-03,  2.5699e-03],\n                [-8.5668e-03,  2.0092e-03, -1.5295e-03]],\n      \n               [[-6.0612e-03,  8.6441e-03,  9.8906e-03],\n                [-1.3150e-02,  9.8471e-03,  1.1952e-02],\n                [-6.2016e-03,  3.9133e-03,  1.4034e-02]],\n      \n               [[ 2.9498e-03, -4.3139e-03,  9.0856e-03],\n                [ 6.5891e-03, -8.8100e-03,  1.0122e-02],\n                [ 1.4713e-02,  7.0518e-03,  6.9726e-03]]],\n      \n      \n              [[[ 1.2452e-02, -6.2607e-04,  5.8241e-03],\n                [ 3.7370e-03, -5.9694e-03,  5.5797e-03],\n                [ 6.8552e-03, -1.2802e-02,  8.6146e-03]],\n      \n               [[-9.5740e-03, -2.1618e-03, -1.1056e-02],\n                [-4.3233e-03,  1.4094e-02,  6.7706e-03],\n                [ 6.2340e-04, -1.3738e-02,  4.4411e-03]],\n      \n               [[-6.6448e-03,  7.7448e-03,  4.6042e-03],\n                [-9.9167e-03, -1.1680e-02, -3.6318e-03],\n                [ 3.1400e-03, -1.0652e-02,  1.1578e-02]],\n      \n               ...,\n      \n               [[ 1.2145e-02, -1.0871e-02, -1.2589e-02],\n                [-9.7776e-03,  9.7894e-03,  1.1579e-02],\n                [-1.4167e-02,  1.0683e-02, -1.0077e-02]],\n      \n               [[-7.7598e-03,  1.0847e-02, -9.2080e-03],\n                [ 1.1527e-02,  7.0528e-03, -3.6675e-03],\n                [-1.3320e-02, -1.0029e-02,  4.6631e-03]],\n      \n               [[ 1.1058e-02,  6.3849e-03, -1.0653e-02],\n                [ 8.6688e-03, -2.3040e-04,  3.7021e-03],\n                [-1.3902e-02,  7.3090e-03,  1.5085e-03]]],\n      \n      \n              [[[ 1.4684e-02, -5.8755e-03,  3.1146e-03],\n                [ 5.1104e-03,  7.0672e-03,  1.1579e-02],\n                [ 4.6482e-03,  1.2523e-02, -8.9994e-03]],\n      \n               [[ 1.0489e-04, -1.3740e-02, -1.2767e-03],\n                [ 8.0195e-03,  1.2940e-02,  1.1633e-02],\n                [-8.5563e-03, -1.4478e-02,  2.0777e-03]],\n      \n               [[ 4.9796e-03,  1.3871e-03, -8.1283e-03],\n                [-2.1150e-03, -1.1962e-02, -2.1781e-03],\n                [-1.1559e-02,  4.2234e-03, -5.7809e-03]],\n      \n               ...,\n      \n               [[-9.1594e-03,  1.4221e-02, -6.5727e-03],\n                [-7.5505e-03,  9.3913e-04, -1.1944e-02],\n                [-9.9042e-03, -1.1947e-02, -3.6851e-03]],\n      \n               [[-3.1213e-03,  1.2176e-02,  8.4452e-03],\n                [-8.2586e-03, -1.0021e-02, -2.5592e-03],\n                [ 4.9044e-03, -1.7904e-03,  2.4101e-04]],\n      \n               [[ 1.2523e-04, -2.5316e-03, -1.2896e-02],\n                [ 6.4744e-03,  6.6829e-03, -3.6452e-04],\n                [ 7.8467e-03,  8.0435e-03, -7.0168e-03]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-9.6627e-03,  2.5859e-03, -7.3062e-03,  4.8992e-05, -4.9974e-03,\n               2.9840e-03, -1.1166e-02,  6.2310e-03, -1.6252e-02, -7.3568e-03,\n              -1.3471e-02, -9.6921e-03,  7.9809e-03, -8.7156e-04,  1.0896e-02,\n               8.0393e-03,  1.1666e-02,  2.6570e-03,  8.4042e-04,  1.1758e-02,\n              -2.0076e-03,  5.4568e-04,  2.4176e-03,  8.9315e-03,  1.0877e-03,\n               5.7224e-03, -1.8612e-02,  9.2820e-03, -2.4436e-03,  2.1247e-03,\n              -1.9197e-04,  6.6744e-03,  2.4778e-04,  1.4685e-02,  1.7074e-02,\n              -1.6931e-03, -2.6524e-03,  1.0856e-02, -3.1915e-03, -1.0382e-02,\n              -5.5467e-04,  1.0580e-02, -1.4099e-03,  2.0011e-03,  7.6277e-03,\n               1.9667e-03,  6.6911e-05,  8.9201e-03, -2.3017e-03, -4.5274e-03,\n              -7.2799e-03,  6.7181e-03,  8.6888e-03, -7.5279e-03, -4.3793e-03,\n              -8.2205e-03, -3.8293e-03, -6.1112e-03, -1.6750e-03,  1.5256e-03,\n              -2.9404e-03,  8.0979e-04,  7.0102e-03, -1.9484e-03,  5.8911e-03,\n              -1.3804e-03, -5.1755e-03,  6.5124e-03,  3.0366e-03,  1.4888e-02,\n               1.2402e-03, -1.1562e-02,  6.5796e-03, -4.6990e-03,  5.6478e-04,\n              -6.5472e-03, -4.5033e-03,  1.3282e-02, -5.5578e-03,  5.1262e-03,\n               1.3931e-03,  8.3614e-05, -2.3201e-02, -1.1350e-02,  1.1635e-02,\n               5.7757e-03, -3.3377e-03,  5.4219e-03,  1.7761e-03,  1.9258e-02,\n              -4.3954e-03,  1.2742e-02, -8.0430e-03, -1.1599e-03, -4.1126e-03,\n               2.0723e-03,  5.4403e-03, -1.6382e-03,  1.4107e-02,  7.2007e-03,\n              -7.7157e-03,  7.6706e-03, -1.5739e-02,  1.5100e-02, -1.5415e-03,\n               1.8788e-04,  1.1194e-02,  3.1623e-03, -1.1732e-02, -7.8029e-03,\n               8.8382e-03,  3.8784e-03, -3.4210e-04,  9.9542e-04, -8.9481e-03,\n               2.7412e-03, -9.6760e-03,  6.1792e-04,  1.1163e-02,  5.5398e-04,\n              -6.1380e-03, -2.2347e-03, -3.8744e-04, -3.8483e-03, -6.1064e-03,\n              -4.3023e-03,  7.4693e-03,  5.9136e-03, -9.3670e-04,  4.3474e-03,\n              -3.1645e-04,  1.0201e-03, -1.3159e-02,  8.0320e-03,  2.6978e-03,\n              -5.4675e-03, -1.4736e-03,  3.1855e-03, -1.7878e-03, -4.7890e-03,\n               6.0327e-03,  9.6337e-04, -5.1539e-03, -4.0591e-03, -1.5609e-02,\n              -7.2874e-03, -2.0155e-02, -1.0015e-02,  1.0507e-04,  7.8669e-03,\n              -3.1014e-03,  2.1329e-03,  7.2945e-03, -8.0631e-03, -1.1295e-02,\n               4.5920e-03, -1.6484e-02, -1.0117e-02,  1.4058e-03,  1.2107e-03,\n              -4.4754e-03,  1.5747e-03, -1.0269e-02,  9.8958e-03, -4.3891e-03,\n              -1.7276e-02, -2.0479e-03, -4.4580e-03,  1.1498e-02,  2.2203e-03,\n              -3.2119e-03,  4.0446e-03,  3.7192e-03, -3.5937e-03, -1.6899e-02,\n              -8.2410e-05,  1.4341e-02, -3.4831e-03, -9.1805e-03, -1.4796e-02,\n               2.4330e-03,  1.3100e-03,  2.4784e-04,  1.5670e-03, -1.4117e-02,\n               6.8042e-03,  4.8702e-03, -6.3297e-03,  8.7898e-04,  1.1040e-02,\n              -8.2749e-03, -9.4140e-03,  1.1304e-04, -7.1813e-03, -9.4829e-03,\n               1.0971e-03,  1.1262e-02,  1.2488e-03,  5.0708e-04, -7.4091e-03,\n               9.7714e-03,  3.5469e-03,  1.9737e-03, -1.5152e-02,  2.4572e-03,\n              -2.5139e-03,  8.2145e-03, -6.9606e-03, -7.5013e-03,  8.4094e-03,\n               2.2384e-03,  2.0407e-03, -5.9202e-04,  1.1063e-02,  1.6743e-03,\n               7.3985e-03,  1.6771e-03, -7.7911e-04,  2.5249e-03,  8.0163e-03,\n               3.0719e-03,  4.0581e-03, -1.5166e-02, -1.1939e-02,  1.9090e-03,\n               9.2768e-03,  5.7217e-03,  2.6849e-03,  7.5817e-03, -2.5502e-03,\n               4.7369e-03, -7.6218e-03, -7.9607e-03,  2.5655e-03,  8.7666e-03,\n               4.9981e-03,  6.2335e-03,  3.1817e-03,  1.2742e-02,  2.7114e-03,\n               7.2234e-03,  1.1067e-04,  2.6951e-03, -3.8269e-03, -7.4505e-03,\n               7.0578e-04,  2.4880e-03, -6.6902e-03,  1.8548e-02, -3.6215e-04,\n              -2.1998e-02, -5.6140e-03,  4.1176e-03,  1.6268e-03,  5.4091e-03,\n              -4.2304e-03,  2.3673e-02,  7.3209e-03,  8.3348e-03, -6.6264e-04,\n              -7.4751e-04,  3.0721e-03,  1.6617e-02,  4.3138e-03,  2.7563e-03,\n              -1.0056e-02,  5.8068e-03, -7.2894e-03,  2.1483e-03,  4.4999e-03,\n               5.4849e-03, -4.0773e-03, -8.4834e-03, -5.9019e-03, -1.1250e-02,\n              -5.4072e-03, -4.1693e-03,  1.7587e-03, -2.8265e-03,  7.0092e-03,\n               1.1199e-02,  1.8364e-04, -2.2277e-03, -5.7951e-04,  5.1347e-03,\n              -2.5899e-03,  2.7815e-03, -8.5477e-03,  3.2053e-03, -6.2279e-03,\n               3.7844e-03, -6.4682e-03,  1.4942e-02,  7.1097e-03,  1.5211e-02,\n              -2.0255e-03, -1.8779e-04, -5.1500e-03,  1.0463e-03, -7.9226e-03,\n              -4.0408e-03, -5.6845e-03,  1.3714e-02, -1.2953e-02,  1.8864e-02,\n              -1.4250e-03, -3.7544e-03,  1.2023e-04, -7.7301e-03, -1.2488e-02,\n               1.1138e-02, -7.6206e-03,  3.6583e-03, -6.1346e-03,  2.3268e-03,\n               7.4344e-03, -6.4979e-03,  6.0332e-03, -9.5140e-03, -7.9657e-03,\n               8.8175e-03, -5.6425e-03, -3.5682e-03,  5.5679e-03,  8.9511e-03,\n               3.1893e-03,  9.2566e-03, -6.3322e-03,  6.1058e-03,  8.0136e-03,\n               1.1498e-02,  2.8902e-03, -1.0347e-02,  3.8497e-03,  1.0219e-02,\n               6.7478e-04,  5.6861e-03,  3.0999e-03,  1.5221e-02,  1.0727e-03,\n               2.0027e-04,  4.4343e-03, -5.3632e-03,  9.3373e-03, -1.6828e-03,\n               7.9935e-03,  6.3402e-03,  6.9175e-03, -9.3940e-03,  5.0278e-03,\n              -5.5952e-03,  3.7591e-03, -1.0472e-02,  1.4861e-02, -2.3931e-03,\n              -3.4152e-03,  6.7917e-03,  1.2300e-03, -1.1016e-02, -7.4185e-03,\n               8.8394e-03,  6.1926e-03, -3.1292e-03,  3.1500e-03, -1.1308e-02,\n               3.4574e-03, -1.1563e-02,  3.3781e-03,  9.5348e-03, -3.0541e-03,\n              -1.9079e-02,  1.4083e-02, -3.2877e-03, -6.4885e-03, -1.1495e-02,\n               1.0657e-03,  7.9288e-03,  2.2538e-03,  1.0770e-02, -4.9197e-03,\n              -1.5141e-02, -5.2542e-04, -8.4561e-03, -6.4359e-03,  2.4077e-03,\n              -1.1875e-02, -1.6681e-02, -1.2209e-02,  1.8456e-03,  1.3082e-02,\n               3.2050e-03, -6.3717e-03,  3.0758e-03, -1.2370e-02,  2.2817e-03,\n              -3.9184e-03,  1.0913e-03, -3.8753e-03,  9.5258e-03,  2.8944e-03,\n               1.6545e-03, -8.1557e-03, -1.1214e-03, -8.1895e-03, -1.8425e-02,\n              -1.0676e-03, -4.2453e-03, -1.0483e-02, -8.3791e-03,  1.2404e-02,\n              -7.9737e-04,  1.1625e-02, -5.2467e-03,  1.0581e-03, -6.1049e-03,\n               1.5282e-03, -8.7749e-03, -4.0793e-03, -3.2188e-03, -1.6648e-03,\n              -1.8531e-03,  9.2343e-04,  4.2888e-03,  1.7403e-02, -5.9032e-03,\n               5.0676e-03, -1.0413e-02, -1.2360e-02,  4.0909e-04,  6.2453e-03,\n              -9.6855e-03,  1.2582e-02, -3.1367e-03,  6.6151e-03,  7.1573e-03,\n              -8.2565e-03, -2.2993e-03,  4.0740e-03, -8.1498e-03, -6.1559e-03,\n              -4.2518e-03, -1.4552e-02, -1.8590e-03, -1.4303e-03, -6.4777e-03,\n               9.0335e-03, -1.0874e-02,  3.9856e-03,  1.7334e-03, -2.0177e-04,\n               3.3010e-04, -4.0856e-03, -7.9378e-03, -9.7514e-03, -5.0678e-03,\n              -8.0528e-03, -5.9823e-03, -7.0982e-03,  1.8516e-02, -1.5880e-03,\n              -1.4359e-02,  5.0331e-03,  6.8210e-03,  2.1519e-03,  4.0025e-03,\n               2.9079e-04, -2.6275e-03,  2.6169e-03,  1.1163e-02,  5.3891e-03,\n               1.8236e-02,  2.0529e-03, -1.0045e-02, -9.4185e-03, -2.2061e-03,\n               6.8791e-03, -9.0630e-03, -1.9235e-03, -8.1186e-03,  1.0754e-03,\n              -3.9560e-04, -1.9457e-02,  1.5993e-02,  9.2737e-04,  9.1270e-03,\n              -1.4372e-03,  1.4933e-02, -1.0817e-03,  4.3506e-03,  1.2623e-02,\n              -5.1336e-03,  2.3347e-03, -2.6127e-05, -1.1102e-02,  3.2044e-03,\n               2.2335e-02,  2.6505e-03, -5.5260e-03, -1.1031e-02,  7.5003e-04,\n              -1.4140e-02,  9.4609e-03,  3.1204e-03, -5.3986e-04, -6.6711e-03,\n              -1.0527e-02, -2.0512e-03, -1.5997e-04, -5.7613e-04, -1.0226e-02,\n               8.6355e-03, -4.5127e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9012, 0.9012, 0.9014, 0.9012, 0.9012, 0.9018, 0.9009, 0.9013, 0.9011,\n              0.9013, 0.9011, 0.9011, 0.9010, 0.9011, 0.9014, 0.9012, 0.9009, 0.9010,\n              0.9012, 0.9015, 0.9011, 0.9011, 0.9013, 0.9013, 0.9013, 0.9014, 0.9014,\n              0.9013, 0.9012, 0.9012, 0.9014, 0.9013, 0.9012, 0.9014, 0.9011, 0.9017,\n              0.9013, 0.9013, 0.9013, 0.9012, 0.9017, 0.9014, 0.9010, 0.9014, 0.9013,\n              0.9016, 0.9010, 0.9012, 0.9012, 0.9013, 0.9013, 0.9016, 0.9011, 0.9012,\n              0.9010, 0.9019, 0.9011, 0.9012, 0.9011, 0.9017, 0.9010, 0.9012, 0.9012,\n              0.9011, 0.9013, 0.9016, 0.9012, 0.9017, 0.9010, 0.9010, 0.9011, 0.9014,\n              0.9011, 0.9013, 0.9012, 0.9018, 0.9013, 0.9014, 0.9011, 0.9011, 0.9012,\n              0.9011, 0.9016, 0.9012, 0.9014, 0.9013, 0.9014, 0.9011, 0.9011, 0.9017,\n              0.9011, 0.9010, 0.9012, 0.9012, 0.9010, 0.9013, 0.9010, 0.9016, 0.9011,\n              0.9011, 0.9010, 0.9015, 0.9019, 0.9008, 0.9012, 0.9012, 0.9011, 0.9012,\n              0.9010, 0.9018, 0.9011, 0.9013, 0.9014, 0.9012, 0.9013, 0.9010, 0.9010,\n              0.9012, 0.9015, 0.9013, 0.9013, 0.9013, 0.9015, 0.9012, 0.9012, 0.9015,\n              0.9013, 0.9012, 0.9011, 0.9011, 0.9012, 0.9013, 0.9011, 0.9011, 0.9009,\n              0.9017, 0.9013, 0.9018, 0.9015, 0.9013, 0.9012, 0.9011, 0.9013, 0.9011,\n              0.9016, 0.9012, 0.9019, 0.9012, 0.9014, 0.9012, 0.9011, 0.9015, 0.9011,\n              0.9010, 0.9013, 0.9013, 0.9011, 0.9011, 0.9014, 0.9013, 0.9017, 0.9010,\n              0.9014, 0.9013, 0.9013, 0.9012, 0.9015, 0.9010, 0.9013, 0.9011, 0.9012,\n              0.9018, 0.9013, 0.9011, 0.9016, 0.9018, 0.9016, 0.9013, 0.9011, 0.9011,\n              0.9013, 0.9015, 0.9011, 0.9012, 0.9012, 0.9013, 0.9008, 0.9016, 0.9012,\n              0.9011, 0.9012, 0.9013, 0.9010, 0.9015, 0.9013, 0.9009, 0.9012, 0.9011,\n              0.9010, 0.9013, 0.9013, 0.9014, 0.9011, 0.9014, 0.9011, 0.9013, 0.9013,\n              0.9011, 0.9014, 0.9009, 0.9011, 0.9010, 0.9015, 0.9014, 0.9012, 0.9013,\n              0.9011, 0.9013, 0.9015, 0.9011, 0.9014, 0.9012, 0.9012, 0.9012, 0.9013,\n              0.9009, 0.9013, 0.9012, 0.9012, 0.9010, 0.9014, 0.9011, 0.9012, 0.9012,\n              0.9012, 0.9012, 0.9014, 0.9013, 0.9010, 0.9015, 0.9012, 0.9013, 0.9015,\n              0.9011, 0.9017, 0.9010, 0.9015, 0.9016, 0.9016, 0.9011, 0.9012, 0.9011,\n              0.9011, 0.9012, 0.9016, 0.9015, 0.9016, 0.9011, 0.9010, 0.9012, 0.9014,\n              0.9010, 0.9012, 0.9011, 0.9013, 0.9015, 0.9015, 0.9012, 0.9013, 0.9011,\n              0.9013, 0.9012, 0.9011, 0.9010, 0.9014, 0.9015, 0.9013, 0.9012, 0.9011,\n              0.9011, 0.9012, 0.9010, 0.9013, 0.9019, 0.9011, 0.9012, 0.9015, 0.9012,\n              0.9015, 0.9010, 0.9023, 0.9012, 0.9013, 0.9014, 0.9012, 0.9011, 0.9013,\n              0.9014, 0.9010, 0.9012, 0.9011, 0.9013, 0.9013, 0.9010, 0.9017, 0.9014,\n              0.9011, 0.9012, 0.9011, 0.9011, 0.9012, 0.9011, 0.9011, 0.9011, 0.9011,\n              0.9013, 0.9011, 0.9014, 0.9012, 0.9024, 0.9011, 0.9009, 0.9011, 0.9016,\n              0.9011, 0.9011, 0.9014, 0.9012, 0.9012, 0.9012, 0.9013, 0.9012, 0.9010,\n              0.9011, 0.9012, 0.9012, 0.9014, 0.9015, 0.9015, 0.9014, 0.9012, 0.9012,\n              0.9012, 0.9018, 0.9014, 0.9009, 0.9012, 0.9013, 0.9012, 0.9014, 0.9013,\n              0.9011, 0.9012, 0.9011, 0.9011, 0.9010, 0.9013, 0.9011, 0.9015, 0.9011,\n              0.9015, 0.9010, 0.9010, 0.9010, 0.9012, 0.9012, 0.9013, 0.9010, 0.9012,\n              0.9015, 0.9020, 0.9013, 0.9013, 0.9010, 0.9011, 0.9014, 0.9013, 0.9011,\n              0.9014, 0.9012, 0.9024, 0.9013, 0.9011, 0.9014, 0.9016, 0.9012, 0.9012,\n              0.9014, 0.9012, 0.9014, 0.9013, 0.9017, 0.9013, 0.9010, 0.9015, 0.9012,\n              0.9011, 0.9011, 0.9011, 0.9012, 0.9012, 0.9012, 0.9011, 0.9011, 0.9014,\n              0.9011, 0.9011, 0.9014, 0.9012, 0.9012, 0.9013, 0.9011, 0.9014, 0.9012,\n              0.9010, 0.9012, 0.9012, 0.9013, 0.9013, 0.9011, 0.9012, 0.9010, 0.9013,\n              0.9010, 0.9010, 0.9010, 0.9012, 0.9012, 0.9016, 0.9015, 0.9015, 0.9012,\n              0.9013, 0.9012, 0.9013, 0.9013, 0.9011, 0.9012, 0.9014, 0.9012, 0.9014,\n              0.9012, 0.9013, 0.9016, 0.9012, 0.9012, 0.9010, 0.9011, 0.9013, 0.9011,\n              0.9017, 0.9011, 0.9014, 0.9013, 0.9016, 0.9013, 0.9013, 0.9014, 0.9010,\n              0.9012, 0.9012, 0.9014, 0.9012, 0.9011, 0.9012, 0.9015, 0.9012, 0.9012,\n              0.9016, 0.9012, 0.9016, 0.9010, 0.9013, 0.9014, 0.9014, 0.9012, 0.9010,\n              0.9014, 0.9013, 0.9011, 0.9012, 0.9013, 0.9018, 0.9012, 0.9011, 0.9012,\n              0.9015, 0.9012, 0.9013, 0.9015, 0.9014, 0.9014, 0.9013, 0.9016, 0.9018,\n              0.9019, 0.9015, 0.9014, 0.9016, 0.9010, 0.9022, 0.9014, 0.9012, 0.9010,\n              0.9013, 0.9010, 0.9012, 0.9010, 0.9013, 0.9015, 0.9011, 0.9012],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[ 0.0153]],\n        \n                 [[-0.0222]],\n        \n                 [[ 0.0276]],\n        \n                 ...,\n        \n                 [[ 0.0610]],\n        \n                 [[ 0.0066]],\n        \n                 [[-0.0521]]],\n        \n        \n                [[[ 0.0235]],\n        \n                 [[-0.0043]],\n        \n                 [[-0.0314]],\n        \n                 ...,\n        \n                 [[ 0.0102]],\n        \n                 [[-0.0489]],\n        \n                 [[ 0.0528]]],\n        \n        \n                [[[ 0.0455]],\n        \n                 [[ 0.0199]],\n        \n                 [[ 0.0482]],\n        \n                 ...,\n        \n                 [[-0.0008]],\n        \n                 [[ 0.0091]],\n        \n                 [[ 0.0472]]],\n        \n        \n                ...,\n        \n        \n                [[[-0.0034]],\n        \n                 [[ 0.0079]],\n        \n                 [[-0.0532]],\n        \n                 ...,\n        \n                 [[ 0.0124]],\n        \n                 [[-0.0053]],\n        \n                 [[ 0.0027]]],\n        \n        \n                [[[ 0.0320]],\n        \n                 [[ 0.0096]],\n        \n                 [[ 0.0080]],\n        \n                 ...,\n        \n                 [[ 0.0037]],\n        \n                 [[ 0.0257]],\n        \n                 [[ 0.0364]]],\n        \n        \n                [[[ 0.0558]],\n        \n                 [[-0.0025]],\n        \n                 [[ 0.0130]],\n        \n                 ...,\n        \n                 [[ 0.0256]],\n        \n                 [[ 0.0544]],\n        \n                 [[-0.0244]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0254, -0.0487,  0.0148, -0.0752, -0.0181,  0.0203,  0.0459,  0.0042,\n                 0.0358,  0.0243, -0.0031,  0.0196,  0.0494,  0.0637,  0.0529,  0.0146,\n                 0.0222,  0.0220,  0.0031,  0.0283,  0.0038,  0.0411, -0.0185,  0.0298,\n                -0.0111, -0.0239,  0.0045,  0.0202,  0.0386,  0.0079,  0.0242,  0.0092,\n                -0.0176,  0.0601, -0.0055,  0.0063, -0.0347, -0.0134,  0.0660,  0.0273,\n                 0.0012,  0.0629,  0.0115, -0.0945,  0.0555,  0.0190, -0.0161, -0.0200,\n                 0.0514, -0.0042, -0.1116, -0.0171,  0.0144,  0.0388, -0.0331, -0.0531,\n                 0.0006,  0.0108,  0.0442, -0.0115,  0.0206, -0.0426,  0.0265, -0.0052,\n                -0.0114, -0.0204, -0.0054, -0.0229,  0.0040, -0.0033, -0.0748,  0.0108,\n                -0.0626,  0.0019,  0.0155,  0.0190,  0.0667,  0.0184, -0.0493, -0.0166,\n                -0.0256,  0.0325,  0.0204,  0.0485, -0.0131, -0.0220, -0.0749, -0.0666,\n                 0.0365, -0.0039,  0.0414,  0.0752, -0.0070,  0.0693, -0.0437, -0.0077,\n                 0.0143, -0.0445,  0.0048,  0.0501,  0.0033,  0.0405, -0.0096, -0.0060,\n                 0.0457, -0.0713, -0.0231,  0.0322, -0.0331,  0.1161, -0.0355,  0.0428,\n                 0.0097, -0.0510,  0.0287, -0.0681, -0.0254, -0.0210,  0.0090, -0.0205,\n                -0.0362,  0.0191,  0.0383,  0.0281,  0.0078, -0.0113, -0.0111,  0.0717,\n                -0.0023, -0.0481,  0.0051, -0.0101, -0.0585,  0.0466, -0.0224, -0.0655,\n                -0.0101, -0.0221,  0.0088,  0.0358,  0.0017,  0.0372,  0.0238, -0.0304,\n                -0.0285, -0.0118, -0.0137, -0.0242,  0.0066,  0.0036, -0.0192, -0.0075,\n                 0.0223, -0.0307,  0.0257,  0.0167,  0.0099,  0.0262,  0.0263, -0.0305,\n                -0.0906, -0.0332,  0.0386, -0.0511, -0.0014, -0.0046, -0.0183, -0.0082,\n                 0.0513, -0.0014,  0.0384,  0.0085, -0.0071,  0.0630,  0.0469, -0.0234,\n                 0.0016, -0.0028, -0.0428,  0.0599,  0.0283,  0.0405,  0.0347,  0.0099,\n                 0.0037, -0.0068, -0.0109, -0.0301,  0.0082,  0.0348, -0.0151, -0.0034,\n                 0.0260,  0.0286, -0.0178,  0.0654,  0.0477,  0.0318, -0.0412,  0.0023,\n                -0.0585, -0.0094,  0.0318, -0.0097,  0.0234, -0.0004, -0.0137,  0.0463,\n                -0.0207, -0.0139,  0.0438,  0.0047, -0.0015, -0.0368,  0.0032,  0.0701,\n                 0.0224, -0.0005, -0.0169,  0.0221,  0.0201,  0.0146, -0.0069, -0.0470,\n                 0.0542, -0.0253,  0.0283, -0.0211,  0.0472,  0.0015,  0.0162, -0.0129,\n                -0.0093, -0.0652,  0.0042,  0.0459, -0.0210, -0.0188, -0.0040,  0.0042,\n                -0.0249, -0.0138, -0.0328, -0.0185,  0.0038,  0.0290, -0.0186, -0.0053,\n                 0.0988, -0.0052, -0.0175, -0.0147,  0.0568,  0.0203,  0.0184,  0.0395,\n                 0.0314,  0.0365,  0.0026,  0.0748,  0.0491,  0.0141, -0.0095, -0.0320,\n                -0.0208,  0.0255,  0.0038,  0.0202, -0.0553, -0.0020, -0.0452, -0.0023,\n                -0.0857,  0.0607, -0.0278,  0.0007, -0.0491, -0.0413,  0.0379, -0.0526,\n                -0.0164,  0.0019,  0.0438,  0.0018, -0.0146,  0.0311, -0.0155,  0.0217,\n                -0.0198, -0.0115,  0.0138,  0.0295, -0.0038, -0.0294,  0.0324,  0.0241,\n                 0.0166,  0.0561,  0.0754, -0.0612,  0.0093,  0.0059, -0.0230,  0.0018,\n                 0.0309, -0.0413, -0.0266, -0.0386,  0.0883, -0.0524, -0.0259,  0.0379,\n                -0.0444, -0.0004, -0.0323, -0.0103,  0.0428,  0.0149, -0.0286,  0.0438,\n                 0.0345, -0.0395,  0.0254, -0.0587,  0.0595, -0.0052, -0.0046, -0.0387,\n                -0.0338,  0.0320,  0.0043,  0.0049, -0.0065,  0.0241, -0.0099, -0.0321,\n                -0.0093, -0.0253, -0.0094,  0.0144, -0.0292, -0.0154,  0.0150,  0.0107,\n                -0.0074,  0.0040,  0.0427, -0.0487, -0.0503,  0.0248,  0.0409,  0.0047,\n                -0.0214, -0.0178, -0.0174, -0.0217,  0.0041, -0.0233,  0.0319, -0.0003,\n                 0.0135,  0.0331, -0.0153, -0.0046,  0.0659, -0.0164, -0.0555,  0.0174,\n                -0.0103, -0.0148, -0.0172,  0.0221, -0.0025,  0.0174, -0.0058, -0.0180,\n                -0.0633,  0.0185,  0.0168,  0.0245,  0.0060, -0.0474,  0.0726, -0.0042,\n                 0.0276, -0.0246,  0.0246,  0.0074, -0.0504,  0.0312, -0.0260, -0.0207,\n                 0.0121,  0.0610,  0.0162,  0.0683, -0.0025,  0.0736,  0.0002, -0.0524,\n                -0.0461, -0.0201, -0.0465,  0.0382,  0.0151,  0.0112,  0.0045,  0.0076,\n                 0.0084, -0.0042, -0.0067, -0.0731,  0.0052, -0.0057, -0.0440,  0.0094,\n                -0.0053,  0.0199,  0.0139, -0.0400, -0.0496, -0.0006, -0.0122,  0.0097,\n                -0.0408, -0.0529,  0.0565, -0.0415,  0.0240, -0.0730,  0.0082,  0.0055,\n                 0.0077,  0.0332, -0.0042, -0.0355,  0.0513, -0.0121,  0.0457,  0.0019,\n                -0.0410, -0.0170,  0.0097,  0.0876,  0.0238,  0.0348, -0.0298, -0.0146,\n                -0.0292, -0.0140, -0.0203, -0.0268,  0.0020,  0.0041, -0.0043,  0.0254,\n                 0.0416, -0.0048, -0.0167,  0.0040,  0.0038,  0.0155,  0.0926,  0.0244,\n                -0.0036, -0.0615, -0.0510,  0.0179,  0.0378,  0.0599, -0.0177,  0.0150,\n                -0.0264,  0.0375,  0.0142,  0.0589, -0.0151,  0.0301, -0.0002, -0.0301,\n                 0.0031, -0.0106, -0.0342,  0.0454,  0.0013,  0.0231,  0.0029, -0.0614,\n                -0.0054, -0.0086, -0.0198,  0.0350, -0.0384, -0.0349, -0.0141, -0.0281,\n                 0.0157,  0.0437,  0.0048, -0.0089,  0.0467, -0.0124, -0.0042, -0.0367,\n                 0.0308,  0.0166, -0.0273, -0.0313,  0.0226, -0.0115,  0.0409,  0.0020],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9173, 0.9234, 0.9123, 0.9232, 0.9142, 0.9145, 0.9234, 0.9167, 0.9166,\n                0.9163, 0.9166, 0.9173, 0.9184, 0.9135, 0.9157, 0.9186, 0.9159, 0.9218,\n                0.9266, 0.9183, 0.9144, 0.9189, 0.9160, 0.9115, 0.9123, 0.9175, 0.9117,\n                0.9170, 0.9213, 0.9207, 0.9108, 0.9159, 0.9141, 0.9162, 0.9155, 0.9123,\n                0.9122, 0.9199, 0.9120, 0.9207, 0.9189, 0.9148, 0.9154, 0.9187, 0.9229,\n                0.9135, 0.9144, 0.9136, 0.9116, 0.9156, 0.9347, 0.9153, 0.9215, 0.9141,\n                0.9337, 0.9260, 0.9230, 0.9204, 0.9153, 0.9129, 0.9216, 0.9174, 0.9223,\n                0.9130, 0.9218, 0.9209, 0.9208, 0.9148, 0.9167, 0.9167, 0.9134, 0.9145,\n                0.9173, 0.9148, 0.9187, 0.9198, 0.9171, 0.9160, 0.9136, 0.9164, 0.9135,\n                0.9130, 0.9115, 0.9248, 0.9146, 0.9163, 0.9225, 0.9312, 0.9147, 0.9153,\n                0.9162, 0.9147, 0.9125, 0.9206, 0.9201, 0.9119, 0.9131, 0.9163, 0.9167,\n                0.9199, 0.9118, 0.9134, 0.9156, 0.9180, 0.9177, 0.9221, 0.9208, 0.9185,\n                0.9181, 0.9271, 0.9157, 0.9126, 0.9134, 0.9179, 0.9146, 0.9236, 0.9159,\n                0.9148, 0.9168, 0.9151, 0.9231, 0.9189, 0.9144, 0.9122, 0.9170, 0.9210,\n                0.9160, 0.9245, 0.9167, 0.9169, 0.9178, 0.9119, 0.9160, 0.9141, 0.9157,\n                0.9186, 0.9161, 0.9137, 0.9150, 0.9174, 0.9151, 0.9118, 0.9136, 0.9188,\n                0.9130, 0.9138, 0.9148, 0.9157, 0.9174, 0.9204, 0.9127, 0.9132, 0.9219,\n                0.9155, 0.9163, 0.9175, 0.9134, 0.9156, 0.9193, 0.9163, 0.9266, 0.9254,\n                0.9273, 0.9165, 0.9137, 0.9149, 0.9179, 0.9233, 0.9260, 0.9187, 0.9158,\n                0.9117, 0.9147, 0.9191, 0.9158, 0.9151, 0.9155, 0.9168, 0.9165, 0.9093,\n                0.9134, 0.9120, 0.9147, 0.9159, 0.9302, 0.9132, 0.9171, 0.9198, 0.9153,\n                0.9128, 0.9118, 0.9158, 0.9146, 0.9186, 0.9132, 0.9186, 0.9125, 0.9122,\n                0.9210, 0.9189, 0.9130, 0.9175, 0.9116, 0.9146, 0.9172, 0.9120, 0.9133,\n                0.9191, 0.9142, 0.9121, 0.9140, 0.9114, 0.9155, 0.9184, 0.9202, 0.9160,\n                0.9120, 0.9160, 0.9186, 0.9125, 0.9216, 0.9290, 0.9119, 0.9136, 0.9254,\n                0.9148, 0.9145, 0.9138, 0.9165, 0.9175, 0.9238, 0.9131, 0.9155, 0.9114,\n                0.9103, 0.9178, 0.9355, 0.9180, 0.9133, 0.9178, 0.9170, 0.9132, 0.9144,\n                0.9144, 0.9204, 0.9169, 0.9125, 0.9194, 0.9226, 0.9139, 0.9212, 0.9161,\n                0.9195, 0.9131, 0.9126, 0.9159, 0.9144, 0.9178, 0.9145, 0.9149, 0.9196,\n                0.9178, 0.9233, 0.9214, 0.9166, 0.9149, 0.9144, 0.9149, 0.9102, 0.9118,\n                0.9177, 0.9151, 0.9181, 0.9193, 0.9191, 0.9182, 0.9155, 0.9440, 0.9171,\n                0.9158, 0.9128, 0.9117, 0.9144, 0.9184, 0.9255, 0.9131, 0.9150, 0.9199,\n                0.9146, 0.9163, 0.9161, 0.9140, 0.9151, 0.9149, 0.9224, 0.9199, 0.9171,\n                0.9203, 0.9255, 0.9198, 0.9112, 0.9231, 0.9108, 0.9119, 0.9181, 0.9169,\n                0.9142, 0.9127, 0.9226, 0.9188, 0.9167, 0.9166, 0.9197, 0.9110, 0.9159,\n                0.9199, 0.9161, 0.9187, 0.9191, 0.9138, 0.9124, 0.9153, 0.9163, 0.9135,\n                0.9181, 0.9229, 0.9095, 0.9224, 0.9144, 0.9160, 0.9145, 0.9180, 0.9162,\n                0.9215, 0.9141, 0.9116, 0.9140, 0.9129, 0.9174, 0.9158, 0.9187, 0.9140,\n                0.9169, 0.9257, 0.9114, 0.9146, 0.9138, 0.9166, 0.9155, 0.9161, 0.9195,\n                0.9178, 0.9142, 0.9148, 0.9293, 0.9132, 0.9170, 0.9156, 0.9266, 0.9171,\n                0.9152, 0.9180, 0.9143, 0.9136, 0.9121, 0.9135, 0.9148, 0.9127, 0.9151,\n                0.9103, 0.9132, 0.9153, 0.9138, 0.9196, 0.9185, 0.9151, 0.9251, 0.9206,\n                0.9143, 0.9196, 0.9142, 0.9124, 0.9201, 0.9127, 0.9131, 0.9163, 0.9125,\n                0.9153, 0.9237, 0.9137, 0.9137, 0.9163, 0.9149, 0.9270, 0.9147, 0.9221,\n                0.9129, 0.9163, 0.9170, 0.9128, 0.9250, 0.9126, 0.9115, 0.9134, 0.9171,\n                0.9147, 0.9147, 0.9134, 0.9200, 0.9121, 0.9124, 0.9147, 0.9144, 0.9127,\n                0.9200, 0.9144, 0.9165, 0.9179, 0.9185, 0.9117, 0.9139, 0.9134, 0.9131,\n                0.9265, 0.9170, 0.9162, 0.9248, 0.9144, 0.9131, 0.9148, 0.9119, 0.9164,\n                0.9115, 0.9198, 0.9151, 0.9136, 0.9174, 0.9187, 0.9235, 0.9223, 0.9175,\n                0.9147, 0.9154, 0.9172, 0.9120, 0.9178, 0.9126, 0.9136, 0.9111, 0.9266,\n                0.9150, 0.9200, 0.9172, 0.9158, 0.9154, 0.9149, 0.9181, 0.9161, 0.9148,\n                0.9264, 0.9174, 0.9152, 0.9286, 0.9182, 0.9157, 0.9105, 0.9139, 0.9168,\n                0.9178, 0.9168, 0.9132, 0.9139, 0.9165, 0.9272, 0.9181, 0.9131, 0.9137,\n                0.9169, 0.9202, 0.9150, 0.9266, 0.9111, 0.9198, 0.9165, 0.9186, 0.9214,\n                0.9196, 0.9125, 0.9200, 0.9128, 0.9138, 0.9167, 0.9180, 0.9132, 0.9154,\n                0.9156, 0.9115, 0.9140, 0.9125, 0.9122, 0.9201, 0.9143, 0.9101, 0.9207,\n                0.9178, 0.9123, 0.9129, 0.9203, 0.9142, 0.9191, 0.9286, 0.9133],\n               grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [512, 256, 3, 3]], ["0.bn1.weight", [512]], ["0.bn1.bias", [512]], ["0.conv2.weight", [512, 512, 3, 3]], ["0.bn2.weight", [512]], ["0.bn2.bias", [512]], ["0.downsample.0.weight", [512, 256, 1, 1]], ["0.downsample.1.weight", [512]], ["0.downsample.1.bias", [512]]], "output_shape": [[512, 512, 1, 1]], "num_parameters": [1179648, 512, 512, 2359296, 512, 512, 131072, 512, 512]}, {"name": "avgpool", "id": 140608652877392, "class_name": "AveragePool()", "parameters": [], "output_shape": [[512, 512]], "num_parameters": []}, {"name": "fc", "id": 140608652877296, "class_name": "Linear(in_features=512, out_features=10, bias=True)", "parameters": [["weight", [10, 512]], ["bias", [10]]], "output_shape": [[512, 10]], "num_parameters": [5120, 10]}], "edges": []}