{"format": "torch", "nodes": [{"name": "conv1", "id": 140311229273712, "class_name": "Conv2d(\n  self.stride=2, self.padding=(3, 3), self.weight=Parameter containing:\n  tensor([[[[ 0.0493,  0.0618, -0.0593,  ..., -0.0753, -0.0169,  0.0080],\n            [-0.0004,  0.0275, -0.0483,  ..., -0.0036, -0.0802, -0.0145],\n            [ 0.0487, -0.0584, -0.0144,  ...,  0.0112,  0.0340,  0.0824],\n            ...,\n            [-0.0281, -0.0639, -0.0799,  ..., -0.0381, -0.0086, -0.0099],\n            [-0.0669, -0.0217,  0.0011,  ..., -0.0467, -0.0522,  0.0665],\n            [ 0.0554, -0.0762,  0.0050,  ...,  0.0203,  0.0567, -0.0537]],\n  \n           [[-0.0067,  0.0757,  0.0251,  ..., -0.0568, -0.0249, -0.0312],\n            [-0.0656, -0.0556, -0.0794,  ..., -0.0371, -0.0289,  0.0415],\n            [-0.0436,  0.0672, -0.0279,  ..., -0.0181, -0.0601, -0.0607],\n            ...,\n            [ 0.0042,  0.0162,  0.0087,  ...,  0.0327, -0.0767, -0.0345],\n            [-0.0791, -0.0146, -0.0048,  ..., -0.0672,  0.0428,  0.0131],\n            [-0.0250,  0.0099,  0.0619,  ...,  0.0624, -0.0112,  0.0688]],\n  \n           [[ 0.0156,  0.0327, -0.0526,  ...,  0.0218, -0.0355, -0.0751],\n            [ 0.0534,  0.0764,  0.0019,  ...,  0.0200, -0.0098, -0.0057],\n            [-0.0514,  0.0313, -0.0738,  ...,  0.0626, -0.0810,  0.0502],\n            ...,\n            [-0.0155, -0.0010, -0.0732,  ..., -0.0153, -0.0254, -0.0498],\n            [ 0.0676,  0.0578,  0.0081,  ...,  0.0145, -0.0100, -0.0379],\n            [ 0.0644, -0.0644, -0.0361,  ...,  0.0782, -0.0436, -0.0367]]],\n  \n  \n          [[[-0.0478, -0.0485, -0.0687,  ..., -0.0572, -0.0427, -0.0816],\n            [-0.0744, -0.0740,  0.0152,  ..., -0.0125,  0.0757, -0.0735],\n            [-0.0310, -0.0666,  0.0515,  ..., -0.0208,  0.0579, -0.0198],\n            ...,\n            [-0.0122,  0.0055,  0.0190,  ..., -0.0001,  0.0478, -0.0508],\n            [ 0.0620,  0.0595,  0.0228,  ..., -0.0180,  0.0730,  0.0464],\n            [-0.0571, -0.0453,  0.0607,  ...,  0.0396,  0.0148,  0.0248]],\n  \n           [[ 0.0165, -0.0282,  0.0406,  ..., -0.0338, -0.0307,  0.0374],\n            [-0.0099, -0.0679, -0.0008,  ..., -0.0785, -0.0507,  0.0011],\n            [ 0.0023,  0.0689, -0.0632,  ...,  0.0693,  0.0411,  0.0736],\n            ...,\n            [-0.0513, -0.0145,  0.0759,  ..., -0.0057, -0.0789,  0.0319],\n            [ 0.0030, -0.0770,  0.0618,  ..., -0.0782,  0.0802, -0.0128],\n            [-0.0776, -0.0320, -0.0514,  ..., -0.0586,  0.0793,  0.0315]],\n  \n           [[ 0.0146,  0.0494, -0.0285,  ..., -0.0438,  0.0617,  0.0027],\n            [-0.0533, -0.0545, -0.0715,  ..., -0.0500, -0.0784, -0.0535],\n            [ 0.0003,  0.0121, -0.0257,  ...,  0.0257, -0.0523, -0.0066],\n            ...,\n            [ 0.0794,  0.0420, -0.0751,  ...,  0.0586,  0.0748,  0.0415],\n            [-0.0660,  0.0375,  0.0458,  ..., -0.0710, -0.0793,  0.0419],\n            [-0.0111,  0.0071,  0.0454,  ...,  0.0020,  0.0080, -0.0048]]],\n  \n  \n          [[[-0.0296, -0.0787,  0.0769,  ...,  0.0481, -0.0326, -0.0463],\n            [ 0.0793,  0.0320, -0.0293,  ..., -0.0450, -0.0309, -0.0472],\n            [ 0.0797,  0.0528, -0.0624,  ...,  0.0161,  0.0619, -0.0795],\n            ...,\n            [ 0.0790, -0.0514,  0.0246,  ..., -0.0166,  0.0050, -0.0068],\n            [ 0.0762,  0.0432, -0.0671,  ..., -0.0549, -0.0681,  0.0523],\n            [ 0.0805, -0.0515,  0.0419,  ...,  0.0686, -0.0355,  0.0041]],\n  \n           [[-0.0753, -0.0141,  0.0405,  ...,  0.0795, -0.0097, -0.0002],\n            [-0.0459, -0.0168,  0.0567,  ...,  0.0670, -0.0090,  0.0195],\n            [-0.0493, -0.0257, -0.0405,  ..., -0.0804, -0.0021, -0.0546],\n            ...,\n            [-0.0328, -0.0819,  0.0795,  ..., -0.0061, -0.0647,  0.0479],\n            [ 0.0126, -0.0300,  0.0604,  ..., -0.0793, -0.0085,  0.0135],\n            [-0.0562, -0.0453,  0.0231,  ...,  0.0144, -0.0234, -0.0157]],\n  \n           [[ 0.0606, -0.0418,  0.0432,  ..., -0.0422, -0.0374,  0.0597],\n            [-0.0176, -0.0481,  0.0688,  ..., -0.0647,  0.0277,  0.0715],\n            [-0.0652,  0.0534,  0.0771,  ..., -0.0193,  0.0565,  0.0336],\n            ...,\n            [ 0.0342,  0.0577,  0.0770,  ...,  0.0564, -0.0525, -0.0305],\n            [ 0.0494, -0.0322, -0.0702,  ...,  0.0818, -0.0521, -0.0812],\n            [ 0.0716, -0.0771, -0.0175,  ...,  0.0347, -0.0245, -0.0300]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0738, -0.0658,  0.0652,  ...,  0.0033,  0.0516,  0.0061],\n            [-0.0524,  0.0510,  0.0609,  ..., -0.0461,  0.0111, -0.0418],\n            [ 0.0329, -0.0500, -0.0406,  ...,  0.0131,  0.0198, -0.0761],\n            ...,\n            [-0.0185, -0.0511, -0.0143,  ...,  0.0336,  0.0187, -0.0377],\n            [-0.0027,  0.0299, -0.0614,  ..., -0.0720, -0.0267,  0.0693],\n            [-0.0677, -0.0148, -0.0819,  ...,  0.0149,  0.0292, -0.0460]],\n  \n           [[ 0.0752,  0.0715,  0.0202,  ..., -0.0660,  0.0779, -0.0038],\n            [ 0.0056,  0.0778, -0.0407,  ...,  0.0376,  0.0553, -0.0123],\n            [-0.0423, -0.0391,  0.0369,  ...,  0.0813, -0.0068, -0.0613],\n            ...,\n            [-0.0453, -0.0515,  0.0310,  ...,  0.0771, -0.0672,  0.0477],\n            [-0.0777, -0.0033,  0.0720,  ..., -0.0360, -0.0202,  0.0087],\n            [ 0.0019,  0.0422, -0.0266,  ...,  0.0550,  0.0105,  0.0208]],\n  \n           [[ 0.0591, -0.0412,  0.0784,  ..., -0.0788, -0.0564, -0.0115],\n            [-0.0199,  0.0186, -0.0693,  ..., -0.0666,  0.0767,  0.0739],\n            [-0.0080, -0.0765,  0.0312,  ..., -0.0496,  0.0357,  0.0303],\n            ...,\n            [ 0.0033, -0.0461,  0.0532,  ...,  0.0815,  0.0809, -0.0347],\n            [-0.0202, -0.0687,  0.0811,  ...,  0.0496,  0.0810, -0.0410],\n            [-0.0377, -0.0426, -0.0372,  ..., -0.0796, -0.0033,  0.0050]]],\n  \n  \n          [[[-0.0793,  0.0004,  0.0047,  ..., -0.0082, -0.0765,  0.0107],\n            [ 0.0006,  0.0711, -0.0337,  ..., -0.0199,  0.0382, -0.0107],\n            [ 0.0538,  0.0070, -0.0584,  ..., -0.0150,  0.0338, -0.0681],\n            ...,\n            [ 0.0657, -0.0630,  0.0135,  ...,  0.0122, -0.0387,  0.0192],\n            [ 0.0723, -0.0488,  0.0138,  ..., -0.0586,  0.0632, -0.0393],\n            [ 0.0811,  0.0750, -0.0759,  ...,  0.0659, -0.0670, -0.0693]],\n  \n           [[ 0.0112,  0.0442, -0.0374,  ..., -0.0510, -0.0757, -0.0645],\n            [-0.0146, -0.0409, -0.0492,  ...,  0.0678,  0.0770, -0.0353],\n            [-0.0329,  0.0427,  0.0373,  ..., -0.0528, -0.0551, -0.0070],\n            ...,\n            [-0.0116, -0.0022,  0.0522,  ..., -0.0152,  0.0172, -0.0677],\n            [ 0.0369,  0.0678,  0.0511,  ...,  0.0267, -0.0541,  0.0811],\n            [-0.0257,  0.0410,  0.0353,  ...,  0.0541,  0.0143,  0.0521]],\n  \n           [[ 0.0210,  0.0222,  0.0397,  ..., -0.0078, -0.0179,  0.0260],\n            [ 0.0242, -0.0703, -0.0264,  ...,  0.0161, -0.0550, -0.0176],\n            [-0.0071, -0.0712, -0.0747,  ..., -0.0494,  0.0677, -0.0196],\n            ...,\n            [-0.0359,  0.0099,  0.0758,  ..., -0.0463,  0.0123,  0.0314],\n            [-0.0512, -0.0532,  0.0337,  ..., -0.0142,  0.0091,  0.0817],\n            [-0.0413,  0.0659, -0.0394,  ...,  0.0113,  0.0510,  0.0097]]],\n  \n  \n          [[[-0.0255, -0.0605, -0.0686,  ..., -0.0207,  0.0568, -0.0224],\n            [ 0.0375, -0.0372, -0.0015,  ...,  0.0744, -0.0312,  0.0642],\n            [ 0.0711, -0.0121,  0.0541,  ..., -0.0421, -0.0010,  0.0597],\n            ...,\n            [-0.0709, -0.0784,  0.0682,  ..., -0.0801, -0.0246, -0.0285],\n            [ 0.0051, -0.0818, -0.0370,  ..., -0.0101,  0.0071, -0.0117],\n            [-0.0237,  0.0355, -0.0532,  ..., -0.0155,  0.0725, -0.0613]],\n  \n           [[-0.0294,  0.0544, -0.0697,  ...,  0.0527,  0.0801, -0.0048],\n            [-0.0087, -0.0633, -0.0303,  ...,  0.0249, -0.0372,  0.0778],\n            [ 0.0079, -0.0420,  0.0084,  ..., -0.0483,  0.0042,  0.0178],\n            ...,\n            [ 0.0511,  0.0093, -0.0484,  ..., -0.0413, -0.0216, -0.0520],\n            [-0.0465,  0.0382, -0.0637,  ..., -0.0681,  0.0673, -0.0156],\n            [ 0.0758,  0.0355, -0.0679,  ...,  0.0637, -0.0463, -0.0138]],\n  \n           [[-0.0191,  0.0705, -0.0650,  ..., -0.0285, -0.0529, -0.0060],\n            [-0.0737, -0.0131, -0.0625,  ...,  0.0747,  0.0396, -0.0325],\n            [-0.0122,  0.0113,  0.0214,  ..., -0.0567,  0.0151,  0.0031],\n            ...,\n            [-0.0454, -0.0132,  0.0485,  ..., -0.0107, -0.0687,  0.0471],\n            [ 0.0229, -0.0811,  0.0808,  ..., -0.0793,  0.0300,  0.0270],\n            [ 0.0175,  0.0514, -0.0066,  ...,  0.0146, -0.0357, -0.0367]]]],\n         requires_grad=True)\n)", "parameters": [["weight", [64, 3, 7, 7]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [9408]}, {"name": "bn1", "id": 140311229273760, "class_name": "BatchNorm2d(\n  self.momentum=0.1, self.weight=Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-3.9063e-03, -2.3172e-03, -4.2204e-04,  2.3021e-04, -6.5349e-04,\n          -2.3018e-04, -1.0130e-04,  1.4837e-03, -1.4968e-03,  3.5189e-05,\n           3.6657e-05,  3.3647e-04, -2.0966e-03, -8.6414e-04,  1.7350e-03,\n          -6.7613e-04, -2.0208e-04,  1.1149e-03,  2.0724e-04,  1.5740e-03,\n           1.9975e-03, -7.1045e-04,  1.3255e-03,  2.2001e-04,  3.8533e-03,\n          -1.3833e-03, -2.2627e-04, -3.3064e-04,  3.3503e-04, -5.3788e-04,\n           5.3253e-04, -1.9281e-03, -9.1594e-04,  4.1517e-03, -7.3942e-04,\n          -8.2882e-04, -2.0322e-03,  7.6017e-04,  1.7862e-03, -1.8373e-04,\n           1.6017e-03,  1.4689e-03,  1.0354e-03, -1.6671e-03,  7.6181e-04,\n          -1.7450e-03,  3.7272e-03,  4.4410e-04, -3.7888e-03,  1.5492e-03,\n          -1.0542e-03,  2.9004e-04,  1.9480e-03, -3.8276e-05,  1.2516e-04,\n           7.0696e-06, -1.7993e-03,  1.6411e-03,  3.6366e-04,  2.1333e-03,\n          -1.6327e-03,  7.3823e-04, -1.5622e-03, -2.8641e-03],\n         grad_fn=<AddBackward0>), self.running_var=tensor([1.0718, 0.9594, 0.9238, 0.9101, 0.9366, 0.9116, 0.9126, 0.9470, 0.9477,\n          0.9152, 0.9355, 0.9624, 0.9818, 0.9325, 0.9601, 0.9338, 0.9428, 0.9759,\n          0.9356, 0.9243, 0.9490, 0.9252, 0.9299, 0.9285, 1.0946, 0.9260, 0.9232,\n          0.9102, 0.9332, 0.9164, 0.9201, 0.9521, 0.9258, 1.0391, 0.9362, 0.9333,\n          0.9480, 0.9368, 0.9403, 0.9340, 0.9350, 0.9752, 0.9343, 0.9446, 0.9311,\n          0.9547, 1.0018, 0.9293, 1.0376, 0.9187, 0.9947, 0.9240, 0.9450, 0.9543,\n          0.9106, 0.9061, 0.9361, 0.9393, 0.9262, 0.9640, 0.9394, 0.9360, 0.9480,\n          0.9859], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n)", "parameters": [["weight", [64]], ["bias", [64]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [64, 64]}, {"name": "relu", "id": 140311229273664, "class_name": "ReLU()", "parameters": [], "output_shape": [[512, 64, 16, 16]], "num_parameters": []}, {"name": "pool", "id": 140311229273616, "class_name": "MaxPool2d(self.kernel_size=(3, 3), self.stride=(3, 3), self.padding=1)", "parameters": [], "output_shape": [[512, 64, 6, 6]], "num_parameters": []}, {"name": "layer1", "id": 140311229273040, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 0.0334, -0.0312,  0.0007],\n                [ 0.0181, -0.0358,  0.0283],\n                [-0.0076, -0.0045,  0.0187]],\n      \n               [[ 0.0382, -0.0218,  0.0079],\n                [ 0.0116,  0.0115,  0.0027],\n                [-0.0164, -0.0393,  0.0323]],\n      \n               [[ 0.0291, -0.0341,  0.0026],\n                [ 0.0127,  0.0234,  0.0387],\n                [-0.0247, -0.0236, -0.0338]],\n      \n               ...,\n      \n               [[-0.0358, -0.0396,  0.0368],\n                [ 0.0040,  0.0101, -0.0155],\n                [-0.0310,  0.0402, -0.0242]],\n      \n               [[ 0.0174, -0.0093, -0.0262],\n                [-0.0343, -0.0035,  0.0176],\n                [-0.0009, -0.0404,  0.0296]],\n      \n               [[-0.0053, -0.0354, -0.0351],\n                [-0.0353,  0.0171, -0.0415],\n                [-0.0376,  0.0102,  0.0388]]],\n      \n      \n              [[[-0.0034,  0.0037, -0.0055],\n                [-0.0046,  0.0328,  0.0415],\n                [ 0.0362, -0.0291, -0.0095]],\n      \n               [[-0.0166, -0.0131,  0.0146],\n                [-0.0164, -0.0174,  0.0198],\n                [-0.0180, -0.0378, -0.0031]],\n      \n               [[ 0.0066, -0.0020,  0.0089],\n                [ 0.0079,  0.0277, -0.0295],\n                [ 0.0358, -0.0175,  0.0128]],\n      \n               ...,\n      \n               [[-0.0036,  0.0409,  0.0311],\n                [ 0.0136, -0.0244,  0.0269],\n                [ 0.0258, -0.0369, -0.0226]],\n      \n               [[-0.0003,  0.0288, -0.0157],\n                [ 0.0060, -0.0201, -0.0107],\n                [ 0.0228, -0.0172, -0.0258]],\n      \n               [[ 0.0323, -0.0199, -0.0213],\n                [-0.0018, -0.0034,  0.0202],\n                [-0.0322, -0.0158,  0.0119]]],\n      \n      \n              [[[-0.0250, -0.0187,  0.0244],\n                [ 0.0009, -0.0032, -0.0011],\n                [-0.0035,  0.0203,  0.0273]],\n      \n               [[-0.0216, -0.0413, -0.0255],\n                [ 0.0417,  0.0022,  0.0395],\n                [ 0.0284, -0.0011, -0.0178]],\n      \n               [[-0.0128, -0.0086,  0.0036],\n                [ 0.0266,  0.0002, -0.0215],\n                [ 0.0143, -0.0302, -0.0377]],\n      \n               ...,\n      \n               [[ 0.0383,  0.0022,  0.0154],\n                [ 0.0132,  0.0370, -0.0186],\n                [ 0.0167,  0.0298,  0.0089]],\n      \n               [[-0.0187,  0.0270, -0.0388],\n                [-0.0347,  0.0014, -0.0128],\n                [ 0.0282, -0.0353, -0.0222]],\n      \n               [[-0.0388,  0.0260,  0.0241],\n                [ 0.0015, -0.0024,  0.0082],\n                [-0.0095,  0.0121, -0.0322]]],\n      \n      \n              ...,\n      \n      \n              [[[-0.0391,  0.0033, -0.0413],\n                [-0.0269,  0.0178, -0.0258],\n                [ 0.0259, -0.0268, -0.0221]],\n      \n               [[ 0.0267, -0.0176, -0.0154],\n                [-0.0314,  0.0322, -0.0133],\n                [-0.0363,  0.0018, -0.0209]],\n      \n               [[ 0.0413,  0.0374, -0.0317],\n                [-0.0079,  0.0020, -0.0049],\n                [ 0.0108,  0.0082, -0.0232]],\n      \n               ...,\n      \n               [[-0.0043, -0.0329, -0.0283],\n                [-0.0221, -0.0155,  0.0404],\n                [ 0.0076,  0.0048, -0.0207]],\n      \n               [[-0.0394, -0.0339, -0.0010],\n                [-0.0080,  0.0205, -0.0261],\n                [-0.0089,  0.0004, -0.0237]],\n      \n               [[-0.0207, -0.0142,  0.0052],\n                [ 0.0240,  0.0176, -0.0067],\n                [ 0.0370, -0.0201,  0.0271]]],\n      \n      \n              [[[ 0.0409,  0.0021, -0.0270],\n                [-0.0376, -0.0220,  0.0247],\n                [ 0.0306, -0.0408,  0.0099]],\n      \n               [[ 0.0315, -0.0064,  0.0224],\n                [ 0.0049,  0.0215, -0.0011],\n                [ 0.0347,  0.0344,  0.0370]],\n      \n               [[ 0.0068,  0.0083,  0.0133],\n                [ 0.0253,  0.0378, -0.0187],\n                [-0.0225, -0.0116,  0.0341]],\n      \n               ...,\n      \n               [[ 0.0328, -0.0177, -0.0189],\n                [-0.0085,  0.0205,  0.0067],\n                [ 0.0112,  0.0374,  0.0289]],\n      \n               [[ 0.0081, -0.0399,  0.0321],\n                [-0.0133,  0.0141, -0.0345],\n                [-0.0180, -0.0078,  0.0083]],\n      \n               [[-0.0011, -0.0006,  0.0377],\n                [ 0.0158, -0.0123, -0.0086],\n                [-0.0405,  0.0343, -0.0159]]],\n      \n      \n              [[[-0.0236, -0.0340,  0.0171],\n                [ 0.0378, -0.0323,  0.0276],\n                [-0.0083, -0.0049, -0.0103]],\n      \n               [[-0.0339,  0.0340,  0.0387],\n                [ 0.0065, -0.0311,  0.0004],\n                [ 0.0176, -0.0071, -0.0050]],\n      \n               [[ 0.0096,  0.0333,  0.0100],\n                [ 0.0407,  0.0285,  0.0208],\n                [ 0.0146, -0.0039, -0.0412]],\n      \n               ...,\n      \n               [[-0.0365, -0.0263, -0.0344],\n                [-0.0338,  0.0265,  0.0053],\n                [-0.0172, -0.0077, -0.0384]],\n      \n               [[ 0.0249,  0.0145,  0.0414],\n                [-0.0209, -0.0115,  0.0094],\n                [ 0.0187, -0.0364,  0.0239]],\n      \n               [[-0.0045,  0.0031,  0.0215],\n                [ 0.0009,  0.0318,  0.0159],\n                [ 0.0380, -0.0340, -0.0087]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-0.0489,  0.0174, -0.0371, -0.0326,  0.0415, -0.0336, -0.0809,  0.0261,\n              -0.0289,  0.0245,  0.0475,  0.0459,  0.0483, -0.0098,  0.0043,  0.0617,\n               0.0283,  0.0060,  0.0194,  0.0015,  0.0062, -0.0146, -0.0598, -0.0224,\n              -0.0238, -0.0580,  0.0454,  0.0267, -0.0041,  0.0298, -0.0103, -0.0052,\n               0.0321,  0.0225,  0.0122, -0.0006,  0.0540, -0.0413,  0.0836, -0.0122,\n              -0.0646, -0.0322,  0.0493, -0.0334, -0.0170, -0.0903,  0.0183, -0.0055,\n               0.0474, -0.0013, -0.0252, -0.0769,  0.0539,  0.0243, -0.0269,  0.0550,\n              -0.0475, -0.0302, -0.0033,  0.0121,  0.0186,  0.0295,  0.0750,  0.0143],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9178, 0.9121, 0.9131, 0.9156, 0.9152, 0.9248, 0.9243, 0.9160, 0.9177,\n              0.9179, 0.9162, 0.9230, 0.9175, 0.9115, 0.9216, 0.9261, 0.9226, 0.9142,\n              0.9112, 0.9241, 0.9260, 0.9120, 0.9410, 0.9304, 0.9089, 0.9226, 0.9138,\n              0.9184, 0.9207, 0.9194, 0.9236, 0.9196, 0.9208, 0.9172, 0.9168, 0.9370,\n              0.9210, 0.9246, 0.9406, 0.9115, 0.9197, 0.9205, 0.9210, 0.9203, 0.9188,\n              0.9337, 0.9102, 0.9234, 0.9159, 0.9158, 0.9178, 0.9372, 0.9135, 0.9280,\n              0.9285, 0.9275, 0.9223, 0.9266, 0.9195, 0.9254, 0.9324, 0.9167, 0.9351,\n              0.9222], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-1.2184e-02,  4.1634e-02, -2.5958e-02],\n                [-2.1367e-02,  2.2485e-02, -4.7149e-03],\n                [-3.1308e-02,  1.5320e-02,  1.4204e-03]],\n      \n               [[ 1.9331e-02, -2.3647e-02, -1.7461e-02],\n                [-1.5852e-02, -2.6018e-03,  2.9826e-02],\n                [ 3.4765e-02,  1.9636e-02, -2.0196e-03]],\n      \n               [[ 3.7254e-02,  1.0623e-02, -5.9603e-03],\n                [ 1.8007e-02,  3.4956e-02,  1.7957e-02],\n                [-1.0683e-02,  3.0308e-02,  2.8202e-03]],\n      \n               ...,\n      \n               [[ 1.8262e-02,  2.3843e-02,  3.0253e-02],\n                [-7.6285e-04,  3.6077e-02,  5.7512e-03],\n                [-7.0397e-03,  3.2032e-02,  3.4689e-02]],\n      \n               [[ 1.8441e-02,  4.0564e-03, -8.6316e-03],\n                [-3.9933e-02,  3.3894e-02,  3.9188e-02],\n                [-2.1470e-02,  2.4033e-02, -9.5495e-03]],\n      \n               [[-1.7912e-02, -2.5106e-02, -3.7509e-02],\n                [ 3.4953e-02, -3.3356e-02, -3.0733e-02],\n                [-1.6370e-02, -3.4610e-02,  2.9213e-02]]],\n      \n      \n              [[[-3.8043e-02, -8.4019e-03,  9.3138e-03],\n                [ 2.9777e-02, -3.7119e-02,  1.3964e-02],\n                [-1.3311e-02, -1.2698e-02, -1.8394e-02]],\n      \n               [[ 8.0700e-03, -1.1880e-03,  3.2144e-02],\n                [-3.8174e-02,  3.3848e-02, -1.5978e-02],\n                [ 3.2024e-03,  2.0874e-02,  1.0660e-02]],\n      \n               [[-1.4517e-02,  4.5731e-03,  2.5615e-02],\n                [-1.4315e-02, -1.6795e-02,  1.2032e-02],\n                [ 3.2120e-03, -1.9356e-03,  2.6641e-02]],\n      \n               ...,\n      \n               [[-1.6189e-02, -7.8175e-03,  7.6528e-03],\n                [-1.4890e-02, -6.9194e-03, -3.0977e-02],\n                [ 3.2561e-02, -5.8723e-04,  4.3843e-03]],\n      \n               [[-3.8862e-02,  1.6378e-02, -7.5674e-03],\n                [ 1.6644e-02, -3.2230e-02, -2.6392e-02],\n                [ 3.7897e-02,  2.1509e-02, -1.0439e-02]],\n      \n               [[-1.2557e-03, -9.3439e-03,  2.7278e-02],\n                [-1.5948e-02, -3.3891e-02, -2.5693e-02],\n                [ 1.8368e-02, -4.1224e-02, -1.8701e-02]]],\n      \n      \n              [[[-3.7579e-02,  1.8724e-02,  3.2090e-02],\n                [-8.7876e-03,  5.1894e-03, -3.9551e-02],\n                [-3.6323e-02, -6.0480e-03,  2.2946e-03]],\n      \n               [[-1.5957e-02, -3.6095e-02,  1.7665e-02],\n                [ 1.5062e-02,  3.8287e-02, -8.3704e-03],\n                [-1.6142e-02, -9.3992e-04, -7.6340e-03]],\n      \n               [[-1.8707e-02, -2.9478e-02, -2.6888e-02],\n                [ 1.9853e-02, -3.0711e-02, -4.0990e-02],\n                [-3.7596e-02,  1.4714e-02,  2.0955e-02]],\n      \n               ...,\n      \n               [[ 3.6661e-02, -3.6403e-02, -4.0476e-02],\n                [-4.0813e-02,  2.8373e-02,  3.5583e-02],\n                [-2.0198e-02, -1.4940e-02, -3.6348e-02]],\n      \n               [[ 1.2134e-02,  3.5093e-03,  3.4233e-02],\n                [-4.5312e-03, -2.5875e-02,  1.4386e-02],\n                [-2.8713e-02,  4.6387e-03,  2.0498e-02]],\n      \n               [[ 2.5064e-02, -1.0592e-02, -2.0659e-03],\n                [-3.9533e-02,  6.4570e-03,  2.8764e-02],\n                [-2.6573e-02, -2.8463e-02, -1.8876e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[-3.9491e-02, -2.4384e-02, -2.8518e-02],\n                [ 3.3279e-02,  2.9693e-02,  2.9542e-02],\n                [ 4.8728e-03, -2.3525e-04, -2.7899e-02]],\n      \n               [[-2.7499e-03, -3.1155e-02, -2.4401e-02],\n                [ 3.6979e-02, -2.4549e-02, -1.0085e-02],\n                [-1.7507e-02,  1.0929e-02, -3.0075e-03]],\n      \n               [[ 1.1403e-02,  2.0448e-02, -3.5044e-02],\n                [ 3.3967e-02, -3.5200e-02, -2.4702e-02],\n                [-7.2911e-03, -3.0378e-02, -3.7715e-02]],\n      \n               ...,\n      \n               [[-4.0079e-02,  3.2147e-02,  6.7694e-03],\n                [ 3.2188e-02, -1.4200e-02, -2.7691e-02],\n                [ 5.2440e-03,  1.7441e-02,  3.9355e-02]],\n      \n               [[ 1.7580e-02, -1.4122e-02, -2.4305e-02],\n                [-9.8563e-03, -4.0965e-02, -2.0069e-02],\n                [-2.3679e-02, -2.8172e-02, -3.9250e-02]],\n      \n               [[-4.0475e-02, -2.8084e-02,  3.4495e-02],\n                [ 2.2150e-02,  2.5732e-02, -3.5630e-02],\n                [-2.2120e-02, -7.8447e-04, -1.6988e-02]]],\n      \n      \n              [[[-9.9188e-03, -1.8729e-02, -2.9198e-02],\n                [-1.8485e-03,  2.7616e-02, -1.3470e-02],\n                [ 1.3542e-02,  1.3640e-02,  3.3347e-02]],\n      \n               [[-4.0222e-02,  2.7768e-02,  2.2163e-02],\n                [-2.6648e-05,  2.0464e-02,  4.0872e-02],\n                [ 2.9735e-02, -2.6406e-02, -1.9144e-02]],\n      \n               [[-2.7324e-02,  1.8241e-02, -1.9285e-02],\n                [ 3.0938e-02, -9.0151e-04,  3.0042e-02],\n                [ 2.3324e-02,  3.6824e-02,  3.5624e-02]],\n      \n               ...,\n      \n               [[ 8.8299e-03,  1.7983e-02,  3.6773e-02],\n                [-1.9907e-02,  2.8646e-02, -3.7089e-02],\n                [ 3.0208e-03,  8.2206e-04, -2.0545e-02]],\n      \n               [[ 1.7764e-02, -3.2869e-02, -1.8931e-02],\n                [-1.9970e-02,  1.3737e-02, -7.7273e-03],\n                [ 4.1638e-02,  1.4578e-02,  9.3102e-03]],\n      \n               [[-2.2606e-02, -2.3982e-02, -1.5811e-02],\n                [-2.7226e-02, -3.6718e-02,  2.8190e-02],\n                [ 3.3654e-02,  3.7627e-02, -2.1494e-02]]],\n      \n      \n              [[[ 1.0586e-02,  2.9447e-02,  1.1061e-02],\n                [-7.9988e-04,  2.0133e-02,  1.0928e-02],\n                [-4.0487e-02,  3.7739e-02, -4.1230e-02]],\n      \n               [[ 2.3916e-03, -3.4781e-02, -2.7257e-02],\n                [-3.6209e-03, -3.3786e-02, -3.8829e-02],\n                [ 2.2472e-02, -4.0610e-02, -7.1123e-03]],\n      \n               [[ 6.9198e-03, -1.8868e-02, -3.1062e-02],\n                [-1.5850e-02, -7.0699e-03, -4.0399e-02],\n                [-3.5618e-02, -3.2703e-02,  3.6440e-02]],\n      \n               ...,\n      \n               [[-2.4423e-03,  2.3180e-02,  3.2546e-02],\n                [-4.7968e-04,  2.0931e-02,  3.7797e-02],\n                [-2.2335e-02, -7.4941e-03, -1.4824e-02]],\n      \n               [[-1.1142e-03, -2.8413e-02,  3.9578e-02],\n                [-5.0356e-03, -3.6929e-02,  7.8832e-03],\n                [ 2.0610e-02,  6.8799e-05, -9.7965e-04]],\n      \n               [[-2.3909e-02,  9.9463e-03, -3.5060e-02],\n                [ 1.2008e-02, -1.1369e-02,  1.4198e-02],\n                [ 3.8488e-02,  3.3350e-02, -3.7660e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0434,  0.0150, -0.0386,  0.0041,  0.0285, -0.0191,  0.0112,  0.0252,\n              -0.0249,  0.0307,  0.0252, -0.0175, -0.0166,  0.0106, -0.0065,  0.0026,\n              -0.0218, -0.0092,  0.0091,  0.0108, -0.0071, -0.0146,  0.0176,  0.0022,\n               0.0308, -0.0374, -0.0150, -0.0044, -0.0321,  0.0277, -0.0172,  0.0420,\n              -0.0303,  0.0078,  0.0169, -0.0077,  0.0003,  0.0322, -0.0230,  0.0053,\n              -0.0444, -0.0043, -0.0101,  0.0170, -0.0064, -0.0036, -0.0121,  0.0024,\n              -0.0308, -0.0135,  0.0078,  0.0230, -0.0094,  0.0006, -0.0040,  0.0188,\n               0.0088,  0.0049,  0.0025,  0.0056, -0.0117, -0.0015,  0.0089, -0.0213],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9147, 0.9109, 0.9124, 0.9108, 0.9123, 0.9086, 0.9109, 0.9102, 0.9067,\n              0.9095, 0.9120, 0.9098, 0.9084, 0.9085, 0.9111, 0.9080, 0.9101, 0.9099,\n              0.9073, 0.9102, 0.9093, 0.9115, 0.9149, 0.9126, 0.9071, 0.9107, 0.9105,\n              0.9143, 0.9136, 0.9076, 0.9089, 0.9143, 0.9150, 0.9132, 0.9081, 0.9087,\n              0.9087, 0.9104, 0.9106, 0.9096, 0.9099, 0.9161, 0.9072, 0.9074, 0.9098,\n              0.9116, 0.9096, 0.9109, 0.9084, 0.9084, 0.9073, 0.9097, 0.9081, 0.9089,\n              0.9072, 0.9107, 0.9081, 0.9076, 0.9098, 0.9139, 0.9100, 0.9099, 0.9092,\n              0.9108], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n  )\n)", "parameters": [["0.conv1.weight", [64, 64, 3, 3]], ["0.bn1.weight", [64]], ["0.bn1.bias", [64]], ["0.conv2.weight", [64, 64, 3, 3]], ["0.bn2.weight", [64]], ["0.bn2.bias", [64]]], "output_shape": [[512, 64, 6, 6]], "num_parameters": [36864, 64, 64, 36864, 64, 64]}, {"name": "layer2", "id": 140311229272128, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 0.0057, -0.0250,  0.0164],\n                [ 0.0400,  0.0175,  0.0126],\n                [ 0.0283,  0.0146, -0.0401]],\n      \n               [[ 0.0236,  0.0396, -0.0362],\n                [ 0.0412, -0.0110,  0.0276],\n                [ 0.0231, -0.0224,  0.0110]],\n      \n               [[ 0.0382,  0.0120, -0.0297],\n                [-0.0311,  0.0009, -0.0250],\n                [-0.0052, -0.0384, -0.0095]],\n      \n               ...,\n      \n               [[-0.0020, -0.0130,  0.0361],\n                [ 0.0023, -0.0018,  0.0253],\n                [-0.0100,  0.0198, -0.0332]],\n      \n               [[ 0.0295, -0.0270,  0.0272],\n                [ 0.0269, -0.0121,  0.0229],\n                [-0.0038, -0.0373,  0.0212]],\n      \n               [[-0.0022,  0.0388, -0.0207],\n                [ 0.0013,  0.0314,  0.0258],\n                [ 0.0246, -0.0366, -0.0174]]],\n      \n      \n              [[[ 0.0224, -0.0179,  0.0225],\n                [ 0.0295,  0.0259, -0.0039],\n                [ 0.0054, -0.0404, -0.0329]],\n      \n               [[ 0.0411,  0.0183,  0.0137],\n                [-0.0212, -0.0176, -0.0038],\n                [-0.0008, -0.0170, -0.0046]],\n      \n               [[-0.0190,  0.0023,  0.0191],\n                [-0.0226, -0.0239,  0.0309],\n                [-0.0227,  0.0045, -0.0062]],\n      \n               ...,\n      \n               [[ 0.0030, -0.0162, -0.0175],\n                [-0.0297, -0.0117, -0.0299],\n                [ 0.0158, -0.0325,  0.0294]],\n      \n               [[ 0.0091,  0.0081, -0.0102],\n                [ 0.0069,  0.0052,  0.0207],\n                [ 0.0136,  0.0229,  0.0075]],\n      \n               [[ 0.0100,  0.0261,  0.0244],\n                [ 0.0009, -0.0169, -0.0095],\n                [-0.0293, -0.0390, -0.0100]]],\n      \n      \n              [[[ 0.0014, -0.0152,  0.0193],\n                [ 0.0396,  0.0127, -0.0010],\n                [ 0.0308, -0.0216, -0.0084]],\n      \n               [[-0.0200, -0.0132,  0.0256],\n                [-0.0216, -0.0036,  0.0090],\n                [ 0.0016, -0.0195,  0.0144]],\n      \n               [[-0.0339, -0.0220,  0.0237],\n                [-0.0070,  0.0390, -0.0312],\n                [ 0.0162,  0.0077,  0.0315]],\n      \n               ...,\n      \n               [[-0.0376,  0.0175,  0.0040],\n                [-0.0266,  0.0057,  0.0188],\n                [ 0.0136,  0.0247, -0.0281]],\n      \n               [[ 0.0199, -0.0213, -0.0272],\n                [-0.0410,  0.0378,  0.0345],\n                [-0.0163, -0.0283, -0.0190]],\n      \n               [[ 0.0343,  0.0304,  0.0360],\n                [ 0.0023, -0.0107,  0.0189],\n                [-0.0237, -0.0134, -0.0154]]],\n      \n      \n              ...,\n      \n      \n              [[[ 0.0392, -0.0104, -0.0348],\n                [-0.0189, -0.0311, -0.0168],\n                [-0.0142, -0.0141,  0.0181]],\n      \n               [[ 0.0048, -0.0108, -0.0263],\n                [-0.0071,  0.0208,  0.0316],\n                [-0.0414, -0.0269,  0.0048]],\n      \n               [[ 0.0091, -0.0118, -0.0128],\n                [-0.0174,  0.0141,  0.0126],\n                [-0.0277,  0.0264, -0.0267]],\n      \n               ...,\n      \n               [[-0.0175, -0.0075, -0.0026],\n                [ 0.0326,  0.0282,  0.0414],\n                [ 0.0067,  0.0170,  0.0196]],\n      \n               [[-0.0160,  0.0342, -0.0343],\n                [-0.0053,  0.0250, -0.0342],\n                [-0.0211,  0.0233,  0.0383]],\n      \n               [[ 0.0367,  0.0351,  0.0027],\n                [-0.0396, -0.0216, -0.0040],\n                [-0.0285, -0.0314, -0.0037]]],\n      \n      \n              [[[ 0.0027, -0.0112,  0.0177],\n                [-0.0042,  0.0080, -0.0230],\n                [ 0.0394, -0.0065, -0.0051]],\n      \n               [[ 0.0264,  0.0158, -0.0310],\n                [ 0.0161,  0.0050, -0.0280],\n                [-0.0343, -0.0182, -0.0258]],\n      \n               [[-0.0199, -0.0059, -0.0371],\n                [-0.0358, -0.0181, -0.0417],\n                [-0.0161,  0.0250,  0.0275]],\n      \n               ...,\n      \n               [[ 0.0386,  0.0326, -0.0135],\n                [-0.0050,  0.0078,  0.0253],\n                [ 0.0081,  0.0167,  0.0050]],\n      \n               [[-0.0315,  0.0066,  0.0095],\n                [-0.0289,  0.0260,  0.0028],\n                [-0.0352, -0.0191,  0.0049]],\n      \n               [[-0.0171,  0.0085, -0.0021],\n                [ 0.0067, -0.0045,  0.0073],\n                [ 0.0095,  0.0035, -0.0017]]],\n      \n      \n              [[[ 0.0402, -0.0410,  0.0270],\n                [ 0.0266, -0.0191,  0.0135],\n                [ 0.0267, -0.0184, -0.0173]],\n      \n               [[ 0.0232,  0.0085, -0.0056],\n                [-0.0249,  0.0256,  0.0190],\n                [-0.0143,  0.0188, -0.0378]],\n      \n               [[ 0.0100,  0.0411, -0.0399],\n                [-0.0080,  0.0073,  0.0229],\n                [ 0.0376, -0.0316,  0.0030]],\n      \n               ...,\n      \n               [[ 0.0208, -0.0193, -0.0196],\n                [-0.0336,  0.0228, -0.0224],\n                [ 0.0128, -0.0258, -0.0097]],\n      \n               [[ 0.0114, -0.0127,  0.0354],\n                [ 0.0203, -0.0273,  0.0006],\n                [ 0.0173,  0.0183,  0.0299]],\n      \n               [[-0.0087,  0.0061, -0.0020],\n                [ 0.0365,  0.0362,  0.0407],\n                [ 0.0228, -0.0323, -0.0191]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 4.1394e-02, -7.2975e-02,  6.2889e-02,  3.3404e-02,  2.3902e-02,\n               6.5906e-02,  6.0861e-02, -7.2144e-02, -7.4606e-02,  2.9694e-02,\n              -2.3581e-02, -2.4060e-02, -5.0286e-02,  8.3339e-03, -1.9893e-02,\n               5.9891e-03, -1.0653e-01, -1.2048e-02,  9.9890e-03, -5.2402e-02,\n               4.1058e-02, -4.4266e-02, -3.3618e-02, -1.2486e-02,  2.9155e-02,\n              -5.4711e-04,  1.1004e-01,  2.6732e-02, -7.2164e-02,  5.2000e-02,\n              -4.1631e-02,  6.1241e-02, -6.9690e-02, -2.2360e-02, -1.4087e-02,\n               1.0292e-01, -9.1550e-02, -1.6588e-02,  2.7741e-02, -6.6358e-03,\n              -4.7811e-02,  1.1507e-01,  2.7581e-03, -8.0427e-02,  6.9451e-02,\n               9.5358e-03,  6.3837e-02, -6.2774e-02,  5.5895e-02, -3.2846e-02,\n               3.4996e-02, -2.2172e-02, -1.8492e-03, -3.5467e-02, -2.4416e-02,\n               1.4857e-02,  7.2552e-02, -6.6090e-02,  5.8888e-02, -3.5019e-03,\n              -4.9368e-02,  1.2250e-01, -2.4371e-03, -1.4051e-02, -3.9909e-03,\n               1.3332e-02,  6.1244e-02, -9.1323e-02,  6.5352e-02,  8.1306e-02,\n              -3.0945e-02,  6.5263e-02,  7.0153e-03,  4.0532e-02,  1.3927e-02,\n              -6.6681e-02, -1.6320e-02, -4.2428e-02, -9.6260e-04,  1.3472e-01,\n               8.1935e-02,  9.3863e-02, -7.4628e-02, -1.1165e-04, -1.0072e-02,\n              -1.0177e-01,  1.0866e-01, -1.3672e-01,  4.1320e-02, -4.1976e-02,\n               2.0686e-02, -5.2758e-02, -5.0570e-02,  2.0135e-02, -4.2090e-02,\n              -7.0135e-02,  1.7970e-02, -1.0763e-01, -1.6302e-02, -3.5309e-02,\n               2.9433e-02, -3.0023e-02, -2.5823e-02, -3.4141e-02,  5.6244e-02,\n              -4.2239e-02,  8.0195e-03,  2.6554e-02, -4.8255e-02, -1.5531e-02,\n               7.1518e-02,  3.4219e-02,  1.2642e-02,  6.3311e-03, -5.7012e-03,\n              -2.1709e-02, -1.0998e-01,  7.1735e-02, -6.4154e-02,  3.6805e-02,\n               9.8784e-02,  6.3355e-02,  7.2252e-03,  1.3621e-01,  6.4102e-03,\n              -7.8662e-03, -9.9782e-03, -9.6905e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9355, 0.9360, 0.9291, 0.9234, 0.9429, 0.9416, 0.9252, 0.9475, 0.9338,\n              0.9274, 0.9267, 0.9274, 0.9277, 0.9237, 0.9331, 0.9219, 0.9398, 0.9400,\n              0.9265, 0.9465, 0.9246, 0.9330, 0.9408, 0.9450, 0.9326, 0.9438, 0.9549,\n              0.9300, 0.9431, 0.9283, 0.9223, 0.9255, 0.9260, 0.9212, 0.9601, 0.9506,\n              0.9265, 0.9346, 0.9245, 0.9282, 0.9444, 0.9387, 0.9237, 0.9384, 0.9219,\n              0.9488, 0.9322, 0.9267, 0.9241, 0.9528, 0.9326, 0.9326, 0.9261, 0.9564,\n              0.9398, 0.9395, 0.9369, 0.9391, 0.9369, 0.9307, 0.9454, 0.9642, 0.9290,\n              0.9303, 0.9354, 0.9372, 0.9254, 0.9444, 0.9374, 0.9223, 0.9300, 0.9537,\n              0.9233, 0.9323, 0.9368, 0.9433, 0.9277, 0.9288, 0.9294, 0.9526, 0.9386,\n              0.9569, 0.9413, 0.9276, 0.9381, 0.9551, 0.9293, 0.9402, 0.9281, 0.9449,\n              0.9228, 0.9508, 0.9467, 0.9333, 0.9318, 0.9274, 0.9348, 0.9319, 0.9240,\n              0.9469, 0.9396, 0.9274, 0.9297, 0.9366, 0.9314, 0.9348, 0.9241, 0.9268,\n              0.9348, 0.9387, 0.9404, 0.9301, 0.9483, 0.9266, 0.9390, 0.9284, 0.9701,\n              0.9380, 0.9336, 0.9445, 0.9715, 0.9238, 0.9229, 0.9474, 0.9278, 0.9445,\n              0.9365, 0.9876], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-1.5953e-03,  1.8052e-02,  7.0025e-03],\n                [ 2.2437e-02,  1.5265e-02,  3.3878e-03],\n                [ 1.1534e-02,  7.5722e-03, -1.2397e-02]],\n      \n               [[-1.4842e-02, -2.1668e-02,  1.5362e-02],\n                [-1.6061e-02, -1.6161e-02, -5.6432e-03],\n                [ 5.8487e-03, -8.8776e-03, -1.6509e-03]],\n      \n               [[ 2.9097e-02,  1.1946e-02,  4.7752e-03],\n                [ 1.0534e-03,  1.9730e-03,  2.8077e-03],\n                [ 1.2703e-02, -3.9130e-03, -1.4390e-03]],\n      \n               ...,\n      \n               [[-8.9228e-03, -1.6689e-02,  2.7425e-02],\n                [-1.8463e-02,  2.0737e-03, -2.6602e-02],\n                [-2.3340e-02, -9.1874e-03,  2.6908e-02]],\n      \n               [[ 1.1707e-02, -3.3963e-03, -1.8703e-02],\n                [-5.7456e-03,  7.0120e-03, -2.8272e-02],\n                [-1.7167e-02, -8.0353e-03,  2.0477e-02]],\n      \n               [[-2.3692e-02, -2.6276e-02,  6.3431e-03],\n                [-2.9398e-02, -4.0953e-03,  1.8283e-02],\n                [-7.5105e-04,  4.6710e-03, -4.9827e-04]]],\n      \n      \n              [[[-6.5788e-03, -3.6612e-04,  7.6848e-03],\n                [ 1.3910e-02,  1.9377e-02, -1.3290e-02],\n                [-6.8569e-03,  2.7322e-02,  1.2846e-02]],\n      \n               [[ 1.9611e-02,  4.7757e-03,  9.8553e-03],\n                [ 2.9213e-03, -1.7884e-02, -2.7794e-02],\n                [-1.5839e-02,  1.4423e-02,  5.8696e-03]],\n      \n               [[-1.8127e-02,  2.4675e-02,  2.4148e-02],\n                [-1.7413e-02,  8.4603e-03,  1.5076e-02],\n                [ 2.2991e-02, -2.9225e-02,  2.7184e-02]],\n      \n               ...,\n      \n               [[ 2.2408e-02,  2.6539e-02, -9.4770e-03],\n                [ 1.1177e-02,  2.0961e-02,  1.3093e-02],\n                [-2.1441e-02,  9.2392e-03,  2.2742e-02]],\n      \n               [[-1.4655e-02, -1.9316e-02,  1.8192e-02],\n                [ 1.0399e-02,  1.2061e-02, -1.5182e-02],\n                [ 1.5710e-02,  2.0419e-02,  2.6292e-02]],\n      \n               [[-1.8181e-02, -6.5433e-04, -2.7689e-02],\n                [ 9.4956e-03, -2.3653e-02,  2.9238e-02],\n                [ 1.0478e-02, -6.6817e-03,  2.2386e-02]]],\n      \n      \n              [[[ 2.3119e-03,  2.4834e-02,  9.5957e-04],\n                [ 2.9179e-03, -1.1631e-02,  1.9504e-02],\n                [ 2.1828e-02, -4.7377e-04,  2.9119e-02]],\n      \n               [[ 1.6630e-02, -1.7014e-02, -1.7250e-02],\n                [-7.0560e-03,  2.0858e-03, -1.7315e-02],\n                [ 2.2273e-02,  1.9748e-02,  1.4125e-02]],\n      \n               [[-1.3415e-02,  5.6145e-03,  9.5251e-03],\n                [ 3.2163e-03, -2.0525e-02,  1.9670e-02],\n                [ 1.2416e-02, -1.7984e-02,  2.8605e-02]],\n      \n               ...,\n      \n               [[-2.5476e-02, -1.1481e-02, -1.9922e-02],\n                [ 2.1265e-02, -2.8618e-02, -4.4724e-03],\n                [ 2.6334e-02,  9.7975e-03,  1.0194e-02]],\n      \n               [[-1.6085e-04, -7.7010e-03, -1.6753e-02],\n                [ 2.0915e-02, -2.0713e-02, -9.1195e-03],\n                [-4.6378e-03, -5.9816e-03,  6.4987e-03]],\n      \n               [[ 1.0731e-02,  5.3495e-03,  1.0126e-02],\n                [ 2.9212e-02,  1.8492e-03,  5.4604e-03],\n                [-2.5326e-02,  1.1677e-02, -1.9238e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[ 3.7879e-03,  2.1875e-02,  1.9221e-02],\n                [-1.2265e-02, -1.1954e-02, -5.5638e-03],\n                [ 2.1294e-02,  1.0375e-02, -2.0505e-02]],\n      \n               [[ 1.1868e-02,  2.5695e-02,  1.1096e-02],\n                [-2.5452e-02, -7.6227e-03, -2.9138e-02],\n                [ 2.3342e-03, -2.5389e-02,  7.4273e-03]],\n      \n               [[ 6.9143e-03,  2.2792e-02, -1.3839e-02],\n                [ 5.3077e-03, -2.7983e-02,  2.3431e-02],\n                [ 2.4576e-02,  1.6581e-02, -1.2419e-02]],\n      \n               ...,\n      \n               [[-8.2517e-03,  1.2231e-02, -2.6687e-02],\n                [-2.6619e-02,  2.0051e-02, -1.4057e-03],\n                [ 1.5177e-02, -1.4175e-03,  1.7025e-02]],\n      \n               [[-3.5421e-05, -1.2953e-02, -1.2010e-02],\n                [ 1.2550e-02, -5.5676e-03, -1.5034e-02],\n                [ 2.3630e-02,  2.9214e-02, -1.5220e-02]],\n      \n               [[-2.3373e-02, -9.2560e-03, -1.3354e-02],\n                [ 4.9568e-03,  1.0235e-03,  1.5108e-02],\n                [ 2.8089e-02,  2.5732e-02, -1.9046e-02]]],\n      \n      \n              [[[-1.1585e-03,  2.4281e-02,  2.2612e-02],\n                [ 7.5911e-03,  2.7157e-02, -8.2164e-03],\n                [-2.1187e-02,  2.6432e-02,  1.0670e-02]],\n      \n               [[ 1.0163e-02, -1.5796e-03,  8.2769e-03],\n                [ 1.1945e-02,  1.3824e-02,  2.3905e-02],\n                [-1.5874e-02,  9.7896e-04,  2.9384e-02]],\n      \n               [[ 2.0054e-02, -2.4699e-02, -3.6114e-03],\n                [ 3.0419e-03, -1.3150e-02,  7.9093e-03],\n                [ 1.5342e-02, -2.0776e-02, -2.1437e-02]],\n      \n               ...,\n      \n               [[-2.7104e-02,  2.7252e-02,  1.3694e-02],\n                [ 1.1497e-02, -5.1294e-03,  1.2147e-02],\n                [-2.0797e-02, -1.2632e-02,  5.0602e-03]],\n      \n               [[-2.8353e-02,  8.5855e-03,  2.8037e-03],\n                [ 1.7520e-02,  1.4392e-02, -1.3022e-03],\n                [-2.3855e-02, -5.4406e-03, -2.8913e-02]],\n      \n               [[-1.0976e-02,  2.7948e-02, -2.3703e-02],\n                [-6.5804e-03, -2.7408e-02,  2.4444e-02],\n                [ 2.8375e-02,  1.0897e-02, -2.0069e-02]]],\n      \n      \n              [[[ 1.2214e-03,  2.6444e-02, -1.8880e-02],\n                [-2.5277e-02,  1.7336e-02,  7.6369e-03],\n                [-1.6450e-02,  3.7809e-04,  2.1228e-02]],\n      \n               [[-5.4853e-03,  2.7393e-02, -1.1689e-02],\n                [ 2.1337e-02, -2.7279e-02, -2.6043e-02],\n                [-2.7840e-02, -2.0722e-02,  1.2136e-02]],\n      \n               [[-1.8499e-02,  2.8445e-02,  3.7041e-03],\n                [-1.1772e-02,  2.5538e-02,  2.0631e-02],\n                [ 1.5385e-02,  7.5943e-03,  2.1373e-02]],\n      \n               ...,\n      \n               [[-9.6564e-03,  6.7467e-03,  2.4687e-02],\n                [ 2.6053e-02, -5.9515e-04, -2.8834e-02],\n                [-2.8261e-02,  2.9438e-02,  1.6594e-02]],\n      \n               [[-3.9441e-03, -2.4578e-04, -1.0109e-02],\n                [ 1.1954e-02,  3.5758e-03,  1.5927e-02],\n                [-1.7191e-02,  1.1857e-02, -1.2790e-02]],\n      \n               [[-2.9097e-02, -1.0914e-02, -2.2528e-03],\n                [ 1.0369e-02, -2.3489e-02, -2.7468e-02],\n                [ 2.6271e-02, -1.9489e-02,  2.2472e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0204,  0.0194, -0.0119, -0.0273, -0.0326, -0.0214, -0.0127,  0.0083,\n               0.0188, -0.0187, -0.0241,  0.0104,  0.0025,  0.0105,  0.0051, -0.0020,\n              -0.0059, -0.0157, -0.0174, -0.0018, -0.0180,  0.0204, -0.0099, -0.0062,\n              -0.0039,  0.0002,  0.0098,  0.0265, -0.0337, -0.0134,  0.0100, -0.0147,\n              -0.0040, -0.0150,  0.0295, -0.0160, -0.0220, -0.0200,  0.0084, -0.0114,\n               0.0139,  0.0029,  0.0176,  0.0010, -0.0079,  0.0094, -0.0161, -0.0247,\n              -0.0088, -0.0043, -0.0166, -0.0191,  0.0042, -0.0085,  0.0157,  0.0261,\n               0.0243, -0.0078,  0.0055, -0.0020,  0.0136, -0.0216, -0.0220, -0.0113,\n               0.0023,  0.0028, -0.0188, -0.0214, -0.0069,  0.0118,  0.0080,  0.0034,\n               0.0178, -0.0004,  0.0378, -0.0171,  0.0293,  0.0297,  0.0130,  0.0126,\n              -0.0027,  0.0129,  0.0111,  0.0371, -0.0005, -0.0032, -0.0264,  0.0214,\n              -0.0033,  0.0094,  0.0151, -0.0234, -0.0076,  0.0119, -0.0126, -0.0052,\n               0.0082, -0.0034,  0.0178,  0.0009, -0.0140, -0.0007,  0.0044, -0.0018,\n              -0.0058, -0.0066,  0.0086, -0.0153, -0.0035,  0.0084,  0.0259,  0.0105,\n              -0.0003, -0.0031,  0.0046,  0.0142, -0.0194, -0.0021, -0.0107,  0.0134,\n              -0.0348,  0.0048,  0.0052,  0.0205,  0.0224, -0.0174, -0.0162, -0.0074],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9130, 0.9070, 0.9129, 0.9075, 0.9101, 0.9089, 0.9085, 0.9105, 0.9087,\n              0.9074, 0.9083, 0.9079, 0.9086, 0.9090, 0.9105, 0.9058, 0.9077, 0.9105,\n              0.9134, 0.9063, 0.9082, 0.9079, 0.9083, 0.9077, 0.9060, 0.9065, 0.9076,\n              0.9069, 0.9069, 0.9076, 0.9057, 0.9083, 0.9097, 0.9085, 0.9106, 0.9081,\n              0.9081, 0.9103, 0.9065, 0.9087, 0.9078, 0.9071, 0.9099, 0.9089, 0.9089,\n              0.9060, 0.9068, 0.9108, 0.9066, 0.9070, 0.9106, 0.9054, 0.9073, 0.9100,\n              0.9077, 0.9123, 0.9107, 0.9081, 0.9067, 0.9075, 0.9073, 0.9084, 0.9126,\n              0.9089, 0.9075, 0.9064, 0.9065, 0.9103, 0.9082, 0.9092, 0.9067, 0.9076,\n              0.9103, 0.9080, 0.9112, 0.9075, 0.9070, 0.9089, 0.9078, 0.9082, 0.9131,\n              0.9106, 0.9071, 0.9129, 0.9100, 0.9072, 0.9132, 0.9068, 0.9076, 0.9072,\n              0.9100, 0.9069, 0.9103, 0.9068, 0.9081, 0.9082, 0.9085, 0.9069, 0.9068,\n              0.9058, 0.9081, 0.9076, 0.9108, 0.9081, 0.9076, 0.9062, 0.9077, 0.9082,\n              0.9073, 0.9087, 0.9077, 0.9081, 0.9080, 0.9068, 0.9067, 0.9087, 0.9100,\n              0.9120, 0.9084, 0.9061, 0.9117, 0.9106, 0.9072, 0.9074, 0.9072, 0.9094,\n              0.9059, 0.9066], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[ 0.0827]],\n        \n                 [[ 0.0261]],\n        \n                 [[-0.1146]],\n        \n                 ...,\n        \n                 [[-0.1205]],\n        \n                 [[ 0.1158]],\n        \n                 [[ 0.0712]]],\n        \n        \n                [[[ 0.1105]],\n        \n                 [[ 0.0518]],\n        \n                 [[-0.0932]],\n        \n                 ...,\n        \n                 [[-0.0399]],\n        \n                 [[ 0.1050]],\n        \n                 [[-0.0308]]],\n        \n        \n                [[[-0.0532]],\n        \n                 [[ 0.0249]],\n        \n                 [[ 0.0253]],\n        \n                 ...,\n        \n                 [[-0.0420]],\n        \n                 [[-0.0191]],\n        \n                 [[ 0.0729]]],\n        \n        \n                ...,\n        \n        \n                [[[ 0.0885]],\n        \n                 [[-0.0425]],\n        \n                 [[ 0.0401]],\n        \n                 ...,\n        \n                 [[-0.1219]],\n        \n                 [[ 0.0768]],\n        \n                 [[-0.0386]]],\n        \n        \n                [[[ 0.0678]],\n        \n                 [[-0.0912]],\n        \n                 [[-0.0521]],\n        \n                 ...,\n        \n                 [[-0.0746]],\n        \n                 [[-0.0514]],\n        \n                 [[-0.0326]]],\n        \n        \n                [[[ 0.0242]],\n        \n                 [[-0.1214]],\n        \n                 [[-0.0601]],\n        \n                 ...,\n        \n                 [[ 0.0930]],\n        \n                 [[ 0.0877]],\n        \n                 [[-0.0346]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-6.7636e-03, -4.9755e-02,  2.1889e-02,  1.1116e-02,  5.2417e-03,\n                 1.3895e-01,  4.4514e-02, -4.1357e-02, -1.5523e-02,  1.3634e-04,\n                 3.7435e-02, -5.4340e-02,  4.2534e-02, -5.1946e-02, -2.6754e-02,\n                 2.4891e-02,  1.1299e-02,  1.7568e-02, -7.7497e-02, -1.0210e-01,\n                -1.9150e-02, -1.9422e-02,  3.0510e-02, -6.3215e-02,  5.7682e-02,\n                -9.4340e-02, -8.9808e-03,  2.9739e-02, -4.8092e-02,  8.2695e-02,\n                -1.4612e-02,  2.2384e-02, -1.1116e-01,  5.9906e-02,  4.2498e-02,\n                -7.8650e-02,  1.2771e-01,  2.8298e-02,  5.0834e-02, -5.1067e-02,\n                -2.5709e-02,  5.9145e-02,  2.4863e-02, -6.4657e-02,  1.2273e-01,\n                 8.2043e-02, -1.2592e-02, -1.3705e-01, -9.0019e-03,  2.4712e-03,\n                -1.0791e-01,  1.1293e-01, -4.9349e-02,  2.4565e-02, -8.3780e-03,\n                 2.1330e-02, -1.5720e-01,  5.1084e-02, -4.9541e-02,  3.2699e-02,\n                 1.0594e-01, -6.0702e-02,  4.9717e-03, -6.4365e-04, -2.2122e-02,\n                 6.7006e-02,  8.2769e-02, -3.3918e-02,  4.2668e-02,  1.2487e-01,\n                -5.7466e-02, -3.2044e-02,  6.4386e-02, -8.1241e-02, -2.3449e-02,\n                -5.6423e-02,  1.0468e-02,  3.4636e-02,  3.2993e-02, -2.4821e-02,\n                 3.8691e-03, -2.9792e-02, -7.0805e-02,  6.4420e-02,  3.1930e-02,\n                 4.1901e-02,  1.9106e-02,  3.7501e-02,  6.5085e-02,  5.2205e-03,\n                -3.5309e-02, -2.9991e-02, -1.9873e-02, -9.8113e-02, -1.3705e-02,\n                 1.2341e-01,  5.9777e-02,  7.7771e-02,  2.8688e-02, -3.6762e-02,\n                -5.1453e-03, -9.6555e-02,  7.4960e-02,  9.0371e-02, -7.3966e-03,\n                 4.7047e-02, -5.3982e-02,  7.5329e-03, -1.3756e-02, -5.1420e-02,\n                -5.1744e-02, -3.4425e-02,  8.4013e-02,  1.1334e-01, -1.6111e-02,\n                 8.0567e-02,  5.9405e-02, -3.6524e-03,  4.5099e-02,  3.6554e-02,\n                 2.5069e-02, -4.0812e-02,  2.5814e-02, -2.3144e-04,  5.5690e-02,\n                 1.6118e-02, -1.6566e-01, -9.0286e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9347, 0.9293, 0.9236, 0.9172, 0.9402, 0.9650, 0.9227, 0.9267, 0.9291,\n                0.9214, 0.9252, 0.9322, 0.9476, 0.9337, 0.9252, 0.9283, 0.9224, 0.9514,\n                0.9275, 0.9477, 0.9361, 0.9324, 0.9312, 0.9430, 0.9312, 0.9275, 0.9255,\n                0.9241, 0.9309, 0.9492, 0.9189, 0.9435, 0.9380, 0.9556, 0.9312, 0.9246,\n                0.9467, 0.9351, 0.9255, 0.9259, 0.9215, 0.9389, 0.9285, 0.9434, 0.9340,\n                0.9532, 0.9347, 0.9432, 0.9355, 0.9213, 0.9389, 0.9383, 0.9272, 0.9190,\n                0.9296, 0.9277, 0.9790, 0.9468, 0.9284, 0.9267, 0.9527, 0.9288, 0.9517,\n                0.9448, 0.9188, 0.9346, 0.9352, 0.9340, 0.9274, 0.9593, 0.9440, 0.9679,\n                0.9385, 0.9661, 0.9313, 0.9341, 0.9375, 0.9168, 0.9287, 0.9288, 0.9304,\n                0.9189, 0.9339, 0.9531, 0.9239, 0.9444, 0.9282, 0.9242, 0.9326, 0.9275,\n                0.9329, 0.9273, 0.9221, 0.9587, 0.9268, 0.9377, 0.9477, 0.9353, 0.9327,\n                0.9315, 0.9308, 0.9343, 0.9426, 0.9431, 0.9495, 0.9474, 0.9375, 0.9395,\n                0.9371, 0.9378, 0.9239, 0.9322, 0.9314, 0.9344, 0.9407, 0.9268, 0.9296,\n                0.9205, 0.9296, 0.9246, 0.9344, 0.9319, 0.9262, 0.9283, 0.9522, 0.9313,\n                0.9854, 0.9404], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [128, 64, 3, 3]], ["0.bn1.weight", [128]], ["0.bn1.bias", [128]], ["0.conv2.weight", [128, 128, 3, 3]], ["0.bn2.weight", [128]], ["0.bn2.bias", [128]], ["0.downsample.0.weight", [128, 64, 1, 1]], ["0.downsample.1.weight", [128]], ["0.downsample.1.bias", [128]]], "output_shape": [[512, 128, 3, 3]], "num_parameters": [73728, 128, 128, 147456, 128, 128, 8192, 128, 128]}, {"name": "layer3", "id": 140310980266016, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 0.0021,  0.0275, -0.0114],\n                [-0.0015,  0.0012, -0.0255],\n                [-0.0129, -0.0050,  0.0185]],\n      \n               [[-0.0112,  0.0184, -0.0097],\n                [-0.0083, -0.0265, -0.0277],\n                [ 0.0051, -0.0132,  0.0161]],\n      \n               [[ 0.0135, -0.0243, -0.0056],\n                [ 0.0037,  0.0158, -0.0258],\n                [-0.0134,  0.0053, -0.0082]],\n      \n               ...,\n      \n               [[-0.0176,  0.0250, -0.0132],\n                [-0.0109, -0.0093,  0.0277],\n                [ 0.0007, -0.0198,  0.0076]],\n      \n               [[ 0.0215,  0.0196, -0.0168],\n                [-0.0258, -0.0120,  0.0166],\n                [ 0.0105,  0.0117,  0.0253]],\n      \n               [[ 0.0086,  0.0013, -0.0282],\n                [-0.0238,  0.0259,  0.0264],\n                [ 0.0271,  0.0088, -0.0029]]],\n      \n      \n              [[[-0.0125, -0.0098, -0.0121],\n                [ 0.0148,  0.0142, -0.0041],\n                [-0.0024, -0.0291, -0.0153]],\n      \n               [[ 0.0098, -0.0245,  0.0278],\n                [ 0.0175,  0.0129, -0.0014],\n                [ 0.0259,  0.0007, -0.0120]],\n      \n               [[-0.0120,  0.0103, -0.0054],\n                [ 0.0024,  0.0042,  0.0037],\n                [ 0.0272,  0.0038,  0.0169]],\n      \n               ...,\n      \n               [[-0.0186, -0.0236,  0.0251],\n                [ 0.0276,  0.0082, -0.0162],\n                [-0.0046,  0.0179,  0.0123]],\n      \n               [[-0.0174,  0.0237,  0.0284],\n                [-0.0243, -0.0028,  0.0255],\n                [ 0.0265,  0.0031, -0.0190]],\n      \n               [[-0.0267, -0.0212,  0.0015],\n                [ 0.0145,  0.0075,  0.0212],\n                [-0.0188, -0.0119, -0.0035]]],\n      \n      \n              [[[ 0.0069, -0.0200, -0.0249],\n                [-0.0241, -0.0176,  0.0103],\n                [ 0.0045,  0.0108,  0.0210]],\n      \n               [[-0.0015, -0.0232, -0.0044],\n                [-0.0045,  0.0078,  0.0038],\n                [ 0.0270, -0.0275, -0.0294]],\n      \n               [[ 0.0016, -0.0079, -0.0060],\n                [ 0.0244, -0.0036, -0.0016],\n                [ 0.0254,  0.0040, -0.0189]],\n      \n               ...,\n      \n               [[ 0.0091, -0.0161,  0.0170],\n                [ 0.0270, -0.0046, -0.0036],\n                [-0.0141,  0.0104, -0.0215]],\n      \n               [[-0.0120, -0.0014, -0.0093],\n                [ 0.0210,  0.0230, -0.0045],\n                [ 0.0259,  0.0140, -0.0091]],\n      \n               [[ 0.0267, -0.0159,  0.0039],\n                [ 0.0145, -0.0219,  0.0043],\n                [ 0.0122, -0.0269,  0.0249]]],\n      \n      \n              ...,\n      \n      \n              [[[ 0.0055,  0.0031, -0.0205],\n                [ 0.0100, -0.0135,  0.0065],\n                [ 0.0160, -0.0279,  0.0039]],\n      \n               [[-0.0068, -0.0175, -0.0239],\n                [-0.0180, -0.0226,  0.0025],\n                [ 0.0075,  0.0158,  0.0246]],\n      \n               [[ 0.0291, -0.0151, -0.0072],\n                [ 0.0228, -0.0088,  0.0289],\n                [-0.0284, -0.0219,  0.0139]],\n      \n               ...,\n      \n               [[-0.0134,  0.0147,  0.0120],\n                [-0.0076, -0.0172, -0.0238],\n                [-0.0236,  0.0092,  0.0083]],\n      \n               [[-0.0231, -0.0038, -0.0072],\n                [-0.0235,  0.0294, -0.0014],\n                [-0.0191,  0.0162, -0.0093]],\n      \n               [[-0.0140, -0.0029,  0.0078],\n                [ 0.0053, -0.0008,  0.0254],\n                [ 0.0198, -0.0248,  0.0079]]],\n      \n      \n              [[[-0.0154, -0.0140,  0.0241],\n                [-0.0242, -0.0258, -0.0053],\n                [ 0.0235,  0.0278,  0.0013]],\n      \n               [[ 0.0048,  0.0292, -0.0208],\n                [ 0.0125,  0.0185, -0.0216],\n                [ 0.0078,  0.0218,  0.0265]],\n      \n               [[-0.0159, -0.0182,  0.0263],\n                [-0.0212, -0.0075,  0.0170],\n                [ 0.0169, -0.0203, -0.0254]],\n      \n               ...,\n      \n               [[ 0.0077, -0.0149, -0.0121],\n                [ 0.0270, -0.0207, -0.0195],\n                [-0.0200, -0.0213,  0.0153]],\n      \n               [[ 0.0201,  0.0108,  0.0132],\n                [-0.0010,  0.0179, -0.0246],\n                [-0.0057, -0.0262,  0.0263]],\n      \n               [[-0.0112,  0.0181, -0.0167],\n                [ 0.0143, -0.0217, -0.0036],\n                [ 0.0021,  0.0142,  0.0044]]],\n      \n      \n              [[[-0.0023,  0.0186, -0.0123],\n                [-0.0127,  0.0045,  0.0288],\n                [ 0.0077, -0.0072, -0.0070]],\n      \n               [[-0.0161,  0.0068, -0.0245],\n                [ 0.0218, -0.0119,  0.0034],\n                [ 0.0070, -0.0189,  0.0028]],\n      \n               [[-0.0210,  0.0015,  0.0112],\n                [ 0.0015, -0.0155,  0.0063],\n                [-0.0239,  0.0260, -0.0044]],\n      \n               ...,\n      \n               [[ 0.0254,  0.0203, -0.0167],\n                [ 0.0024, -0.0276,  0.0116],\n                [ 0.0138,  0.0144,  0.0244]],\n      \n               [[ 0.0197, -0.0227,  0.0251],\n                [ 0.0149,  0.0068,  0.0283],\n                [ 0.0277, -0.0247, -0.0272]],\n      \n               [[ 0.0068, -0.0286, -0.0229],\n                [-0.0144,  0.0007,  0.0075],\n                [ 0.0007, -0.0154, -0.0089]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 3.4269e-02, -1.8253e-03,  1.1393e-02,  5.0545e-02,  3.9535e-05,\n              -9.8790e-03, -1.2233e-02, -2.4505e-02, -4.9849e-03,  1.2429e-02,\n               3.9268e-03,  2.4176e-02,  2.1897e-02,  1.1417e-02,  1.8762e-02,\n              -1.7178e-02, -1.7420e-02,  2.1169e-02,  8.5558e-03,  1.4898e-02,\n              -6.3762e-03, -7.0085e-03,  8.3945e-03,  3.2633e-03,  2.9170e-02,\n              -1.1388e-02, -2.5443e-02,  9.0465e-03,  8.8752e-03, -7.1415e-03,\n               7.5938e-03, -3.9360e-03, -4.6255e-02,  3.0428e-02,  1.3617e-02,\n              -9.2145e-03, -2.0176e-02,  2.6652e-02,  4.7335e-03, -9.8120e-03,\n              -1.1564e-02,  6.9949e-03, -1.2919e-04,  1.0155e-02,  4.4847e-03,\n               2.7506e-03, -1.8000e-02,  2.7155e-03,  8.2249e-03,  7.8712e-03,\n               1.9172e-02, -8.0771e-03,  2.3911e-03,  2.6464e-02,  2.9042e-03,\n               2.1641e-02, -1.0400e-02, -2.8078e-02,  1.6104e-02,  1.0030e-02,\n              -3.2334e-03,  2.3973e-02, -1.7365e-03,  1.8931e-02,  1.5353e-02,\n              -5.1831e-03, -2.9659e-03, -1.7913e-02, -9.9742e-03, -9.2630e-03,\n              -1.1516e-02,  4.0415e-02,  1.5274e-02, -4.8198e-03,  1.0595e-02,\n               2.7371e-02,  8.2669e-04,  2.0481e-02,  2.4745e-02, -6.2374e-03,\n              -2.0055e-02, -2.7657e-02, -4.5853e-03, -7.1576e-03, -3.5362e-02,\n              -1.8701e-02,  1.4495e-02, -3.6353e-02, -5.6375e-03, -1.9798e-02,\n               3.7406e-03, -7.9622e-04, -5.1472e-03, -3.5730e-03,  6.4371e-03,\n              -1.1422e-02,  3.1388e-02,  7.7105e-03, -6.5119e-03, -1.0730e-02,\n              -3.9476e-03,  1.0948e-02, -4.4770e-04, -2.0261e-02, -7.3504e-03,\n              -2.6048e-02, -1.4854e-02,  3.1943e-03, -1.9447e-02, -8.6597e-03,\n              -2.4292e-02, -7.5833e-03,  1.9655e-02, -1.9036e-02, -1.9892e-02,\n              -2.1400e-02, -3.1481e-03, -3.0472e-02, -6.2389e-03, -8.1244e-03,\n              -6.1486e-03,  1.3747e-02,  1.0165e-02, -1.8217e-02,  8.0714e-03,\n               1.1994e-02, -1.7166e-02,  3.2580e-02, -2.4355e-02,  2.1328e-02,\n              -4.1778e-03, -1.8808e-02,  3.5182e-03,  3.5109e-03, -2.3983e-02,\n              -9.5473e-03,  1.7763e-02,  1.8751e-02, -1.5233e-02,  1.0153e-02,\n               5.0280e-03,  1.0964e-03,  1.2027e-02,  2.2958e-03,  3.5324e-03,\n              -2.4502e-04,  1.5267e-02, -1.1605e-02, -8.6018e-03,  1.0743e-02,\n              -1.5825e-02,  3.4998e-02, -1.3984e-03, -1.0099e-03, -1.3682e-02,\n              -2.3014e-02, -2.0783e-02, -6.6221e-04, -1.0050e-02,  1.7485e-02,\n              -7.2680e-03, -2.9127e-03,  1.4378e-02, -7.7410e-03,  2.0754e-02,\n              -2.1062e-02, -1.1105e-02,  7.5903e-04, -9.7442e-03, -2.1951e-02,\n              -3.3964e-03, -2.3652e-02, -6.0928e-03,  6.4725e-03,  1.1153e-03,\n               7.5104e-03,  1.1840e-02,  1.8322e-02, -7.9534e-03,  1.3178e-03,\n              -1.3653e-02,  2.1693e-02,  2.3458e-02, -5.0886e-03, -1.1199e-02,\n              -2.0230e-03,  1.1048e-02,  6.4347e-03, -2.9956e-02, -5.6667e-03,\n               1.8057e-02, -6.3370e-03,  2.6812e-02,  2.3654e-02, -1.3509e-02,\n               3.6303e-02,  2.1643e-02, -8.4415e-03,  1.6594e-02, -2.5427e-03,\n              -4.2957e-02, -2.2136e-02,  7.2385e-03, -4.3529e-02, -2.0946e-02,\n              -1.5992e-02, -9.1761e-03, -3.7423e-02,  8.3677e-03,  3.4133e-03,\n              -3.0356e-03, -2.2641e-02, -2.3365e-02, -1.0534e-02, -1.7232e-03,\n               1.3415e-02, -2.1063e-02,  1.9955e-02, -6.5855e-03, -8.5828e-03,\n              -2.1299e-02,  2.1027e-02,  2.3041e-02,  1.8194e-02,  3.8914e-02,\n               1.2013e-02, -1.3739e-02, -1.8871e-02, -3.0791e-03, -2.0474e-02,\n               8.3684e-03,  2.0086e-02, -1.0106e-02, -2.1987e-02,  1.1284e-02,\n              -1.1899e-02, -6.4134e-03,  3.0771e-03, -1.6071e-02, -1.0694e-02,\n               1.9746e-02,  1.3991e-02, -3.4793e-03,  2.6709e-03,  3.7088e-03,\n              -8.8587e-03, -5.7000e-03, -2.3458e-02, -1.2401e-02, -2.7171e-03,\n              -1.6454e-02, -6.7489e-03,  2.7204e-02,  1.8144e-02, -1.8290e-03,\n               4.9719e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9186, 0.9109, 0.9241, 0.9115, 0.9140, 0.9133, 0.9105, 0.9146, 0.9126,\n              0.9097, 0.9115, 0.9116, 0.9120, 0.9091, 0.9125, 0.9147, 0.9182, 0.9181,\n              0.9089, 0.9092, 0.9170, 0.9152, 0.9108, 0.9109, 0.9122, 0.9125, 0.9169,\n              0.9119, 0.9107, 0.9135, 0.9207, 0.9168, 0.9201, 0.9149, 0.9107, 0.9133,\n              0.9098, 0.9169, 0.9108, 0.9115, 0.9098, 0.9163, 0.9144, 0.9096, 0.9125,\n              0.9156, 0.9111, 0.9193, 0.9129, 0.9119, 0.9110, 0.9134, 0.9172, 0.9113,\n              0.9131, 0.9121, 0.9165, 0.9182, 0.9182, 0.9195, 0.9088, 0.9159, 0.9127,\n              0.9126, 0.9098, 0.9163, 0.9130, 0.9129, 0.9108, 0.9142, 0.9162, 0.9112,\n              0.9111, 0.9116, 0.9111, 0.9103, 0.9133, 0.9097, 0.9156, 0.9149, 0.9100,\n              0.9089, 0.9084, 0.9090, 0.9106, 0.9121, 0.9092, 0.9185, 0.9128, 0.9192,\n              0.9119, 0.9152, 0.9123, 0.9101, 0.9100, 0.9117, 0.9096, 0.9090, 0.9133,\n              0.9176, 0.9083, 0.9114, 0.9144, 0.9148, 0.9117, 0.9139, 0.9129, 0.9142,\n              0.9106, 0.9214, 0.9132, 0.9114, 0.9122, 0.9140, 0.9110, 0.9110, 0.9180,\n              0.9089, 0.9127, 0.9095, 0.9131, 0.9110, 0.9128, 0.9145, 0.9160, 0.9116,\n              0.9129, 0.9146, 0.9137, 0.9119, 0.9128, 0.9120, 0.9101, 0.9080, 0.9119,\n              0.9187, 0.9157, 0.9159, 0.9113, 0.9163, 0.9173, 0.9098, 0.9183, 0.9121,\n              0.9138, 0.9179, 0.9193, 0.9155, 0.9108, 0.9121, 0.9152, 0.9159, 0.9127,\n              0.9195, 0.9115, 0.9161, 0.9098, 0.9129, 0.9157, 0.9090, 0.9107, 0.9136,\n              0.9109, 0.9152, 0.9138, 0.9198, 0.9165, 0.9165, 0.9209, 0.9168, 0.9151,\n              0.9150, 0.9091, 0.9129, 0.9145, 0.9112, 0.9096, 0.9219, 0.9194, 0.9112,\n              0.9110, 0.9150, 0.9168, 0.9157, 0.9113, 0.9129, 0.9096, 0.9151, 0.9232,\n              0.9144, 0.9098, 0.9129, 0.9094, 0.9100, 0.9120, 0.9099, 0.9105, 0.9136,\n              0.9102, 0.9098, 0.9150, 0.9168, 0.9104, 0.9128, 0.9125, 0.9118, 0.9134,\n              0.9130, 0.9095, 0.9146, 0.9212, 0.9124, 0.9111, 0.9105, 0.9108, 0.9124,\n              0.9139, 0.9179, 0.9097, 0.9149, 0.9129, 0.9099, 0.9152, 0.9105, 0.9164,\n              0.9107, 0.9154, 0.9143, 0.9209, 0.9183, 0.9096, 0.9113, 0.9107, 0.9126,\n              0.9122, 0.9116, 0.9101, 0.9115, 0.9180, 0.9100, 0.9094, 0.9124, 0.9298,\n              0.9114, 0.9110, 0.9210, 0.9089, 0.9115, 0.9123, 0.9096, 0.9112, 0.9153,\n              0.9125, 0.9156, 0.9105, 0.9098], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 1.2464e-02,  9.1472e-03,  9.6678e-03],\n                [ 1.0522e-02, -1.1668e-02,  1.9331e-03],\n                [ 8.0864e-03,  1.8217e-02, -1.2830e-05]],\n      \n               [[-7.5132e-03,  1.4344e-02, -1.6575e-02],\n                [-1.1329e-02, -4.2691e-03,  7.7907e-03],\n                [ 1.8559e-02, -4.4267e-03, -9.7335e-03]],\n      \n               [[ 1.5495e-02, -9.3076e-03,  1.7144e-02],\n                [ 1.5823e-02, -1.4329e-02,  6.4300e-04],\n                [ 8.4912e-03, -1.8345e-02,  1.8794e-02]],\n      \n               ...,\n      \n               [[-2.1355e-03, -5.0317e-03,  1.9811e-02],\n                [ 1.2084e-02,  1.9163e-02, -1.1040e-02],\n                [ 1.4859e-02,  9.5206e-04,  4.0751e-03]],\n      \n               [[ 1.9038e-02, -2.9231e-04,  6.0399e-03],\n                [-1.6397e-02,  7.3497e-03,  1.6952e-02],\n                [ 1.1609e-02, -5.2966e-03,  1.4087e-02]],\n      \n               [[-8.7059e-03,  1.7425e-02, -1.3370e-02],\n                [ 9.9621e-03,  2.2953e-03, -4.2678e-04],\n                [ 4.3538e-03,  1.8229e-02, -3.8423e-03]]],\n      \n      \n              [[[ 1.9036e-02,  6.4903e-03, -1.1289e-02],\n                [ 8.0255e-03,  1.3031e-02, -1.2649e-02],\n                [-5.7859e-03,  1.9929e-02,  1.3809e-02]],\n      \n               [[-1.4214e-02, -1.0853e-02,  2.0074e-02],\n                [-1.0101e-02, -9.6161e-03,  6.7404e-03],\n                [ 2.0609e-02, -9.1640e-03,  1.2312e-02]],\n      \n               [[-5.3512e-03,  3.7941e-03, -1.5983e-02],\n                [ 6.9582e-03, -8.6943e-03,  4.6030e-03],\n                [ 8.8897e-03, -1.4331e-03,  4.9944e-03]],\n      \n               ...,\n      \n               [[ 1.2895e-02,  1.4303e-02, -1.9806e-02],\n                [-2.8295e-03,  1.1470e-02,  1.3028e-02],\n                [-1.0912e-02,  1.3962e-02, -9.4490e-03]],\n      \n               [[-2.0595e-02, -1.1860e-02,  1.1125e-02],\n                [-9.1051e-04,  5.3046e-03, -2.0632e-02],\n                [-3.6025e-03, -8.9005e-04,  1.5400e-03]],\n      \n               [[-1.7521e-02, -1.3438e-02,  7.5237e-03],\n                [-6.9651e-05,  1.7553e-02, -9.1198e-04],\n                [-4.0457e-03,  9.3773e-03,  1.1608e-02]]],\n      \n      \n              [[[-1.6019e-02, -1.7410e-02,  2.0091e-02],\n                [-1.7156e-02,  1.9314e-02, -6.2308e-04],\n                [ 1.0835e-02, -3.5959e-03,  1.2770e-02]],\n      \n               [[ 7.0539e-04,  1.0753e-02, -1.7882e-02],\n                [-2.0705e-02,  1.6486e-02,  1.1224e-02],\n                [ 2.0622e-02, -2.2591e-03, -1.7418e-02]],\n      \n               [[ 1.1971e-02,  1.8538e-02, -1.4642e-02],\n                [ 1.3446e-02, -1.6387e-02,  1.3202e-02],\n                [-5.4037e-03, -1.9744e-02,  1.8977e-02]],\n      \n               ...,\n      \n               [[-1.0373e-02,  7.7367e-03, -1.9389e-02],\n                [ 7.0256e-04,  3.1268e-03, -1.6298e-02],\n                [-4.4999e-03, -1.9946e-02,  3.6184e-04]],\n      \n               [[-3.0306e-03, -1.7704e-03, -7.1792e-03],\n                [ 1.2489e-02, -1.2896e-02,  1.0909e-02],\n                [ 4.4757e-03, -7.2899e-03, -3.7145e-03]],\n      \n               [[-1.3798e-02, -1.9153e-02,  3.9011e-03],\n                [-6.1856e-03, -1.6351e-03,  4.7388e-03],\n                [ 1.2362e-02, -4.5748e-03, -2.7734e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[ 8.4995e-03, -8.0489e-03,  2.6852e-03],\n                [ 1.3309e-02, -9.8296e-03, -1.3883e-02],\n                [-1.8756e-02, -9.7676e-03,  1.2458e-02]],\n      \n               [[-6.4833e-03,  1.1962e-02, -9.2666e-03],\n                [-1.1497e-02, -3.7383e-03,  1.5729e-02],\n                [-5.6978e-03,  1.3036e-02, -1.6079e-02]],\n      \n               [[-1.1328e-02, -1.8713e-02, -1.4693e-02],\n                [-9.9724e-03,  8.8507e-03, -3.0726e-03],\n                [ 1.7020e-02, -1.4599e-02,  1.1929e-02]],\n      \n               ...,\n      \n               [[-2.5907e-03,  1.6464e-02, -1.3681e-02],\n                [-1.3172e-02,  1.3486e-02, -2.0120e-02],\n                [ 1.5417e-02,  2.3542e-03, -1.9321e-02]],\n      \n               [[-1.0059e-02, -1.0087e-02, -1.3125e-02],\n                [ 1.7125e-03,  8.7849e-03,  4.1127e-06],\n                [-6.1701e-03,  1.0992e-02, -9.4856e-03]],\n      \n               [[ 9.8921e-03,  1.3712e-02, -4.0057e-03],\n                [-1.9914e-02, -6.0625e-03,  4.8398e-03],\n                [ 8.8885e-03, -2.5846e-03,  1.3924e-02]]],\n      \n      \n              [[[-6.2453e-03, -1.7753e-02,  3.8959e-03],\n                [ 1.9021e-02,  1.2866e-02, -1.4584e-02],\n                [-4.9091e-03, -7.6231e-03, -1.2846e-02]],\n      \n               [[ 1.2085e-02,  1.9217e-02,  2.1966e-03],\n                [ 1.8476e-02, -1.3123e-02, -1.0224e-02],\n                [ 7.0011e-03,  7.3568e-03, -2.0064e-02]],\n      \n               [[-8.8084e-03,  1.8409e-02, -1.2514e-02],\n                [ 1.4717e-03, -4.0661e-03,  9.5618e-03],\n                [-1.8782e-04,  1.1730e-02,  1.6980e-02]],\n      \n               ...,\n      \n               [[-6.2145e-03, -1.1790e-02, -1.0397e-02],\n                [-1.9749e-03,  9.8121e-03,  1.7384e-02],\n                [-1.7208e-02,  3.6667e-04, -1.9189e-02]],\n      \n               [[ 9.4207e-03,  2.1193e-03, -7.0029e-04],\n                [-1.6288e-02,  8.6954e-03,  1.5657e-02],\n                [-1.2201e-02,  3.5147e-03, -1.8667e-02]],\n      \n               [[ 1.7695e-02, -6.4381e-03,  1.5806e-02],\n                [ 7.8834e-03, -1.4523e-02,  5.5330e-04],\n                [ 1.5375e-02,  1.8771e-02,  1.3927e-02]]],\n      \n      \n              [[[ 1.2953e-02,  4.7702e-03,  1.4618e-02],\n                [-1.4435e-02,  9.5123e-03,  1.6321e-02],\n                [-5.6445e-03,  3.1162e-03, -1.7399e-02]],\n      \n               [[ 2.0383e-02, -1.0340e-02, -1.4563e-02],\n                [-1.7951e-02,  1.8217e-02,  3.8723e-03],\n                [-1.5423e-02,  2.8019e-03, -6.8081e-03]],\n      \n               [[ 1.4723e-02, -3.4191e-03, -5.5637e-03],\n                [-1.5104e-02, -9.2018e-03, -7.7630e-03],\n                [-3.0746e-03, -1.6355e-02, -4.9954e-03]],\n      \n               ...,\n      \n               [[-3.2093e-03,  1.7071e-02, -7.5533e-03],\n                [ 1.9364e-02, -1.9596e-02, -1.1811e-02],\n                [-1.8596e-02, -2.0145e-02, -1.6761e-03]],\n      \n               [[-6.3617e-03, -2.8796e-03,  1.6174e-02],\n                [ 9.2402e-03, -1.8727e-02,  8.3621e-03],\n                [-1.5932e-02, -1.5243e-02,  1.0501e-03]],\n      \n               [[-1.4659e-02,  4.8619e-03,  3.2865e-03],\n                [-1.1283e-03,  3.5705e-03,  1.7027e-02],\n                [-4.3214e-03, -7.1573e-04,  3.4083e-04]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-1.9208e-02,  2.2365e-03,  1.1523e-02,  1.8750e-02,  1.3822e-03,\n              -9.1819e-03, -6.0165e-04, -9.6319e-03,  1.0601e-02, -2.1317e-03,\n              -1.4911e-02, -3.6411e-03,  2.8172e-03, -4.0340e-03, -1.1600e-02,\n              -1.5314e-02,  9.4704e-04,  9.3283e-03,  7.8901e-03, -3.5424e-02,\n               1.0150e-04, -7.6672e-03, -1.6329e-02,  2.6516e-03, -7.6609e-03,\n               7.2853e-03, -9.7973e-03, -5.4997e-03,  2.0007e-02, -3.1837e-03,\n               1.8663e-04,  8.7774e-03,  9.8454e-03,  1.3346e-02, -1.9199e-02,\n              -6.2716e-03, -1.3501e-02, -4.1684e-03,  1.1782e-03, -4.7767e-04,\n              -2.8314e-03, -1.1684e-02, -7.7642e-03,  6.5080e-03, -4.6161e-03,\n               1.5328e-02,  4.8005e-03,  2.0129e-02,  1.7279e-02, -2.4157e-02,\n               2.8873e-03, -6.5172e-03,  6.5838e-03, -7.3394e-04,  2.4440e-02,\n               1.6513e-02, -8.4515e-03, -3.1404e-03,  1.1044e-03, -6.2703e-03,\n               1.9912e-02,  1.7912e-02, -4.8919e-03, -1.1889e-02,  1.6519e-02,\n              -2.2528e-02,  7.6725e-03,  6.1582e-03, -1.3006e-02,  2.5265e-02,\n              -5.1642e-03, -1.5380e-02, -6.1477e-03,  2.2356e-03, -2.9161e-02,\n               6.1115e-03,  1.0712e-02, -1.9691e-02,  3.5230e-03,  1.2450e-02,\n              -1.0784e-02, -1.6786e-03, -1.0587e-02,  1.3833e-02,  3.0707e-02,\n              -1.0217e-02, -8.3275e-03,  3.4558e-03,  1.8748e-04, -2.3493e-02,\n               2.1646e-03, -1.8437e-03, -5.5247e-04,  1.0828e-02,  2.2649e-02,\n              -7.6101e-03,  7.2990e-03, -4.5191e-05,  1.4667e-02, -5.4344e-03,\n              -4.3759e-03, -5.1514e-03, -7.7600e-03, -1.2909e-04,  1.9064e-02,\n              -1.0386e-02, -2.4042e-03, -6.6591e-03, -1.0760e-02, -3.7112e-03,\n               2.3292e-02,  3.8009e-03, -1.7637e-02, -5.4022e-03,  8.0759e-03,\n              -3.4618e-03,  8.9328e-03,  6.8351e-03, -3.6972e-03,  1.1972e-02,\n              -3.0331e-03,  9.0521e-03, -1.0431e-02,  3.6625e-03, -9.0923e-03,\n               5.3908e-03, -8.0001e-03,  1.2256e-02, -3.3482e-03,  7.8353e-04,\n               2.1134e-02,  1.3880e-03,  1.7543e-03, -1.2003e-02,  2.9325e-02,\n              -2.1609e-02, -9.8417e-03, -1.4847e-02,  7.2340e-03,  5.1490e-03,\n              -1.6234e-05,  1.7799e-02, -1.2375e-02, -3.4942e-03,  9.2927e-03,\n               9.9973e-04, -1.3630e-02,  1.7636e-02,  1.1063e-02,  8.9657e-03,\n               3.8700e-03, -5.6943e-03,  4.0077e-03, -2.1873e-02,  2.2642e-05,\n              -1.4674e-02, -3.1914e-04,  1.5238e-02, -8.8689e-05,  1.7044e-03,\n               1.7555e-02, -1.6818e-02, -6.8793e-03,  5.2464e-03,  3.5920e-03,\n              -2.6342e-04, -1.4909e-02,  3.6876e-03, -1.9664e-02, -6.8150e-03,\n               1.7733e-03, -6.0327e-03,  1.0103e-02,  9.7896e-03, -4.7082e-03,\n              -1.3650e-02,  7.7535e-03, -2.4895e-03,  7.4017e-03,  1.4287e-02,\n              -9.1587e-04, -5.7893e-03,  1.1040e-02, -6.8211e-03,  1.1168e-02,\n              -3.4423e-02, -1.2436e-02,  1.3118e-02, -3.6933e-03, -1.9429e-02,\n               1.0121e-02, -9.0124e-03, -6.6982e-03,  8.2793e-03, -1.2502e-02,\n               6.2118e-03,  1.8309e-02,  1.8354e-02, -4.4960e-03, -2.0216e-02,\n               1.9244e-04, -1.4066e-03,  1.7225e-02,  1.4078e-02, -8.0809e-03,\n              -6.5785e-03, -7.5821e-03, -2.3891e-02,  3.2263e-03, -2.0023e-02,\n               7.0726e-03,  1.7954e-02, -2.2525e-04, -9.3343e-03,  7.5292e-03,\n               1.0566e-02,  1.6201e-02, -7.1486e-03, -9.7063e-03, -1.3163e-02,\n              -2.5466e-03,  1.7280e-03, -1.1659e-03,  2.2658e-03,  6.5823e-03,\n               2.1493e-02, -2.6533e-02, -7.1222e-03,  9.1471e-03, -2.2965e-02,\n               1.3067e-02,  1.4457e-02,  2.0843e-02, -1.5470e-03,  1.3902e-02,\n              -2.0509e-02, -1.0866e-02,  1.6751e-02, -1.0651e-02,  1.7022e-03,\n              -8.5279e-03, -1.7157e-02,  6.7369e-03,  6.6227e-03, -1.0438e-02,\n              -5.6203e-03,  2.6061e-03,  4.5834e-03,  3.7909e-03, -1.7090e-02,\n               4.9220e-03,  1.8326e-02,  3.2512e-03, -2.5062e-02,  3.5577e-02,\n              -2.4904e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9050, 0.9087, 0.9046, 0.9070, 0.9052, 0.9068, 0.9049, 0.9042, 0.9041,\n              0.9049, 0.9067, 0.9056, 0.9050, 0.9043, 0.9077, 0.9056, 0.9083, 0.9048,\n              0.9051, 0.9045, 0.9047, 0.9062, 0.9055, 0.9047, 0.9067, 0.9055, 0.9062,\n              0.9068, 0.9075, 0.9056, 0.9040, 0.9063, 0.9050, 0.9051, 0.9052, 0.9059,\n              0.9057, 0.9046, 0.9055, 0.9098, 0.9050, 0.9048, 0.9063, 0.9048, 0.9055,\n              0.9059, 0.9067, 0.9049, 0.9049, 0.9069, 0.9061, 0.9055, 0.9041, 0.9043,\n              0.9044, 0.9056, 0.9041, 0.9038, 0.9048, 0.9051, 0.9083, 0.9091, 0.9056,\n              0.9056, 0.9089, 0.9048, 0.9057, 0.9060, 0.9078, 0.9079, 0.9054, 0.9056,\n              0.9046, 0.9048, 0.9070, 0.9064, 0.9048, 0.9050, 0.9063, 0.9062, 0.9035,\n              0.9052, 0.9042, 0.9079, 0.9046, 0.9074, 0.9075, 0.9088, 0.9057, 0.9044,\n              0.9052, 0.9038, 0.9072, 0.9062, 0.9058, 0.9076, 0.9095, 0.9062, 0.9071,\n              0.9048, 0.9045, 0.9068, 0.9050, 0.9084, 0.9067, 0.9040, 0.9046, 0.9066,\n              0.9054, 0.9053, 0.9061, 0.9038, 0.9052, 0.9045, 0.9058, 0.9049, 0.9065,\n              0.9039, 0.9048, 0.9043, 0.9080, 0.9042, 0.9061, 0.9070, 0.9051, 0.9040,\n              0.9051, 0.9056, 0.9053, 0.9048, 0.9057, 0.9042, 0.9063, 0.9059, 0.9055,\n              0.9045, 0.9046, 0.9068, 0.9045, 0.9064, 0.9048, 0.9081, 0.9062, 0.9044,\n              0.9055, 0.9058, 0.9058, 0.9047, 0.9048, 0.9131, 0.9085, 0.9047, 0.9118,\n              0.9117, 0.9088, 0.9057, 0.9050, 0.9055, 0.9061, 0.9056, 0.9069, 0.9051,\n              0.9043, 0.9066, 0.9034, 0.9073, 0.9042, 0.9060, 0.9113, 0.9056, 0.9053,\n              0.9095, 0.9067, 0.9053, 0.9072, 0.9050, 0.9041, 0.9060, 0.9054, 0.9040,\n              0.9042, 0.9047, 0.9087, 0.9058, 0.9094, 0.9062, 0.9056, 0.9049, 0.9040,\n              0.9043, 0.9109, 0.9066, 0.9082, 0.9066, 0.9037, 0.9045, 0.9069, 0.9043,\n              0.9078, 0.9047, 0.9061, 0.9080, 0.9104, 0.9045, 0.9087, 0.9052, 0.9061,\n              0.9058, 0.9070, 0.9066, 0.9040, 0.9048, 0.9065, 0.9046, 0.9047, 0.9056,\n              0.9062, 0.9049, 0.9076, 0.9104, 0.9064, 0.9069, 0.9051, 0.9075, 0.9050,\n              0.9063, 0.9076, 0.9058, 0.9060, 0.9072, 0.9073, 0.9112, 0.9081, 0.9038,\n              0.9046, 0.9057, 0.9052, 0.9068, 0.9072, 0.9056, 0.9071, 0.9048, 0.9047,\n              0.9052, 0.9050, 0.9095, 0.9083, 0.9050, 0.9057, 0.9059, 0.9043, 0.9047,\n              0.9074, 0.9055, 0.9108, 0.9062], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[-0.0769]],\n        \n                 [[-0.0384]],\n        \n                 [[ 0.0240]],\n        \n                 ...,\n        \n                 [[ 0.0172]],\n        \n                 [[-0.0670]],\n        \n                 [[ 0.0556]]],\n        \n        \n                [[[-0.0774]],\n        \n                 [[-0.0026]],\n        \n                 [[-0.0705]],\n        \n                 ...,\n        \n                 [[-0.0186]],\n        \n                 [[-0.0678]],\n        \n                 [[ 0.0609]]],\n        \n        \n                [[[ 0.0665]],\n        \n                 [[ 0.0664]],\n        \n                 [[ 0.0077]],\n        \n                 ...,\n        \n                 [[ 0.0220]],\n        \n                 [[-0.0310]],\n        \n                 [[-0.0871]]],\n        \n        \n                ...,\n        \n        \n                [[[-0.0182]],\n        \n                 [[-0.0162]],\n        \n                 [[-0.0282]],\n        \n                 ...,\n        \n                 [[ 0.0827]],\n        \n                 [[ 0.0070]],\n        \n                 [[ 0.0807]]],\n        \n        \n                [[[-0.0777]],\n        \n                 [[-0.0647]],\n        \n                 [[ 0.0322]],\n        \n                 ...,\n        \n                 [[-0.0200]],\n        \n                 [[ 0.0524]],\n        \n                 [[-0.0553]]],\n        \n        \n                [[[ 0.0690]],\n        \n                 [[-0.0526]],\n        \n                 [[-0.0536]],\n        \n                 ...,\n        \n                 [[-0.0610]],\n        \n                 [[-0.0524]],\n        \n                 [[ 0.0465]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n               requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0138, -0.0033,  0.0065, -0.0009, -0.0272, -0.0342,  0.0100, -0.0015,\n                 0.0291,  0.0525, -0.0124,  0.0468, -0.0085, -0.0206,  0.0300, -0.0359,\n                 0.0494,  0.0096,  0.0097,  0.0150, -0.0071, -0.0024,  0.0432, -0.0078,\n                -0.0220, -0.0133,  0.0282, -0.0303,  0.0543,  0.0225,  0.0530,  0.0197,\n                 0.0339, -0.0052, -0.0334,  0.0146,  0.0360,  0.0205,  0.0206, -0.0377,\n                 0.0531,  0.0258, -0.0188,  0.0368,  0.0566,  0.0047,  0.0220,  0.0009,\n                -0.0036, -0.0162,  0.0108,  0.0125, -0.0243,  0.0016,  0.0242,  0.0101,\n                -0.0492, -0.0024,  0.0077,  0.0516,  0.0253,  0.0180,  0.0041,  0.0489,\n                -0.0123,  0.0164,  0.0347,  0.0024,  0.0358, -0.0093,  0.0143,  0.0221,\n                -0.0359, -0.0509, -0.0733,  0.0388, -0.0262, -0.0106,  0.0851,  0.0307,\n                -0.0004, -0.0176, -0.0146,  0.0280, -0.0312,  0.0888, -0.0125,  0.0144,\n                 0.0301,  0.0039,  0.0171, -0.0344, -0.0293, -0.0188, -0.0556, -0.0189,\n                -0.0053, -0.0165, -0.0269,  0.0466,  0.0219,  0.0113, -0.0023, -0.0301,\n                -0.0193, -0.0255, -0.0138, -0.0220, -0.0694, -0.0214,  0.0284, -0.0256,\n                -0.0284,  0.0018, -0.0354,  0.0615, -0.0440, -0.0135, -0.0291,  0.0124,\n                 0.0100, -0.0029, -0.0109, -0.0114,  0.0284,  0.0140,  0.0206, -0.0310,\n                 0.0182, -0.0030, -0.0431, -0.0176, -0.0023, -0.0133,  0.0121,  0.0233,\n                -0.0259, -0.0272, -0.0131,  0.0041,  0.0271,  0.0297, -0.0144, -0.0114,\n                -0.0364, -0.0516, -0.0100, -0.0040, -0.0166,  0.0142,  0.0041, -0.0054,\n                 0.0094,  0.0373,  0.0787, -0.0234, -0.0029, -0.0337,  0.0280,  0.0272,\n                -0.0124, -0.0613, -0.0515, -0.0247,  0.0115, -0.0271,  0.0018,  0.0586,\n                 0.0125,  0.0279,  0.0233, -0.0008,  0.0061,  0.0060,  0.0030, -0.0273,\n                -0.0259,  0.0322,  0.0188, -0.0940, -0.0749,  0.0726,  0.0416,  0.0051,\n                -0.0393,  0.0542, -0.0479,  0.0303, -0.0243,  0.0100,  0.0365,  0.0265,\n                -0.0055,  0.0158, -0.0172,  0.0304,  0.1140, -0.0137, -0.0364, -0.0512,\n                 0.0339, -0.0091,  0.0142,  0.0105,  0.0149,  0.0516,  0.0324, -0.0127,\n                -0.0166, -0.0460, -0.0184,  0.0189,  0.0044, -0.0275,  0.0213,  0.0263,\n                 0.0208,  0.0443,  0.0659, -0.0282,  0.0238,  0.0220, -0.0104, -0.0446,\n                 0.0263,  0.0137, -0.0055, -0.0065,  0.0043,  0.0201,  0.0030, -0.0146,\n                 0.0100, -0.0132,  0.0671,  0.0062, -0.0256, -0.0392, -0.0738,  0.0495,\n                 0.0399,  0.0150, -0.0166, -0.0095, -0.0028,  0.0530, -0.0025, -0.0198,\n                 0.0087, -0.0279,  0.0257, -0.0060, -0.0009,  0.0482, -0.0381, -0.0192],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9177, 0.9187, 0.9202, 0.9193, 0.9231, 0.9239, 0.9153, 0.9198, 0.9182,\n                0.9175, 0.9156, 0.9124, 0.9231, 0.9190, 0.9225, 0.9170, 0.9168, 0.9246,\n                0.9147, 0.9167, 0.9245, 0.9188, 0.9132, 0.9148, 0.9192, 0.9153, 0.9191,\n                0.9194, 0.9196, 0.9160, 0.9157, 0.9152, 0.9164, 0.9205, 0.9160, 0.9173,\n                0.9317, 0.9145, 0.9178, 0.9224, 0.9228, 0.9201, 0.9251, 0.9165, 0.9153,\n                0.9192, 0.9195, 0.9229, 0.9168, 0.9142, 0.9184, 0.9244, 0.9180, 0.9301,\n                0.9184, 0.9156, 0.9183, 0.9146, 0.9161, 0.9283, 0.9197, 0.9232, 0.9442,\n                0.9209, 0.9225, 0.9136, 0.9163, 0.9205, 0.9176, 0.9174, 0.9173, 0.9178,\n                0.9139, 0.9130, 0.9220, 0.9123, 0.9193, 0.9169, 0.9116, 0.9202, 0.9119,\n                0.9179, 0.9170, 0.9219, 0.9196, 0.9190, 0.9143, 0.9147, 0.9151, 0.9207,\n                0.9197, 0.9159, 0.9199, 0.9194, 0.9133, 0.9227, 0.9192, 0.9152, 0.9137,\n                0.9159, 0.9178, 0.9147, 0.9179, 0.9179, 0.9174, 0.9205, 0.9155, 0.9227,\n                0.9254, 0.9215, 0.9202, 0.9126, 0.9190, 0.9192, 0.9143, 0.9173, 0.9180,\n                0.9247, 0.9149, 0.9153, 0.9254, 0.9160, 0.9180, 0.9219, 0.9240, 0.9170,\n                0.9138, 0.9216, 0.9191, 0.9181, 0.9198, 0.9227, 0.9236, 0.9234, 0.9162,\n                0.9195, 0.9161, 0.9168, 0.9139, 0.9219, 0.9173, 0.9212, 0.9205, 0.9153,\n                0.9146, 0.9185, 0.9131, 0.9214, 0.9188, 0.9182, 0.9170, 0.9177, 0.9166,\n                0.9210, 0.9150, 0.9253, 0.9155, 0.9172, 0.9203, 0.9246, 0.9163, 0.9208,\n                0.9215, 0.9193, 0.9138, 0.9178, 0.9142, 0.9190, 0.9227, 0.9237, 0.9184,\n                0.9145, 0.9286, 0.9155, 0.9252, 0.9169, 0.9268, 0.9363, 0.9208, 0.9351,\n                0.9189, 0.9172, 0.9124, 0.9163, 0.9210, 0.9237, 0.9246, 0.9289, 0.9213,\n                0.9159, 0.9205, 0.9236, 0.9184, 0.9161, 0.9126, 0.9290, 0.9175, 0.9344,\n                0.9201, 0.9278, 0.9148, 0.9175, 0.9240, 0.9171, 0.9188, 0.9158, 0.9262,\n                0.9219, 0.9122, 0.9261, 0.9175, 0.9148, 0.9148, 0.9214, 0.9213, 0.9264,\n                0.9287, 0.9159, 0.9216, 0.9159, 0.9133, 0.9214, 0.9195, 0.9323, 0.9151,\n                0.9247, 0.9131, 0.9194, 0.9147, 0.9140, 0.9163, 0.9186, 0.9121, 0.9192,\n                0.9236, 0.9174, 0.9152, 0.9197, 0.9217, 0.9144, 0.9190, 0.9299, 0.9165,\n                0.9194, 0.9187, 0.9172, 0.9202, 0.9238, 0.9210, 0.9188, 0.9175, 0.9157,\n                0.9186, 0.9161, 0.9142, 0.9224], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [256, 128, 3, 3]], ["0.bn1.weight", [256]], ["0.bn1.bias", [256]], ["0.conv2.weight", [256, 256, 3, 3]], ["0.bn2.weight", [256]], ["0.bn2.bias", [256]], ["0.downsample.0.weight", [256, 128, 1, 1]], ["0.downsample.1.weight", [256]], ["0.downsample.1.bias", [256]]], "output_shape": [[512, 256, 2, 2]], "num_parameters": [294912, 256, 256, 589824, 256, 256, 32768, 256, 256]}, {"name": "layer4", "id": 140311229200992, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-1.8237e-03,  3.4795e-03,  6.8091e-03],\n                [ 1.1782e-03, -1.8746e-02,  6.2656e-03],\n                [ 1.1697e-03, -1.2283e-02, -1.9951e-02]],\n      \n               [[-1.0931e-02,  1.1723e-02, -3.4867e-04],\n                [ 2.8326e-03,  4.9270e-03, -9.7176e-03],\n                [ 5.4476e-03, -1.0714e-02,  1.0847e-02]],\n      \n               [[ 1.7063e-02,  4.7896e-03, -9.1924e-03],\n                [-1.2960e-02,  1.9519e-02, -8.9356e-03],\n                [ 2.9631e-03, -1.6828e-02, -6.2435e-04]],\n      \n               ...,\n      \n               [[-1.6900e-02, -2.8979e-03, -1.8340e-02],\n                [ 7.7507e-04,  5.8947e-03, -1.6853e-03],\n                [-2.6863e-03, -1.8900e-03,  2.6960e-03]],\n      \n               [[ 6.2418e-03,  1.6020e-02,  1.0957e-02],\n                [ 1.7917e-02, -9.2715e-03,  1.3121e-04],\n                [-2.5335e-03,  8.0172e-03, -1.4583e-02]],\n      \n               [[-5.3975e-03, -1.8751e-02, -2.0140e-02],\n                [ 1.8522e-02, -1.8613e-02,  7.9573e-03],\n                [ 1.1747e-02,  1.9667e-02, -7.3874e-03]]],\n      \n      \n              [[[ 1.7891e-02,  1.1984e-02,  9.4589e-03],\n                [ 1.3145e-02, -1.8885e-02, -1.8286e-02],\n                [ 1.9269e-02, -1.9457e-02,  1.9067e-02]],\n      \n               [[-1.6555e-02, -1.8261e-02,  6.1180e-03],\n                [ 1.1415e-02, -1.4147e-02, -5.2355e-05],\n                [ 1.1326e-03,  6.1426e-03, -1.2739e-02]],\n      \n               [[-1.4737e-02,  2.1913e-03, -7.8416e-04],\n                [-1.1247e-03, -1.0248e-03, -1.8045e-02],\n                [-1.1620e-02, -6.6115e-03, -1.5957e-02]],\n      \n               ...,\n      \n               [[-1.1485e-03, -2.0341e-02,  2.2698e-03],\n                [-6.8946e-03, -7.1096e-03, -1.2158e-02],\n                [ 1.7741e-02, -1.2081e-02, -5.0763e-03]],\n      \n               [[-1.1598e-02, -4.2600e-03,  1.1298e-02],\n                [ 1.3058e-02,  9.5154e-03,  1.1609e-02],\n                [-1.9777e-02,  2.0232e-03,  4.6372e-03]],\n      \n               [[ 7.7103e-03, -4.3864e-03,  1.9386e-02],\n                [ 5.4919e-03, -1.5316e-02,  1.1102e-02],\n                [-1.8398e-02,  2.0355e-02,  6.1712e-03]]],\n      \n      \n              [[[ 1.3487e-02,  8.1675e-03,  1.2884e-02],\n                [ 9.2019e-03,  6.9511e-03, -6.7596e-03],\n                [ 1.2917e-02,  1.0009e-02, -1.8799e-02]],\n      \n               [[ 8.4572e-03,  6.2920e-03, -5.1797e-03],\n                [ 1.2497e-02, -9.5917e-03,  8.5442e-03],\n                [ 7.9589e-03, -1.3976e-02, -1.6760e-02]],\n      \n               [[-1.9492e-02, -3.9608e-03, -1.4101e-02],\n                [-1.8183e-02,  1.3885e-02, -8.8831e-03],\n                [-1.6839e-02, -3.8074e-03, -1.9900e-02]],\n      \n               ...,\n      \n               [[-1.6102e-03,  1.8919e-02, -7.4243e-03],\n                [ 6.4333e-03,  1.3908e-02,  4.8865e-03],\n                [ 1.5323e-03, -1.8583e-02,  1.0798e-03]],\n      \n               [[ 4.3522e-03,  1.8151e-02,  5.6920e-03],\n                [-1.7298e-02,  1.9426e-02, -9.6754e-03],\n                [-5.9743e-03,  1.3290e-02, -1.2941e-02]],\n      \n               [[ 5.8322e-03,  1.3277e-03, -1.8570e-02],\n                [ 1.7791e-02,  2.0765e-02,  2.1949e-03],\n                [-1.2531e-03,  1.7741e-02, -1.0421e-04]]],\n      \n      \n              ...,\n      \n      \n              [[[ 3.9550e-03,  2.0594e-02, -1.7394e-02],\n                [ 1.8264e-03,  1.6097e-02, -1.7272e-02],\n                [-1.0599e-03,  1.5300e-02, -8.4129e-03]],\n      \n               [[-1.6291e-02, -9.7579e-03, -1.3043e-02],\n                [-1.1661e-02, -9.4540e-04,  7.5293e-03],\n                [ 2.6656e-03,  6.4687e-04, -6.6278e-03]],\n      \n               [[-1.5038e-02, -1.9427e-02, -9.0612e-03],\n                [ 2.2808e-03,  1.2348e-02, -4.6772e-03],\n                [ 1.8367e-02,  8.7933e-03,  6.5055e-03]],\n      \n               ...,\n      \n               [[ 4.8495e-03, -1.9124e-02,  1.6063e-03],\n                [ 3.5952e-03, -1.1577e-03, -2.5971e-03],\n                [ 1.9429e-02, -1.9297e-02, -1.7268e-03]],\n      \n               [[ 8.4934e-03, -2.3830e-04,  9.4779e-03],\n                [-5.0207e-04, -1.9274e-02,  2.7900e-05],\n                [-7.7808e-03,  9.2362e-03,  1.0039e-02]],\n      \n               [[ 1.3510e-02, -1.3727e-03, -1.2051e-02],\n                [ 1.1521e-03,  4.4777e-03,  9.0249e-03],\n                [ 3.6453e-03, -5.9763e-03,  1.6982e-02]]],\n      \n      \n              [[[-3.7458e-03,  3.4058e-03, -2.0499e-02],\n                [-2.2211e-03, -4.8939e-03, -1.6479e-02],\n                [ 1.4465e-02,  1.8710e-02, -2.0103e-03]],\n      \n               [[-2.0155e-02, -1.4429e-02, -1.1936e-02],\n                [ 1.9978e-02,  8.8954e-04, -6.9757e-03],\n                [ 5.0952e-03,  1.7610e-02, -1.0112e-02]],\n      \n               [[ 1.4888e-02,  2.0078e-02, -6.2378e-03],\n                [-7.4882e-03,  1.7237e-02, -1.6850e-02],\n                [ 4.3140e-03, -9.3895e-03,  9.7731e-03]],\n      \n               ...,\n      \n               [[ 1.9376e-02, -7.6391e-04,  1.3376e-03],\n                [-1.6481e-03, -2.0032e-02, -8.1393e-03],\n                [ 1.6077e-02,  3.0858e-03,  1.3707e-02]],\n      \n               [[ 1.5538e-02,  1.3191e-02, -1.6564e-02],\n                [-1.1951e-02, -3.2521e-03,  2.0781e-02],\n                [ 1.3243e-02, -3.8857e-03, -1.0865e-02]],\n      \n               [[-2.0002e-02, -8.4983e-03, -1.0793e-02],\n                [-1.0253e-03,  1.2089e-02, -1.2874e-02],\n                [-1.0097e-03,  7.1281e-03, -1.8480e-02]]],\n      \n      \n              [[[-6.6838e-03,  1.1312e-02,  1.8970e-02],\n                [-1.9493e-02,  1.1041e-02,  9.3785e-03],\n                [ 7.6987e-03, -4.6606e-03, -1.2600e-02]],\n      \n               [[ 1.4092e-02, -1.5227e-02,  8.6672e-03],\n                [-5.0953e-03,  1.9673e-02, -1.4295e-02],\n                [ 1.6477e-02,  1.6756e-02, -8.2796e-04]],\n      \n               [[-2.8202e-03,  1.2012e-02,  4.9300e-03],\n                [-1.3558e-02, -6.0392e-03,  1.4896e-02],\n                [ 3.7689e-03,  1.7667e-02,  1.5570e-02]],\n      \n               ...,\n      \n               [[-7.5980e-03,  7.3635e-03, -1.9181e-04],\n                [ 1.8342e-02,  8.4115e-03, -1.2418e-02],\n                [ 5.7779e-04, -6.3613e-03,  9.2087e-03]],\n      \n               [[ 7.5614e-03, -1.0094e-02,  5.1622e-03],\n                [-1.8723e-02,  6.8181e-03, -2.1998e-03],\n                [ 6.2381e-04, -9.7369e-03,  8.9960e-03]],\n      \n               [[ 7.9478e-03,  1.0303e-02, -3.2754e-04],\n                [ 1.4547e-02, -3.3827e-03,  7.4530e-03],\n                [ 1.5418e-02, -1.0496e-02, -7.1824e-03]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-0.0197, -0.0197,  0.0099, -0.0054, -0.0193, -0.0692, -0.0113,  0.0806,\n               0.0368,  0.0055,  0.0012,  0.0028, -0.0520,  0.0077, -0.0197, -0.0015,\n               0.0129,  0.0136, -0.0522, -0.0086, -0.0304,  0.0404, -0.0110, -0.0141,\n               0.0142,  0.0260, -0.0292, -0.0145, -0.0232,  0.0043, -0.0258, -0.0102,\n              -0.0374, -0.0249, -0.0216,  0.0549,  0.0275, -0.0367, -0.0113,  0.0057,\n              -0.0095,  0.0314, -0.0630, -0.0067,  0.0029,  0.0053, -0.0429,  0.0012,\n              -0.0213,  0.0374,  0.0169,  0.0125,  0.0182,  0.0189, -0.0052,  0.0213,\n               0.0262, -0.0101,  0.0079,  0.0418, -0.0015,  0.0446, -0.0456, -0.0270,\n              -0.0289,  0.0115,  0.0116, -0.0148,  0.0230,  0.0093, -0.0139, -0.0256,\n               0.0406,  0.0036, -0.0273,  0.0052, -0.0371, -0.0230,  0.0290, -0.0050,\n              -0.0306,  0.0485, -0.0109, -0.0228, -0.0066,  0.0052, -0.0299, -0.0011,\n               0.0175, -0.0420,  0.0048,  0.0068, -0.0116,  0.0304, -0.0071,  0.0137,\n              -0.0043,  0.0154,  0.0340,  0.0562, -0.0004,  0.0437,  0.0253, -0.0450,\n               0.0108,  0.0320, -0.0215, -0.0815, -0.0018, -0.0327,  0.0299, -0.0509,\n               0.0146,  0.0033,  0.0536, -0.0270, -0.0258,  0.0132, -0.0030, -0.0283,\n              -0.0150, -0.0570, -0.0274, -0.0078,  0.0471, -0.0098,  0.0216, -0.0148,\n               0.0022,  0.0317, -0.0080,  0.0008, -0.0142, -0.0062,  0.0083,  0.0446,\n               0.0165, -0.0091,  0.0217,  0.0402,  0.0141, -0.0221,  0.0460,  0.0039,\n              -0.0198,  0.0152,  0.0160,  0.0328,  0.0136, -0.0164, -0.0260,  0.0063,\n               0.0020, -0.0085,  0.0137, -0.0063, -0.0072,  0.0093, -0.0509,  0.0160,\n               0.0249, -0.0013, -0.0175, -0.0021, -0.0183,  0.0042,  0.0185, -0.0108,\n               0.0082,  0.0206, -0.0068,  0.0186,  0.0134, -0.0501,  0.0123, -0.0657,\n              -0.0082, -0.0170,  0.0290, -0.0111,  0.0239,  0.0426,  0.0239,  0.0223,\n              -0.0075,  0.0142, -0.0008, -0.0356, -0.0301, -0.0140,  0.0267,  0.0233,\n               0.0185, -0.0099,  0.0051, -0.0190,  0.0222,  0.0096, -0.0574, -0.0220,\n               0.0145,  0.0057,  0.0451,  0.0169, -0.0005,  0.0369,  0.0247,  0.0038,\n               0.0105,  0.0191, -0.0049,  0.0056, -0.0355, -0.0108,  0.0052, -0.0139,\n               0.0232,  0.0005, -0.0314,  0.0042, -0.0306,  0.0073,  0.0385,  0.0129,\n              -0.0414, -0.0190, -0.0038,  0.0168, -0.0047, -0.0363, -0.0615,  0.0267,\n              -0.0098, -0.0624, -0.0394,  0.0081,  0.0024,  0.0214,  0.0330,  0.0021,\n              -0.0206, -0.0151, -0.0370, -0.0256,  0.0281,  0.0081,  0.0202, -0.0506,\n               0.0154, -0.0335, -0.0138,  0.0198, -0.0166, -0.0102,  0.0136, -0.0017,\n               0.0066,  0.0121, -0.0364, -0.0049,  0.0052, -0.0160,  0.0202,  0.0003,\n               0.0297, -0.0338,  0.0187,  0.0092,  0.0192,  0.0192, -0.0185, -0.0105,\n              -0.0054, -0.0066,  0.0042, -0.0138,  0.0160,  0.0194,  0.0005, -0.0507,\n               0.0278,  0.0693, -0.0095, -0.0509,  0.0057, -0.0049, -0.0172,  0.0001,\n               0.0217,  0.0441,  0.0223,  0.0063, -0.0033, -0.0143,  0.0085, -0.0189,\n              -0.0083,  0.0278,  0.0449, -0.0099, -0.0037, -0.0101, -0.0227, -0.0276,\n               0.0237,  0.0218, -0.0201, -0.0277,  0.0054, -0.0266, -0.0123,  0.0094,\n               0.0379,  0.0171, -0.0445,  0.0172,  0.0065,  0.0126, -0.0412, -0.0034,\n              -0.0019, -0.0003, -0.0398, -0.0221, -0.0154,  0.0659, -0.0391,  0.0347,\n               0.0312, -0.0388, -0.0315, -0.0096,  0.0240,  0.0326, -0.0158, -0.0082,\n               0.0193,  0.0060, -0.0100, -0.0016,  0.0484, -0.0153,  0.0078, -0.0488,\n               0.0365,  0.0352,  0.0104, -0.0063, -0.0080,  0.0122, -0.0323,  0.0211,\n              -0.0116, -0.0201, -0.0046, -0.0134,  0.0201,  0.0031,  0.0153,  0.0159,\n               0.0107, -0.0248, -0.0296, -0.0030,  0.0066, -0.0568, -0.0051, -0.0244,\n               0.0016,  0.0253, -0.0199,  0.0193,  0.0119, -0.0052,  0.0259,  0.0234,\n              -0.0382,  0.0131,  0.0127,  0.0034, -0.0229, -0.0437,  0.0223, -0.0069,\n              -0.0061, -0.0264, -0.0192, -0.0119, -0.0082, -0.0049,  0.0050, -0.0073,\n              -0.0206, -0.0172,  0.0110,  0.0133,  0.0048, -0.0331,  0.0118, -0.0256,\n               0.0352, -0.0209,  0.0092, -0.0123, -0.0153,  0.0060, -0.0195,  0.0273,\n              -0.0487,  0.0015, -0.0224, -0.0184,  0.0024, -0.0286, -0.0504, -0.0081,\n               0.0537,  0.0081, -0.0287, -0.0126,  0.0201,  0.0078, -0.0094,  0.0069,\n               0.0625, -0.0005,  0.0118, -0.0189, -0.0066, -0.0427, -0.0355,  0.0517,\n               0.0089,  0.0216, -0.0147,  0.0360, -0.0171,  0.0179,  0.0181,  0.0161,\n               0.0118, -0.0183, -0.0149,  0.0346,  0.0454, -0.0265,  0.0526,  0.0116,\n               0.0724,  0.0162, -0.0163,  0.0078, -0.0463,  0.0028,  0.0032, -0.0188,\n               0.0171, -0.0539, -0.0078,  0.0217,  0.0327,  0.0146,  0.0293, -0.0019,\n              -0.0071, -0.0107, -0.0023,  0.0207, -0.0343,  0.0132,  0.0014, -0.0333,\n              -0.0134,  0.0331,  0.0018,  0.0025, -0.0113,  0.0004,  0.0520, -0.0015,\n              -0.0047,  0.0043, -0.0159, -0.0022, -0.0144,  0.0570, -0.0541, -0.0057,\n               0.0156, -0.0609,  0.0020,  0.0032, -0.0189,  0.0483, -0.0105,  0.0572,\n               0.0009, -0.0145,  0.0486, -0.0051, -0.0066, -0.0074,  0.0097,  0.0038,\n               0.0232, -0.0199,  0.0179,  0.0262,  0.0159, -0.0047, -0.0411, -0.0454],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9083, 0.9082, 0.9084, 0.9080, 0.9109, 0.9122, 0.9092, 0.9104, 0.9079,\n              0.9091, 0.9079, 0.9085, 0.9098, 0.9079, 0.9073, 0.9099, 0.9079, 0.9097,\n              0.9102, 0.9074, 0.9115, 0.9077, 0.9095, 0.9079, 0.9072, 0.9126, 0.9096,\n              0.9109, 0.9125, 0.9109, 0.9093, 0.9075, 0.9098, 0.9098, 0.9076, 0.9084,\n              0.9084, 0.9064, 0.9072, 0.9076, 0.9092, 0.9080, 0.9124, 0.9081, 0.9063,\n              0.9065, 0.9071, 0.9082, 0.9076, 0.9078, 0.9105, 0.9073, 0.9080, 0.9091,\n              0.9077, 0.9087, 0.9083, 0.9073, 0.9069, 0.9113, 0.9076, 0.9088, 0.9084,\n              0.9091, 0.9067, 0.9096, 0.9107, 0.9103, 0.9104, 0.9079, 0.9092, 0.9085,\n              0.9078, 0.9084, 0.9105, 0.9104, 0.9096, 0.9099, 0.9090, 0.9069, 0.9081,\n              0.9081, 0.9063, 0.9083, 0.9110, 0.9087, 0.9067, 0.9102, 0.9096, 0.9127,\n              0.9089, 0.9067, 0.9096, 0.9113, 0.9098, 0.9126, 0.9106, 0.9096, 0.9095,\n              0.9074, 0.9073, 0.9091, 0.9081, 0.9130, 0.9083, 0.9068, 0.9113, 0.9116,\n              0.9083, 0.9091, 0.9068, 0.9141, 0.9088, 0.9085, 0.9099, 0.9125, 0.9088,\n              0.9078, 0.9133, 0.9078, 0.9098, 0.9087, 0.9093, 0.9087, 0.9100, 0.9096,\n              0.9085, 0.9067, 0.9094, 0.9102, 0.9062, 0.9076, 0.9091, 0.9074, 0.9090,\n              0.9091, 0.9069, 0.9107, 0.9093, 0.9083, 0.9092, 0.9081, 0.9076, 0.9086,\n              0.9086, 0.9080, 0.9106, 0.9096, 0.9097, 0.9078, 0.9061, 0.9072, 0.9086,\n              0.9074, 0.9070, 0.9088, 0.9114, 0.9083, 0.9065, 0.9085, 0.9107, 0.9071,\n              0.9073, 0.9107, 0.9092, 0.9087, 0.9080, 0.9084, 0.9074, 0.9107, 0.9078,\n              0.9096, 0.9080, 0.9112, 0.9088, 0.9076, 0.9071, 0.9093, 0.9068, 0.9079,\n              0.9073, 0.9103, 0.9069, 0.9094, 0.9084, 0.9075, 0.9064, 0.9069, 0.9081,\n              0.9099, 0.9105, 0.9093, 0.9096, 0.9116, 0.9093, 0.9102, 0.9090, 0.9069,\n              0.9093, 0.9086, 0.9081, 0.9077, 0.9078, 0.9103, 0.9068, 0.9086, 0.9093,\n              0.9094, 0.9095, 0.9089, 0.9068, 0.9073, 0.9083, 0.9081, 0.9098, 0.9093,\n              0.9089, 0.9079, 0.9085, 0.9097, 0.9098, 0.9085, 0.9077, 0.9098, 0.9091,\n              0.9089, 0.9067, 0.9083, 0.9087, 0.9081, 0.9110, 0.9073, 0.9128, 0.9105,\n              0.9120, 0.9085, 0.9076, 0.9085, 0.9073, 0.9070, 0.9080, 0.9078, 0.9071,\n              0.9071, 0.9081, 0.9076, 0.9087, 0.9083, 0.9087, 0.9080, 0.9066, 0.9076,\n              0.9081, 0.9080, 0.9081, 0.9092, 0.9084, 0.9071, 0.9070, 0.9064, 0.9060,\n              0.9099, 0.9116, 0.9077, 0.9088, 0.9093, 0.9110, 0.9085, 0.9086, 0.9083,\n              0.9066, 0.9081, 0.9100, 0.9115, 0.9097, 0.9093, 0.9087, 0.9110, 0.9072,\n              0.9096, 0.9070, 0.9156, 0.9074, 0.9109, 0.9092, 0.9080, 0.9117, 0.9082,\n              0.9085, 0.9091, 0.9079, 0.9100, 0.9114, 0.9093, 0.9071, 0.9100, 0.9121,\n              0.9095, 0.9097, 0.9081, 0.9085, 0.9077, 0.9137, 0.9089, 0.9073, 0.9072,\n              0.9086, 0.9074, 0.9092, 0.9084, 0.9079, 0.9073, 0.9080, 0.9072, 0.9120,\n              0.9070, 0.9077, 0.9072, 0.9121, 0.9096, 0.9096, 0.9067, 0.9077, 0.9074,\n              0.9071, 0.9107, 0.9062, 0.9105, 0.9079, 0.9084, 0.9081, 0.9077, 0.9078,\n              0.9098, 0.9084, 0.9084, 0.9092, 0.9065, 0.9094, 0.9094, 0.9091, 0.9092,\n              0.9068, 0.9085, 0.9081, 0.9067, 0.9083, 0.9134, 0.9085, 0.9076, 0.9086,\n              0.9077, 0.9090, 0.9066, 0.9082, 0.9070, 0.9076, 0.9082, 0.9094, 0.9081,\n              0.9089, 0.9097, 0.9096, 0.9072, 0.9076, 0.9070, 0.9068, 0.9086, 0.9066,\n              0.9086, 0.9069, 0.9096, 0.9078, 0.9069, 0.9075, 0.9091, 0.9115, 0.9067,\n              0.9100, 0.9088, 0.9075, 0.9082, 0.9069, 0.9077, 0.9075, 0.9086, 0.9082,\n              0.9082, 0.9100, 0.9094, 0.9106, 0.9092, 0.9073, 0.9086, 0.9063, 0.9090,\n              0.9071, 0.9079, 0.9087, 0.9121, 0.9069, 0.9063, 0.9098, 0.9073, 0.9073,\n              0.9099, 0.9083, 0.9088, 0.9065, 0.9075, 0.9080, 0.9081, 0.9107, 0.9076,\n              0.9101, 0.9086, 0.9112, 0.9071, 0.9088, 0.9064, 0.9086, 0.9083, 0.9076,\n              0.9072, 0.9123, 0.9090, 0.9095, 0.9082, 0.9068, 0.9085, 0.9086, 0.9089,\n              0.9075, 0.9073, 0.9078, 0.9077, 0.9079, 0.9120, 0.9072, 0.9073, 0.9063,\n              0.9079, 0.9078, 0.9092, 0.9062, 0.9067, 0.9092, 0.9076, 0.9102, 0.9092,\n              0.9095, 0.9070, 0.9077, 0.9091, 0.9058, 0.9090, 0.9079, 0.9120, 0.9072,\n              0.9068, 0.9084, 0.9106, 0.9079, 0.9061, 0.9091, 0.9075, 0.9078, 0.9077,\n              0.9097, 0.9091, 0.9081, 0.9062, 0.9080, 0.9085, 0.9068, 0.9081, 0.9090,\n              0.9103, 0.9103, 0.9104, 0.9099, 0.9089, 0.9072, 0.9103, 0.9065, 0.9123,\n              0.9162, 0.9094, 0.9100, 0.9103, 0.9067, 0.9087, 0.9068, 0.9098, 0.9080,\n              0.9104, 0.9173, 0.9072, 0.9075, 0.9085, 0.9088, 0.9075, 0.9093, 0.9064,\n              0.9083, 0.9108, 0.9121, 0.9094, 0.9076, 0.9087, 0.9073, 0.9072],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 0.0132,  0.0039,  0.0062],\n                [ 0.0002,  0.0051,  0.0050],\n                [-0.0132,  0.0068, -0.0087]],\n      \n               [[-0.0049, -0.0032,  0.0094],\n                [-0.0029, -0.0107, -0.0027],\n                [-0.0062, -0.0120,  0.0048]],\n      \n               [[-0.0094,  0.0132, -0.0093],\n                [ 0.0035,  0.0090, -0.0066],\n                [ 0.0096, -0.0110,  0.0009]],\n      \n               ...,\n      \n               [[ 0.0059, -0.0012, -0.0112],\n                [-0.0135, -0.0073,  0.0072],\n                [-0.0066,  0.0147, -0.0020]],\n      \n               [[-0.0117,  0.0076,  0.0100],\n                [-0.0034,  0.0034,  0.0105],\n                [ 0.0110,  0.0110, -0.0023]],\n      \n               [[ 0.0054,  0.0055, -0.0051],\n                [ 0.0025, -0.0023,  0.0031],\n                [ 0.0050, -0.0124,  0.0121]]],\n      \n      \n              [[[ 0.0109,  0.0031, -0.0031],\n                [ 0.0117, -0.0087,  0.0056],\n                [ 0.0116,  0.0112,  0.0081]],\n      \n               [[ 0.0135,  0.0044, -0.0036],\n                [ 0.0002, -0.0099, -0.0090],\n                [ 0.0040,  0.0100, -0.0126]],\n      \n               [[-0.0061, -0.0093,  0.0021],\n                [ 0.0134,  0.0144, -0.0035],\n                [-0.0137, -0.0133, -0.0028]],\n      \n               ...,\n      \n               [[-0.0051,  0.0058, -0.0026],\n                [ 0.0114, -0.0074,  0.0132],\n                [ 0.0097,  0.0147,  0.0092]],\n      \n               [[ 0.0052,  0.0140,  0.0096],\n                [-0.0066, -0.0127, -0.0040],\n                [ 0.0110, -0.0067,  0.0038]],\n      \n               [[ 0.0062,  0.0141,  0.0032],\n                [-0.0130,  0.0081,  0.0088],\n                [-0.0024, -0.0104, -0.0113]]],\n      \n      \n              [[[ 0.0145,  0.0145,  0.0049],\n                [-0.0103,  0.0143, -0.0032],\n                [-0.0121, -0.0014, -0.0111]],\n      \n               [[-0.0115,  0.0062, -0.0035],\n                [-0.0114, -0.0005, -0.0006],\n                [-0.0013, -0.0018,  0.0047]],\n      \n               [[ 0.0111,  0.0020,  0.0146],\n                [-0.0013, -0.0090,  0.0048],\n                [ 0.0019, -0.0062, -0.0101]],\n      \n               ...,\n      \n               [[-0.0035, -0.0142,  0.0035],\n                [-0.0039, -0.0020, -0.0030],\n                [ 0.0108,  0.0030,  0.0015]],\n      \n               [[ 0.0092,  0.0136, -0.0137],\n                [-0.0022, -0.0026, -0.0109],\n                [-0.0098,  0.0091, -0.0051]],\n      \n               [[-0.0147,  0.0084, -0.0141],\n                [ 0.0019, -0.0024,  0.0090],\n                [ 0.0108,  0.0136,  0.0039]]],\n      \n      \n              ...,\n      \n      \n              [[[ 0.0087,  0.0068,  0.0003],\n                [ 0.0111,  0.0100, -0.0034],\n                [-0.0010, -0.0055,  0.0135]],\n      \n               [[-0.0071,  0.0014, -0.0059],\n                [-0.0035, -0.0052, -0.0063],\n                [ 0.0108,  0.0128, -0.0017]],\n      \n               [[-0.0127, -0.0052, -0.0087],\n                [ 0.0089,  0.0063,  0.0140],\n                [-0.0071, -0.0032,  0.0120]],\n      \n               ...,\n      \n               [[ 0.0011,  0.0111,  0.0045],\n                [ 0.0024, -0.0132,  0.0135],\n                [ 0.0051,  0.0120, -0.0126]],\n      \n               [[-0.0146, -0.0139,  0.0010],\n                [ 0.0104, -0.0082,  0.0036],\n                [ 0.0038,  0.0014,  0.0065]],\n      \n               [[ 0.0108, -0.0069, -0.0145],\n                [ 0.0096, -0.0088,  0.0051],\n                [-0.0041,  0.0053,  0.0119]]],\n      \n      \n              [[[ 0.0042,  0.0071, -0.0074],\n                [-0.0130,  0.0014,  0.0095],\n                [-0.0113,  0.0087, -0.0005]],\n      \n               [[-0.0098, -0.0140,  0.0088],\n                [-0.0080,  0.0077, -0.0026],\n                [ 0.0065,  0.0103, -0.0055]],\n      \n               [[-0.0059,  0.0014, -0.0034],\n                [-0.0084,  0.0099, -0.0092],\n                [-0.0022,  0.0106, -0.0103]],\n      \n               ...,\n      \n               [[ 0.0019,  0.0144,  0.0034],\n                [ 0.0137, -0.0041,  0.0076],\n                [ 0.0141, -0.0135,  0.0022]],\n      \n               [[-0.0097, -0.0034, -0.0144],\n                [-0.0003, -0.0007,  0.0077],\n                [-0.0129, -0.0087,  0.0103]],\n      \n               [[-0.0003,  0.0058, -0.0088],\n                [ 0.0134, -0.0042, -0.0077],\n                [ 0.0017, -0.0032,  0.0034]]],\n      \n      \n              [[[-0.0065,  0.0021, -0.0030],\n                [-0.0082, -0.0018,  0.0025],\n                [-0.0113,  0.0059,  0.0125]],\n      \n               [[-0.0122, -0.0080,  0.0103],\n                [-0.0102, -0.0123,  0.0060],\n                [-0.0119,  0.0073,  0.0105]],\n      \n               [[ 0.0077,  0.0072,  0.0087],\n                [ 0.0032,  0.0047, -0.0086],\n                [ 0.0074, -0.0136,  0.0087]],\n      \n               ...,\n      \n               [[-0.0050, -0.0142, -0.0121],\n                [-0.0143, -0.0115, -0.0002],\n                [-0.0062,  0.0114,  0.0083]],\n      \n               [[-0.0050, -0.0001, -0.0125],\n                [-0.0084, -0.0056,  0.0072],\n                [-0.0123, -0.0083,  0.0001]],\n      \n               [[ 0.0052,  0.0099,  0.0099],\n                [-0.0008,  0.0008,  0.0110],\n                [-0.0108, -0.0094, -0.0126]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 1.0294e-05,  6.4790e-03, -1.0033e-02, -2.6261e-04, -4.3739e-03,\n               3.7256e-03, -1.3415e-03, -2.0845e-03, -3.9640e-03,  9.1738e-03,\n              -1.5431e-02, -5.5264e-03,  2.4683e-03, -6.2480e-03, -4.6263e-03,\n              -2.0071e-02, -2.1231e-02, -2.8889e-03, -2.8619e-03, -6.4351e-03,\n               3.2300e-03,  1.2285e-02,  1.9022e-03,  5.0468e-03,  9.5008e-03,\n              -1.5693e-03, -2.9383e-03, -9.1721e-03,  3.9941e-03,  2.9781e-04,\n               1.9416e-03, -1.9041e-02,  2.0319e-03,  4.5793e-03, -8.4152e-03,\n              -6.1016e-03, -5.2496e-03, -1.8038e-02, -4.2900e-03, -8.7425e-03,\n              -7.7112e-03, -1.4278e-03, -5.8726e-03, -6.7496e-03, -3.9392e-03,\n               8.9380e-03,  8.6457e-03, -4.2161e-03, -1.0878e-02,  7.3712e-03,\n              -3.0424e-04, -2.9335e-03, -6.8527e-03,  4.7556e-03, -1.4390e-02,\n               2.0923e-03, -6.7611e-03, -3.9606e-03, -5.2198e-04,  1.4445e-02,\n               6.3581e-03, -1.0688e-02,  1.2386e-02,  5.3490e-03, -1.4911e-02,\n              -4.6113e-03, -2.5698e-03, -5.8662e-03,  3.7712e-03, -3.9541e-03,\n              -1.2975e-02,  3.2954e-03,  3.8488e-03, -1.6587e-03,  1.5751e-02,\n              -7.6924e-03,  1.2036e-03, -9.3269e-03, -2.0546e-03, -1.0474e-02,\n              -9.4249e-03, -3.1985e-03,  2.5840e-03,  4.6746e-03,  7.6680e-03,\n              -9.2421e-03,  4.7229e-03,  8.3762e-03,  1.5610e-02,  1.8640e-03,\n               8.5580e-04, -1.2648e-02, -3.1190e-03,  5.3747e-03,  8.9461e-03,\n               2.8538e-03, -1.3557e-02,  8.3837e-03, -9.4555e-04, -1.3657e-02,\n               2.5039e-03,  4.1848e-03,  9.7807e-04, -8.9768e-03, -4.0174e-03,\n               4.7394e-03, -1.4203e-02,  3.3136e-03, -9.8863e-04,  1.3380e-02,\n               4.4123e-03, -8.5682e-04,  3.2488e-03,  5.6312e-03, -5.5477e-03,\n              -3.6053e-03, -1.2287e-02,  7.5523e-05,  8.8647e-04,  1.5334e-02,\n              -1.1908e-02, -3.3926e-03,  6.4635e-03,  6.9469e-03,  1.1704e-02,\n              -1.3527e-02, -3.6989e-03, -4.9391e-03,  1.3149e-02, -1.0659e-02,\n               4.8953e-04,  3.6977e-03,  5.1014e-03,  5.1087e-03, -5.3181e-03,\n              -3.7099e-03, -5.5582e-03,  1.2151e-03, -1.7188e-03,  1.0360e-02,\n               7.8373e-03, -1.5450e-03,  8.8462e-03, -5.2155e-03,  4.5329e-03,\n               8.0654e-03,  4.6062e-03,  6.5825e-03, -2.2207e-04,  1.4187e-03,\n              -3.9514e-03, -1.0914e-02,  2.2634e-03,  1.7707e-03, -3.0213e-03,\n               5.8682e-03, -3.2340e-03, -1.1872e-02,  5.4958e-04, -7.6981e-04,\n              -4.4340e-04, -5.5083e-04,  2.9902e-03, -6.6654e-03,  9.6625e-03,\n              -1.0852e-03, -7.5973e-03,  8.0701e-03,  1.9799e-03,  1.4639e-03,\n              -5.0303e-03, -2.9661e-03, -1.1776e-02, -8.2642e-03, -9.8926e-03,\n               5.0740e-03, -2.5136e-03,  3.3641e-03,  1.1672e-02, -8.3272e-03,\n              -1.7158e-03,  1.3101e-04, -2.6913e-03, -1.2330e-02,  3.7317e-03,\n               6.6655e-03,  2.2467e-03,  6.3915e-03,  3.4590e-03, -1.7675e-03,\n              -5.0990e-03, -2.1738e-02, -2.8286e-03, -2.8117e-03, -3.1777e-03,\n              -5.6732e-03, -1.2428e-02,  1.2161e-02, -7.0285e-03, -2.0216e-03,\n              -7.7511e-03, -7.7276e-03, -3.7501e-03, -3.0429e-03, -7.0591e-03,\n               5.3037e-03,  6.3391e-03,  3.9778e-03, -9.8565e-03, -5.3129e-03,\n              -1.0713e-02, -3.1703e-03, -8.6614e-03, -3.9513e-03, -1.3969e-02,\n              -2.6297e-04, -1.1766e-02, -1.0007e-02, -6.6947e-04, -3.2374e-04,\n               5.3739e-03, -5.8243e-03, -2.0506e-03,  6.3001e-03,  6.7856e-03,\n              -1.1749e-02, -6.9685e-03, -2.0613e-03,  9.1659e-04,  3.1364e-03,\n               9.6284e-03,  4.0518e-03, -5.3284e-03,  2.1806e-03,  3.8664e-03,\n               7.6911e-04,  1.1871e-02, -1.6699e-03, -1.2011e-03,  5.8522e-04,\n               5.3947e-03, -1.8832e-02,  6.1819e-03,  8.6311e-05,  4.8475e-03,\n               9.1225e-03,  3.9352e-03, -1.4474e-02,  7.7210e-03, -5.7186e-03,\n               6.7926e-03, -2.4138e-04, -1.2017e-02, -4.0280e-03, -3.0268e-03,\n               1.9426e-04, -4.1396e-03,  7.4958e-03, -5.0740e-03,  2.6776e-03,\n              -5.0652e-03,  3.6183e-03,  2.1108e-03,  4.8744e-03, -2.6198e-03,\n               5.2912e-03,  4.4627e-03, -6.5194e-03,  1.0610e-02, -8.6248e-03,\n               7.1255e-03,  1.4181e-02, -1.5401e-02, -6.3426e-04,  6.5866e-03,\n              -1.1012e-02, -1.1171e-02,  1.1499e-02,  2.4821e-03, -1.0330e-02,\n              -2.7304e-03,  6.3623e-04, -2.9745e-03, -8.3250e-03, -2.1642e-03,\n              -5.7163e-03, -1.9213e-03,  4.2400e-03, -8.7932e-03, -5.2712e-03,\n              -2.2937e-04,  5.7549e-04, -1.3781e-03,  4.2111e-03,  2.6385e-03,\n               1.1077e-02,  7.0552e-03, -6.7032e-03, -3.8493e-03,  6.3032e-03,\n               1.4143e-02,  1.3632e-03,  2.2824e-04,  5.9337e-03,  6.5388e-03,\n               3.6462e-04,  9.6070e-03, -3.0480e-03, -9.6890e-03,  4.3155e-03,\n               1.0416e-02, -1.8171e-03, -2.6612e-03, -3.0431e-03, -6.1656e-03,\n               5.0282e-03,  5.9497e-03,  1.5449e-02, -4.0992e-03, -1.1792e-03,\n              -2.6535e-03, -1.0824e-03, -5.6823e-03, -3.3937e-03, -3.8006e-03,\n               5.7284e-03,  3.2871e-04, -1.0261e-02, -8.0247e-03,  1.4658e-02,\n              -1.6705e-03, -3.5333e-04,  1.1924e-02, -8.6773e-03, -2.4073e-03,\n              -4.0905e-03, -1.1189e-02, -9.7800e-03,  3.9282e-03, -1.0626e-02,\n               3.9308e-03,  7.5859e-03,  3.5046e-03, -1.2843e-02, -4.0819e-04,\n              -7.2608e-03,  1.4066e-02, -1.0087e-03, -3.6293e-03,  7.6698e-03,\n               2.7718e-03,  1.2396e-03, -1.2252e-02,  3.9003e-03,  1.1731e-03,\n               5.0363e-03, -5.9688e-03, -1.4518e-02,  5.5618e-04,  4.6282e-03,\n               7.1286e-03, -1.3760e-03,  8.0081e-03, -8.3215e-04, -8.7072e-04,\n               1.0150e-02, -1.0779e-03,  3.8576e-03, -1.6377e-02,  6.7729e-03,\n               1.9495e-03, -6.3025e-03, -7.9399e-03,  1.0028e-02, -2.5767e-03,\n              -1.5507e-02, -8.8180e-03, -2.6453e-03, -4.8373e-03,  6.6999e-05,\n               6.3346e-04, -3.3291e-03, -3.1090e-03,  4.4646e-03, -6.7873e-03,\n               1.1105e-03,  9.9981e-03,  2.0119e-03,  7.7136e-03, -2.2451e-04,\n              -1.9170e-03,  7.9057e-03,  1.2884e-02, -3.2317e-04, -1.0033e-02,\n               4.8011e-03,  1.7881e-03, -3.6221e-04, -2.4119e-03,  4.6816e-03,\n               7.6434e-03,  8.7248e-03, -6.7678e-03,  3.4557e-03, -5.3347e-04,\n               6.4053e-03,  7.9943e-03, -4.5823e-03, -5.4603e-03,  4.1160e-03,\n               7.2548e-03,  7.5118e-03,  4.2389e-03, -9.3524e-03,  2.4071e-02,\n              -1.7928e-03, -1.8998e-02,  1.6362e-02,  2.7967e-03, -8.1343e-03,\n              -4.9796e-03,  6.4136e-03,  6.3079e-03,  2.5314e-03,  3.9388e-03,\n              -2.6949e-03, -1.5170e-02, -8.9250e-03,  6.7081e-04,  7.4692e-03,\n               2.5878e-03,  3.4338e-03,  2.4975e-03,  1.3657e-03,  1.5160e-03,\n              -7.3031e-03,  5.4298e-04,  7.1379e-04,  7.0516e-03,  1.1268e-02,\n              -2.6269e-03, -6.4633e-03, -8.4695e-03, -1.0046e-02, -5.5451e-03,\n              -8.8904e-03,  1.0376e-02, -2.3635e-03,  2.4601e-03, -1.0322e-02,\n               9.0097e-03,  1.9568e-03, -7.3724e-04,  3.4907e-03,  1.1958e-03,\n               8.0987e-03,  6.3563e-03,  7.3715e-03,  9.4552e-03,  6.5389e-03,\n               9.6428e-03,  1.2634e-02, -1.0891e-02, -1.0550e-03,  7.7763e-04,\n               5.1608e-03, -2.8200e-03,  7.0263e-03, -3.5177e-03, -2.0500e-03,\n              -4.7401e-03, -1.8405e-03,  4.9311e-03, -5.9553e-03,  1.3171e-03,\n              -1.1459e-02,  2.1896e-03, -5.0336e-03,  2.3524e-03, -9.0814e-03,\n               1.4278e-03,  1.1199e-03,  2.5078e-03,  6.2734e-03,  2.6541e-03,\n               6.5258e-03, -8.0217e-03,  4.1126e-03,  3.7619e-03,  1.1391e-02,\n               1.5037e-02, -6.7292e-03,  1.2235e-03, -1.1864e-03,  2.3129e-03,\n              -5.9993e-03, -1.1167e-02,  2.7364e-03, -1.1823e-02, -1.4267e-03,\n               6.4395e-03,  6.0667e-03, -2.0831e-03, -1.2782e-02, -1.2343e-03,\n               5.9886e-03,  2.0222e-03,  3.6122e-03, -8.3613e-03, -1.8969e-02,\n               8.4997e-03, -2.1486e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9015, 0.9014, 0.9013, 0.9015, 0.9014, 0.9015, 0.9011, 0.9013, 0.9012,\n              0.9014, 0.9011, 0.9012, 0.9012, 0.9010, 0.9014, 0.9015, 0.9014, 0.9011,\n              0.9011, 0.9011, 0.9014, 0.9012, 0.9012, 0.9011, 0.9014, 0.9010, 0.9013,\n              0.9012, 0.9012, 0.9015, 0.9014, 0.9012, 0.9013, 0.9010, 0.9016, 0.9011,\n              0.9010, 0.9020, 0.9012, 0.9012, 0.9015, 0.9012, 0.9011, 0.9011, 0.9013,\n              0.9013, 0.9010, 0.9011, 0.9015, 0.9017, 0.9015, 0.9010, 0.9012, 0.9013,\n              0.9013, 0.9013, 0.9011, 0.9010, 0.9012, 0.9010, 0.9018, 0.9015, 0.9013,\n              0.9014, 0.9021, 0.9012, 0.9010, 0.9013, 0.9012, 0.9017, 0.9010, 0.9009,\n              0.9014, 0.9012, 0.9012, 0.9015, 0.9012, 0.9011, 0.9013, 0.9012, 0.9017,\n              0.9014, 0.9015, 0.9015, 0.9013, 0.9014, 0.9012, 0.9011, 0.9012, 0.9012,\n              0.9012, 0.9016, 0.9015, 0.9011, 0.9011, 0.9016, 0.9019, 0.9010, 0.9015,\n              0.9013, 0.9016, 0.9010, 0.9019, 0.9011, 0.9009, 0.9012, 0.9013, 0.9009,\n              0.9013, 0.9013, 0.9012, 0.9013, 0.9013, 0.9014, 0.9013, 0.9012, 0.9015,\n              0.9011, 0.9011, 0.9015, 0.9014, 0.9014, 0.9010, 0.9014, 0.9016, 0.9025,\n              0.9012, 0.9011, 0.9012, 0.9013, 0.9011, 0.9013, 0.9011, 0.9011, 0.9014,\n              0.9013, 0.9014, 0.9010, 0.9012, 0.9014, 0.9013, 0.9014, 0.9015, 0.9013,\n              0.9011, 0.9010, 0.9014, 0.9012, 0.9011, 0.9016, 0.9014, 0.9012, 0.9013,\n              0.9013, 0.9016, 0.9013, 0.9011, 0.9012, 0.9010, 0.9013, 0.9016, 0.9012,\n              0.9012, 0.9011, 0.9020, 0.9011, 0.9014, 0.9011, 0.9016, 0.9011, 0.9010,\n              0.9014, 0.9012, 0.9015, 0.9015, 0.9013, 0.9011, 0.9009, 0.9013, 0.9017,\n              0.9012, 0.9012, 0.9010, 0.9011, 0.9010, 0.9015, 0.9012, 0.9014, 0.9010,\n              0.9013, 0.9012, 0.9014, 0.9014, 0.9012, 0.9013, 0.9011, 0.9012, 0.9013,\n              0.9013, 0.9012, 0.9012, 0.9012, 0.9012, 0.9011, 0.9011, 0.9009, 0.9011,\n              0.9014, 0.9014, 0.9012, 0.9016, 0.9014, 0.9014, 0.9015, 0.9015, 0.9014,\n              0.9013, 0.9014, 0.9012, 0.9014, 0.9013, 0.9012, 0.9013, 0.9014, 0.9012,\n              0.9013, 0.9015, 0.9013, 0.9012, 0.9011, 0.9011, 0.9012, 0.9011, 0.9013,\n              0.9014, 0.9014, 0.9019, 0.9015, 0.9014, 0.9012, 0.9013, 0.9015, 0.9011,\n              0.9016, 0.9013, 0.9015, 0.9010, 0.9012, 0.9014, 0.9013, 0.9016, 0.9012,\n              0.9012, 0.9011, 0.9011, 0.9012, 0.9012, 0.9012, 0.9014, 0.9010, 0.9014,\n              0.9013, 0.9012, 0.9011, 0.9010, 0.9017, 0.9016, 0.9015, 0.9013, 0.9012,\n              0.9017, 0.9012, 0.9011, 0.9011, 0.9012, 0.9014, 0.9011, 0.9012, 0.9011,\n              0.9012, 0.9013, 0.9011, 0.9012, 0.9017, 0.9013, 0.9012, 0.9010, 0.9013,\n              0.9013, 0.9012, 0.9014, 0.9011, 0.9013, 0.9013, 0.9014, 0.9014, 0.9011,\n              0.9012, 0.9013, 0.9012, 0.9020, 0.9010, 0.9011, 0.9010, 0.9014, 0.9013,\n              0.9012, 0.9012, 0.9011, 0.9012, 0.9011, 0.9010, 0.9011, 0.9012, 0.9012,\n              0.9015, 0.9011, 0.9016, 0.9011, 0.9012, 0.9014, 0.9011, 0.9014, 0.9014,\n              0.9011, 0.9012, 0.9012, 0.9013, 0.9012, 0.9012, 0.9010, 0.9013, 0.9011,\n              0.9013, 0.9014, 0.9015, 0.9014, 0.9015, 0.9012, 0.9010, 0.9011, 0.9012,\n              0.9014, 0.9013, 0.9011, 0.9015, 0.9013, 0.9013, 0.9012, 0.9017, 0.9012,\n              0.9011, 0.9014, 0.9011, 0.9011, 0.9012, 0.9014, 0.9018, 0.9012, 0.9013,\n              0.9012, 0.9014, 0.9012, 0.9014, 0.9014, 0.9012, 0.9012, 0.9013, 0.9011,\n              0.9012, 0.9013, 0.9014, 0.9012, 0.9010, 0.9012, 0.9012, 0.9012, 0.9011,\n              0.9010, 0.9012, 0.9011, 0.9010, 0.9013, 0.9012, 0.9014, 0.9011, 0.9020,\n              0.9011, 0.9012, 0.9017, 0.9011, 0.9010, 0.9012, 0.9012, 0.9015, 0.9013,\n              0.9014, 0.9011, 0.9012, 0.9011, 0.9013, 0.9012, 0.9012, 0.9012, 0.9014,\n              0.9014, 0.9011, 0.9014, 0.9012, 0.9014, 0.9018, 0.9014, 0.9009, 0.9010,\n              0.9012, 0.9012, 0.9011, 0.9015, 0.9010, 0.9014, 0.9013, 0.9012, 0.9013,\n              0.9012, 0.9011, 0.9012, 0.9012, 0.9012, 0.9012, 0.9010, 0.9015, 0.9013,\n              0.9012, 0.9013, 0.9014, 0.9012, 0.9015, 0.9013, 0.9012, 0.9012, 0.9014,\n              0.9012, 0.9013, 0.9012, 0.9014, 0.9013, 0.9014, 0.9011, 0.9019, 0.9012,\n              0.9013, 0.9009, 0.9012, 0.9011, 0.9011, 0.9011, 0.9010, 0.9015, 0.9012,\n              0.9012, 0.9014, 0.9015, 0.9012, 0.9014, 0.9014, 0.9013, 0.9012, 0.9015,\n              0.9010, 0.9014, 0.9016, 0.9011, 0.9012, 0.9014, 0.9012, 0.9012, 0.9011,\n              0.9013, 0.9011, 0.9012, 0.9014, 0.9012, 0.9012, 0.9010, 0.9012, 0.9012,\n              0.9012, 0.9011, 0.9011, 0.9011, 0.9013, 0.9010, 0.9012, 0.9011, 0.9014,\n              0.9014, 0.9011, 0.9018, 0.9019, 0.9012, 0.9013, 0.9012, 0.9009, 0.9013,\n              0.9012, 0.9014, 0.9011, 0.9013, 0.9010, 0.9011, 0.9019, 0.9011],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[-0.0184]],\n        \n                 [[-0.0546]],\n        \n                 [[ 0.0116]],\n        \n                 ...,\n        \n                 [[ 0.0270]],\n        \n                 [[-0.0034]],\n        \n                 [[ 0.0379]]],\n        \n        \n                [[[ 0.0340]],\n        \n                 [[-0.0593]],\n        \n                 [[-0.0305]],\n        \n                 ...,\n        \n                 [[-0.0043]],\n        \n                 [[ 0.0178]],\n        \n                 [[-0.0041]]],\n        \n        \n                [[[-0.0137]],\n        \n                 [[ 0.0059]],\n        \n                 [[ 0.0246]],\n        \n                 ...,\n        \n                 [[-0.0431]],\n        \n                 [[ 0.0257]],\n        \n                 [[ 0.0177]]],\n        \n        \n                ...,\n        \n        \n                [[[ 0.0344]],\n        \n                 [[ 0.0214]],\n        \n                 [[ 0.0424]],\n        \n                 ...,\n        \n                 [[ 0.0128]],\n        \n                 [[ 0.0321]],\n        \n                 [[-0.0465]]],\n        \n        \n                [[[ 0.0231]],\n        \n                 [[ 0.0551]],\n        \n                 [[ 0.0196]],\n        \n                 ...,\n        \n                 [[-0.0197]],\n        \n                 [[ 0.0501]],\n        \n                 [[-0.0386]]],\n        \n        \n                [[[ 0.0520]],\n        \n                 [[ 0.0300]],\n        \n                 [[-0.0155]],\n        \n                 ...,\n        \n                 [[-0.0373]],\n        \n                 [[-0.0076]],\n        \n                 [[-0.0087]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-0.0151, -0.0705, -0.0064, -0.0367,  0.0459,  0.0787, -0.0338,  0.0260,\n                -0.0083, -0.0079,  0.0661,  0.0212, -0.0698, -0.0054,  0.0127, -0.0779,\n                -0.0654, -0.0146,  0.0412, -0.0282,  0.0421, -0.0804, -0.0083, -0.0027,\n                 0.0021, -0.0314, -0.0764,  0.0226, -0.0165, -0.0218,  0.0135,  0.0227,\n                 0.0549, -0.0921,  0.0007, -0.0195,  0.0319,  0.0606, -0.0509,  0.0307,\n                 0.0323,  0.0524,  0.0439, -0.0386,  0.0601,  0.0605,  0.0445,  0.0209,\n                 0.0779,  0.0381,  0.0037, -0.0669,  0.0010,  0.0165,  0.0076, -0.0213,\n                 0.0611,  0.0148, -0.0247, -0.0329,  0.0754,  0.0322, -0.0195,  0.0291,\n                -0.0639, -0.0502,  0.0037, -0.0168,  0.0208, -0.0127, -0.0062, -0.0119,\n                -0.0063, -0.0509,  0.0130, -0.0584,  0.0007,  0.0116, -0.0167,  0.0292,\n                -0.0493, -0.0505, -0.0019,  0.0729, -0.0011, -0.0213,  0.0012,  0.0477,\n                -0.0143,  0.0042,  0.0509,  0.0078, -0.0690, -0.0052, -0.0236,  0.0636,\n                 0.0511,  0.0014,  0.0127,  0.0079, -0.0338, -0.0660,  0.0133,  0.0170,\n                -0.0015,  0.0756, -0.0469,  0.0447,  0.0149,  0.0163, -0.0556, -0.0194,\n                -0.0501,  0.0421,  0.0023,  0.0108, -0.0271, -0.0342,  0.0631, -0.0557,\n                 0.0366,  0.0135,  0.0493,  0.0109, -0.0019,  0.0641, -0.0117, -0.0269,\n                 0.0145,  0.0064, -0.0349, -0.0165, -0.0062,  0.0550,  0.0420, -0.0231,\n                 0.0602, -0.0676, -0.0392,  0.0473,  0.0167,  0.0158, -0.0560,  0.0777,\n                 0.0196, -0.0169, -0.0179, -0.0578,  0.0044,  0.0303, -0.0042,  0.0002,\n                 0.0052, -0.0038, -0.0478, -0.0066,  0.0375, -0.0351, -0.0161,  0.0612,\n                 0.0871,  0.0201, -0.0273,  0.0449, -0.0017,  0.0547,  0.0338, -0.0494,\n                 0.0322,  0.0132, -0.0303, -0.0077,  0.0603, -0.0419, -0.0181, -0.0006,\n                -0.0224,  0.0843,  0.0900, -0.0274, -0.0922, -0.0270, -0.0310,  0.0050,\n                -0.0299,  0.0849, -0.0015, -0.0122, -0.0248,  0.0364,  0.0475,  0.0689,\n                 0.0806, -0.0405, -0.0198, -0.0493, -0.0610, -0.0153, -0.0647, -0.0286,\n                -0.0153,  0.0193, -0.0427,  0.0049, -0.0808,  0.0189, -0.0112,  0.0236,\n                -0.0067,  0.0362,  0.0086, -0.0374, -0.0650, -0.0035, -0.0892, -0.0685,\n                -0.0670, -0.0068, -0.0456,  0.0311,  0.0026, -0.0393, -0.0183, -0.0205,\n                 0.0004,  0.0253,  0.0605,  0.0447, -0.0146,  0.0542, -0.0733,  0.0461,\n                 0.0724, -0.0484,  0.0410, -0.0349,  0.0113, -0.0111,  0.0174,  0.0108,\n                 0.0401, -0.0260,  0.0194,  0.0121,  0.0273,  0.0151, -0.0004, -0.0316,\n                -0.0607, -0.0234, -0.0195, -0.0842, -0.0550, -0.0300, -0.0160,  0.0084,\n                 0.0342, -0.0009,  0.0406,  0.0586,  0.0135, -0.0052,  0.0517,  0.0371,\n                 0.0017, -0.0457,  0.0115, -0.0543,  0.0045, -0.0547, -0.0040, -0.0017,\n                -0.0271,  0.0202, -0.0034, -0.0020, -0.0057, -0.0065, -0.0342,  0.0260,\n                 0.0570, -0.0022, -0.0128, -0.0215, -0.0573, -0.0130, -0.0511,  0.0647,\n                 0.0227,  0.0209,  0.0426, -0.0361, -0.0487,  0.0471,  0.0489,  0.0641,\n                 0.0193, -0.0087,  0.0609, -0.0263,  0.0708, -0.0155,  0.0308, -0.0221,\n                 0.0488, -0.0257,  0.0360,  0.0494, -0.0092,  0.0258, -0.0361, -0.0364,\n                 0.0059, -0.0325, -0.0113, -0.0187,  0.0436, -0.0122,  0.0324,  0.0119,\n                -0.0535,  0.0229, -0.0432,  0.0128, -0.0020,  0.0648,  0.0086,  0.0022,\n                 0.0294, -0.0296, -0.0357,  0.0030,  0.0265,  0.0595, -0.0672,  0.0223,\n                -0.0507,  0.0043,  0.0224,  0.0240, -0.0501,  0.0340,  0.0359, -0.0032,\n                -0.0198, -0.0076,  0.0617,  0.0655,  0.0094, -0.0485,  0.0615,  0.0073,\n                -0.0146,  0.0273,  0.0145,  0.0116,  0.0230,  0.0159, -0.0266,  0.0102,\n                -0.0164,  0.0372, -0.0268, -0.0915,  0.0035, -0.0598,  0.0438, -0.0643,\n                -0.0111, -0.0091,  0.0096,  0.0068,  0.0304,  0.0240, -0.0085, -0.0284,\n                 0.0034, -0.0010,  0.0032, -0.0188, -0.0399, -0.0214, -0.0161,  0.0276,\n                -0.0507,  0.0076, -0.0272,  0.0525,  0.0066, -0.0437, -0.0949,  0.0109,\n                 0.0070,  0.0082,  0.0157, -0.0072,  0.0059, -0.0230,  0.0407,  0.0821,\n                -0.0137, -0.0060, -0.0522,  0.0438,  0.0411, -0.0234, -0.0204,  0.0191,\n                 0.0220, -0.0347,  0.0208,  0.0677, -0.0367, -0.0021, -0.0045, -0.0086,\n                 0.0348, -0.0018,  0.0212, -0.0166,  0.0407, -0.0139, -0.0049, -0.0213,\n                -0.0111, -0.0375, -0.0427, -0.0156,  0.0174, -0.0096, -0.0234, -0.0291,\n                 0.0166, -0.0265,  0.0111, -0.0101,  0.0736, -0.0256,  0.0394,  0.0502,\n                -0.0436, -0.0463,  0.0554, -0.0485, -0.0581,  0.0257, -0.0019,  0.0350,\n                 0.0371,  0.0305,  0.0308, -0.0503,  0.0586, -0.0215,  0.0061,  0.0446,\n                -0.0675, -0.0469, -0.0383,  0.0225, -0.0156, -0.0051,  0.0551,  0.0213,\n                -0.0533,  0.0372,  0.0388, -0.0002, -0.0222, -0.0681,  0.0831, -0.0373,\n                 0.0002, -0.0014,  0.0178, -0.0530, -0.0398,  0.0409,  0.0309, -0.0406,\n                -0.1116, -0.0042, -0.0515, -0.0407,  0.0275,  0.0116,  0.0351, -0.0266,\n                 0.0320, -0.0637,  0.0360,  0.0068, -0.0211,  0.0158,  0.0073, -0.0757,\n                -0.0098, -0.0111, -0.0644,  0.0427,  0.0264, -0.0030,  0.0078,  0.0457,\n                -0.0382, -0.0112, -0.0264, -0.0109,  0.0024, -0.0643,  0.0044,  0.0277],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9226, 0.9183, 0.9121, 0.9156, 0.9179, 0.9177, 0.9159, 0.9160, 0.9150,\n                0.9163, 0.9202, 0.9159, 0.9162, 0.9192, 0.9146, 0.9140, 0.9124, 0.9188,\n                0.9163, 0.9182, 0.9130, 0.9185, 0.9134, 0.9216, 0.9222, 0.9134, 0.9195,\n                0.9141, 0.9123, 0.9162, 0.9140, 0.9142, 0.9142, 0.9153, 0.9190, 0.9140,\n                0.9169, 0.9204, 0.9186, 0.9197, 0.9194, 0.9149, 0.9294, 0.9151, 0.9176,\n                0.9161, 0.9138, 0.9155, 0.9327, 0.9157, 0.9204, 0.9174, 0.9197, 0.9142,\n                0.9237, 0.9161, 0.9178, 0.9147, 0.9235, 0.9190, 0.9217, 0.9142, 0.9153,\n                0.9239, 0.9226, 0.9286, 0.9232, 0.9129, 0.9124, 0.9157, 0.9190, 0.9126,\n                0.9246, 0.9178, 0.9268, 0.9169, 0.9156, 0.9135, 0.9195, 0.9160, 0.9207,\n                0.9212, 0.9180, 0.9329, 0.9151, 0.9158, 0.9141, 0.9233, 0.9147, 0.9300,\n                0.9162, 0.9129, 0.9196, 0.9160, 0.9182, 0.9174, 0.9183, 0.9227, 0.9114,\n                0.9158, 0.9132, 0.9149, 0.9191, 0.9202, 0.9208, 0.9182, 0.9170, 0.9220,\n                0.9145, 0.9187, 0.9210, 0.9146, 0.9189, 0.9239, 0.9218, 0.9144, 0.9193,\n                0.9167, 0.9185, 0.9142, 0.9160, 0.9184, 0.9151, 0.9206, 0.9148, 0.9186,\n                0.9182, 0.9166, 0.9203, 0.9163, 0.9132, 0.9140, 0.9155, 0.9161, 0.9207,\n                0.9123, 0.9186, 0.9209, 0.9160, 0.9139, 0.9215, 0.9193, 0.9209, 0.9179,\n                0.9179, 0.9151, 0.9151, 0.9171, 0.9161, 0.9246, 0.9257, 0.9159, 0.9161,\n                0.9121, 0.9209, 0.9166, 0.9187, 0.9163, 0.9120, 0.9120, 0.9178, 0.9144,\n                0.9143, 0.9163, 0.9116, 0.9158, 0.9202, 0.9132, 0.9149, 0.9164, 0.9133,\n                0.9237, 0.9148, 0.9223, 0.9169, 0.9135, 0.9168, 0.9187, 0.9161, 0.9260,\n                0.9248, 0.9150, 0.9169, 0.9224, 0.9132, 0.9280, 0.9115, 0.9248, 0.9152,\n                0.9138, 0.9233, 0.9198, 0.9149, 0.9151, 0.9208, 0.9169, 0.9258, 0.9151,\n                0.9186, 0.9282, 0.9133, 0.9168, 0.9213, 0.9223, 0.9198, 0.9149, 0.9131,\n                0.9148, 0.9152, 0.9130, 0.9189, 0.9134, 0.9172, 0.9268, 0.9116, 0.9160,\n                0.9189, 0.9145, 0.9190, 0.9147, 0.9178, 0.9196, 0.9152, 0.9182, 0.9139,\n                0.9149, 0.9142, 0.9174, 0.9156, 0.9255, 0.9166, 0.9176, 0.9146, 0.9159,\n                0.9141, 0.9225, 0.9198, 0.9203, 0.9174, 0.9206, 0.9230, 0.9172, 0.9127,\n                0.9250, 0.9140, 0.9165, 0.9212, 0.9192, 0.9192, 0.9145, 0.9144, 0.9256,\n                0.9178, 0.9210, 0.9148, 0.9136, 0.9192, 0.9171, 0.9165, 0.9200, 0.9156,\n                0.9162, 0.9148, 0.9257, 0.9153, 0.9142, 0.9163, 0.9158, 0.9192, 0.9166,\n                0.9367, 0.9165, 0.9194, 0.9123, 0.9145, 0.9172, 0.9149, 0.9163, 0.9177,\n                0.9124, 0.9151, 0.9237, 0.9149, 0.9190, 0.9158, 0.9181, 0.9162, 0.9216,\n                0.9188, 0.9236, 0.9185, 0.9169, 0.9143, 0.9231, 0.9159, 0.9182, 0.9142,\n                0.9218, 0.9200, 0.9169, 0.9158, 0.9222, 0.9137, 0.9152, 0.9186, 0.9164,\n                0.9145, 0.9273, 0.9200, 0.9149, 0.9143, 0.9161, 0.9147, 0.9193, 0.9159,\n                0.9176, 0.9155, 0.9125, 0.9227, 0.9241, 0.9173, 0.9128, 0.9213, 0.9173,\n                0.9154, 0.9155, 0.9163, 0.9308, 0.9153, 0.9155, 0.9146, 0.9173, 0.9222,\n                0.9229, 0.9158, 0.9110, 0.9135, 0.9145, 0.9140, 0.9115, 0.9207, 0.9257,\n                0.9131, 0.9163, 0.9149, 0.9213, 0.9166, 0.9332, 0.9218, 0.9181, 0.9249,\n                0.9148, 0.9175, 0.9158, 0.9214, 0.9186, 0.9156, 0.9175, 0.9129, 0.9149,\n                0.9220, 0.9116, 0.9159, 0.9364, 0.9160, 0.9194, 0.9189, 0.9227, 0.9108,\n                0.9131, 0.9141, 0.9174, 0.9147, 0.9238, 0.9246, 0.9155, 0.9187, 0.9129,\n                0.9144, 0.9236, 0.9293, 0.9158, 0.9181, 0.9153, 0.9274, 0.9193, 0.9174,\n                0.9148, 0.9191, 0.9195, 0.9174, 0.9170, 0.9181, 0.9154, 0.9184, 0.9271,\n                0.9238, 0.9134, 0.9179, 0.9146, 0.9141, 0.9169, 0.9169, 0.9238, 0.9136,\n                0.9165, 0.9151, 0.9176, 0.9197, 0.9185, 0.9159, 0.9195, 0.9189, 0.9144,\n                0.9167, 0.9232, 0.9215, 0.9151, 0.9164, 0.9152, 0.9204, 0.9146, 0.9120,\n                0.9139, 0.9172, 0.9132, 0.9246, 0.9176, 0.9128, 0.9140, 0.9255, 0.9164,\n                0.9188, 0.9215, 0.9207, 0.9118, 0.9146, 0.9155, 0.9168, 0.9250, 0.9191,\n                0.9205, 0.9168, 0.9150, 0.9152, 0.9175, 0.9147, 0.9157, 0.9213, 0.9184,\n                0.9138, 0.9179, 0.9365, 0.9170, 0.9166, 0.9142, 0.9193, 0.9180, 0.9247,\n                0.9185, 0.9209, 0.9170, 0.9183, 0.9195, 0.9123, 0.9127, 0.9165, 0.9261,\n                0.9151, 0.9169, 0.9152, 0.9339, 0.9139, 0.9151, 0.9150, 0.9263, 0.9145,\n                0.9202, 0.9149, 0.9216, 0.9166, 0.9172, 0.9289, 0.9179, 0.9216, 0.9157,\n                0.9155, 0.9167, 0.9141, 0.9197, 0.9127, 0.9257, 0.9191, 0.9205, 0.9198,\n                0.9207, 0.9169, 0.9171, 0.9188, 0.9172, 0.9137, 0.9178, 0.9183, 0.9163,\n                0.9167, 0.9252, 0.9141, 0.9150, 0.9170, 0.9115, 0.9185, 0.9149],\n               grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [512, 256, 3, 3]], ["0.bn1.weight", [512]], ["0.bn1.bias", [512]], ["0.conv2.weight", [512, 512, 3, 3]], ["0.bn2.weight", [512]], ["0.bn2.bias", [512]], ["0.downsample.0.weight", [512, 256, 1, 1]], ["0.downsample.1.weight", [512]], ["0.downsample.1.bias", [512]]], "output_shape": [[512, 512, 1, 1]], "num_parameters": [1179648, 512, 512, 2359296, 512, 512, 131072, 512, 512]}, {"name": "avgpool", "id": 140311229273520, "class_name": "AveragePool()", "parameters": [], "output_shape": [[512, 512]], "num_parameters": []}, {"name": "fc", "id": 140311229272464, "class_name": "Linear(in_features=512, out_features=10, bias=True)", "parameters": [["weight", [10, 512]], ["bias", [10]]], "output_shape": [[512, 10]], "num_parameters": [5120, 10]}], "edges": []}