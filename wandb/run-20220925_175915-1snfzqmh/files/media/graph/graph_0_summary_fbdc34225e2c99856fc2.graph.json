{"format": "torch", "nodes": [{"name": "conv1", "id": 140209539497840, "class_name": "Conv2d(\n  self.stride=2, self.padding=(3, 3), self.weight=Parameter containing:\n  tensor([[[[-0.0494,  0.0185, -0.0045,  ..., -0.0563, -0.0334,  0.0167],\n            [-0.0661, -0.0638,  0.0433,  ...,  0.0540,  0.0783, -0.0197],\n            [ 0.0782, -0.0801,  0.0540,  ..., -0.0630, -0.0344,  0.0402],\n            ...,\n            [ 0.0683,  0.0724, -0.0401,  ...,  0.0616, -0.0520,  0.0247],\n            [-0.0631, -0.0485,  0.0119,  ..., -0.0113,  0.0151,  0.0550],\n            [ 0.0482, -0.0438, -0.0150,  ...,  0.0792,  0.0307,  0.0587]],\n  \n           [[-0.0720,  0.0298,  0.0092,  ...,  0.0122,  0.0124,  0.0623],\n            [-0.0358,  0.0219, -0.0083,  ..., -0.0693, -0.0169,  0.0571],\n            [-0.0470, -0.0058,  0.0731,  ..., -0.0065,  0.0284, -0.0694],\n            ...,\n            [-0.0749,  0.0418,  0.0509,  ...,  0.0114, -0.0605, -0.0519],\n            [-0.0660, -0.0292, -0.0759,  ..., -0.0527,  0.0617,  0.0306],\n            [-0.0638,  0.0056,  0.0154,  ..., -0.0743,  0.0310, -0.0186]],\n  \n           [[-0.0049,  0.0654,  0.0006,  ...,  0.0212, -0.0455, -0.0665],\n            [-0.0457,  0.0741,  0.0741,  ..., -0.0377, -0.0808, -0.0139],\n            [-0.0147, -0.0087,  0.0705,  ..., -0.0156, -0.0455, -0.0681],\n            ...,\n            [ 0.0036, -0.0322, -0.0572,  ...,  0.0624,  0.0350, -0.0533],\n            [ 0.0601,  0.0522, -0.0053,  ..., -0.0497,  0.0710, -0.0819],\n            [-0.0678,  0.0436, -0.0031,  ...,  0.0251, -0.0515,  0.0146]]],\n  \n  \n          [[[-0.0300,  0.0804, -0.0528,  ...,  0.0494, -0.0770, -0.0066],\n            [ 0.0256, -0.0346,  0.0188,  ..., -0.0031,  0.0041, -0.0050],\n            [-0.0467, -0.0074,  0.0260,  ...,  0.0067, -0.0707,  0.0274],\n            ...,\n            [-0.0447, -0.0675, -0.0518,  ..., -0.0457, -0.0234,  0.0520],\n            [ 0.0301, -0.0238,  0.0302,  ..., -0.0291, -0.0077,  0.0604],\n            [ 0.0140,  0.0674, -0.0637,  ..., -0.0742,  0.0492,  0.0037]],\n  \n           [[-0.0239, -0.0264,  0.0041,  ..., -0.0166, -0.0354, -0.0401],\n            [ 0.0273,  0.0384, -0.0173,  ...,  0.0429, -0.0561, -0.0307],\n            [ 0.0150, -0.0149, -0.0787,  ...,  0.0400, -0.0079, -0.0020],\n            ...,\n            [ 0.0549, -0.0041, -0.0014,  ..., -0.0372,  0.0617,  0.0519],\n            [ 0.0391, -0.0065, -0.0144,  ...,  0.0262,  0.0718,  0.0704],\n            [-0.0594, -0.0342,  0.0662,  ..., -0.0702, -0.0681,  0.0474]],\n  \n           [[ 0.0332, -0.0207,  0.0449,  ..., -0.0301,  0.0594, -0.0169],\n            [-0.0108,  0.0045, -0.0654,  ..., -0.0445,  0.0222, -0.0754],\n            [-0.0379, -0.0809,  0.0040,  ..., -0.0447,  0.0077,  0.0532],\n            ...,\n            [ 0.0014, -0.0474, -0.0248,  ..., -0.0407, -0.0130, -0.0699],\n            [ 0.0650, -0.0563,  0.0341,  ..., -0.0685,  0.0053, -0.0219],\n            [ 0.0695, -0.0436, -0.0727,  ..., -0.0668, -0.0119,  0.0644]]],\n  \n  \n          [[[-0.0306, -0.0359,  0.0817,  ..., -0.0272, -0.0157,  0.0367],\n            [ 0.0320, -0.0323,  0.0271,  ..., -0.0731,  0.0280,  0.0271],\n            [-0.0744, -0.0624,  0.0320,  ...,  0.0132,  0.0711,  0.0817],\n            ...,\n            [-0.0516, -0.0513,  0.0766,  ..., -0.0083,  0.0005,  0.0676],\n            [-0.0401,  0.0465, -0.0424,  ...,  0.0647,  0.0356, -0.0051],\n            [-0.0109, -0.0067,  0.0751,  ..., -0.0672, -0.0521, -0.0518]],\n  \n           [[-0.0673,  0.0266,  0.0422,  ...,  0.0472, -0.0274,  0.0494],\n            [-0.0800, -0.0139,  0.0042,  ..., -0.0348, -0.0600,  0.0232],\n            [ 0.0643, -0.0135,  0.0314,  ..., -0.0690, -0.0747,  0.0753],\n            ...,\n            [ 0.0173, -0.0024, -0.0816,  ...,  0.0598, -0.0005,  0.0484],\n            [ 0.0149,  0.0564, -0.0819,  ...,  0.0304, -0.0403, -0.0784],\n            [-0.0080, -0.0374, -0.0542,  ...,  0.0784, -0.0375,  0.0176]],\n  \n           [[ 0.0763,  0.0086,  0.0798,  ...,  0.0544, -0.0142, -0.0429],\n            [-0.0138, -0.0550, -0.0581,  ..., -0.0426,  0.0782, -0.0378],\n            [ 0.0141,  0.0671, -0.0025,  ..., -0.0444, -0.0625, -0.0530],\n            ...,\n            [ 0.0521, -0.0548,  0.0797,  ...,  0.0637,  0.0414, -0.0569],\n            [ 0.0519,  0.0764,  0.0148,  ..., -0.0613, -0.0290, -0.0229],\n            [ 0.0251,  0.0230,  0.0382,  ..., -0.0732,  0.0553,  0.0253]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0362,  0.0677,  0.0665,  ..., -0.0404, -0.0076,  0.0604],\n            [ 0.0081, -0.0253,  0.0606,  ..., -0.0531,  0.0491,  0.0626],\n            [ 0.0494, -0.0140,  0.0108,  ..., -0.0390, -0.0046, -0.0516],\n            ...,\n            [ 0.0400, -0.0029,  0.0042,  ...,  0.0731,  0.0533, -0.0083],\n            [-0.0266,  0.0448,  0.0584,  ..., -0.0524,  0.0644,  0.0790],\n            [-0.0662,  0.0177,  0.0584,  ...,  0.0115, -0.0724, -0.0629]],\n  \n           [[-0.0558, -0.0146, -0.0673,  ..., -0.0012, -0.0596, -0.0465],\n            [-0.0498, -0.0085,  0.0489,  ...,  0.0724,  0.0401, -0.0715],\n            [ 0.0265,  0.0042,  0.0511,  ...,  0.0358, -0.0441, -0.0032],\n            ...,\n            [ 0.0219,  0.0768, -0.0467,  ..., -0.0472, -0.0398,  0.0371],\n            [-0.0531,  0.0179, -0.0399,  ...,  0.0648, -0.0695,  0.0715],\n            [ 0.0748, -0.0774, -0.0243,  ..., -0.0395, -0.0372, -0.0781]],\n  \n           [[-0.0328,  0.0646, -0.0676,  ..., -0.0430,  0.0014,  0.0751],\n            [ 0.0328,  0.0654,  0.0144,  ..., -0.0389,  0.0010,  0.0225],\n            [ 0.0262,  0.0441,  0.0173,  ...,  0.0587,  0.0167,  0.0147],\n            ...,\n            [-0.0733, -0.0331, -0.0394,  ..., -0.0319,  0.0468, -0.0674],\n            [ 0.0607, -0.0717, -0.0262,  ..., -0.0243,  0.0554,  0.0701],\n            [ 0.0020, -0.0637,  0.0291,  ..., -0.0241,  0.0543,  0.0410]]],\n  \n  \n          [[[ 0.0073,  0.0058,  0.0601,  ..., -0.0471,  0.0250, -0.0642],\n            [-0.0388, -0.0149, -0.0565,  ..., -0.0181,  0.0134, -0.0784],\n            [ 0.0266,  0.0495,  0.0392,  ...,  0.0039,  0.0756, -0.0044],\n            ...,\n            [ 0.0443, -0.0689,  0.0650,  ...,  0.0579, -0.0633,  0.0002],\n            [-0.0720,  0.0158, -0.0183,  ..., -0.0749, -0.0804,  0.0249],\n            [-0.0400,  0.0156, -0.0550,  ...,  0.0028, -0.0508, -0.0772]],\n  \n           [[ 0.0124,  0.0004,  0.0795,  ...,  0.0033, -0.0099,  0.0576],\n            [ 0.0804, -0.0071,  0.0750,  ...,  0.0413,  0.0237,  0.0678],\n            [-0.0751,  0.0620,  0.0780,  ..., -0.0758, -0.0350,  0.0099],\n            ...,\n            [ 0.0049, -0.0678,  0.0330,  ...,  0.0694,  0.0560, -0.0679],\n            [-0.0213,  0.0011, -0.0244,  ...,  0.0236, -0.0569, -0.0330],\n            [-0.0654,  0.0160,  0.0673,  ...,  0.0765, -0.0736, -0.0258]],\n  \n           [[ 0.0113,  0.0308, -0.0416,  ...,  0.0595, -0.0612, -0.0235],\n            [ 0.0801,  0.0500,  0.0269,  ..., -0.0473,  0.0759, -0.0280],\n            [ 0.0056, -0.0095, -0.0484,  ...,  0.0363,  0.0255,  0.0056],\n            ...,\n            [ 0.0543, -0.0746,  0.0745,  ..., -0.0537,  0.0231,  0.0158],\n            [-0.0079,  0.0517, -0.0416,  ..., -0.0379, -0.0196, -0.0393],\n            [ 0.0583,  0.0025,  0.0436,  ..., -0.0317, -0.0129,  0.0168]]],\n  \n  \n          [[[ 0.0766,  0.0179, -0.0553,  ..., -0.0309,  0.0489,  0.0611],\n            [ 0.0245, -0.0180,  0.0191,  ...,  0.0426,  0.0748, -0.0704],\n            [-0.0267, -0.0382, -0.0198,  ...,  0.0472,  0.0008,  0.0009],\n            ...,\n            [ 0.0823, -0.0360, -0.0643,  ..., -0.0132, -0.0583,  0.0467],\n            [ 0.0547, -0.0278, -0.0212,  ...,  0.0326, -0.0172,  0.0803],\n            [ 0.0013,  0.0737, -0.0738,  ..., -0.0426, -0.0197, -0.0777]],\n  \n           [[-0.0770,  0.0344, -0.0352,  ...,  0.0372, -0.0094,  0.0373],\n            [ 0.0552,  0.0283,  0.0176,  ..., -0.0716, -0.0080,  0.0162],\n            [ 0.0451,  0.0394,  0.0089,  ..., -0.0814,  0.0442,  0.0213],\n            ...,\n            [-0.0732, -0.0314, -0.0447,  ...,  0.0542,  0.0709, -0.0811],\n            [-0.0526,  0.0460,  0.0732,  ..., -0.0431, -0.0342,  0.0658],\n            [ 0.0101,  0.0417,  0.0054,  ...,  0.0120,  0.0325, -0.0799]],\n  \n           [[-0.0576, -0.0783,  0.0439,  ...,  0.0820, -0.0014,  0.0575],\n            [ 0.0102, -0.0381, -0.0105,  ..., -0.0531,  0.0433, -0.0322],\n            [-0.0426, -0.0425, -0.0505,  ..., -0.0480, -0.0810, -0.0222],\n            ...,\n            [ 0.0762, -0.0617, -0.0041,  ...,  0.0758,  0.0458, -0.0027],\n            [-0.0434, -0.0651, -0.0520,  ...,  0.0182,  0.0078,  0.0540],\n            [-0.0418, -0.0671,  0.0388,  ..., -0.0725,  0.0570,  0.0542]]]],\n         requires_grad=True)\n)", "parameters": [["weight", [64, 3, 7, 7]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [9408]}, {"name": "bn1", "id": 140209539497696, "class_name": "BatchNorm2d(\n  self.momentum=0.1, self.weight=Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 1.7934e-03,  1.1897e-03, -1.1161e-03,  6.1861e-04, -1.1927e-04,\n           3.7240e-04,  2.0410e-03,  2.2397e-04, -2.4273e-04, -1.7306e-03,\n          -1.5298e-03, -1.5178e-04, -8.7137e-04, -1.0743e-03, -1.0705e-03,\n           7.0965e-05,  2.1928e-03,  2.0212e-03,  8.2170e-04,  1.3145e-03,\n           7.4723e-04,  3.9270e-04, -2.2842e-04,  1.4855e-03,  3.3302e-04,\n          -2.0116e-04, -6.7498e-04, -1.7000e-03, -8.4019e-04, -1.2353e-03,\n           8.2052e-04, -8.7619e-04, -6.6328e-04, -1.1205e-03, -8.3951e-04,\n          -4.0914e-04,  1.0753e-04, -1.3001e-04, -3.6563e-05,  5.9559e-04,\n           3.9608e-04,  8.2044e-04,  1.3962e-03,  1.5637e-03,  1.9816e-03,\n          -8.9780e-04, -6.1559e-04, -1.2779e-03, -1.4454e-03, -4.0114e-04,\n           2.6757e-04, -3.9494e-04, -9.1088e-04,  2.2168e-03, -3.9231e-04,\n          -4.7288e-04, -8.5007e-04, -3.9621e-04, -1.4188e-03, -1.7997e-04,\n           5.9626e-04, -7.8655e-04, -5.3762e-04,  1.8185e-03],\n         grad_fn=<AddBackward0>), self.running_var=tensor([0.9356, 0.9676, 0.9430, 0.9480, 0.9232, 0.9304, 0.9580, 0.9361, 0.9268,\n          1.0116, 0.9619, 0.9295, 0.9401, 0.9242, 0.9200, 0.9227, 0.9548, 1.0788,\n          0.9153, 0.9921, 0.9638, 0.9304, 0.9339, 0.9202, 0.9165, 0.9268, 0.9131,\n          0.9465, 0.9214, 0.9292, 0.9306, 0.9444, 0.9421, 0.9591, 0.9638, 0.9394,\n          0.9371, 0.9444, 0.9375, 0.9277, 0.9098, 0.9405, 0.9416, 0.9885, 1.1003,\n          0.9258, 0.9388, 0.9492, 0.9913, 0.9266, 0.9108, 0.9518, 0.9341, 1.0496,\n          0.9146, 0.9193, 0.9201, 0.9179, 0.9310, 0.9126, 0.9186, 0.9201, 0.9778,\n          0.9880], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n)", "parameters": [["weight", [64]], ["bias", [64]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [64, 64]}, {"name": "relu", "id": 140209539497744, "class_name": "ReLU()", "parameters": [], "output_shape": [[512, 64, 16, 16]], "num_parameters": []}, {"name": "pool", "id": 140209539497552, "class_name": "MaxPool2d(self.kernel_size=(3, 3), self.stride=(3, 3), self.padding=1)", "parameters": [], "output_shape": [[512, 64, 6, 6]], "num_parameters": []}, {"name": "layer1", "id": 140209539496544, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-2.3336e-02, -3.7683e-02,  3.6240e-02],\n                [-3.3593e-02, -2.8458e-02,  1.2801e-02],\n                [-2.7208e-03, -2.2294e-02,  3.8332e-02]],\n      \n               [[-1.1015e-02, -3.9448e-02,  2.3664e-02],\n                [-7.2824e-03, -2.6135e-02,  3.0531e-02],\n                [ 8.2134e-03,  3.6529e-02, -3.8337e-02]],\n      \n               [[ 3.2054e-02,  3.6147e-02, -2.1757e-02],\n                [ 2.3136e-02, -7.1410e-03, -1.8697e-02],\n                [ 2.5671e-02, -2.3116e-02,  8.6600e-03]],\n      \n               ...,\n      \n               [[ 3.0528e-02,  4.5365e-04, -1.2366e-02],\n                [ 2.1139e-02, -4.1534e-02,  1.7248e-03],\n                [-2.3812e-02,  2.0596e-02, -2.2140e-02]],\n      \n               [[ 6.9507e-03, -1.3515e-02, -1.1460e-02],\n                [-4.6647e-03, -2.0797e-02,  1.2567e-02],\n                [-1.7286e-02, -1.1625e-02, -3.7652e-02]],\n      \n               [[ 1.4681e-02,  1.8957e-02,  2.9002e-03],\n                [-3.3982e-02,  3.8591e-02, -4.0988e-02],\n                [ 2.9836e-02,  1.9338e-02,  6.8523e-03]]],\n      \n      \n              [[[-3.1363e-03,  1.5706e-02,  2.7208e-02],\n                [-3.8962e-03,  3.2022e-02, -3.2535e-02],\n                [ 1.8425e-02,  1.2886e-02,  8.3191e-03]],\n      \n               [[-1.6628e-02, -4.3953e-05,  2.1379e-02],\n                [ 3.1825e-02,  3.0813e-02, -4.0657e-02],\n                [ 1.5312e-02, -6.8886e-03,  2.8171e-02]],\n      \n               [[ 3.4883e-02, -2.3125e-02, -7.7041e-03],\n                [ 2.2329e-02, -3.8584e-02, -1.9916e-02],\n                [ 4.0387e-02,  3.3164e-02,  1.2754e-02]],\n      \n               ...,\n      \n               [[ 3.8422e-02, -3.5450e-04,  2.3392e-02],\n                [-4.2370e-03,  2.8589e-03,  3.3198e-02],\n                [-4.3366e-03,  3.0048e-03, -2.2559e-02]],\n      \n               [[ 3.8550e-02,  2.5551e-02, -5.2764e-03],\n                [-3.8166e-03,  1.4220e-03, -1.9550e-02],\n                [-2.4628e-02, -4.0714e-02,  2.6708e-02]],\n      \n               [[-3.8101e-02,  2.6174e-02, -1.5325e-02],\n                [-7.3530e-03, -2.3077e-02, -2.3333e-02],\n                [ 3.7985e-02,  4.1294e-02, -2.6898e-02]]],\n      \n      \n              [[[-2.4850e-02,  3.1309e-03,  2.0735e-02],\n                [ 3.9801e-02,  8.7949e-03,  2.4095e-02],\n                [-1.7322e-03, -3.4445e-02, -3.6167e-02]],\n      \n               [[ 1.6972e-02, -6.6248e-03,  3.3250e-02],\n                [ 3.7492e-03, -1.2803e-02, -3.7788e-02],\n                [ 9.9330e-03, -3.6360e-02,  3.4976e-02]],\n      \n               [[-7.4546e-04,  3.6134e-04, -3.3279e-02],\n                [ 2.8818e-02, -1.7351e-02, -2.9034e-02],\n                [ 1.4857e-02,  2.2464e-02, -4.9387e-03]],\n      \n               ...,\n      \n               [[ 1.6714e-02, -1.0799e-02,  9.8646e-03],\n                [-1.5095e-03, -2.8078e-02,  1.4737e-03],\n                [-2.3133e-02,  3.4474e-02, -1.4870e-02]],\n      \n               [[ 6.6737e-03, -2.9895e-02, -6.4353e-03],\n                [ 1.7231e-02, -1.3516e-02, -1.4419e-02],\n                [ 3.2907e-03, -2.7512e-02, -3.2158e-02]],\n      \n               [[ 3.0937e-02,  2.7811e-02,  9.7497e-03],\n                [-1.1404e-02,  3.4217e-02,  9.0145e-03],\n                [ 4.0569e-02,  9.2766e-03,  3.8141e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[ 2.3711e-02,  3.1329e-02,  4.0778e-02],\n                [-3.5909e-02, -5.2542e-03,  2.1175e-02],\n                [ 2.1845e-02,  2.0473e-03,  1.7798e-02]],\n      \n               [[-6.9379e-03,  3.8531e-02,  3.3588e-02],\n                [-3.9780e-02, -3.1302e-02,  1.5973e-02],\n                [ 3.9831e-02,  3.4736e-02, -2.8111e-02]],\n      \n               [[-1.0988e-02,  1.3934e-02, -2.9839e-02],\n                [-6.7097e-03, -3.0717e-02, -1.9979e-02],\n                [ 2.5546e-02, -2.7862e-02,  3.7214e-04]],\n      \n               ...,\n      \n               [[ 2.1231e-02, -3.1750e-02, -1.5750e-03],\n                [ 3.0930e-02,  3.9144e-02, -3.0946e-03],\n                [-2.7037e-02, -1.1977e-02, -1.6937e-02]],\n      \n               [[ 4.0054e-02, -4.4517e-03,  2.0407e-02],\n                [ 1.0419e-02, -1.3757e-02, -7.7487e-04],\n                [-2.6293e-02, -4.1123e-02, -2.3034e-03]],\n      \n               [[-2.1467e-02,  1.7545e-02,  3.9674e-02],\n                [ 2.7668e-02,  4.0863e-02, -3.1682e-02],\n                [-1.4871e-02, -3.0532e-02,  1.6167e-02]]],\n      \n      \n              [[[ 2.3632e-02, -2.0863e-02,  3.4479e-02],\n                [-3.8399e-02,  2.5813e-02,  5.3601e-03],\n                [ 1.0051e-02,  3.6227e-02,  1.8053e-02]],\n      \n               [[-4.2230e-03, -3.3157e-02,  2.9874e-02],\n                [ 3.0583e-02, -2.9187e-02,  2.4923e-02],\n                [ 2.4874e-03,  1.7381e-02, -2.0169e-02]],\n      \n               [[ 3.3500e-02,  1.7612e-02,  3.0126e-02],\n                [ 5.7839e-03,  3.6443e-02,  4.2064e-04],\n                [ 1.5862e-02, -1.8761e-03, -1.3822e-02]],\n      \n               ...,\n      \n               [[-3.5339e-02, -2.6315e-02, -2.3030e-03],\n                [-3.8215e-02, -1.1898e-02,  3.2162e-02],\n                [-5.4484e-04, -3.5001e-02,  1.0260e-02]],\n      \n               [[-2.9791e-02, -2.3966e-02, -2.5006e-02],\n                [ 1.9078e-02, -3.3155e-03,  2.1370e-03],\n                [-5.1780e-03,  2.9605e-02, -3.2663e-02]],\n      \n               [[-4.3850e-03,  9.4691e-03, -3.6481e-02],\n                [-1.7409e-02,  4.3043e-04,  3.0028e-02],\n                [-3.0497e-02, -1.2954e-02,  3.7297e-02]]],\n      \n      \n              [[[ 2.8407e-02, -2.9167e-02, -1.5529e-02],\n                [-4.0984e-02,  1.0004e-02,  1.1266e-02],\n                [ 1.1659e-02, -2.4472e-02, -1.4115e-02]],\n      \n               [[ 1.5151e-02,  8.9234e-03, -3.7747e-02],\n                [ 1.9828e-02,  4.0450e-02,  2.8294e-03],\n                [-1.5708e-02,  3.0039e-02,  3.2176e-02]],\n      \n               [[ 6.4959e-04, -1.6595e-02, -1.8257e-02],\n                [-3.6290e-02, -3.2496e-02, -3.7876e-02],\n                [-3.1854e-02,  3.4533e-02,  3.5971e-03]],\n      \n               ...,\n      \n               [[ 1.5165e-02,  3.6401e-02,  3.3498e-02],\n                [ 2.4498e-02, -2.5386e-02,  2.6294e-02],\n                [-2.9338e-02,  2.8290e-02,  2.8985e-02]],\n      \n               [[-4.0488e-02,  3.4769e-02,  5.4737e-06],\n                [-6.4426e-04,  2.1431e-02, -3.3127e-02],\n                [-2.8150e-02,  1.6109e-02,  1.8460e-03]],\n      \n               [[-2.8554e-02, -8.2003e-03,  1.0238e-02],\n                [ 2.0566e-02, -2.3549e-02,  2.8376e-02],\n                [ 2.0194e-02,  3.7356e-02,  3.8627e-02]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 1.2009e-02,  1.2617e-01, -6.8984e-03, -3.1608e-03,  4.9303e-02,\n              -7.3327e-02, -5.9972e-02, -2.1847e-02,  1.9909e-03,  4.2370e-02,\n              -5.6365e-02,  1.8770e-02, -1.2157e-01,  7.1832e-03,  2.0126e-03,\n              -7.3264e-02, -4.2503e-02,  1.5500e-02,  2.7994e-02, -5.9059e-03,\n               2.1367e-02, -1.0834e-01,  4.6871e-02, -6.7139e-02,  7.7237e-02,\n              -2.5472e-02, -2.2800e-02,  3.0831e-02,  3.8374e-02, -3.7469e-02,\n              -7.5019e-03,  4.2183e-02,  2.5031e-02, -2.1580e-02,  3.5115e-02,\n               1.2225e-02,  7.4168e-03,  8.9289e-03,  3.0093e-02, -2.7919e-02,\n               4.6971e-02, -1.8980e-02, -6.3353e-02, -2.9095e-02, -8.9587e-03,\n               1.0220e-02, -1.1965e-02,  1.2615e-02,  3.4074e-02,  6.9544e-03,\n              -1.0642e-04, -4.3051e-02, -2.9722e-02,  1.0208e-02,  4.9999e-02,\n               2.3440e-02,  4.9973e-03,  8.3499e-03, -3.1556e-02, -3.9343e-02,\n               1.2700e-03,  6.3672e-02,  1.6417e-02, -6.8132e-02],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9185, 0.9555, 0.9175, 0.9099, 0.9292, 0.9149, 0.9263, 0.9206, 0.9093,\n              0.9133, 0.9189, 0.9122, 0.9501, 0.9197, 0.9157, 0.9272, 0.9196, 0.9156,\n              0.9228, 0.9101, 0.9142, 0.9312, 0.9395, 0.9182, 0.9225, 0.9153, 0.9086,\n              0.9156, 0.9190, 0.9306, 0.9173, 0.9173, 0.9089, 0.9135, 0.9173, 0.9103,\n              0.9271, 0.9080, 0.9166, 0.9179, 0.9179, 0.9197, 0.9395, 0.9146, 0.9103,\n              0.9176, 0.9138, 0.9150, 0.9149, 0.9128, 0.9211, 0.9162, 0.9155, 0.9167,\n              0.9223, 0.9177, 0.9175, 0.9305, 0.9177, 0.9156, 0.9230, 0.9298, 0.9149,\n              0.9238], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-3.8217e-02, -1.1365e-02, -8.5676e-04],\n                [-1.5219e-02, -1.8263e-02, -3.4579e-02],\n                [-1.9290e-02,  7.3581e-03,  2.6293e-03]],\n      \n               [[ 1.1130e-02, -2.3591e-02,  1.6848e-02],\n                [ 1.7336e-03, -2.3604e-02, -3.2413e-02],\n                [ 8.7138e-03,  4.4772e-03,  1.8518e-02]],\n      \n               [[-2.0856e-02,  2.8718e-02, -2.0413e-02],\n                [-7.5468e-03, -3.9755e-02, -1.6967e-02],\n                [ 1.2068e-02,  1.1960e-02,  3.1598e-02]],\n      \n               ...,\n      \n               [[-4.0527e-03,  7.6360e-03,  2.4183e-02],\n                [ 3.7278e-02,  4.4425e-03, -3.7899e-02],\n                [ 2.5216e-02, -4.0757e-02,  3.2808e-02]],\n      \n               [[ 5.7587e-03,  4.9322e-03, -3.9830e-02],\n                [ 4.0877e-02,  3.5776e-02,  3.5737e-02],\n                [ 1.4737e-05, -3.1577e-02,  1.1218e-02]],\n      \n               [[-4.0153e-02, -2.4108e-03, -3.9694e-02],\n                [-3.9645e-02, -1.4064e-02, -1.4472e-02],\n                [-2.2554e-02,  1.6332e-02,  8.1037e-03]]],\n      \n      \n              [[[ 5.0999e-03,  2.1278e-02, -4.1693e-03],\n                [ 1.2671e-02,  3.4097e-02,  3.1309e-03],\n                [ 3.6912e-03, -3.3968e-02, -3.1760e-02]],\n      \n               [[ 7.1249e-03,  3.8281e-02,  3.7442e-02],\n                [-3.8885e-02,  4.3688e-03,  2.2075e-02],\n                [-2.5030e-02,  1.4362e-02, -1.2570e-02]],\n      \n               [[-1.7353e-02, -3.3271e-02,  3.8816e-02],\n                [ 2.0979e-02, -2.6876e-02,  1.9355e-02],\n                [ 2.5117e-02, -3.1754e-02,  1.8940e-02]],\n      \n               ...,\n      \n               [[ 6.6055e-03, -4.0079e-02,  6.9515e-03],\n                [ 3.1696e-02, -2.3654e-03, -2.4889e-02],\n                [ 3.7927e-02, -5.2540e-03,  1.9656e-02]],\n      \n               [[-2.1857e-02,  4.0508e-02, -2.1035e-02],\n                [ 2.8787e-02,  3.6959e-02,  2.6513e-02],\n                [ 2.5083e-02,  2.5114e-02, -3.3269e-03]],\n      \n               [[ 6.1943e-03, -2.4968e-03,  3.4330e-02],\n                [ 3.5311e-03,  2.6571e-02, -1.0232e-02],\n                [-3.5710e-02, -5.7450e-03, -3.8261e-03]]],\n      \n      \n              [[[ 3.5771e-02,  2.8624e-03,  4.3607e-03],\n                [-3.1072e-02, -4.1904e-03,  3.2670e-02],\n                [-2.7838e-02, -3.6516e-02,  2.3557e-02]],\n      \n               [[-1.5450e-02, -3.8966e-02,  3.4615e-02],\n                [ 4.1445e-02,  1.2385e-02,  2.7901e-02],\n                [-1.4462e-02,  1.7560e-03,  3.5910e-02]],\n      \n               [[ 2.1384e-02,  1.4434e-02,  1.6693e-02],\n                [-2.9864e-02, -2.7332e-02,  6.0932e-03],\n                [ 1.6403e-02,  3.2948e-02, -3.6306e-02]],\n      \n               ...,\n      \n               [[ 1.3314e-02, -2.2005e-02,  3.1824e-02],\n                [-3.7574e-02, -1.5075e-02, -2.1837e-02],\n                [ 3.4633e-02,  2.1208e-02, -2.3837e-02]],\n      \n               [[-3.7480e-02, -7.3681e-03,  1.1989e-02],\n                [ 2.9295e-02, -1.1859e-02, -7.8710e-03],\n                [ 2.1167e-02,  1.7383e-02,  2.9068e-02]],\n      \n               [[ 4.0694e-03,  5.7923e-03,  2.9696e-02],\n                [ 2.2376e-02,  1.1526e-02,  1.1499e-02],\n                [-3.9526e-02, -3.4178e-02,  3.6024e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[ 3.5406e-02, -3.6944e-02, -2.1492e-02],\n                [ 1.6963e-02,  1.3886e-02,  2.0948e-02],\n                [-5.6573e-03,  2.9502e-02,  2.4237e-02]],\n      \n               [[-1.4824e-02,  2.3824e-02, -4.0666e-02],\n                [-1.0615e-02,  1.6505e-02,  2.9839e-02],\n                [ 4.8207e-03,  1.8459e-02,  2.5115e-02]],\n      \n               [[-1.3851e-02,  3.0927e-03,  2.0627e-02],\n                [ 2.3670e-03,  2.9751e-02, -9.1114e-03],\n                [-2.6742e-02, -1.8400e-02, -2.9843e-02]],\n      \n               ...,\n      \n               [[-3.1675e-02,  2.5610e-02,  1.8805e-02],\n                [-3.7717e-02,  2.7700e-02,  2.0968e-02],\n                [-2.3002e-02,  2.0755e-02,  3.0752e-03]],\n      \n               [[-4.1589e-02, -3.0772e-02,  1.0945e-02],\n                [ 9.4414e-03,  3.3529e-02,  1.5759e-02],\n                [ 3.3802e-02, -2.7641e-02, -3.2059e-02]],\n      \n               [[-2.3227e-02,  1.5449e-02,  2.2895e-02],\n                [-3.0689e-03,  2.0830e-02,  7.5497e-03],\n                [-1.9602e-02,  2.1064e-02, -2.0531e-02]]],\n      \n      \n              [[[ 2.4588e-02,  8.5825e-03, -3.8026e-02],\n                [ 3.0012e-04, -1.1505e-02,  1.7085e-02],\n                [-3.4376e-02,  4.7622e-03, -3.8017e-02]],\n      \n               [[-1.9384e-02,  4.0487e-02, -4.1385e-03],\n                [ 2.3779e-02,  7.5177e-03,  1.0554e-02],\n                [ 3.8541e-02, -1.8225e-02, -1.2325e-02]],\n      \n               [[ 1.1990e-02,  2.5669e-02, -1.4706e-02],\n                [ 1.1528e-02, -4.9557e-03,  3.5623e-02],\n                [ 1.8146e-02, -1.2692e-02, -5.8777e-03]],\n      \n               ...,\n      \n               [[ 3.4728e-02,  3.5910e-02, -2.3969e-02],\n                [ 5.6539e-03, -7.5766e-03,  3.8027e-02],\n                [-2.1399e-02, -2.5973e-02, -4.1073e-02]],\n      \n               [[ 5.8760e-06,  3.7691e-02,  9.2611e-03],\n                [-2.0388e-02, -5.8805e-04, -3.4807e-02],\n                [ 1.9984e-02,  1.8509e-02,  2.2375e-02]],\n      \n               [[-3.0595e-02, -9.3895e-03, -1.6165e-02],\n                [-4.7983e-03,  2.3349e-02,  2.6088e-02],\n                [ 8.1516e-03, -2.6628e-02, -2.0963e-02]]],\n      \n      \n              [[[ 1.9607e-02,  6.6655e-03, -3.0880e-02],\n                [ 2.8219e-02, -3.2752e-02,  8.4903e-03],\n                [-2.2051e-02, -3.2632e-02, -3.7820e-02]],\n      \n               [[ 3.0769e-02,  1.9350e-02, -1.7132e-02],\n                [-3.8156e-02,  2.0008e-02,  1.1587e-02],\n                [ 2.9115e-02,  2.3698e-02, -3.7677e-02]],\n      \n               [[ 2.7044e-02, -8.1781e-03, -2.0277e-02],\n                [-2.2336e-02, -3.2735e-02, -5.2977e-03],\n                [-3.1414e-02, -1.6915e-02,  6.0458e-03]],\n      \n               ...,\n      \n               [[-2.9230e-02, -1.8644e-02,  3.0486e-02],\n                [-5.7315e-03,  2.8077e-03, -1.9811e-02],\n                [ 6.4166e-03,  1.4240e-02,  2.5954e-02]],\n      \n               [[ 1.0932e-02,  2.3045e-02, -1.1090e-02],\n                [-1.0068e-02,  7.7736e-03,  2.2373e-02],\n                [ 2.2705e-02,  1.7550e-02, -9.1880e-03]],\n      \n               [[-3.3716e-02,  2.2904e-02, -4.1036e-02],\n                [ 2.9729e-02, -2.9382e-02,  3.8470e-02],\n                [ 1.2961e-02,  2.4200e-02, -3.6023e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-0.0406,  0.0003,  0.0005, -0.0137, -0.0168, -0.0186,  0.0157, -0.0256,\n              -0.0267, -0.0087,  0.0203, -0.0135,  0.0081,  0.0004, -0.0073,  0.0161,\n              -0.0012, -0.0081, -0.0438,  0.0241, -0.0328, -0.0093, -0.0198, -0.0174,\n               0.0233, -0.0193, -0.0006,  0.0232,  0.0392,  0.0019,  0.0028,  0.0202,\n               0.0036,  0.0385, -0.0250, -0.0128, -0.0119, -0.0092,  0.0243, -0.0099,\n              -0.0019, -0.0118,  0.0217,  0.0379,  0.0116,  0.0068,  0.0069, -0.0002,\n               0.0128, -0.0088, -0.0166,  0.0023, -0.0025,  0.0052, -0.0236, -0.0532,\n              -0.0111,  0.0210,  0.0025,  0.0259, -0.0036,  0.0092,  0.0115,  0.0158],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9112, 0.9069, 0.9134, 0.9123, 0.9086, 0.9104, 0.9110, 0.9108, 0.9117,\n              0.9072, 0.9102, 0.9092, 0.9200, 0.9093, 0.9080, 0.9152, 0.9132, 0.9073,\n              0.9126, 0.9133, 0.9178, 0.9103, 0.9105, 0.9092, 0.9087, 0.9102, 0.9080,\n              0.9129, 0.9113, 0.9101, 0.9063, 0.9108, 0.9077, 0.9123, 0.9084, 0.9110,\n              0.9081, 0.9113, 0.9140, 0.9100, 0.9082, 0.9068, 0.9079, 0.9182, 0.9092,\n              0.9088, 0.9112, 0.9070, 0.9114, 0.9078, 0.9098, 0.9118, 0.9087, 0.9093,\n              0.9101, 0.9175, 0.9091, 0.9124, 0.9078, 0.9095, 0.9103, 0.9082, 0.9081,\n              0.9106], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n  )\n)", "parameters": [["0.conv1.weight", [64, 64, 3, 3]], ["0.bn1.weight", [64]], ["0.bn1.bias", [64]], ["0.conv2.weight", [64, 64, 3, 3]], ["0.bn2.weight", [64]], ["0.bn2.bias", [64]]], "output_shape": [[512, 64, 6, 6]], "num_parameters": [36864, 64, 64, 36864, 64, 64]}, {"name": "layer2", "id": 140209539495488, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-0.0300,  0.0317, -0.0388],\n                [ 0.0063,  0.0263, -0.0050],\n                [-0.0229,  0.0282,  0.0008]],\n      \n               [[ 0.0356, -0.0392, -0.0231],\n                [-0.0189, -0.0330,  0.0366],\n                [-0.0304, -0.0410, -0.0118]],\n      \n               [[-0.0200,  0.0290,  0.0205],\n                [ 0.0150, -0.0243, -0.0113],\n                [-0.0114,  0.0299, -0.0090]],\n      \n               ...,\n      \n               [[-0.0200, -0.0151,  0.0271],\n                [ 0.0150,  0.0084,  0.0237],\n                [ 0.0023, -0.0095, -0.0293]],\n      \n               [[-0.0365,  0.0338,  0.0002],\n                [ 0.0238,  0.0207,  0.0284],\n                [-0.0380,  0.0039,  0.0104]],\n      \n               [[ 0.0089, -0.0183,  0.0230],\n                [ 0.0317,  0.0393,  0.0415],\n                [ 0.0218,  0.0236, -0.0180]]],\n      \n      \n              [[[-0.0403,  0.0146, -0.0253],\n                [ 0.0063, -0.0343,  0.0107],\n                [-0.0126, -0.0173, -0.0123]],\n      \n               [[ 0.0395, -0.0370,  0.0386],\n                [ 0.0050, -0.0364,  0.0240],\n                [ 0.0156,  0.0292,  0.0145]],\n      \n               [[ 0.0082,  0.0013, -0.0172],\n                [ 0.0128,  0.0314,  0.0244],\n                [ 0.0255,  0.0363,  0.0011]],\n      \n               ...,\n      \n               [[ 0.0002,  0.0022,  0.0328],\n                [ 0.0304,  0.0112, -0.0201],\n                [-0.0399, -0.0022,  0.0318]],\n      \n               [[-0.0403,  0.0337,  0.0409],\n                [ 0.0222, -0.0013,  0.0217],\n                [ 0.0350, -0.0367,  0.0348]],\n      \n               [[ 0.0213, -0.0378, -0.0361],\n                [ 0.0031,  0.0283, -0.0391],\n                [ 0.0369, -0.0348, -0.0145]]],\n      \n      \n              [[[-0.0064,  0.0145, -0.0074],\n                [-0.0408,  0.0343,  0.0084],\n                [ 0.0193,  0.0119,  0.0212]],\n      \n               [[-0.0050, -0.0416, -0.0381],\n                [ 0.0107, -0.0008, -0.0298],\n                [ 0.0167, -0.0152,  0.0236]],\n      \n               [[-0.0298, -0.0091, -0.0403],\n                [-0.0209, -0.0241,  0.0062],\n                [ 0.0166, -0.0109, -0.0332]],\n      \n               ...,\n      \n               [[ 0.0360,  0.0358,  0.0098],\n                [-0.0028,  0.0307, -0.0347],\n                [ 0.0197,  0.0360, -0.0145]],\n      \n               [[-0.0341,  0.0303, -0.0009],\n                [ 0.0367,  0.0242, -0.0287],\n                [-0.0341, -0.0322, -0.0023]],\n      \n               [[ 0.0368,  0.0235, -0.0070],\n                [-0.0413,  0.0158,  0.0121],\n                [ 0.0326, -0.0268,  0.0292]]],\n      \n      \n              ...,\n      \n      \n              [[[-0.0074,  0.0037, -0.0159],\n                [ 0.0297,  0.0292, -0.0070],\n                [ 0.0383, -0.0130, -0.0235]],\n      \n               [[-0.0262,  0.0103, -0.0047],\n                [ 0.0180, -0.0131, -0.0256],\n                [ 0.0104, -0.0201, -0.0190]],\n      \n               [[-0.0031, -0.0185, -0.0410],\n                [ 0.0195,  0.0289,  0.0140],\n                [ 0.0317,  0.0167, -0.0212]],\n      \n               ...,\n      \n               [[ 0.0029, -0.0113, -0.0016],\n                [ 0.0281, -0.0066,  0.0413],\n                [-0.0259, -0.0372, -0.0071]],\n      \n               [[-0.0211,  0.0221,  0.0354],\n                [ 0.0412,  0.0254,  0.0211],\n                [-0.0001, -0.0247, -0.0312]],\n      \n               [[-0.0305, -0.0367,  0.0271],\n                [-0.0150, -0.0324,  0.0069],\n                [ 0.0275,  0.0245,  0.0133]]],\n      \n      \n              [[[ 0.0147, -0.0305,  0.0103],\n                [-0.0406, -0.0048,  0.0190],\n                [ 0.0166, -0.0171, -0.0340]],\n      \n               [[ 0.0186,  0.0381, -0.0184],\n                [-0.0039,  0.0096, -0.0158],\n                [-0.0224, -0.0277,  0.0223]],\n      \n               [[ 0.0314,  0.0307, -0.0072],\n                [-0.0256, -0.0400, -0.0404],\n                [ 0.0068, -0.0096, -0.0196]],\n      \n               ...,\n      \n               [[ 0.0133, -0.0221,  0.0186],\n                [-0.0408,  0.0252,  0.0110],\n                [-0.0189, -0.0191,  0.0269]],\n      \n               [[-0.0060,  0.0078, -0.0372],\n                [ 0.0171,  0.0323, -0.0143],\n                [ 0.0236, -0.0215,  0.0214]],\n      \n               [[ 0.0414, -0.0197,  0.0118],\n                [-0.0332,  0.0405, -0.0024],\n                [-0.0339, -0.0311, -0.0183]]],\n      \n      \n              [[[-0.0098,  0.0201,  0.0370],\n                [-0.0285, -0.0272, -0.0309],\n                [-0.0226,  0.0207,  0.0049]],\n      \n               [[ 0.0158, -0.0045, -0.0059],\n                [ 0.0204,  0.0355, -0.0239],\n                [ 0.0010, -0.0337,  0.0295]],\n      \n               [[-0.0260, -0.0331, -0.0248],\n                [-0.0122,  0.0057,  0.0229],\n                [-0.0371, -0.0326,  0.0242]],\n      \n               ...,\n      \n               [[-0.0345, -0.0229,  0.0062],\n                [-0.0331, -0.0059, -0.0032],\n                [-0.0374,  0.0231,  0.0088]],\n      \n               [[-0.0366, -0.0002,  0.0118],\n                [ 0.0187,  0.0028, -0.0225],\n                [ 0.0154,  0.0296,  0.0368]],\n      \n               [[-0.0414, -0.0133,  0.0352],\n                [ 0.0150,  0.0133,  0.0244],\n                [ 0.0254, -0.0205, -0.0095]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0322,  0.0230, -0.0023, -0.0276, -0.0650,  0.0085, -0.0605,  0.0251,\n              -0.0078,  0.0167,  0.0029, -0.0752, -0.0072, -0.0794,  0.0112, -0.0637,\n              -0.0044, -0.0022, -0.1018,  0.0051,  0.0617,  0.0542, -0.0275, -0.0587,\n               0.0195,  0.0450,  0.0773,  0.0458,  0.0368, -0.0342, -0.0162,  0.0551,\n              -0.0274, -0.0293, -0.0652, -0.0516, -0.0366,  0.0026, -0.0081, -0.0112,\n               0.0037,  0.0547, -0.0092, -0.0988, -0.0183, -0.0535, -0.0275,  0.0067,\n               0.0520,  0.1008,  0.0453, -0.0013, -0.0346, -0.0464, -0.0338, -0.0350,\n               0.0023,  0.0703,  0.0302,  0.0324, -0.0774,  0.0586,  0.0044,  0.0581,\n              -0.0174, -0.0065, -0.0410, -0.0727, -0.0615, -0.0212, -0.0573, -0.0041,\n               0.0260,  0.0143,  0.0789, -0.0523, -0.0490,  0.0080,  0.0180,  0.0081,\n               0.0502,  0.0435,  0.0126,  0.0140, -0.0285,  0.0138, -0.0111, -0.0007,\n              -0.0463, -0.0763,  0.0721, -0.0657,  0.0742, -0.0013,  0.0401, -0.0490,\n               0.0208,  0.0452,  0.0563, -0.0607,  0.0722, -0.0233,  0.0064, -0.0627,\n               0.0437,  0.0249,  0.0397, -0.0459,  0.0245,  0.0709,  0.0282,  0.0320,\n               0.1026, -0.0110,  0.0469, -0.0503,  0.0174, -0.0822,  0.0431,  0.0566,\n              -0.0159,  0.0346, -0.0219,  0.0772, -0.0741,  0.0391,  0.0227, -0.0463],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9348, 0.9315, 0.9252, 0.9277, 0.9328, 0.9298, 0.9315, 0.9233, 0.9503,\n              0.9327, 0.9359, 0.9282, 0.9234, 0.9402, 0.9222, 0.9384, 0.9296, 0.9158,\n              0.9457, 0.9524, 0.9229, 0.9367, 0.9270, 0.9288, 0.9192, 0.9533, 0.9293,\n              0.9242, 0.9384, 0.9307, 0.9243, 0.9417, 0.9291, 0.9275, 0.9389, 0.9245,\n              0.9420, 0.9239, 0.9277, 0.9280, 0.9302, 0.9240, 0.9399, 0.9397, 0.9403,\n              0.9855, 0.9302, 0.9271, 0.9287, 0.9386, 0.9276, 0.9310, 0.9290, 0.9461,\n              0.9430, 0.9431, 0.9168, 0.9400, 0.9294, 0.9254, 0.9549, 0.9355, 0.9315,\n              0.9264, 0.9291, 0.9277, 0.9290, 0.9251, 0.9267, 0.9348, 0.9463, 0.9460,\n              0.9287, 0.9284, 0.9540, 0.9518, 0.9247, 0.9389, 0.9510, 0.9471, 0.9342,\n              0.9375, 0.9228, 0.9288, 0.9243, 0.9292, 0.9276, 0.9183, 0.9398, 0.9381,\n              0.9417, 0.9311, 0.9345, 0.9249, 0.9420, 0.9272, 0.9363, 0.9293, 0.9233,\n              0.9265, 0.9656, 0.9364, 0.9211, 0.9548, 0.9353, 0.9292, 0.9206, 0.9461,\n              0.9261, 0.9425, 0.9270, 0.9359, 0.9452, 0.9301, 0.9226, 0.9233, 0.9250,\n              0.9446, 0.9371, 0.9526, 0.9340, 0.9291, 0.9300, 0.9407, 0.9275, 0.9344,\n              0.9342, 0.9558], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-0.0068,  0.0142, -0.0139],\n                [-0.0039,  0.0159, -0.0067],\n                [ 0.0104,  0.0123,  0.0076]],\n      \n               [[-0.0045, -0.0188,  0.0154],\n                [ 0.0156,  0.0143,  0.0248],\n                [-0.0064, -0.0095,  0.0022]],\n      \n               [[ 0.0100, -0.0252,  0.0162],\n                [ 0.0181, -0.0105,  0.0249],\n                [-0.0137, -0.0085, -0.0125]],\n      \n               ...,\n      \n               [[ 0.0005,  0.0251, -0.0090],\n                [-0.0098, -0.0033,  0.0256],\n                [ 0.0152, -0.0178, -0.0057]],\n      \n               [[-0.0002, -0.0119, -0.0201],\n                [ 0.0138,  0.0011, -0.0145],\n                [-0.0098, -0.0138, -0.0081]],\n      \n               [[-0.0150, -0.0225, -0.0292],\n                [-0.0151,  0.0286, -0.0113],\n                [-0.0202,  0.0094, -0.0084]]],\n      \n      \n              [[[-0.0152,  0.0220, -0.0063],\n                [-0.0273,  0.0021,  0.0081],\n                [ 0.0026, -0.0105, -0.0246]],\n      \n               [[ 0.0028,  0.0162, -0.0021],\n                [-0.0152,  0.0089, -0.0009],\n                [-0.0040,  0.0073,  0.0147]],\n      \n               [[ 0.0256,  0.0019, -0.0181],\n                [-0.0227, -0.0091, -0.0166],\n                [-0.0199,  0.0183,  0.0274]],\n      \n               ...,\n      \n               [[-0.0273,  0.0116, -0.0051],\n                [ 0.0253, -0.0161,  0.0105],\n                [-0.0221, -0.0231, -0.0211]],\n      \n               [[-0.0240, -0.0262,  0.0076],\n                [-0.0135,  0.0289,  0.0152],\n                [-0.0277,  0.0065,  0.0218]],\n      \n               [[ 0.0185,  0.0223,  0.0090],\n                [ 0.0015, -0.0040,  0.0014],\n                [-0.0176, -0.0195,  0.0295]]],\n      \n      \n              [[[-0.0256,  0.0200, -0.0119],\n                [-0.0125,  0.0113,  0.0091],\n                [-0.0171,  0.0160,  0.0179]],\n      \n               [[-0.0149, -0.0162, -0.0035],\n                [-0.0104,  0.0084,  0.0026],\n                [-0.0183,  0.0031,  0.0181]],\n      \n               [[-0.0060, -0.0101, -0.0144],\n                [-0.0115,  0.0057, -0.0052],\n                [-0.0043,  0.0003, -0.0277]],\n      \n               ...,\n      \n               [[-0.0235, -0.0053,  0.0095],\n                [ 0.0275,  0.0247,  0.0240],\n                [ 0.0153, -0.0017,  0.0203]],\n      \n               [[-0.0238,  0.0088, -0.0106],\n                [ 0.0155,  0.0219,  0.0113],\n                [ 0.0069, -0.0256,  0.0059]],\n      \n               [[ 0.0022, -0.0232,  0.0184],\n                [ 0.0287,  0.0226,  0.0152],\n                [ 0.0229, -0.0102, -0.0006]]],\n      \n      \n              ...,\n      \n      \n              [[[-0.0076,  0.0270,  0.0114],\n                [-0.0020,  0.0289,  0.0114],\n                [-0.0181,  0.0214,  0.0142]],\n      \n               [[-0.0097, -0.0135,  0.0263],\n                [ 0.0052, -0.0049,  0.0284],\n                [-0.0197,  0.0249,  0.0032]],\n      \n               [[-0.0038,  0.0171, -0.0017],\n                [-0.0137,  0.0007, -0.0177],\n                [-0.0294,  0.0180, -0.0081]],\n      \n               ...,\n      \n               [[-0.0204,  0.0037,  0.0231],\n                [-0.0047, -0.0222,  0.0199],\n                [ 0.0209,  0.0072,  0.0079]],\n      \n               [[-0.0142,  0.0279,  0.0084],\n                [-0.0119,  0.0275, -0.0282],\n                [ 0.0047,  0.0011,  0.0096]],\n      \n               [[-0.0147,  0.0010,  0.0017],\n                [-0.0198,  0.0229, -0.0046],\n                [-0.0223, -0.0210,  0.0278]]],\n      \n      \n              [[[-0.0069,  0.0084,  0.0012],\n                [ 0.0219,  0.0094, -0.0156],\n                [ 0.0124, -0.0190, -0.0273]],\n      \n               [[ 0.0103, -0.0006,  0.0285],\n                [ 0.0106, -0.0102, -0.0021],\n                [-0.0243,  0.0075,  0.0058]],\n      \n               [[-0.0116, -0.0084,  0.0034],\n                [ 0.0222,  0.0180,  0.0218],\n                [ 0.0256,  0.0124, -0.0220]],\n      \n               ...,\n      \n               [[ 0.0062,  0.0236,  0.0011],\n                [-0.0045,  0.0155,  0.0221],\n                [-0.0111,  0.0033,  0.0180]],\n      \n               [[-0.0114, -0.0254, -0.0285],\n                [ 0.0289, -0.0285, -0.0240],\n                [ 0.0043, -0.0138, -0.0066]],\n      \n               [[ 0.0276,  0.0160, -0.0012],\n                [ 0.0193, -0.0109,  0.0019],\n                [-0.0248,  0.0246,  0.0254]]],\n      \n      \n              [[[-0.0160,  0.0221, -0.0175],\n                [ 0.0111, -0.0233, -0.0096],\n                [-0.0189, -0.0119,  0.0267]],\n      \n               [[ 0.0150,  0.0234,  0.0078],\n                [-0.0169,  0.0277, -0.0244],\n                [ 0.0274, -0.0187, -0.0107]],\n      \n               [[ 0.0217, -0.0181,  0.0111],\n                [-0.0220, -0.0171,  0.0247],\n                [ 0.0181, -0.0058,  0.0201]],\n      \n               ...,\n      \n               [[ 0.0075,  0.0054, -0.0140],\n                [ 0.0061, -0.0153, -0.0103],\n                [ 0.0054, -0.0192,  0.0091]],\n      \n               [[ 0.0050, -0.0157,  0.0108],\n                [-0.0243, -0.0259,  0.0031],\n                [ 0.0051, -0.0167, -0.0123]],\n      \n               [[ 0.0240,  0.0031, -0.0059],\n                [-0.0174,  0.0156, -0.0239],\n                [-0.0223,  0.0218,  0.0051]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 1.1690e-02, -1.5897e-02,  2.4327e-03,  1.5820e-02,  1.3228e-02,\n               2.7015e-02, -3.9093e-03, -2.0977e-02,  9.4326e-03, -2.4114e-02,\n              -8.4030e-03, -5.3722e-03,  6.0826e-03,  1.0173e-02, -6.3866e-03,\n              -9.4348e-03,  2.9798e-02,  6.4080e-03,  1.6458e-02,  1.9541e-02,\n               8.6719e-04,  2.7989e-03,  2.2275e-02, -9.8524e-03,  1.4589e-02,\n               1.5293e-02,  1.2897e-03, -1.9537e-03,  9.3870e-03,  2.9501e-02,\n               3.7081e-02, -5.6135e-03,  1.6905e-03, -9.0308e-03, -3.0605e-02,\n              -2.2020e-04,  2.1412e-02, -2.3894e-02,  4.2796e-02, -9.3824e-03,\n               6.4407e-04, -1.5472e-02,  7.6047e-03, -9.1964e-03, -1.0253e-02,\n              -1.5307e-02,  1.6703e-02, -1.8940e-02, -7.4112e-03,  4.0918e-03,\n               1.0158e-02, -1.7421e-02,  1.2522e-02, -1.4262e-02, -3.1320e-03,\n               3.0950e-02,  6.5845e-03, -1.0782e-02,  6.5178e-03, -4.2659e-03,\n               2.3398e-02, -7.3964e-03, -4.3523e-03,  1.8523e-02,  1.6988e-02,\n              -3.0386e-03,  8.0007e-03, -5.1990e-03, -2.2886e-05, -1.5181e-02,\n               2.2849e-02,  1.8258e-02,  3.2061e-04, -5.3432e-03, -1.3200e-02,\n              -1.1963e-02,  1.0714e-02,  2.0021e-02,  2.2322e-02, -1.2664e-02,\n               5.0831e-03,  6.5962e-03, -1.5443e-02,  6.5974e-03, -1.0791e-02,\n               6.4044e-03, -6.3933e-03,  8.0897e-03, -4.5951e-03, -3.3445e-03,\n              -9.6905e-03, -2.3473e-02, -7.1105e-03, -5.4735e-03,  4.2404e-05,\n               6.5244e-03,  1.0721e-02, -1.7845e-02,  1.7479e-03,  1.7049e-02,\n               8.7305e-03,  5.6968e-03,  3.1059e-03, -5.1597e-03,  8.7420e-03,\n               2.1462e-03, -1.6742e-02,  2.2476e-02, -2.7127e-03, -4.2374e-02,\n               7.1145e-03, -8.2849e-03,  2.0802e-02,  1.5616e-02, -1.5730e-02,\n              -1.1084e-02, -2.5386e-03, -3.3119e-03,  1.8629e-02,  3.6050e-02,\n               2.2157e-02,  7.1966e-03, -7.7043e-03,  1.3268e-02,  1.6844e-02,\n              -1.5997e-03,  6.3527e-03,  1.1290e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9066, 0.9080, 0.9069, 0.9076, 0.9063, 0.9100, 0.9070, 0.9089, 0.9090,\n              0.9100, 0.9095, 0.9076, 0.9065, 0.9065, 0.9103, 0.9089, 0.9087, 0.9082,\n              0.9081, 0.9092, 0.9069, 0.9080, 0.9096, 0.9062, 0.9095, 0.9076, 0.9072,\n              0.9087, 0.9067, 0.9092, 0.9097, 0.9082, 0.9097, 0.9067, 0.9274, 0.9081,\n              0.9083, 0.9084, 0.9080, 0.9075, 0.9095, 0.9073, 0.9069, 0.9074, 0.9059,\n              0.9074, 0.9074, 0.9079, 0.9058, 0.9088, 0.9096, 0.9055, 0.9087, 0.9061,\n              0.9087, 0.9077, 0.9074, 0.9071, 0.9091, 0.9069, 0.9077, 0.9094, 0.9067,\n              0.9082, 0.9060, 0.9079, 0.9084, 0.9084, 0.9071, 0.9082, 0.9078, 0.9056,\n              0.9091, 0.9079, 0.9117, 0.9074, 0.9067, 0.9086, 0.9100, 0.9087, 0.9087,\n              0.9098, 0.9111, 0.9066, 0.9082, 0.9077, 0.9062, 0.9079, 0.9097, 0.9071,\n              0.9062, 0.9075, 0.9098, 0.9073, 0.9084, 0.9086, 0.9059, 0.9100, 0.9078,\n              0.9097, 0.9087, 0.9067, 0.9060, 0.9084, 0.9073, 0.9072, 0.9091, 0.9079,\n              0.9064, 0.9096, 0.9087, 0.9080, 0.9082, 0.9088, 0.9084, 0.9081, 0.9099,\n              0.9093, 0.9110, 0.9091, 0.9084, 0.9095, 0.9081, 0.9092, 0.9088, 0.9069,\n              0.9060, 0.9097], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[-0.0606]],\n        \n                 [[-0.0566]],\n        \n                 [[ 0.0408]],\n        \n                 ...,\n        \n                 [[ 0.0566]],\n        \n                 [[-0.0647]],\n        \n                 [[ 0.0341]]],\n        \n        \n                [[[-0.0643]],\n        \n                 [[-0.1051]],\n        \n                 [[-0.0897]],\n        \n                 ...,\n        \n                 [[ 0.0938]],\n        \n                 [[-0.0337]],\n        \n                 [[-0.0091]]],\n        \n        \n                [[[-0.0796]],\n        \n                 [[ 0.0399]],\n        \n                 [[ 0.0104]],\n        \n                 ...,\n        \n                 [[-0.0628]],\n        \n                 [[ 0.0889]],\n        \n                 [[-0.0807]]],\n        \n        \n                ...,\n        \n        \n                [[[ 0.0170]],\n        \n                 [[-0.0255]],\n        \n                 [[ 0.0428]],\n        \n                 ...,\n        \n                 [[ 0.0218]],\n        \n                 [[-0.0914]],\n        \n                 [[-0.0188]]],\n        \n        \n                [[[ 0.0605]],\n        \n                 [[ 0.1101]],\n        \n                 [[-0.0019]],\n        \n                 ...,\n        \n                 [[-0.0780]],\n        \n                 [[-0.0425]],\n        \n                 [[-0.0201]]],\n        \n        \n                [[[-0.0215]],\n        \n                 [[ 0.0161]],\n        \n                 [[-0.1240]],\n        \n                 ...,\n        \n                 [[-0.1203]],\n        \n                 [[-0.1169]],\n        \n                 [[ 0.0137]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-2.9970e-03,  7.1130e-02,  3.9402e-03, -9.1127e-02, -2.1613e-02,\n                 1.0169e-01,  2.6819e-02,  4.1272e-02,  7.3464e-03, -1.1864e-02,\n                -5.0355e-02,  3.6434e-02,  5.7449e-02,  5.2996e-02, -5.2345e-02,\n                -5.8037e-02, -7.5344e-02,  4.2355e-02, -8.7114e-02,  4.2252e-02,\n                 1.2040e-02, -2.7225e-02,  4.4984e-02, -6.5354e-02, -4.2592e-02,\n                 2.8517e-02,  2.8793e-02,  3.4559e-03, -1.0274e-02, -2.4933e-02,\n                 3.3918e-02,  2.1521e-02, -4.0923e-02,  4.4725e-02,  8.5451e-02,\n                 1.7245e-02,  2.5052e-02, -2.0979e-02, -5.6010e-02, -5.1412e-05,\n                 1.6381e-02,  9.3989e-02, -5.6622e-02,  1.2629e-02,  2.5431e-02,\n                -1.0977e-01, -2.9163e-02,  1.3615e-01, -3.7803e-02, -5.3844e-03,\n                 5.7616e-02,  2.1995e-02, -5.1311e-02,  8.2502e-02,  8.6472e-02,\n                -2.8599e-02,  1.2656e-01,  4.7443e-02,  1.9090e-02, -1.5070e-01,\n                -4.6989e-02,  2.6142e-03,  7.0545e-04, -8.4871e-02, -3.5912e-02,\n                 2.9279e-02,  7.4537e-02, -3.3499e-03, -2.5564e-02,  1.2223e-02,\n                -7.3748e-02, -2.0591e-02, -2.6012e-02,  1.4833e-02,  6.1760e-02,\n                -7.2584e-02,  2.5527e-02, -2.0576e-02, -7.1910e-03, -3.0184e-02,\n                 4.5889e-04, -3.6448e-02, -9.7862e-02, -4.2674e-02,  8.8548e-02,\n                -9.8260e-03, -2.4150e-02, -1.0416e-02,  7.3559e-03, -6.3506e-02,\n                 4.8866e-03,  3.6995e-03,  5.4894e-02,  1.3721e-02,  3.8575e-02,\n                 1.6751e-02,  4.9137e-02,  1.4849e-03, -5.2666e-02,  8.4735e-02,\n                 3.3151e-02, -5.7646e-03, -2.8374e-02,  7.2760e-03,  1.1093e-01,\n                -2.5842e-02, -5.2574e-03,  1.0830e-01,  6.1210e-02, -3.9724e-02,\n                 1.4859e-02,  8.5888e-03,  8.9658e-02,  3.9201e-02, -1.2906e-02,\n                -5.9373e-03, -3.4082e-02, -5.9786e-02, -3.8584e-03, -9.3184e-03,\n                -2.7203e-02, -3.6253e-03,  1.0275e-02, -1.3164e-02,  4.4280e-02,\n                 1.2890e-02, -2.6529e-02, -2.9927e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9318, 0.9484, 0.9283, 0.9405, 0.9419, 0.9356, 0.9313, 0.9429, 0.9333,\n                0.9317, 0.9195, 0.9302, 0.9305, 0.9271, 0.9356, 0.9371, 0.9471, 0.9286,\n                0.9342, 0.9239, 0.9245, 0.9375, 0.9509, 0.9396, 0.9324, 0.9349, 0.9375,\n                0.9326, 0.9314, 0.9460, 0.9224, 0.9349, 0.9320, 0.9385, 0.9298, 0.9475,\n                0.9345, 0.9309, 0.9221, 0.9315, 0.9194, 0.9511, 0.9223, 0.9252, 0.9278,\n                0.9359, 0.9353, 0.9599, 0.9299, 0.9216, 0.9410, 0.9185, 0.9427, 0.9643,\n                0.9432, 0.9423, 0.9386, 0.9295, 0.9652, 0.9661, 0.9350, 0.9219, 0.9169,\n                0.9310, 0.9224, 0.9355, 0.9299, 0.9252, 0.9224, 0.9412, 0.9370, 0.9459,\n                0.9378, 0.9266, 0.9231, 0.9408, 0.9252, 0.9412, 0.9225, 0.9196, 0.9417,\n                0.9294, 0.9403, 0.9237, 0.9592, 0.9279, 0.9260, 0.9226, 0.9459, 0.9423,\n                0.9213, 0.9160, 0.9362, 0.9195, 0.9498, 0.9200, 0.9289, 0.9383, 0.9257,\n                0.9502, 0.9194, 0.9339, 0.9297, 0.9289, 0.9334, 0.9372, 0.9196, 0.9524,\n                0.9497, 0.9269, 0.9261, 0.9343, 0.9510, 0.9309, 0.9201, 0.9264, 0.9544,\n                0.9350, 0.9227, 0.9249, 0.9355, 0.9305, 0.9195, 0.9255, 0.9279, 0.9340,\n                0.9366, 0.9396], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [128, 64, 3, 3]], ["0.bn1.weight", [128]], ["0.bn1.bias", [128]], ["0.conv2.weight", [128, 128, 3, 3]], ["0.bn2.weight", [128]], ["0.bn2.bias", [128]], ["0.downsample.0.weight", [128, 64, 1, 1]], ["0.downsample.1.weight", [128]], ["0.downsample.1.bias", [128]]], "output_shape": [[512, 128, 3, 3]], "num_parameters": [73728, 128, 128, 147456, 128, 128, 8192, 128, 128]}, {"name": "layer3", "id": 140209539495872, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 1.5516e-02, -1.5253e-02, -2.4216e-02],\n                [ 4.9724e-03, -1.2779e-02,  1.3861e-02],\n                [ 4.8603e-03, -9.2644e-03, -2.6342e-02]],\n      \n               [[ 2.2017e-02,  1.1259e-02,  2.1665e-02],\n                [-6.9530e-03,  1.9141e-02,  7.9870e-03],\n                [ 2.7968e-02, -1.1973e-02, -2.2145e-02]],\n      \n               [[ 2.3520e-02,  1.8710e-02, -1.1953e-02],\n                [ 2.2783e-02,  1.8119e-02, -2.4327e-02],\n                [ 1.0870e-02,  1.2512e-02,  1.4116e-02]],\n      \n               ...,\n      \n               [[-2.1865e-02,  2.0671e-02,  1.8170e-02],\n                [ 3.4299e-04, -2.7188e-02, -1.4589e-02],\n                [ 4.2781e-03, -2.5161e-02,  7.1905e-03]],\n      \n               [[ 9.4584e-03,  1.1084e-02, -1.4890e-02],\n                [ 2.4777e-02,  1.8236e-02,  2.8953e-02],\n                [-6.5377e-03, -1.5245e-02,  2.0388e-02]],\n      \n               [[ 1.7739e-02, -2.7598e-02,  5.8566e-03],\n                [-8.7893e-03,  2.6889e-02, -2.0804e-02],\n                [-7.7183e-03, -2.7988e-02,  2.6331e-02]]],\n      \n      \n              [[[-1.4248e-02, -3.9555e-03, -2.4668e-02],\n                [-1.2897e-02,  2.3507e-02, -3.5469e-03],\n                [-7.8937e-03, -2.9372e-02,  1.7812e-02]],\n      \n               [[ 3.3552e-04, -1.2111e-02, -2.4430e-02],\n                [ 4.6874e-03, -6.4020e-03,  2.9157e-02],\n                [-2.3972e-02, -6.8402e-03, -8.6443e-03]],\n      \n               [[-2.4330e-02, -2.7007e-02,  2.2979e-02],\n                [ 1.9406e-02,  1.5497e-02,  2.7361e-02],\n                [ 2.2260e-02,  1.6595e-02,  2.1909e-02]],\n      \n               ...,\n      \n               [[ 2.6822e-02,  5.9174e-03,  2.5158e-02],\n                [-1.7725e-02,  6.2665e-03,  9.7768e-03],\n                [-7.6644e-03,  1.3363e-02, -2.0711e-02]],\n      \n               [[ 1.1328e-02, -1.1775e-02, -2.5333e-02],\n                [ 1.9523e-02, -1.3897e-02,  4.8438e-03],\n                [ 1.3025e-02,  5.8317e-03, -2.5864e-02]],\n      \n               [[-2.5521e-02,  2.6327e-02,  8.7534e-03],\n                [ 2.1669e-02,  6.4497e-04, -1.5886e-02],\n                [-1.0621e-02,  1.3810e-02,  2.1064e-02]]],\n      \n      \n              [[[-2.2015e-02, -1.5042e-02,  1.9555e-03],\n                [-1.1928e-02,  2.3464e-03,  9.6425e-03],\n                [-1.8482e-02, -1.4389e-02,  1.1460e-02]],\n      \n               [[-8.7222e-03,  2.9956e-03,  2.8106e-02],\n                [-2.5924e-02, -2.8081e-02,  9.4448e-03],\n                [ 2.4646e-02,  2.8362e-02, -4.4104e-03]],\n      \n               [[ 1.3337e-02, -1.1493e-02,  1.5442e-02],\n                [ 2.7688e-02,  2.6468e-02, -1.5922e-02],\n                [-3.8177e-03,  2.1446e-02, -2.6189e-02]],\n      \n               ...,\n      \n               [[ 2.6577e-02,  2.9125e-02, -1.6628e-03],\n                [-2.0162e-02, -5.7988e-03,  2.0107e-02],\n                [-1.5446e-02, -1.7729e-03,  4.5445e-03]],\n      \n               [[-1.7232e-02,  5.2304e-04, -1.5798e-02],\n                [ 2.4784e-02, -1.4774e-02, -1.0628e-03],\n                [ 1.3871e-02, -1.4180e-02,  2.0142e-02]],\n      \n               [[-1.7995e-02, -9.1876e-03,  2.1869e-02],\n                [ 2.1557e-02, -1.9051e-02,  2.3783e-02],\n                [ 1.6307e-02,  2.6775e-03,  1.6496e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[ 1.8014e-02, -1.2595e-02,  9.3172e-03],\n                [-2.9029e-02,  6.1471e-03, -1.7873e-02],\n                [ 1.1906e-02,  6.0947e-03,  2.6362e-02]],\n      \n               [[-1.7696e-02,  3.6828e-03,  6.6090e-03],\n                [-1.5973e-02, -2.4523e-02,  1.8565e-02],\n                [-2.4993e-02, -1.9971e-02, -2.7424e-02]],\n      \n               [[-2.3002e-02, -1.5275e-02, -2.6019e-02],\n                [-7.2730e-03,  2.6539e-02,  1.0600e-02],\n                [-7.9111e-03, -3.0651e-03, -7.8228e-05]],\n      \n               ...,\n      \n               [[ 2.2197e-02,  1.7243e-02,  2.7745e-02],\n                [ 2.5578e-02, -2.6396e-02,  7.3923e-03],\n                [ 3.2477e-03,  6.8172e-03,  3.4549e-03]],\n      \n               [[-1.6949e-02,  1.2818e-02,  2.1930e-02],\n                [-1.3253e-02, -5.0919e-04,  2.0480e-02],\n                [-3.7806e-03, -1.1682e-02, -1.0663e-02]],\n      \n               [[-1.8690e-02, -1.7457e-02,  1.6779e-02],\n                [ 6.7711e-03,  8.3632e-03,  1.7983e-02],\n                [-1.0966e-02,  1.7863e-02, -1.2134e-02]]],\n      \n      \n              [[[-1.7476e-02,  1.4019e-02,  1.8349e-03],\n                [-1.5083e-02, -1.3255e-02,  2.5392e-02],\n                [-1.3064e-02,  2.0952e-02,  1.3658e-02]],\n      \n               [[-1.4084e-02, -2.0378e-02,  2.8215e-02],\n                [-2.3153e-02, -1.0623e-02,  3.9232e-03],\n                [-2.9407e-02, -1.2535e-02,  1.6651e-02]],\n      \n               [[ 1.2784e-02, -1.8456e-02,  2.5718e-02],\n                [ 1.0428e-02,  4.6866e-03, -4.9430e-03],\n                [-7.6001e-03,  1.8847e-02, -1.3581e-02]],\n      \n               ...,\n      \n               [[-1.0974e-02, -2.2810e-03, -3.7875e-03],\n                [-7.1416e-03,  2.5704e-03, -9.5576e-04],\n                [ 6.0344e-03,  2.4309e-02, -9.9671e-03]],\n      \n               [[-2.3138e-02, -1.5142e-02,  1.7481e-02],\n                [ 2.4425e-02, -2.0064e-02, -1.1357e-02],\n                [-8.9322e-03, -1.1074e-02, -2.7225e-02]],\n      \n               [[-1.7973e-02, -8.0816e-03,  2.2451e-02],\n                [-1.2136e-02,  1.8143e-02, -1.4764e-03],\n                [ 2.0232e-04, -3.8992e-03, -9.4019e-03]]],\n      \n      \n              [[[ 1.9543e-02, -1.3915e-02,  2.5107e-02],\n                [ 8.8922e-04, -2.0700e-02,  5.1136e-03],\n                [ 1.0388e-03,  1.3157e-02,  5.9830e-03]],\n      \n               [[ 3.9359e-03,  2.7233e-02,  8.7847e-03],\n                [ 2.5863e-02,  1.8913e-02,  1.1518e-02],\n                [ 2.9182e-02,  1.0190e-02, -1.4564e-02]],\n      \n               [[-1.0108e-02, -2.7395e-02,  1.3646e-02],\n                [-1.2084e-02, -1.6826e-02, -1.7049e-03],\n                [ 8.9118e-03, -2.8662e-02,  3.4728e-03]],\n      \n               ...,\n      \n               [[-1.6892e-02, -1.1871e-02, -1.4895e-02],\n                [-1.2051e-02, -7.0222e-03,  1.0451e-02],\n                [ 1.4991e-03,  1.6771e-02, -6.4216e-04]],\n      \n               [[-2.3200e-02,  1.7502e-02,  7.1892e-03],\n                [ 1.3102e-03, -2.0371e-02,  2.1188e-02],\n                [-1.7429e-02,  2.2718e-02,  1.7971e-02]],\n      \n               [[-8.5438e-03, -1.7433e-02,  5.4019e-03],\n                [-1.6121e-02,  2.3239e-02,  2.0378e-02],\n                [-2.6269e-02, -1.5614e-02, -2.2885e-02]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-0.0016, -0.0001,  0.0121, -0.0064,  0.0218,  0.0185,  0.0117, -0.0152,\n              -0.0057,  0.0029, -0.0032, -0.0156, -0.0028,  0.0350,  0.0104, -0.0002,\n              -0.0119, -0.0097, -0.0024, -0.0352,  0.0158,  0.0146,  0.0217,  0.0447,\n              -0.0004,  0.0027,  0.0143, -0.0291,  0.0192, -0.0017,  0.0166,  0.0074,\n              -0.0059, -0.0046,  0.0182, -0.0065,  0.0227, -0.0197, -0.0096,  0.0112,\n               0.0182,  0.0144, -0.0071,  0.0128,  0.0014,  0.0015,  0.0028, -0.0112,\n              -0.0039, -0.0257,  0.0233, -0.0199, -0.0204, -0.0151, -0.0047,  0.0162,\n              -0.0057,  0.0143,  0.0089, -0.0321,  0.0357, -0.0277,  0.0036,  0.0125,\n              -0.0061, -0.0054, -0.0108, -0.0068, -0.0086,  0.0143,  0.0236, -0.0047,\n              -0.0036,  0.0179,  0.0084,  0.0010, -0.0306,  0.0132, -0.0193,  0.0238,\n              -0.0046, -0.0531,  0.0108, -0.0130,  0.0060, -0.0376, -0.0196, -0.0233,\n               0.0139,  0.0050, -0.0074,  0.0125, -0.0114, -0.0111,  0.0139,  0.0084,\n               0.0193, -0.0203, -0.0135, -0.0030,  0.0072, -0.0018,  0.0109,  0.0280,\n              -0.0092, -0.0099,  0.0159,  0.0083, -0.0097, -0.0070, -0.0104, -0.0081,\n              -0.0073, -0.0056,  0.0084,  0.0167,  0.0203,  0.0270,  0.0107,  0.0038,\n               0.0265,  0.0351,  0.0133,  0.0242, -0.0031,  0.0274, -0.0074, -0.0119,\n              -0.0025, -0.0124,  0.0101, -0.0037, -0.0088,  0.0033,  0.0029,  0.0325,\n              -0.0047, -0.0120, -0.0128,  0.0224, -0.0115,  0.0150, -0.0062, -0.0129,\n              -0.0236, -0.0041, -0.0150,  0.0155, -0.0106, -0.0190, -0.0261,  0.0337,\n              -0.0003,  0.0040,  0.0058,  0.0022, -0.0350,  0.0061, -0.0124, -0.0264,\n               0.0347,  0.0056,  0.0164, -0.0031, -0.0055, -0.0012, -0.0201, -0.0116,\n               0.0038,  0.0284, -0.0313,  0.0314, -0.0327, -0.0162,  0.0108,  0.0213,\n              -0.0082, -0.0173, -0.0179, -0.0192,  0.0012,  0.0004,  0.0244, -0.0122,\n              -0.0182,  0.0141,  0.0002, -0.0195, -0.0192,  0.0157, -0.0083, -0.0039,\n               0.0034, -0.0140, -0.0005,  0.0084, -0.0365, -0.0167, -0.0047,  0.0092,\n               0.0183, -0.0128, -0.0126,  0.0269, -0.0156, -0.0055,  0.0085, -0.0093,\n               0.0290, -0.0042,  0.0152,  0.0061, -0.0044, -0.0168, -0.0201, -0.0012,\n              -0.0356, -0.0054,  0.0041, -0.0161,  0.0086, -0.0304, -0.0012,  0.0109,\n               0.0096,  0.0102, -0.0183, -0.0004, -0.0090, -0.0225,  0.0386, -0.0247,\n               0.0214, -0.0246,  0.0112, -0.0221, -0.0126,  0.0039, -0.0082,  0.0271,\n              -0.0356,  0.0032,  0.0065,  0.0021,  0.0012,  0.0151, -0.0035, -0.0224,\n               0.0002,  0.0023, -0.0144,  0.0164, -0.0208, -0.0002, -0.0109, -0.0136],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9095, 0.9119, 0.9197, 0.9099, 0.9132, 0.9127, 0.9183, 0.9105, 0.9115,\n              0.9229, 0.9217, 0.9169, 0.9136, 0.9105, 0.9188, 0.9157, 0.9125, 0.9120,\n              0.9104, 0.9130, 0.9108, 0.9182, 0.9098, 0.9148, 0.9146, 0.9163, 0.9103,\n              0.9148, 0.9098, 0.9111, 0.9223, 0.9093, 0.9116, 0.9120, 0.9116, 0.9134,\n              0.9110, 0.9154, 0.9158, 0.9096, 0.9165, 0.9231, 0.9125, 0.9147, 0.9196,\n              0.9110, 0.9150, 0.9121, 0.9118, 0.9120, 0.9193, 0.9153, 0.9163, 0.9109,\n              0.9119, 0.9098, 0.9133, 0.9120, 0.9107, 0.9186, 0.9157, 0.9124, 0.9112,\n              0.9088, 0.9106, 0.9109, 0.9092, 0.9122, 0.9134, 0.9204, 0.9095, 0.9107,\n              0.9137, 0.9107, 0.9109, 0.9161, 0.9237, 0.9143, 0.9177, 0.9191, 0.9181,\n              0.9121, 0.9108, 0.9106, 0.9160, 0.9171, 0.9119, 0.9139, 0.9108, 0.9147,\n              0.9176, 0.9119, 0.9098, 0.9094, 0.9125, 0.9202, 0.9107, 0.9131, 0.9115,\n              0.9122, 0.9098, 0.9175, 0.9102, 0.9106, 0.9104, 0.9129, 0.9146, 0.9129,\n              0.9124, 0.9176, 0.9144, 0.9175, 0.9118, 0.9103, 0.9164, 0.9136, 0.9165,\n              0.9102, 0.9197, 0.9154, 0.9189, 0.9111, 0.9141, 0.9094, 0.9125, 0.9144,\n              0.9106, 0.9107, 0.9128, 0.9138, 0.9105, 0.9121, 0.9130, 0.9142, 0.9117,\n              0.9113, 0.9133, 0.9128, 0.9099, 0.9110, 0.9143, 0.9113, 0.9100, 0.9113,\n              0.9107, 0.9216, 0.9115, 0.9186, 0.9108, 0.9143, 0.9107, 0.9114, 0.9170,\n              0.9101, 0.9106, 0.9127, 0.9144, 0.9257, 0.9104, 0.9209, 0.9107, 0.9141,\n              0.9091, 0.9138, 0.9142, 0.9080, 0.9130, 0.9109, 0.9105, 0.9100, 0.9144,\n              0.9119, 0.9195, 0.9103, 0.9094, 0.9103, 0.9147, 0.9139, 0.9100, 0.9156,\n              0.9142, 0.9146, 0.9099, 0.9133, 0.9114, 0.9137, 0.9104, 0.9254, 0.9213,\n              0.9092, 0.9146, 0.9162, 0.9115, 0.9159, 0.9136, 0.9169, 0.9208, 0.9166,\n              0.9183, 0.9204, 0.9133, 0.9115, 0.9110, 0.9155, 0.9223, 0.9154, 0.9137,\n              0.9123, 0.9114, 0.9145, 0.9103, 0.9165, 0.9103, 0.9159, 0.9177, 0.9135,\n              0.9147, 0.9128, 0.9093, 0.9118, 0.9101, 0.9129, 0.9141, 0.9151, 0.9116,\n              0.9229, 0.9106, 0.9125, 0.9191, 0.9164, 0.9118, 0.9124, 0.9115, 0.9180,\n              0.9146, 0.9229, 0.9107, 0.9118, 0.9132, 0.9162, 0.9145, 0.9100, 0.9102,\n              0.9226, 0.9153, 0.9106, 0.9115, 0.9171, 0.9164, 0.9116, 0.9105, 0.9113,\n              0.9111, 0.9142, 0.9106, 0.9105], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-5.4663e-03,  1.1275e-02, -1.0041e-02],\n                [ 1.7777e-02, -2.6049e-04,  1.6172e-02],\n                [-4.1402e-03, -3.8237e-03,  1.5995e-02]],\n      \n               [[-1.9285e-02, -1.3866e-02,  1.5059e-02],\n                [-1.9738e-02, -1.2253e-02, -6.3254e-03],\n                [-9.9108e-03, -1.5551e-02, -1.0269e-02]],\n      \n               [[-7.0165e-03,  7.5471e-03, -1.1553e-02],\n                [-1.2156e-03,  9.3962e-05, -4.3071e-03],\n                [ 7.0013e-03, -1.5206e-02,  8.6066e-03]],\n      \n               ...,\n      \n               [[-3.0326e-03, -1.4173e-02, -9.0556e-03],\n                [ 1.0119e-03,  3.0358e-03,  7.9499e-03],\n                [-1.3994e-02,  1.7063e-02,  1.6788e-02]],\n      \n               [[-1.3243e-03, -1.4991e-02,  1.5792e-02],\n                [-1.4016e-02, -1.1000e-02,  3.8523e-04],\n                [ 2.5853e-03,  1.5131e-02, -5.4581e-03]],\n      \n               [[-1.9688e-02,  1.1852e-02, -5.6679e-03],\n                [ 1.5445e-02,  1.7447e-02,  1.7493e-02],\n                [ 7.3825e-03, -2.0710e-02, -9.0107e-03]]],\n      \n      \n              [[[ 1.9791e-02,  1.0990e-02,  1.1376e-02],\n                [-8.7931e-03,  3.2307e-03,  9.6098e-03],\n                [-1.1173e-02, -4.7578e-03, -5.3793e-03]],\n      \n               [[-1.7467e-02, -3.2872e-03,  5.2192e-03],\n                [-1.1217e-03, -1.3222e-02, -1.4775e-02],\n                [-1.3158e-02, -1.9023e-02, -7.4412e-03]],\n      \n               [[ 9.0219e-03,  9.4825e-03, -1.9334e-02],\n                [-2.0268e-02,  4.8155e-03,  1.8204e-02],\n                [ 8.1820e-04, -1.2737e-02,  1.0044e-02]],\n      \n               ...,\n      \n               [[ 1.8925e-02, -1.4261e-02, -1.7112e-02],\n                [ 6.2632e-03, -1.4960e-03,  8.7005e-03],\n                [-1.6140e-02, -8.3245e-03,  1.1912e-02]],\n      \n               [[ 1.5935e-02,  6.6787e-03,  1.5160e-02],\n                [-1.0934e-02, -7.7543e-03, -9.0985e-04],\n                [-1.5886e-02, -1.7723e-02,  1.5203e-02]],\n      \n               [[ 2.0870e-03, -7.6804e-03,  1.9208e-02],\n                [-8.4168e-03,  7.2026e-04, -8.5956e-03],\n                [ 6.5800e-03, -4.0562e-03, -3.2615e-04]]],\n      \n      \n              [[[-1.9264e-02, -1.3930e-02, -1.2548e-02],\n                [ 9.3840e-03,  1.4260e-02, -2.4683e-03],\n                [-1.1393e-02, -1.4334e-02, -1.8075e-02]],\n      \n               [[-1.2498e-02,  9.7743e-03,  2.3253e-03],\n                [ 5.2667e-03, -1.0840e-02, -4.2404e-03],\n                [-5.4845e-03, -8.9689e-04,  1.1333e-02]],\n      \n               [[ 1.6983e-02,  1.7106e-02, -1.2131e-02],\n                [-1.0152e-03, -1.4270e-02, -7.5585e-03],\n                [ 1.7857e-02,  1.3378e-02,  2.9503e-03]],\n      \n               ...,\n      \n               [[-7.7182e-03,  8.4110e-03,  1.1110e-02],\n                [-4.2576e-03, -1.9551e-02, -1.2462e-02],\n                [ 2.0161e-02,  3.3265e-03,  5.2625e-03]],\n      \n               [[ 6.2018e-03, -5.0972e-03, -3.4520e-03],\n                [-1.4931e-02,  3.6455e-03, -1.0270e-02],\n                [-6.6794e-03,  2.0210e-02, -1.1964e-02]],\n      \n               [[ 9.2103e-03,  1.8212e-02, -4.5127e-03],\n                [-1.6402e-02,  3.9300e-04, -9.6559e-03],\n                [ 1.1794e-02,  1.3532e-02,  4.9754e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[ 8.3111e-03, -1.0510e-02,  1.7620e-02],\n                [-1.8730e-03, -1.4843e-02, -1.4698e-02],\n                [ 3.8077e-03, -9.4780e-03, -1.3928e-02]],\n      \n               [[ 1.6550e-03,  2.3224e-03,  6.9863e-04],\n                [-6.5461e-03, -2.0367e-03, -1.6171e-02],\n                [ 1.6299e-02, -7.6246e-04, -4.1635e-03]],\n      \n               [[-4.1031e-03, -9.2379e-03, -9.8070e-03],\n                [-4.9666e-03,  1.7341e-02, -9.6604e-03],\n                [-5.0619e-03,  1.2017e-04, -9.8532e-03]],\n      \n               ...,\n      \n               [[-1.4320e-02,  7.5451e-03,  1.3523e-03],\n                [-6.0101e-03,  8.1205e-03, -1.1438e-02],\n                [ 9.2784e-03, -1.1792e-03, -1.8128e-02]],\n      \n               [[-1.1172e-02,  1.1496e-02,  9.1718e-03],\n                [-7.0329e-03,  1.2965e-02, -2.9530e-03],\n                [-2.0548e-02,  1.3477e-02, -6.1864e-04]],\n      \n               [[-1.0360e-02, -1.4434e-02, -6.9486e-03],\n                [-1.1076e-02, -1.1113e-02, -1.0571e-02],\n                [ 1.4761e-02,  6.0613e-03,  7.5636e-03]]],\n      \n      \n              [[[ 1.3815e-02,  1.0758e-02,  8.4184e-04],\n                [-4.5351e-03,  1.9133e-03, -1.5433e-02],\n                [ 5.8161e-03,  1.0875e-02,  9.0770e-03]],\n      \n               [[-4.3384e-04, -1.7031e-02, -4.0914e-03],\n                [ 1.3098e-02,  1.6402e-02, -7.1401e-03],\n                [-2.1398e-03, -1.2326e-02, -1.5424e-02]],\n      \n               [[ 1.7078e-02, -4.9334e-03,  1.0495e-02],\n                [ 1.5147e-02, -5.2804e-03, -7.4826e-03],\n                [-8.4853e-03,  1.8304e-02,  1.2862e-02]],\n      \n               ...,\n      \n               [[-1.5622e-02,  1.1810e-02, -1.1843e-02],\n                [-1.1590e-02, -1.2121e-02,  9.2555e-03],\n                [-1.2303e-02,  9.7288e-03,  2.6996e-03]],\n      \n               [[-1.9137e-02, -4.7320e-03,  8.6958e-03],\n                [-1.8827e-02,  1.5150e-02, -1.8789e-03],\n                [ 6.2889e-04,  1.9208e-03,  4.5127e-04]],\n      \n               [[-1.4430e-02, -1.7637e-02, -1.5462e-02],\n                [ 1.8325e-02,  1.5926e-02, -7.2440e-03],\n                [ 1.9038e-02, -9.1490e-03, -7.0277e-03]]],\n      \n      \n              [[[ 1.9808e-02,  1.1499e-02, -2.7598e-03],\n                [-1.0275e-02,  5.6927e-03,  1.8158e-02],\n                [ 1.4908e-03,  1.2659e-02,  1.5009e-02]],\n      \n               [[ 1.5979e-02,  3.8770e-03, -1.9546e-02],\n                [-1.4898e-02, -1.2246e-02,  4.7931e-03],\n                [ 1.0272e-02,  1.8440e-03, -4.2528e-04]],\n      \n               [[ 1.9176e-02,  1.3500e-02, -9.1369e-03],\n                [ 1.7589e-02,  1.1573e-02, -4.2710e-04],\n                [ 1.0903e-02,  9.5749e-03, -1.8133e-02]],\n      \n               ...,\n      \n               [[-1.2926e-02,  1.3197e-02, -4.9772e-03],\n                [ 3.6071e-03, -1.4228e-02,  6.1735e-04],\n                [ 1.3121e-02,  1.9329e-02, -1.2167e-02]],\n      \n               [[ 1.3576e-03,  4.1507e-03,  1.6966e-02],\n                [ 1.8642e-03, -7.2439e-04,  1.8927e-02],\n                [ 9.4632e-03, -9.7591e-03,  1.7613e-02]],\n      \n               [[ 1.3244e-02, -1.0179e-02, -1.9092e-02],\n                [ 2.0751e-02,  1.0738e-04, -8.5360e-04],\n                [ 1.0980e-02, -4.3271e-03, -1.4493e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 7.3124e-03, -1.4411e-02,  9.7323e-03, -2.7263e-03, -1.8272e-02,\n               2.0302e-02, -2.4814e-02,  3.0196e-04,  6.1171e-03,  3.2059e-02,\n               2.7020e-03, -1.6808e-02,  8.7984e-03, -1.5428e-02,  6.2096e-03,\n               1.4069e-03,  1.1869e-02, -5.2098e-03, -4.0411e-03,  2.7154e-02,\n               1.5298e-03, -2.4152e-03, -4.3450e-04, -7.9662e-03, -1.1073e-02,\n              -3.0841e-03,  6.1334e-03, -8.4361e-03,  1.1911e-02, -6.3994e-03,\n              -6.8198e-04, -4.1444e-03,  6.4120e-03,  7.3876e-03,  2.3902e-03,\n              -2.7215e-02,  6.5325e-03, -1.3364e-02,  1.2288e-02,  1.7718e-03,\n               8.6262e-03,  1.0965e-02, -3.8549e-03, -3.3304e-03,  4.1498e-03,\n              -1.3547e-03,  4.4134e-03, -5.6610e-03, -8.4559e-03, -1.1045e-02,\n              -1.7308e-02, -2.2872e-02,  3.9853e-03, -2.3154e-02,  1.9012e-02,\n              -1.4449e-03, -1.2642e-02, -1.1919e-02,  2.2149e-03, -1.5859e-02,\n               6.0151e-03, -9.9622e-03,  4.9417e-03, -8.7411e-03,  1.3331e-02,\n              -4.1293e-03, -9.8682e-03,  1.5073e-02,  5.3204e-03,  1.3364e-02,\n              -4.9202e-03,  1.4911e-02, -1.3234e-03, -9.3444e-03, -1.7136e-02,\n               8.5388e-03,  1.3218e-02,  4.8077e-03,  3.6933e-03, -3.1669e-05,\n              -1.9888e-03, -1.4717e-02, -3.7391e-03,  2.7674e-02, -2.7761e-03,\n               1.3391e-02,  1.4060e-02,  3.1591e-03, -3.7160e-03,  8.8029e-03,\n              -1.5231e-02,  4.6250e-03, -9.4369e-03,  1.1087e-02,  5.0400e-03,\n              -1.1108e-03,  7.0804e-03,  5.7221e-03, -1.1283e-02, -1.1691e-02,\n               2.7713e-03,  9.5071e-03, -4.0702e-03, -1.4814e-02,  7.2394e-03,\n              -1.6206e-03, -6.9021e-03, -7.3529e-03,  2.5399e-03, -2.1089e-03,\n              -2.0490e-03, -6.6025e-03, -2.9324e-03,  3.4932e-03, -1.2239e-02,\n               2.9558e-03,  3.8826e-03,  4.9502e-03, -3.4086e-03, -1.5530e-02,\n              -1.9602e-03,  1.8743e-02, -6.2659e-04, -2.3739e-03, -2.0265e-02,\n               3.6371e-03,  4.9157e-03, -1.7101e-02, -1.7042e-03, -1.9908e-02,\n              -1.1654e-02,  1.4577e-03, -2.0219e-02, -5.9488e-06,  8.2821e-03,\n               2.1303e-02, -3.4048e-03, -3.0985e-03, -1.4743e-02,  7.3578e-03,\n              -2.6512e-03,  1.5844e-02, -1.8521e-02, -5.9484e-03,  1.1212e-03,\n              -6.8494e-03, -1.8663e-02, -6.5946e-03,  6.1697e-03,  1.2092e-02,\n               2.6906e-02,  4.5643e-03, -1.3673e-02,  1.8795e-02, -2.5107e-02,\n               6.3104e-04,  5.9864e-03,  9.7975e-03,  2.2023e-03, -4.0712e-03,\n               7.9388e-03,  1.4834e-02, -1.2769e-02,  8.4907e-03, -2.5800e-03,\n              -1.4124e-02, -6.4922e-03, -2.7855e-03, -2.7803e-02, -1.0995e-02,\n              -1.4119e-02, -7.6153e-03, -1.9298e-02, -1.5092e-03, -1.9076e-02,\n               5.7135e-03, -1.6536e-02, -1.4114e-02,  5.3350e-03,  3.7918e-02,\n               1.5131e-03,  2.5613e-04,  3.3407e-02, -1.1101e-02, -1.5797e-02,\n               3.0949e-03, -2.3422e-02, -2.7438e-02, -5.9249e-03,  1.0054e-02,\n               5.1011e-03,  7.5251e-03,  9.6976e-03, -4.8771e-03, -1.5278e-03,\n               1.4873e-02,  2.1384e-03, -9.1224e-04, -5.5713e-03, -1.1604e-04,\n               2.2227e-03, -5.0512e-03, -2.8569e-02, -2.1930e-03,  5.6146e-03,\n              -3.3262e-03,  4.2069e-03, -8.9483e-03,  1.7297e-02,  5.9110e-04,\n               7.2502e-03,  3.1878e-03,  4.6998e-03, -4.5061e-03,  8.5431e-04,\n              -1.3634e-02, -4.8690e-03, -2.7267e-02, -4.4633e-03, -1.9808e-02,\n               8.0742e-03,  1.4076e-02,  9.3255e-03,  1.2944e-02, -4.5757e-03,\n               1.6088e-02,  2.2333e-03,  1.5279e-03, -5.0285e-03,  2.8821e-03,\n              -5.9986e-03, -1.1118e-03,  5.2256e-03, -1.1699e-02,  3.2929e-03,\n               3.4899e-03,  9.1605e-03, -2.6965e-04, -1.0303e-02, -1.8297e-03,\n              -1.0189e-02,  1.1474e-02,  1.6164e-02, -1.5079e-03, -2.0604e-03,\n              -5.7956e-03,  1.8854e-02,  1.9735e-02, -2.1305e-03,  1.8547e-02,\n               6.1094e-03,  2.1818e-03,  1.3191e-02,  2.4202e-04,  1.2395e-02,\n              -1.9122e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9042, 0.9063, 0.9057, 0.9042, 0.9085, 0.9051, 0.9048, 0.9083, 0.9067,\n              0.9071, 0.9043, 0.9054, 0.9053, 0.9045, 0.9061, 0.9064, 0.9061, 0.9046,\n              0.9062, 0.9054, 0.9059, 0.9041, 0.9052, 0.9065, 0.9050, 0.9050, 0.9054,\n              0.9058, 0.9045, 0.9098, 0.9036, 0.9064, 0.9039, 0.9074, 0.9076, 0.9054,\n              0.9042, 0.9046, 0.9072, 0.9086, 0.9047, 0.9058, 0.9073, 0.9051, 0.9065,\n              0.9039, 0.9054, 0.9044, 0.9063, 0.9049, 0.9061, 0.9091, 0.9072, 0.9073,\n              0.9059, 0.9050, 0.9038, 0.9069, 0.9065, 0.9101, 0.9051, 0.9052, 0.9061,\n              0.9035, 0.9058, 0.9053, 0.9091, 0.9097, 0.9055, 0.9093, 0.9055, 0.9057,\n              0.9057, 0.9094, 0.9040, 0.9050, 0.9050, 0.9047, 0.9092, 0.9076, 0.9075,\n              0.9057, 0.9112, 0.9051, 0.9066, 0.9053, 0.9038, 0.9042, 0.9056, 0.9043,\n              0.9057, 0.9063, 0.9046, 0.9043, 0.9043, 0.9057, 0.9050, 0.9063, 0.9065,\n              0.9075, 0.9045, 0.9066, 0.9074, 0.9085, 0.9065, 0.9044, 0.9050, 0.9049,\n              0.9059, 0.9050, 0.9070, 0.9068, 0.9046, 0.9049, 0.9080, 0.9056, 0.9054,\n              0.9065, 0.9067, 0.9059, 0.9060, 0.9058, 0.9066, 0.9052, 0.9047, 0.9049,\n              0.9051, 0.9049, 0.9068, 0.9060, 0.9055, 0.9075, 0.9055, 0.9058, 0.9052,\n              0.9044, 0.9048, 0.9051, 0.9069, 0.9067, 0.9050, 0.9057, 0.9069, 0.9057,\n              0.9048, 0.9058, 0.9048, 0.9041, 0.9036, 0.9046, 0.9082, 0.9100, 0.9075,\n              0.9038, 0.9071, 0.9046, 0.9078, 0.9052, 0.9058, 0.9059, 0.9063, 0.9068,\n              0.9068, 0.9058, 0.9048, 0.9072, 0.9044, 0.9069, 0.9048, 0.9066, 0.9084,\n              0.9043, 0.9048, 0.9052, 0.9054, 0.9043, 0.9080, 0.9047, 0.9066, 0.9070,\n              0.9047, 0.9096, 0.9060, 0.9064, 0.9071, 0.9061, 0.9060, 0.9070, 0.9075,\n              0.9053, 0.9041, 0.9041, 0.9048, 0.9038, 0.9047, 0.9063, 0.9046, 0.9052,\n              0.9043, 0.9062, 0.9062, 0.9040, 0.9052, 0.9048, 0.9060, 0.9065, 0.9076,\n              0.9055, 0.9060, 0.9051, 0.9047, 0.9073, 0.9090, 0.9057, 0.9063, 0.9050,\n              0.9055, 0.9045, 0.9060, 0.9100, 0.9049, 0.9045, 0.9077, 0.9057, 0.9045,\n              0.9046, 0.9060, 0.9048, 0.9112, 0.9048, 0.9054, 0.9064, 0.9052, 0.9045,\n              0.9043, 0.9047, 0.9060, 0.9050, 0.9083, 0.9058, 0.9067, 0.9054, 0.9051,\n              0.9101, 0.9087, 0.9048, 0.9059, 0.9049, 0.9044, 0.9055, 0.9047, 0.9043,\n              0.9072, 0.9053, 0.9038, 0.9048], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[-0.0574]],\n        \n                 [[ 0.0651]],\n        \n                 [[ 0.0279]],\n        \n                 ...,\n        \n                 [[-0.0287]],\n        \n                 [[-0.0644]],\n        \n                 [[-0.0690]]],\n        \n        \n                [[[-0.0336]],\n        \n                 [[ 0.0305]],\n        \n                 [[-0.0356]],\n        \n                 ...,\n        \n                 [[-0.0523]],\n        \n                 [[ 0.0389]],\n        \n                 [[-0.0410]]],\n        \n        \n                [[[-0.0559]],\n        \n                 [[-0.0576]],\n        \n                 [[ 0.0041]],\n        \n                 ...,\n        \n                 [[ 0.0141]],\n        \n                 [[-0.0243]],\n        \n                 [[-0.0625]]],\n        \n        \n                ...,\n        \n        \n                [[[ 0.0293]],\n        \n                 [[-0.0696]],\n        \n                 [[-0.0717]],\n        \n                 ...,\n        \n                 [[-0.0326]],\n        \n                 [[-0.0553]],\n        \n                 [[ 0.0457]]],\n        \n        \n                [[[ 0.0170]],\n        \n                 [[ 0.0016]],\n        \n                 [[ 0.0179]],\n        \n                 ...,\n        \n                 [[ 0.0779]],\n        \n                 [[-0.0249]],\n        \n                 [[-0.0412]]],\n        \n        \n                [[[ 0.0132]],\n        \n                 [[-0.0010]],\n        \n                 [[-0.0458]],\n        \n                 ...,\n        \n                 [[ 0.0075]],\n        \n                 [[-0.0421]],\n        \n                 [[ 0.0254]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n               requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-0.0503,  0.0480,  0.0130,  0.0442,  0.0087, -0.0498, -0.0163,  0.0403,\n                -0.0198,  0.0107, -0.0698, -0.0658, -0.0238, -0.0289,  0.0335, -0.0518,\n                -0.0070,  0.0227, -0.0285, -0.0260,  0.0076, -0.0340,  0.0233, -0.0261,\n                -0.0472,  0.0301,  0.0637,  0.0075, -0.0607,  0.0292, -0.0326, -0.0667,\n                -0.0345,  0.0347, -0.0201, -0.0435, -0.0859, -0.0066,  0.0156, -0.0275,\n                 0.0077, -0.0118,  0.0373,  0.0207, -0.0002,  0.0065, -0.0519, -0.0214,\n                 0.0119, -0.0333, -0.0214, -0.0050, -0.0651,  0.0119, -0.0092, -0.0212,\n                 0.0002, -0.0056, -0.0225,  0.0154, -0.0379,  0.0195, -0.0013,  0.0131,\n                -0.0149,  0.0241,  0.0152, -0.0046, -0.0389, -0.0344, -0.0088, -0.0492,\n                -0.0067,  0.0233, -0.0283, -0.0444, -0.0295, -0.0176,  0.0371,  0.0221,\n                -0.0606,  0.0368, -0.0272,  0.0043,  0.0322,  0.0401,  0.0228,  0.0631,\n                -0.0422, -0.0316, -0.0147, -0.0002, -0.0069,  0.0126,  0.0431,  0.0329,\n                 0.0041, -0.0010,  0.0229, -0.0608, -0.0129, -0.0398, -0.0150,  0.0119,\n                -0.0099, -0.0222,  0.0541,  0.0298,  0.0218,  0.0318,  0.0250, -0.0338,\n                -0.0470, -0.0435,  0.0410,  0.0180,  0.0186,  0.0284, -0.0116,  0.0103,\n                -0.0329, -0.0334,  0.0054, -0.0124,  0.0300, -0.0006, -0.0009,  0.0521,\n                -0.0404,  0.0255, -0.0093, -0.0071, -0.0193,  0.0020,  0.0013,  0.0245,\n                -0.0051, -0.0383,  0.0680,  0.0087, -0.0151,  0.0021,  0.0394, -0.0165,\n                -0.0349, -0.0156, -0.0132, -0.0347, -0.0068, -0.0298,  0.0214,  0.0240,\n                 0.0214,  0.0150,  0.0387,  0.0149, -0.0035, -0.0038,  0.0004, -0.0167,\n                -0.0208,  0.0209,  0.0574,  0.0118,  0.0456, -0.0404, -0.0263,  0.0199,\n                 0.0378,  0.0669,  0.0594, -0.0153, -0.0242,  0.0108,  0.0452,  0.0126,\n                -0.0461,  0.0204, -0.0500,  0.0034, -0.0125, -0.0396,  0.0204,  0.0295,\n                -0.0078, -0.0217, -0.0183,  0.0038, -0.0137, -0.0113,  0.0423, -0.0074,\n                 0.0465, -0.0091, -0.0024,  0.0243, -0.0392,  0.0241, -0.0346,  0.0099,\n                 0.0428,  0.0185,  0.0261,  0.0271,  0.0473,  0.0086,  0.0147,  0.0292,\n                 0.0277, -0.0159,  0.0433, -0.0107,  0.0208, -0.0195, -0.0056,  0.0091,\n                 0.0083, -0.0215,  0.0202,  0.0281,  0.0260, -0.0262, -0.0138,  0.0244,\n                -0.0163,  0.0239, -0.0375, -0.0191,  0.0061, -0.0205,  0.0123,  0.0378,\n                 0.0102, -0.0315,  0.0360, -0.0316,  0.0040, -0.0032,  0.0072,  0.0061,\n                -0.0136, -0.0551, -0.0306,  0.0307, -0.0073, -0.0401, -0.0223, -0.0310,\n                -0.0180, -0.0204,  0.0012,  0.0744,  0.0565, -0.0460, -0.0098,  0.0647],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9175, 0.9146, 0.9157, 0.9223, 0.9284, 0.9217, 0.9205, 0.9176, 0.9215,\n                0.9157, 0.9135, 0.9180, 0.9326, 0.9136, 0.9196, 0.9150, 0.9220, 0.9111,\n                0.9207, 0.9246, 0.9140, 0.9173, 0.9154, 0.9110, 0.9200, 0.9142, 0.9256,\n                0.9240, 0.9179, 0.9148, 0.9165, 0.9190, 0.9215, 0.9175, 0.9237, 0.9202,\n                0.9168, 0.9137, 0.9272, 0.9130, 0.9134, 0.9170, 0.9209, 0.9260, 0.9166,\n                0.9159, 0.9268, 0.9180, 0.9114, 0.9263, 0.9246, 0.9194, 0.9219, 0.9177,\n                0.9159, 0.9161, 0.9173, 0.9186, 0.9129, 0.9163, 0.9209, 0.9170, 0.9162,\n                0.9152, 0.9186, 0.9269, 0.9205, 0.9194, 0.9156, 0.9227, 0.9214, 0.9204,\n                0.9190, 0.9115, 0.9192, 0.9146, 0.9194, 0.9206, 0.9201, 0.9110, 0.9222,\n                0.9165, 0.9244, 0.9156, 0.9098, 0.9295, 0.9160, 0.9167, 0.9183, 0.9203,\n                0.9236, 0.9127, 0.9185, 0.9128, 0.9168, 0.9199, 0.9169, 0.9301, 0.9143,\n                0.9224, 0.9219, 0.9196, 0.9185, 0.9214, 0.9198, 0.9189, 0.9137, 0.9176,\n                0.9243, 0.9251, 0.9174, 0.9210, 0.9182, 0.9238, 0.9238, 0.9169, 0.9165,\n                0.9167, 0.9176, 0.9113, 0.9298, 0.9168, 0.9205, 0.9181, 0.9185, 0.9178,\n                0.9215, 0.9144, 0.9205, 0.9146, 0.9212, 0.9177, 0.9176, 0.9166, 0.9195,\n                0.9191, 0.9208, 0.9208, 0.9170, 0.9195, 0.9127, 0.9152, 0.9157, 0.9195,\n                0.9267, 0.9160, 0.9138, 0.9195, 0.9179, 0.9234, 0.9199, 0.9111, 0.9199,\n                0.9151, 0.9155, 0.9220, 0.9220, 0.9153, 0.9155, 0.9219, 0.9145, 0.9199,\n                0.9151, 0.9200, 0.9230, 0.9268, 0.9149, 0.9132, 0.9218, 0.9135, 0.9221,\n                0.9116, 0.9153, 0.9122, 0.9200, 0.9234, 0.9114, 0.9233, 0.9210, 0.9145,\n                0.9227, 0.9213, 0.9193, 0.9119, 0.9124, 0.9223, 0.9231, 0.9169, 0.9113,\n                0.9189, 0.9145, 0.9158, 0.9184, 0.9196, 0.9246, 0.9182, 0.9182, 0.9255,\n                0.9149, 0.9228, 0.9168, 0.9131, 0.9139, 0.9160, 0.9150, 0.9165, 0.9118,\n                0.9164, 0.9137, 0.9125, 0.9270, 0.9232, 0.9175, 0.9166, 0.9123, 0.9173,\n                0.9185, 0.9187, 0.9264, 0.9144, 0.9176, 0.9207, 0.9182, 0.9128, 0.9186,\n                0.9142, 0.9168, 0.9233, 0.9181, 0.9177, 0.9252, 0.9137, 0.9128, 0.9135,\n                0.9190, 0.9134, 0.9125, 0.9191, 0.9162, 0.9196, 0.9155, 0.9153, 0.9138,\n                0.9247, 0.9187, 0.9206, 0.9142, 0.9179, 0.9253, 0.9111, 0.9143, 0.9217,\n                0.9211, 0.9251, 0.9173, 0.9258], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [256, 128, 3, 3]], ["0.bn1.weight", [256]], ["0.bn1.bias", [256]], ["0.conv2.weight", [256, 256, 3, 3]], ["0.bn2.weight", [256]], ["0.bn2.bias", [256]], ["0.downsample.0.weight", [256, 128, 1, 1]], ["0.downsample.1.weight", [256]], ["0.downsample.1.bias", [256]]], "output_shape": [[512, 256, 2, 2]], "num_parameters": [294912, 256, 256, 589824, 256, 256, 32768, 256, 256]}, {"name": "layer4", "id": 140209539495680, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 2.2684e-03,  1.7327e-02,  1.8472e-02],\n                [ 6.9044e-03,  2.9075e-03, -4.1914e-03],\n                [-5.5283e-03, -1.0763e-02, -1.1357e-04]],\n      \n               [[ 7.8431e-03,  5.2744e-03, -7.1670e-04],\n                [ 1.9730e-02, -8.7374e-03,  1.6428e-02],\n                [ 6.9430e-03, -6.2265e-03, -1.8607e-02]],\n      \n               [[-1.6846e-02, -4.9777e-03,  8.0604e-03],\n                [ 1.0384e-02,  2.5844e-03, -1.0565e-02],\n                [-1.5231e-02,  1.4227e-02,  1.2781e-02]],\n      \n               ...,\n      \n               [[-3.9930e-03, -1.4996e-02,  1.6526e-02],\n                [-8.8295e-03,  4.4283e-03,  7.3383e-03],\n                [ 8.1419e-03,  6.2699e-03, -1.0031e-02]],\n      \n               [[ 5.3662e-03, -1.0777e-03, -9.5558e-03],\n                [-1.7147e-02,  5.6155e-03, -7.2221e-03],\n                [-9.5075e-03,  4.1371e-03,  1.8690e-02]],\n      \n               [[-1.4132e-02, -1.5022e-02,  9.4179e-03],\n                [ 1.1362e-02, -1.0288e-03, -1.7111e-02],\n                [-5.0544e-03, -4.3690e-03, -1.8249e-02]]],\n      \n      \n              [[[ 1.5486e-02,  1.6090e-02, -4.9978e-03],\n                [ 9.4216e-03, -1.4967e-02,  4.8078e-04],\n                [ 1.2193e-02,  8.1196e-03,  1.1319e-02]],\n      \n               [[-8.4847e-03,  1.5325e-02, -1.1540e-02],\n                [-1.9876e-02,  6.0241e-03,  8.8550e-03],\n                [-2.0615e-02,  1.5178e-02,  2.0122e-02]],\n      \n               [[-1.4343e-02, -1.7151e-02,  1.4752e-02],\n                [ 9.3368e-03, -1.3119e-02,  1.8776e-02],\n                [-1.4710e-02, -8.7875e-03,  4.3853e-03]],\n      \n               ...,\n      \n               [[-1.6160e-02, -1.9823e-02, -1.1200e-02],\n                [ 6.7799e-03, -4.7101e-03, -1.1982e-02],\n                [-1.5048e-02,  9.7483e-03, -9.3742e-03]],\n      \n               [[-2.0142e-02,  6.3116e-03, -1.0886e-02],\n                [-7.6285e-03, -1.2852e-02, -1.0516e-02],\n                [-1.3832e-03, -5.3720e-03, -1.9384e-02]],\n      \n               [[-1.9764e-02, -1.1613e-02,  1.7091e-02],\n                [ 8.8071e-04, -1.6148e-02,  7.7638e-04],\n                [ 1.8120e-02, -9.2619e-03,  1.7265e-02]]],\n      \n      \n              [[[ 1.6927e-02, -6.9815e-03, -6.0714e-03],\n                [ 1.7151e-02,  1.0630e-02,  2.0540e-02],\n                [-9.7692e-03, -1.5474e-02, -1.1900e-03]],\n      \n               [[-3.0434e-03,  7.4219e-03,  9.6041e-03],\n                [ 2.2544e-03,  1.5646e-02, -1.4944e-02],\n                [ 1.6475e-02,  1.8877e-02, -1.2871e-02]],\n      \n               [[ 1.9815e-02,  1.2845e-02, -7.1875e-03],\n                [-1.7260e-02, -2.2978e-03, -1.3961e-02],\n                [-2.2971e-03, -1.3937e-03, -1.3624e-02]],\n      \n               ...,\n      \n               [[-1.9644e-02,  1.5703e-02, -1.9932e-02],\n                [ 1.5519e-02, -1.2354e-02,  1.9963e-02],\n                [-3.1838e-03, -1.3043e-02, -1.3877e-02]],\n      \n               [[-1.5763e-03, -1.7343e-02,  2.0231e-02],\n                [ 5.6153e-03, -3.2665e-03,  5.3013e-03],\n                [ 9.2867e-03, -1.6100e-02,  1.5739e-02]],\n      \n               [[ 2.0209e-02,  1.2184e-02,  3.5830e-03],\n                [ 1.7033e-02,  1.7688e-02, -3.6094e-03],\n                [ 2.6278e-03,  3.4654e-03, -1.4856e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[-2.0406e-02, -2.5188e-03, -3.9974e-03],\n                [-9.3105e-05, -1.2208e-02, -4.4603e-03],\n                [ 1.6883e-02,  1.4953e-02, -9.1007e-03]],\n      \n               [[ 5.7675e-03, -2.0454e-02, -1.2660e-02],\n                [-1.2339e-02,  1.1395e-02,  1.1805e-02],\n                [-8.8344e-03,  5.6330e-03, -1.4365e-02]],\n      \n               [[ 9.3654e-03,  1.4472e-04, -1.3724e-02],\n                [-9.3779e-03,  1.3461e-02, -1.2366e-02],\n                [-1.9256e-03, -1.6938e-02,  8.3870e-03]],\n      \n               ...,\n      \n               [[ 1.1639e-03, -1.1478e-02, -1.9791e-02],\n                [-3.6464e-03,  2.0736e-02, -9.6194e-03],\n                [ 6.0483e-03,  9.6282e-03,  1.2903e-02]],\n      \n               [[-1.4095e-02,  4.9800e-03,  3.4874e-03],\n                [-1.4693e-03,  1.9673e-02,  5.7906e-03],\n                [ 4.1334e-03, -6.5319e-03,  1.5516e-02]],\n      \n               [[-9.3297e-03, -1.6957e-02, -3.6270e-03],\n                [-1.9610e-02, -1.0202e-02, -1.4482e-02],\n                [-1.1302e-02, -7.2483e-03,  4.7595e-03]]],\n      \n      \n              [[[ 1.1762e-02, -1.4122e-02, -8.3636e-03],\n                [ 2.8736e-03, -2.0625e-02, -1.9645e-02],\n                [ 9.6533e-03,  1.1898e-02,  5.0894e-03]],\n      \n               [[-3.3783e-03,  1.3955e-02,  1.2808e-02],\n                [-7.5902e-03, -1.3476e-02,  6.2096e-03],\n                [-1.8900e-02, -1.7166e-04,  1.6009e-03]],\n      \n               [[-7.9272e-03, -3.4958e-03, -1.8581e-02],\n                [-3.7135e-03,  2.0033e-02, -1.5541e-02],\n                [-5.2078e-03, -7.3524e-03, -5.7186e-03]],\n      \n               ...,\n      \n               [[ 1.5981e-02, -5.0711e-03,  1.2665e-02],\n                [ 1.5498e-02, -2.0195e-02,  2.0613e-02],\n                [-1.0152e-02, -1.7818e-02, -1.9595e-02]],\n      \n               [[-1.2903e-02, -8.6535e-03, -1.6813e-03],\n                [ 1.4155e-02, -1.1316e-03, -1.6027e-02],\n                [-1.9336e-02,  8.4324e-03,  1.1154e-02]],\n      \n               [[ 1.1787e-02, -1.8923e-02,  3.2947e-03],\n                [ 4.7247e-03, -4.0056e-03,  1.5481e-03],\n                [-1.8792e-02, -5.7600e-03,  1.4082e-02]]],\n      \n      \n              [[[ 1.9010e-02, -8.5494e-03,  9.1563e-03],\n                [-8.8607e-03, -8.7664e-04,  1.2829e-02],\n                [-2.0461e-02,  7.3560e-03, -5.3572e-03]],\n      \n               [[ 3.8984e-03, -1.1676e-02,  1.0833e-02],\n                [ 1.9720e-02, -1.6428e-02,  5.0073e-03],\n                [-1.6675e-02, -1.3512e-02, -1.6851e-02]],\n      \n               [[-1.8573e-02, -1.5719e-02,  1.4900e-02],\n                [ 1.7074e-02, -1.7376e-02,  3.6258e-03],\n                [ 5.8847e-03, -8.2086e-03,  1.3148e-02]],\n      \n               ...,\n      \n               [[-6.5455e-03, -1.4052e-02,  1.0822e-02],\n                [ 1.3392e-02,  9.8547e-03,  2.0574e-02],\n                [-1.1738e-02,  1.4165e-02, -1.5210e-02]],\n      \n               [[-1.1521e-02,  7.6845e-03,  7.2780e-03],\n                [-1.1362e-02, -1.3830e-02, -2.0642e-02],\n                [-9.7138e-03,  5.3047e-03,  1.7398e-03]],\n      \n               [[-3.6156e-03,  2.0226e-02, -1.9201e-02],\n                [ 1.4266e-02,  1.1320e-02, -7.5833e-03],\n                [ 5.2405e-03, -7.2926e-03, -9.6874e-04]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 4.2844e-02, -1.2236e-02, -8.4304e-03,  2.5643e-02,  1.2489e-02,\n              -9.2493e-03,  2.4208e-03,  6.5057e-03,  4.8571e-03,  1.0076e-02,\n              -4.0015e-02,  2.3838e-02,  1.1919e-02, -3.2521e-03, -8.7161e-03,\n              -5.4768e-02,  1.8015e-02,  2.7010e-03,  2.2752e-02, -1.6765e-02,\n              -2.0050e-02,  2.9432e-03, -2.2571e-02, -1.5638e-02, -2.1911e-02,\n              -2.1289e-02, -2.6579e-02,  2.6547e-02,  1.2457e-02, -4.7979e-02,\n              -7.6045e-03, -6.4934e-02, -2.3016e-02, -2.4808e-02, -2.8184e-02,\n               1.5747e-02,  7.2186e-04, -1.7904e-02, -2.0416e-02,  7.0758e-03,\n              -2.9415e-02,  1.7777e-02, -2.3491e-02, -4.8185e-03,  3.3055e-03,\n              -3.0199e-02, -5.4154e-02,  6.3562e-03,  1.3781e-02, -3.6083e-04,\n               1.9589e-02,  3.8761e-02,  1.9565e-02,  2.4863e-02,  1.6537e-02,\n               3.6679e-02,  1.0867e-02,  1.5191e-02, -2.8948e-02, -1.8123e-02,\n              -3.8808e-02, -7.0234e-03,  6.0480e-02,  2.4708e-02, -3.1911e-02,\n               9.1172e-03, -1.2488e-03,  8.7014e-04,  2.9546e-02, -3.0235e-02,\n               2.8692e-02,  1.2877e-02, -2.5210e-02,  1.8674e-02,  6.2285e-03,\n               7.6210e-04, -3.6594e-02, -5.2590e-02, -2.7551e-02,  1.3609e-02,\n              -1.0435e-02,  1.3852e-02, -9.2599e-03,  2.2184e-02,  3.0608e-02,\n              -1.6793e-04, -2.4481e-02, -1.3866e-02, -1.3391e-02, -3.7300e-02,\n              -3.9465e-02, -2.1893e-02, -6.2463e-03, -6.6974e-02,  3.4196e-02,\n               2.8268e-02, -1.7960e-02, -2.3021e-02, -2.2751e-02,  6.8731e-02,\n              -3.1730e-02, -3.3997e-02,  1.9770e-02,  3.9103e-03,  4.3017e-02,\n               7.2131e-04,  1.1437e-02,  4.5726e-03,  1.1767e-02, -1.2981e-03,\n              -4.9388e-03,  9.9993e-04, -2.6417e-03,  4.9291e-02, -1.2693e-02,\n               1.5768e-02,  4.9700e-02, -2.4115e-04, -3.5969e-02,  1.4909e-02,\n              -1.3162e-02,  1.0957e-02,  1.2448e-03, -2.5227e-02,  1.6234e-02,\n               1.9615e-02,  7.9707e-03,  4.9874e-03, -5.5871e-02,  1.4732e-02,\n              -2.2010e-03, -4.5504e-03, -6.0973e-04,  3.1973e-02,  2.3773e-02,\n              -3.3508e-02, -2.1556e-02,  1.4845e-02, -5.0383e-03, -1.2402e-02,\n              -2.9534e-02,  4.6065e-02,  2.0219e-02, -6.8031e-03,  3.9855e-02,\n              -2.4018e-02, -2.3109e-02,  7.1518e-03, -2.6902e-03, -2.8097e-02,\n              -1.1288e-02, -2.8673e-02,  1.6233e-02, -8.4872e-03,  4.4646e-02,\n              -7.2891e-02, -3.6205e-02,  1.9781e-02,  6.8998e-03, -2.4435e-02,\n              -4.4969e-04, -2.5626e-02,  1.0833e-02, -9.6826e-03, -3.9044e-02,\n              -4.7585e-02,  1.6841e-02,  2.0230e-02, -2.6004e-02,  1.8530e-02,\n               6.8847e-03, -1.6363e-02, -7.3379e-03,  4.3573e-02, -6.6164e-03,\n              -1.5791e-02,  5.2531e-02,  1.6309e-02,  3.2602e-02,  6.1390e-02,\n              -1.0379e-02,  3.2795e-03, -2.4365e-03, -4.8864e-03, -3.2693e-02,\n              -5.9389e-02, -3.3228e-02,  6.1740e-02, -4.3169e-02,  1.8025e-02,\n              -4.0732e-02, -2.7329e-02, -2.1269e-03, -1.2113e-03,  4.5510e-03,\n               4.4319e-02, -4.9131e-03,  3.3231e-02,  2.5377e-02,  1.2373e-02,\n               4.1446e-03,  1.7837e-02, -8.1315e-03,  1.2020e-02, -4.0757e-03,\n              -2.0918e-02, -4.2145e-03, -4.5413e-03,  1.2110e-02, -6.1347e-03,\n              -1.0400e-02,  6.8373e-03, -7.3053e-03,  2.1116e-02,  2.5039e-03,\n               6.2146e-02, -5.8740e-02,  1.0079e-03, -5.3459e-02,  4.1278e-02,\n               5.6115e-03, -2.5105e-02, -4.3527e-03,  1.2147e-03,  2.1023e-02,\n               1.1365e-02, -9.1522e-03, -2.7275e-02,  2.0869e-02, -9.3135e-03,\n              -1.3483e-02,  1.8452e-02,  4.8335e-02, -7.5421e-03, -2.0053e-02,\n              -5.0373e-02, -5.6819e-03,  1.1657e-03, -2.8150e-02, -7.7635e-02,\n              -2.0491e-03, -2.3842e-02,  7.9996e-04, -3.4063e-02, -2.4640e-02,\n              -4.3152e-02, -1.5629e-02, -1.7150e-02, -9.5633e-03, -3.4718e-03,\n              -1.6571e-03, -2.3351e-02, -3.5018e-02,  4.8038e-02, -9.6792e-05,\n               2.8303e-03, -1.4076e-02, -3.0676e-02,  3.6634e-02,  2.7456e-02,\n               2.8142e-02, -4.8791e-03,  2.4726e-02, -2.5887e-02, -2.9677e-02,\n              -2.8060e-02, -2.6422e-02,  2.2624e-02, -5.6217e-03,  2.4698e-02,\n               7.5682e-04,  1.2661e-02,  1.8645e-02,  2.7651e-02,  3.3632e-02,\n               3.4583e-02, -3.1673e-02,  1.8086e-02, -1.2074e-02,  1.1840e-02,\n               3.7852e-02, -7.9551e-04,  4.9057e-02, -2.0571e-02,  1.2246e-02,\n               8.2833e-03,  5.0676e-03,  2.3798e-03,  1.4377e-02, -2.4346e-02,\n              -3.0816e-02, -1.5586e-02, -1.7480e-02, -2.9293e-02,  2.4264e-02,\n              -1.4598e-03, -1.6441e-02,  1.1895e-02, -4.6739e-03, -4.0077e-03,\n              -1.2924e-02, -1.4647e-02,  4.9247e-02,  1.0715e-02,  5.8007e-02,\n              -3.0256e-02,  4.4003e-02,  2.3471e-02,  8.0279e-04,  1.5967e-02,\n               1.8470e-02, -2.4973e-02, -2.8990e-02,  4.6870e-03,  1.1402e-02,\n              -7.7548e-03, -1.0657e-02, -1.2363e-02, -9.1716e-03,  1.8832e-02,\n              -2.7268e-02,  7.3050e-03, -4.5445e-03, -2.1602e-02, -2.6673e-02,\n              -1.0606e-02, -3.5220e-02,  9.5738e-03, -5.4243e-02, -5.8489e-03,\n              -1.6217e-02, -3.4037e-02,  1.6348e-02,  1.5706e-02, -8.7913e-03,\n               2.7741e-02,  5.0502e-02, -7.8171e-03,  2.3470e-02,  4.4671e-02,\n               2.4857e-02, -1.2140e-02, -2.1498e-03, -4.5374e-03,  3.1720e-03,\n               2.1834e-02, -1.7623e-02, -7.1937e-03, -2.1432e-02,  2.1702e-02,\n               1.6780e-02, -3.4280e-03,  3.3537e-02,  3.1460e-03, -7.0628e-03,\n               1.8949e-02, -1.1449e-02, -1.3657e-02, -2.9581e-02, -6.3946e-02,\n               1.8845e-02,  1.0085e-02, -1.6708e-02,  1.0029e-02, -1.5238e-02,\n              -2.8994e-03,  3.0602e-02, -7.4864e-03, -1.6037e-02,  3.1860e-03,\n              -1.9091e-03,  1.3591e-02, -1.2185e-02, -8.8482e-03, -2.3641e-03,\n               1.0944e-02, -1.6544e-02, -3.3119e-02,  1.8619e-03,  3.2193e-02,\n              -1.4357e-02,  2.4508e-04, -1.7290e-02, -3.6547e-02,  7.1818e-03,\n               2.0343e-02,  3.2935e-02,  2.3665e-03,  6.5819e-03,  6.0990e-02,\n               3.4074e-02, -2.7457e-02, -1.4486e-02,  2.4217e-02,  1.5525e-02,\n              -2.2878e-02,  5.9757e-03, -3.7943e-02,  2.7684e-02,  4.5964e-02,\n               5.4669e-02, -1.9094e-02,  6.0574e-03, -2.5773e-02, -5.3982e-03,\n              -3.2633e-02, -3.5658e-02, -7.4466e-03,  2.4440e-02,  1.4330e-02,\n              -1.2871e-03,  1.0722e-02, -2.8781e-03,  5.8574e-02, -3.9182e-02,\n               1.0405e-02,  1.4426e-02,  2.1897e-03, -2.8065e-02,  5.8154e-02,\n              -4.1799e-02, -3.7809e-03,  1.5610e-02, -4.0148e-02, -1.1391e-02,\n              -4.3069e-04, -1.4897e-02,  2.0946e-02,  2.0793e-02,  4.0367e-02,\n               2.4919e-02, -1.1185e-04,  4.0436e-04,  4.1949e-03,  1.1090e-02,\n               1.1855e-02,  2.4736e-02, -1.7674e-02,  2.5706e-02,  4.5361e-02,\n               2.4859e-02,  3.1759e-02, -3.3006e-03,  2.2518e-02,  2.2525e-02,\n               5.4664e-02,  1.5598e-02, -1.9653e-02, -1.4111e-02,  1.3262e-02,\n               8.8081e-04,  2.8604e-02, -2.1679e-02, -3.1372e-02, -2.2626e-02,\n              -2.8424e-03, -1.4960e-02, -1.9621e-02, -1.3347e-03, -2.6654e-02,\n              -1.3236e-02, -3.0510e-02, -4.0548e-03,  4.9067e-02,  4.1763e-02,\n               2.5354e-02, -3.7586e-02, -3.2298e-03, -4.0183e-02, -2.5603e-02,\n               5.0868e-03,  4.9451e-03,  7.2171e-03,  1.4350e-02, -4.4224e-03,\n              -3.7434e-02, -1.0269e-02, -3.6149e-02, -2.0079e-02, -5.3644e-02,\n              -1.8145e-02, -1.1986e-02,  5.3894e-02, -1.2089e-02, -5.0108e-03,\n              -3.3951e-03,  4.6005e-03,  3.2236e-02, -1.4550e-02, -1.6336e-02,\n              -1.8251e-03, -1.3285e-02,  9.4945e-03, -5.2830e-03,  7.8553e-04,\n               9.9613e-03, -1.2691e-02, -1.3238e-02, -4.5462e-02,  3.6246e-02,\n               7.2569e-03, -5.0458e-04,  2.1248e-02,  4.0374e-02, -6.6869e-02,\n              -6.5121e-03,  1.3268e-02, -1.2948e-02, -4.1399e-02, -3.3843e-02,\n               1.2495e-02,  7.5184e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9101, 0.9101, 0.9067, 0.9119, 0.9064, 0.9090, 0.9083, 0.9086, 0.9105,\n              0.9079, 0.9082, 0.9097, 0.9081, 0.9100, 0.9074, 0.9097, 0.9094, 0.9080,\n              0.9083, 0.9072, 0.9065, 0.9091, 0.9064, 0.9070, 0.9081, 0.9113, 0.9085,\n              0.9088, 0.9070, 0.9073, 0.9071, 0.9091, 0.9094, 0.9086, 0.9077, 0.9093,\n              0.9089, 0.9119, 0.9075, 0.9066, 0.9113, 0.9084, 0.9084, 0.9083, 0.9074,\n              0.9085, 0.9128, 0.9073, 0.9078, 0.9098, 0.9071, 0.9093, 0.9074, 0.9171,\n              0.9063, 0.9129, 0.9087, 0.9069, 0.9146, 0.9078, 0.9101, 0.9080, 0.9115,\n              0.9079, 0.9133, 0.9083, 0.9082, 0.9098, 0.9119, 0.9079, 0.9073, 0.9073,\n              0.9077, 0.9073, 0.9087, 0.9065, 0.9074, 0.9106, 0.9068, 0.9089, 0.9091,\n              0.9082, 0.9075, 0.9087, 0.9065, 0.9076, 0.9082, 0.9070, 0.9099, 0.9078,\n              0.9084, 0.9110, 0.9077, 0.9098, 0.9083, 0.9115, 0.9090, 0.9064, 0.9090,\n              0.9077, 0.9072, 0.9094, 0.9083, 0.9102, 0.9089, 0.9077, 0.9097, 0.9082,\n              0.9069, 0.9077, 0.9076, 0.9069, 0.9082, 0.9071, 0.9084, 0.9065, 0.9107,\n              0.9072, 0.9114, 0.9071, 0.9078, 0.9106, 0.9081, 0.9088, 0.9077, 0.9084,\n              0.9074, 0.9071, 0.9069, 0.9094, 0.9074, 0.9079, 0.9070, 0.9085, 0.9086,\n              0.9160, 0.9057, 0.9078, 0.9077, 0.9080, 0.9096, 0.9087, 0.9109, 0.9099,\n              0.9085, 0.9098, 0.9067, 0.9083, 0.9075, 0.9077, 0.9092, 0.9059, 0.9090,\n              0.9078, 0.9083, 0.9179, 0.9104, 0.9100, 0.9078, 0.9109, 0.9100, 0.9105,\n              0.9077, 0.9080, 0.9075, 0.9084, 0.9077, 0.9075, 0.9088, 0.9090, 0.9101,\n              0.9063, 0.9087, 0.9063, 0.9071, 0.9078, 0.9116, 0.9062, 0.9095, 0.9105,\n              0.9076, 0.9084, 0.9069, 0.9107, 0.9072, 0.9112, 0.9088, 0.9136, 0.9074,\n              0.9119, 0.9086, 0.9084, 0.9084, 0.9088, 0.9076, 0.9087, 0.9083, 0.9083,\n              0.9067, 0.9057, 0.9118, 0.9076, 0.9081, 0.9099, 0.9071, 0.9071, 0.9076,\n              0.9090, 0.9091, 0.9077, 0.9057, 0.9075, 0.9101, 0.9093, 0.9099, 0.9097,\n              0.9094, 0.9071, 0.9119, 0.9090, 0.9086, 0.9065, 0.9124, 0.9084, 0.9106,\n              0.9058, 0.9071, 0.9103, 0.9065, 0.9079, 0.9079, 0.9073, 0.9074, 0.9096,\n              0.9066, 0.9076, 0.9094, 0.9074, 0.9079, 0.9153, 0.9088, 0.9091, 0.9087,\n              0.9111, 0.9083, 0.9102, 0.9099, 0.9086, 0.9111, 0.9113, 0.9086, 0.9077,\n              0.9101, 0.9075, 0.9078, 0.9084, 0.9098, 0.9101, 0.9123, 0.9076, 0.9079,\n              0.9074, 0.9084, 0.9090, 0.9085, 0.9082, 0.9107, 0.9106, 0.9083, 0.9086,\n              0.9082, 0.9087, 0.9093, 0.9070, 0.9079, 0.9102, 0.9088, 0.9074, 0.9099,\n              0.9069, 0.9096, 0.9068, 0.9176, 0.9086, 0.9117, 0.9076, 0.9106, 0.9066,\n              0.9088, 0.9076, 0.9106, 0.9087, 0.9084, 0.9128, 0.9088, 0.9081, 0.9068,\n              0.9089, 0.9094, 0.9105, 0.9077, 0.9073, 0.9091, 0.9079, 0.9117, 0.9115,\n              0.9090, 0.9070, 0.9078, 0.9094, 0.9066, 0.9098, 0.9100, 0.9089, 0.9081,\n              0.9073, 0.9070, 0.9131, 0.9072, 0.9075, 0.9088, 0.9079, 0.9083, 0.9077,\n              0.9105, 0.9078, 0.9122, 0.9084, 0.9084, 0.9096, 0.9083, 0.9092, 0.9073,\n              0.9082, 0.9081, 0.9077, 0.9102, 0.9092, 0.9081, 0.9106, 0.9095, 0.9091,\n              0.9067, 0.9069, 0.9091, 0.9088, 0.9077, 0.9100, 0.9068, 0.9078, 0.9087,\n              0.9076, 0.9094, 0.9092, 0.9070, 0.9102, 0.9085, 0.9084, 0.9087, 0.9091,\n              0.9081, 0.9073, 0.9086, 0.9080, 0.9096, 0.9076, 0.9125, 0.9090, 0.9087,\n              0.9067, 0.9072, 0.9094, 0.9091, 0.9120, 0.9091, 0.9100, 0.9104, 0.9087,\n              0.9081, 0.9085, 0.9076, 0.9084, 0.9122, 0.9101, 0.9092, 0.9117, 0.9075,\n              0.9084, 0.9093, 0.9088, 0.9080, 0.9073, 0.9072, 0.9072, 0.9070, 0.9087,\n              0.9100, 0.9081, 0.9081, 0.9107, 0.9084, 0.9076, 0.9087, 0.9072, 0.9072,\n              0.9097, 0.9090, 0.9086, 0.9097, 0.9089, 0.9079, 0.9067, 0.9093, 0.9152,\n              0.9134, 0.9094, 0.9083, 0.9091, 0.9082, 0.9106, 0.9088, 0.9070, 0.9076,\n              0.9079, 0.9094, 0.9083, 0.9087, 0.9079, 0.9096, 0.9091, 0.9074, 0.9081,\n              0.9077, 0.9078, 0.9071, 0.9081, 0.9103, 0.9085, 0.9080, 0.9084, 0.9093,\n              0.9092, 0.9080, 0.9118, 0.9089, 0.9090, 0.9077, 0.9096, 0.9074, 0.9102,\n              0.9091, 0.9097, 0.9131, 0.9126, 0.9101, 0.9103, 0.9077, 0.9113, 0.9074,\n              0.9084, 0.9097, 0.9118, 0.9072, 0.9107, 0.9094, 0.9068, 0.9076, 0.9074,\n              0.9111, 0.9118, 0.9087, 0.9075, 0.9082, 0.9099, 0.9106, 0.9080, 0.9061,\n              0.9114, 0.9068, 0.9101, 0.9082, 0.9081, 0.9070, 0.9082, 0.9077, 0.9076,\n              0.9080, 0.9114, 0.9070, 0.9065, 0.9119, 0.9066, 0.9104, 0.9086, 0.9082,\n              0.9101, 0.9080, 0.9063, 0.9087, 0.9085, 0.9093, 0.9083, 0.9090, 0.9084,\n              0.9124, 0.9083, 0.9114, 0.9109, 0.9080, 0.9068, 0.9124, 0.9058],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 6.5867e-03, -9.3285e-03,  1.2821e-02],\n                [-1.2397e-02,  9.6715e-03, -2.6805e-03],\n                [-6.6004e-03, -2.0827e-03, -8.5107e-03]],\n      \n               [[-9.6296e-03, -6.9987e-03,  1.4352e-02],\n                [-9.7529e-03,  1.0692e-02, -2.9065e-03],\n                [-6.7072e-03, -1.8343e-03,  3.3515e-03]],\n      \n               [[-3.9623e-03,  1.3051e-02,  8.5097e-03],\n                [ 1.2955e-02, -1.1219e-02, -6.6977e-05],\n                [-5.0361e-04,  2.7297e-03, -9.6871e-03]],\n      \n               ...,\n      \n               [[ 1.1918e-02,  6.5557e-03,  1.1636e-02],\n                [ 1.1169e-02,  6.7308e-03, -1.3678e-02],\n                [-1.0713e-02, -4.8400e-03,  6.0387e-03]],\n      \n               [[-7.2308e-03,  1.0483e-03,  9.6820e-03],\n                [-8.5026e-03,  6.1583e-03,  6.1258e-03],\n                [-1.0096e-02,  3.6022e-03,  6.9328e-03]],\n      \n               [[-1.0333e-02, -4.8481e-03,  1.2605e-02],\n                [ 9.3647e-03, -1.2839e-02, -1.4102e-02],\n                [ 1.7134e-03, -3.0223e-03,  1.5940e-03]]],\n      \n      \n              [[[ 3.1037e-03,  5.9130e-03, -4.7895e-04],\n                [ 3.5826e-03,  4.7949e-03, -5.5185e-03],\n                [ 3.5461e-03,  1.1705e-02,  1.3171e-02]],\n      \n               [[ 6.3705e-03,  4.0516e-03,  4.4965e-03],\n                [ 1.1104e-02, -1.0746e-02,  9.6030e-03],\n                [-1.7872e-03, -1.1259e-02, -3.9642e-03]],\n      \n               [[-4.8973e-03, -3.4648e-03,  9.3627e-03],\n                [-7.1370e-03,  5.2624e-03, -1.0188e-02],\n                [ 1.1116e-02, -6.6987e-03,  6.5327e-03]],\n      \n               ...,\n      \n               [[-7.2571e-03, -1.2089e-02, -3.9621e-03],\n                [ 7.3647e-03,  1.0714e-02,  8.0385e-03],\n                [ 4.4700e-03,  2.3275e-03,  1.2301e-02]],\n      \n               [[ 7.1026e-03, -6.8469e-03, -1.0678e-02],\n                [ 7.3807e-03,  2.8257e-03,  4.1661e-03],\n                [ 1.0029e-02, -6.8902e-03, -1.2080e-02]],\n      \n               [[ 4.4296e-03,  1.2516e-02, -9.7447e-03],\n                [-6.1660e-03, -8.7815e-03,  5.2025e-03],\n                [ 5.1065e-03,  1.4141e-02, -1.3333e-02]]],\n      \n      \n              [[[-7.3599e-03, -4.0086e-03,  1.0977e-02],\n                [ 1.0660e-02,  1.3133e-02, -3.1585e-04],\n                [ 7.7979e-03,  1.2479e-02,  1.2150e-02]],\n      \n               [[ 8.7809e-03, -1.1691e-02,  7.7846e-04],\n                [ 3.4797e-03, -9.2159e-03,  6.4221e-03],\n                [ 1.4473e-02, -7.9867e-03, -1.3923e-02]],\n      \n               [[ 8.2413e-03,  2.8716e-03, -1.4307e-02],\n                [ 9.4359e-03, -1.4374e-02, -8.7126e-03],\n                [ 4.3346e-03, -8.2446e-03,  4.6342e-03]],\n      \n               ...,\n      \n               [[-7.1676e-03,  3.8106e-03,  8.5436e-03],\n                [ 3.9801e-03, -4.0333e-03,  1.0945e-02],\n                [ 7.4128e-04,  5.0691e-03,  1.3421e-02]],\n      \n               [[-2.5690e-03, -1.2510e-02, -1.2145e-02],\n                [ 1.2746e-02,  2.2833e-03, -1.0634e-02],\n                [ 1.0469e-02, -1.2487e-02, -1.6995e-03]],\n      \n               [[-1.2252e-02,  1.3929e-02, -8.3631e-03],\n                [ 2.0571e-03,  4.5514e-03, -1.1894e-02],\n                [ 7.2202e-03,  4.5179e-04, -1.0545e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[-2.8471e-03, -9.1987e-05, -7.9942e-03],\n                [ 8.4434e-04,  2.8804e-03, -1.3499e-02],\n                [-1.9597e-03,  1.4272e-03,  1.2538e-02]],\n      \n               [[-1.0424e-02, -1.4042e-02,  7.8074e-03],\n                [ 1.1806e-02, -5.7267e-03, -8.5790e-03],\n                [-1.3391e-02, -1.3421e-02, -2.9534e-03]],\n      \n               [[ 7.4649e-03, -8.6052e-03,  4.0945e-03],\n                [ 8.8478e-03, -9.1367e-03, -4.1791e-03],\n                [ 6.0027e-03, -1.3632e-02,  6.3724e-03]],\n      \n               ...,\n      \n               [[-9.3883e-03,  5.3048e-03,  1.5672e-05],\n                [ 8.9749e-03,  3.0801e-03,  5.8035e-03],\n                [ 1.2381e-02,  1.2998e-02,  4.0825e-03]],\n      \n               [[-1.2238e-02,  6.4927e-03, -2.3827e-03],\n                [ 1.3721e-02, -1.2688e-02,  4.3811e-03],\n                [ 7.5916e-03, -1.2963e-02, -5.6602e-03]],\n      \n               [[ 7.9940e-03,  1.1749e-02,  4.9927e-03],\n                [ 1.2743e-02, -6.6501e-03, -8.4978e-04],\n                [ 4.1099e-03, -1.2117e-02, -7.4944e-03]]],\n      \n      \n              [[[ 1.0025e-02,  4.8788e-03, -2.8862e-03],\n                [ 1.0511e-03, -1.3907e-02, -2.6894e-04],\n                [-9.1575e-03,  3.9045e-03, -1.8527e-04]],\n      \n               [[ 1.1200e-02, -1.2451e-02,  9.7673e-04],\n                [-8.0052e-03, -3.5207e-03,  1.0887e-02],\n                [-1.3372e-03,  9.3524e-03,  3.7458e-03]],\n      \n               [[-3.7663e-03, -1.2788e-02, -2.6949e-03],\n                [ 9.9037e-03,  6.0956e-03,  7.8091e-03],\n                [-1.4307e-02,  5.8415e-03,  1.3055e-02]],\n      \n               ...,\n      \n               [[ 7.9148e-03,  1.4173e-02, -1.1187e-03],\n                [-1.0414e-02,  1.4104e-02,  8.2093e-03],\n                [ 5.1929e-03, -6.8134e-03, -1.4020e-02]],\n      \n               [[ 1.1369e-02,  5.0497e-03,  5.8853e-03],\n                [ 2.4915e-03,  7.7691e-06, -1.3733e-02],\n                [-9.3692e-03, -1.1703e-02,  6.3505e-03]],\n      \n               [[ 4.6301e-04, -1.3261e-02,  3.7060e-03],\n                [ 1.4299e-02, -3.7127e-03, -8.2253e-03],\n                [ 1.0573e-02, -1.0588e-02, -9.0298e-03]]],\n      \n      \n              [[[-2.8317e-03,  1.6703e-03, -6.5878e-03],\n                [ 7.9191e-04, -1.3022e-02,  3.0497e-03],\n                [ 1.2333e-02,  1.2150e-02, -1.1334e-02]],\n      \n               [[-7.7652e-03, -1.9253e-03,  1.0051e-02],\n                [ 2.5817e-03,  1.1876e-02, -7.4185e-03],\n                [ 7.0576e-04,  5.9186e-03,  1.2437e-02]],\n      \n               [[ 9.4458e-03,  2.6142e-03,  5.5114e-03],\n                [-7.5637e-03, -1.0736e-02, -9.8745e-03],\n                [ 5.2135e-03, -1.4668e-02,  2.1663e-03]],\n      \n               ...,\n      \n               [[-3.0726e-03,  1.4107e-02, -2.9289e-03],\n                [-3.0707e-03,  1.2472e-02,  9.2475e-04],\n                [ 1.3155e-02, -1.3363e-02, -1.1516e-02]],\n      \n               [[ 8.7279e-03,  2.8681e-03, -1.4557e-02],\n                [-1.2566e-02,  1.3102e-02,  4.3552e-03],\n                [-4.3440e-03, -4.0219e-03, -1.0429e-02]],\n      \n               [[-1.3517e-02, -9.5544e-03,  1.1058e-02],\n                [ 8.9000e-03,  6.2739e-04,  1.2877e-02],\n                [ 1.1451e-02, -1.0362e-02,  4.8543e-03]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-1.2222e-03,  1.1090e-02,  3.7180e-04, -1.1000e-03,  8.4765e-03,\n              -2.5759e-03,  6.8241e-03,  2.8199e-03,  5.6535e-03,  5.6590e-03,\n              -3.8835e-04,  4.1998e-03, -4.6723e-03,  8.1772e-04, -2.9402e-03,\n               6.5983e-03, -5.8483e-04, -7.7734e-03,  1.0281e-02, -1.2602e-02,\n              -2.9749e-03,  5.7229e-03,  1.0963e-03,  4.2494e-03,  2.9583e-03,\n              -7.4302e-03,  6.1446e-03, -1.7178e-02,  3.4523e-03, -8.6486e-03,\n               3.9669e-03,  3.0949e-03, -7.5590e-03,  9.2260e-03, -3.1862e-03,\n              -1.2194e-02,  1.0801e-02, -1.0322e-03, -1.0638e-02,  4.8946e-03,\n              -8.2729e-03,  7.5456e-03, -5.8665e-03, -1.0152e-02,  4.8971e-03,\n               2.4595e-03, -7.3020e-03, -5.0757e-05,  7.1188e-03,  1.6316e-03,\n              -4.8006e-04, -5.1260e-03, -2.4803e-04,  6.2442e-03,  5.8742e-03,\n               5.9899e-03,  5.8700e-03, -8.1678e-03,  6.9464e-03, -1.7911e-02,\n               3.9551e-03, -3.7996e-03,  1.8523e-03,  1.2159e-02, -3.1262e-03,\n              -2.9443e-03, -1.1705e-02, -5.3309e-03, -7.0191e-03,  2.9604e-03,\n              -5.9351e-03,  1.2610e-02, -1.3821e-02, -5.5497e-03,  6.4379e-03,\n              -1.2294e-02, -2.1921e-03,  8.8271e-03,  1.2784e-02,  9.6390e-03,\n              -5.0020e-04, -6.3127e-03,  1.2190e-02,  1.1177e-02, -8.1882e-03,\n               1.1547e-02,  1.1223e-02,  1.0473e-02,  5.5900e-04, -2.0928e-03,\n               8.3745e-04, -2.1941e-03, -6.4465e-03,  2.2991e-03,  2.4235e-03,\n              -6.2075e-03,  1.1024e-02,  1.0461e-02,  9.2354e-03,  1.4009e-02,\n               8.4091e-03, -8.4866e-04, -2.7962e-03,  2.6665e-03,  6.9627e-03,\n              -2.2656e-04, -2.3685e-02,  5.7283e-03,  3.8104e-03,  7.3111e-03,\n               4.1472e-04,  4.3388e-03, -2.3534e-03, -4.3267e-03, -2.0232e-03,\n               2.8755e-03,  5.0452e-03, -2.7228e-03,  3.6021e-03, -1.1546e-02,\n               8.1064e-03, -3.7941e-04, -2.5653e-03, -7.6184e-03,  8.3692e-03,\n              -5.7391e-03,  9.7461e-03, -1.2428e-02, -1.1345e-02,  5.0449e-04,\n               3.0711e-03,  6.9548e-03, -9.8186e-03,  9.7956e-03, -3.2581e-03,\n              -2.2292e-03,  7.8355e-03,  6.8844e-03,  1.4264e-02, -1.4810e-03,\n              -1.2908e-02, -3.2924e-03, -4.6968e-05, -4.5429e-03, -1.2465e-02,\n              -1.0614e-02,  1.7452e-02,  4.5223e-03, -1.5845e-03,  1.0812e-02,\n              -1.2815e-02, -2.1471e-03,  1.1566e-03,  4.3349e-03,  1.8134e-02,\n              -4.2483e-03,  1.3362e-02,  4.2825e-03, -4.6646e-03, -8.6999e-03,\n               2.0726e-03, -1.0126e-02, -4.8468e-03, -5.1314e-03,  1.6218e-03,\n               1.9393e-03, -4.9619e-03,  6.2659e-03, -4.7262e-03, -2.0494e-03,\n              -9.7256e-03, -6.2872e-03,  5.7377e-03, -1.1928e-02, -4.1802e-03,\n              -1.0507e-02,  1.0851e-03, -1.4572e-02, -1.2037e-02,  1.3702e-03,\n              -1.5336e-02, -6.2101e-03, -2.5402e-03,  5.2380e-03, -9.1612e-03,\n               1.0350e-03,  4.6345e-03, -1.0673e-02, -4.4452e-03,  5.2840e-04,\n              -1.3621e-02,  4.3251e-03, -4.4356e-03, -7.2761e-04, -2.6887e-03,\n               2.9859e-03, -6.3804e-03,  1.3291e-02,  9.1413e-03,  2.0206e-03,\n              -4.6036e-03, -2.9297e-03, -4.6543e-04, -3.7646e-03, -3.0259e-03,\n              -5.3076e-03,  1.1694e-03,  1.3670e-02,  3.7265e-03, -8.8811e-04,\n              -9.2042e-03,  4.0748e-03,  1.0608e-03,  1.1646e-03, -1.6553e-03,\n              -1.7154e-03, -8.8167e-03, -1.1133e-03, -1.1663e-02,  3.1885e-03,\n               8.1645e-03,  5.9787e-03,  7.4704e-03, -1.2480e-02,  5.3680e-03,\n               1.6086e-02,  1.7938e-02, -1.0310e-02,  8.1363e-03, -2.8537e-03,\n              -1.6266e-02, -5.6541e-03,  4.6615e-04,  8.1515e-03,  9.3719e-04,\n              -1.4403e-02, -2.2005e-03, -2.5685e-03, -8.1574e-03, -4.8290e-04,\n              -1.1757e-02, -3.2049e-03, -8.8152e-03, -2.2573e-03,  1.0515e-02,\n               4.2052e-03, -3.5166e-03, -2.8934e-04,  2.4982e-03, -1.5867e-04,\n               4.0522e-04, -4.9313e-03,  6.2010e-03,  6.7818e-04, -3.6924e-03,\n              -6.8646e-04, -1.2489e-02, -1.0083e-03, -8.5467e-03,  4.0290e-03,\n               5.9018e-03,  2.0100e-04, -4.7925e-03, -1.0725e-02, -5.0448e-03,\n               5.7314e-03, -1.5915e-02, -8.6452e-03,  7.5777e-04, -2.5600e-03,\n              -1.2144e-02, -1.5918e-03, -1.0627e-02,  1.3071e-03,  3.4096e-03,\n              -9.3706e-03,  1.1535e-02, -8.9023e-03,  2.2656e-03,  1.6594e-02,\n               3.9840e-03, -2.9326e-03, -1.1248e-02, -5.8721e-03,  1.2326e-03,\n              -2.7790e-03,  1.3242e-04,  6.0744e-03, -8.1788e-03,  8.5976e-03,\n               8.5414e-03, -7.9670e-03,  6.2964e-03, -1.1230e-02,  1.3161e-02,\n               3.0681e-03, -4.9960e-03,  7.2027e-03,  5.4250e-03, -6.2281e-03,\n              -4.0603e-03, -1.9192e-04,  1.6491e-02, -3.6261e-03, -4.1259e-03,\n              -5.9182e-03, -6.0434e-03, -2.3565e-03,  2.5017e-03, -8.4098e-03,\n               5.3521e-03, -9.1274e-05,  1.3279e-03,  2.7808e-03,  2.2991e-03,\n              -3.3621e-03, -1.1061e-02,  1.0655e-03,  1.3452e-03,  1.1429e-04,\n              -3.3309e-03,  8.5607e-03, -6.8251e-03,  2.6155e-03, -8.7613e-04,\n               1.2647e-02,  1.5235e-03, -3.5168e-03, -1.4296e-03, -2.1293e-02,\n               3.4881e-03, -1.1772e-03,  1.6986e-03,  3.1403e-03,  9.9168e-03,\n               8.7866e-04, -1.3353e-02,  1.6749e-02,  6.7202e-03, -4.7949e-03,\n              -3.1456e-03,  7.8375e-03, -8.8589e-04, -6.3111e-03,  8.0656e-03,\n               3.9616e-03,  2.1133e-04,  7.0170e-04, -3.0645e-03, -4.7876e-03,\n               1.2793e-02, -2.3669e-03, -1.5544e-03,  2.5536e-03, -5.2940e-03,\n              -1.0633e-03, -4.3619e-03, -4.6990e-03,  5.6369e-03,  4.1405e-03,\n               1.3036e-03, -8.7548e-03,  1.1465e-02, -6.2913e-03, -6.7059e-04,\n              -1.6573e-03,  2.5494e-03, -1.1689e-02, -6.7008e-03, -2.5461e-03,\n              -1.1628e-03,  2.8626e-03, -1.4085e-03, -1.0869e-02,  3.2819e-03,\n              -4.0741e-03,  2.1066e-03,  5.6294e-03, -5.5127e-03,  6.2833e-03,\n              -1.4082e-02, -4.0968e-03, -9.3173e-03,  4.9037e-03, -1.2664e-03,\n              -8.1741e-03, -9.2395e-03, -7.5405e-03,  1.6475e-03,  2.9875e-03,\n              -1.3434e-02, -8.1073e-04, -5.3276e-03,  2.5372e-03, -1.5661e-02,\n               1.1391e-04,  6.5464e-04, -5.2929e-03,  1.7677e-03, -1.9621e-03,\n               9.7048e-05,  3.6639e-04,  9.4798e-03, -1.1155e-02, -9.4631e-03,\n               6.7425e-03,  2.9426e-04,  6.0171e-03, -5.7327e-03,  1.0542e-02,\n               8.0432e-03,  2.8337e-03,  1.2012e-02, -4.3819e-04, -7.9580e-04,\n              -4.5769e-03, -2.6121e-02,  1.2465e-02, -8.0777e-03, -3.0041e-03,\n               1.2624e-04, -1.4102e-02, -1.0900e-02, -7.9572e-03,  4.7040e-03,\n               7.8784e-03, -1.7544e-03,  8.8291e-03, -3.4330e-03, -7.4561e-03,\n              -2.4704e-03, -2.0461e-03,  1.0628e-02,  4.1941e-03,  8.4064e-03,\n               3.7866e-03, -5.5236e-03,  2.3767e-03,  7.9217e-04, -6.9935e-03,\n              -9.1281e-04, -1.4202e-02,  7.1072e-04, -8.3217e-04,  6.7162e-03,\n              -1.1353e-02, -4.2940e-03, -1.1621e-02,  5.3469e-03, -1.0711e-02,\n               3.8544e-04,  3.8907e-03,  9.2707e-03,  3.9062e-03, -1.2212e-02,\n              -5.8612e-04, -7.2669e-03,  6.6165e-04, -9.5546e-03,  4.6906e-03,\n               1.1969e-03, -1.0713e-02, -3.1856e-03,  7.6668e-03,  5.5755e-03,\n               2.5381e-03,  5.2616e-03, -2.1793e-03,  1.0777e-02, -5.0660e-03,\n               4.1887e-03,  4.6802e-03,  5.2993e-03,  9.9548e-03, -6.7991e-03,\n              -3.7859e-03,  1.3845e-02,  3.6774e-03, -3.9713e-03,  4.2620e-03,\n              -1.1963e-02,  1.5687e-02,  1.0991e-04, -2.7160e-03, -2.9270e-03,\n              -2.2810e-03, -6.5969e-03, -9.4726e-03, -1.0697e-02,  8.0606e-03,\n               2.0389e-03,  2.7073e-03, -5.0433e-03,  1.8584e-03, -1.7025e-02,\n               4.7483e-03, -6.9877e-03,  6.3881e-04, -1.2056e-04,  4.1177e-03,\n               1.8389e-02,  5.1034e-03,  1.6296e-02, -5.6222e-03,  1.7780e-03,\n              -2.7191e-03, -3.7531e-03,  8.0507e-03, -6.9440e-03,  2.4436e-03,\n              -4.6193e-03,  9.4523e-04], grad_fn=<AddBackward0>), self.running_var=tensor([0.9011, 0.9013, 0.9014, 0.9011, 0.9015, 0.9012, 0.9011, 0.9016, 0.9011,\n              0.9010, 0.9011, 0.9012, 0.9011, 0.9012, 0.9012, 0.9013, 0.9010, 0.9013,\n              0.9019, 0.9018, 0.9013, 0.9017, 0.9017, 0.9012, 0.9014, 0.9012, 0.9016,\n              0.9014, 0.9013, 0.9015, 0.9015, 0.9012, 0.9010, 0.9014, 0.9012, 0.9015,\n              0.9011, 0.9015, 0.9011, 0.9012, 0.9013, 0.9018, 0.9016, 0.9015, 0.9012,\n              0.9012, 0.9011, 0.9011, 0.9014, 0.9014, 0.9013, 0.9015, 0.9013, 0.9011,\n              0.9012, 0.9017, 0.9010, 0.9013, 0.9016, 0.9015, 0.9013, 0.9012, 0.9013,\n              0.9012, 0.9014, 0.9010, 0.9012, 0.9010, 0.9015, 0.9013, 0.9012, 0.9017,\n              0.9016, 0.9014, 0.9013, 0.9014, 0.9013, 0.9013, 0.9014, 0.9010, 0.9013,\n              0.9010, 0.9013, 0.9016, 0.9012, 0.9010, 0.9012, 0.9012, 0.9011, 0.9011,\n              0.9010, 0.9013, 0.9018, 0.9013, 0.9014, 0.9011, 0.9010, 0.9012, 0.9016,\n              0.9012, 0.9011, 0.9012, 0.9016, 0.9017, 0.9012, 0.9016, 0.9020, 0.9014,\n              0.9016, 0.9013, 0.9010, 0.9012, 0.9011, 0.9012, 0.9014, 0.9013, 0.9010,\n              0.9011, 0.9015, 0.9016, 0.9010, 0.9012, 0.9016, 0.9011, 0.9020, 0.9012,\n              0.9013, 0.9014, 0.9015, 0.9011, 0.9012, 0.9012, 0.9013, 0.9012, 0.9011,\n              0.9013, 0.9013, 0.9012, 0.9015, 0.9010, 0.9011, 0.9014, 0.9010, 0.9012,\n              0.9011, 0.9016, 0.9016, 0.9014, 0.9011, 0.9016, 0.9016, 0.9013, 0.9016,\n              0.9014, 0.9012, 0.9011, 0.9012, 0.9011, 0.9010, 0.9014, 0.9010, 0.9011,\n              0.9013, 0.9011, 0.9010, 0.9011, 0.9010, 0.9011, 0.9017, 0.9014, 0.9014,\n              0.9011, 0.9009, 0.9012, 0.9011, 0.9012, 0.9014, 0.9013, 0.9012, 0.9010,\n              0.9011, 0.9013, 0.9011, 0.9013, 0.9011, 0.9013, 0.9011, 0.9013, 0.9013,\n              0.9017, 0.9012, 0.9012, 0.9012, 0.9013, 0.9012, 0.9016, 0.9015, 0.9013,\n              0.9011, 0.9014, 0.9011, 0.9014, 0.9013, 0.9013, 0.9015, 0.9012, 0.9010,\n              0.9019, 0.9012, 0.9010, 0.9011, 0.9015, 0.9012, 0.9017, 0.9011, 0.9013,\n              0.9017, 0.9014, 0.9015, 0.9012, 0.9013, 0.9010, 0.9012, 0.9017, 0.9013,\n              0.9012, 0.9014, 0.9012, 0.9014, 0.9012, 0.9010, 0.9013, 0.9011, 0.9015,\n              0.9014, 0.9020, 0.9014, 0.9014, 0.9015, 0.9013, 0.9015, 0.9011, 0.9013,\n              0.9011, 0.9011, 0.9013, 0.9010, 0.9009, 0.9012, 0.9013, 0.9012, 0.9011,\n              0.9011, 0.9012, 0.9008, 0.9012, 0.9011, 0.9011, 0.9014, 0.9011, 0.9013,\n              0.9012, 0.9012, 0.9015, 0.9011, 0.9010, 0.9014, 0.9014, 0.9012, 0.9012,\n              0.9014, 0.9013, 0.9017, 0.9011, 0.9011, 0.9012, 0.9012, 0.9014, 0.9013,\n              0.9012, 0.9011, 0.9011, 0.9013, 0.9011, 0.9014, 0.9014, 0.9010, 0.9012,\n              0.9015, 0.9012, 0.9016, 0.9019, 0.9018, 0.9015, 0.9011, 0.9010, 0.9012,\n              0.9014, 0.9012, 0.9010, 0.9013, 0.9013, 0.9012, 0.9011, 0.9012, 0.9013,\n              0.9011, 0.9013, 0.9013, 0.9010, 0.9011, 0.9012, 0.9014, 0.9012, 0.9012,\n              0.9012, 0.9012, 0.9011, 0.9013, 0.9012, 0.9010, 0.9011, 0.9011, 0.9010,\n              0.9016, 0.9017, 0.9011, 0.9014, 0.9015, 0.9018, 0.9013, 0.9013, 0.9012,\n              0.9015, 0.9013, 0.9012, 0.9017, 0.9012, 0.9010, 0.9011, 0.9014, 0.9013,\n              0.9014, 0.9013, 0.9013, 0.9013, 0.9010, 0.9012, 0.9012, 0.9017, 0.9011,\n              0.9014, 0.9012, 0.9012, 0.9011, 0.9012, 0.9012, 0.9011, 0.9012, 0.9010,\n              0.9013, 0.9012, 0.9012, 0.9013, 0.9012, 0.9013, 0.9013, 0.9017, 0.9011,\n              0.9011, 0.9014, 0.9012, 0.9010, 0.9010, 0.9013, 0.9014, 0.9011, 0.9011,\n              0.9011, 0.9014, 0.9013, 0.9013, 0.9013, 0.9013, 0.9012, 0.9012, 0.9020,\n              0.9013, 0.9014, 0.9012, 0.9012, 0.9013, 0.9013, 0.9013, 0.9015, 0.9010,\n              0.9011, 0.9013, 0.9010, 0.9014, 0.9012, 0.9012, 0.9016, 0.9015, 0.9011,\n              0.9012, 0.9012, 0.9011, 0.9012, 0.9013, 0.9012, 0.9012, 0.9011, 0.9012,\n              0.9013, 0.9012, 0.9023, 0.9013, 0.9010, 0.9012, 0.9016, 0.9012, 0.9013,\n              0.9015, 0.9012, 0.9013, 0.9015, 0.9013, 0.9013, 0.9012, 0.9013, 0.9010,\n              0.9012, 0.9018, 0.9014, 0.9014, 0.9011, 0.9017, 0.9011, 0.9012, 0.9014,\n              0.9017, 0.9014, 0.9011, 0.9016, 0.9013, 0.9010, 0.9012, 0.9010, 0.9011,\n              0.9010, 0.9013, 0.9012, 0.9012, 0.9012, 0.9016, 0.9011, 0.9011, 0.9013,\n              0.9010, 0.9013, 0.9015, 0.9010, 0.9012, 0.9010, 0.9011, 0.9015, 0.9013,\n              0.9013, 0.9012, 0.9012, 0.9013, 0.9016, 0.9011, 0.9011, 0.9010, 0.9012,\n              0.9014, 0.9011, 0.9012, 0.9017, 0.9012, 0.9011, 0.9013, 0.9009, 0.9013,\n              0.9012, 0.9011, 0.9017, 0.9014, 0.9012, 0.9011, 0.9013, 0.9012, 0.9012,\n              0.9012, 0.9016, 0.9011, 0.9011, 0.9015, 0.9015, 0.9010, 0.9012, 0.9015,\n              0.9011, 0.9011, 0.9014, 0.9010, 0.9017, 0.9013, 0.9016, 0.9016],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[ 5.2755e-02]],\n        \n                 [[-4.1950e-02]],\n        \n                 [[-5.0308e-02]],\n        \n                 ...,\n        \n                 [[-2.1610e-02]],\n        \n                 [[-5.0322e-02]],\n        \n                 [[ 2.1676e-02]]],\n        \n        \n                [[[-5.9286e-02]],\n        \n                 [[ 2.0178e-02]],\n        \n                 [[ 4.4433e-02]],\n        \n                 ...,\n        \n                 [[-3.5707e-02]],\n        \n                 [[-4.2539e-02]],\n        \n                 [[ 4.3980e-02]]],\n        \n        \n                [[[ 2.7995e-02]],\n        \n                 [[-1.5403e-02]],\n        \n                 [[ 4.5316e-02]],\n        \n                 ...,\n        \n                 [[ 2.7411e-02]],\n        \n                 [[-3.2188e-02]],\n        \n                 [[ 4.2694e-02]]],\n        \n        \n                ...,\n        \n        \n                [[[-3.2924e-02]],\n        \n                 [[ 2.6173e-02]],\n        \n                 [[ 3.5715e-02]],\n        \n                 ...,\n        \n                 [[ 4.2688e-02]],\n        \n                 [[ 2.7468e-02]],\n        \n                 [[ 1.2576e-02]]],\n        \n        \n                [[[ 4.6733e-03]],\n        \n                 [[ 7.6106e-03]],\n        \n                 [[-5.4409e-02]],\n        \n                 ...,\n        \n                 [[ 5.0759e-02]],\n        \n                 [[ 4.3978e-02]],\n        \n                 [[-2.8622e-02]]],\n        \n        \n                [[[ 5.6089e-02]],\n        \n                 [[ 1.2534e-02]],\n        \n                 [[-4.7881e-03]],\n        \n                 ...,\n        \n                 [[ 1.2558e-02]],\n        \n                 [[ 2.3432e-05]],\n        \n                 [[ 5.6431e-02]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 1.0343e-02,  2.3179e-02, -3.0819e-02, -8.3786e-02,  1.8357e-02,\n                 3.3835e-03, -3.5146e-02, -2.0895e-02,  3.6025e-02,  1.5697e-02,\n                 3.8436e-02,  3.1731e-02,  5.9121e-03,  1.3811e-02, -3.1541e-02,\n                -1.5684e-03, -3.0581e-02, -2.9045e-02,  2.1908e-02, -3.2655e-02,\n                 2.2278e-02,  1.0096e-02, -1.6112e-02, -3.5577e-02,  6.7638e-02,\n                -9.6771e-03, -4.3358e-02,  1.1384e-02, -4.6235e-02,  8.8156e-02,\n                -7.6881e-02, -3.4687e-02, -6.2792e-02, -2.4366e-02, -3.7518e-02,\n                -4.7470e-02, -3.6347e-02,  1.5214e-02,  1.0884e-03,  9.4593e-03,\n                 3.3735e-02,  2.2819e-02,  2.6628e-02, -1.1188e-02,  1.0881e-03,\n                 5.4474e-02, -2.9796e-03,  5.5647e-02, -2.8217e-02, -1.5567e-02,\n                 6.1284e-03,  5.6186e-03, -4.6838e-02, -5.2628e-02,  1.2320e-02,\n                 3.3238e-02, -5.0086e-02, -2.4017e-02, -6.2491e-02, -2.2202e-02,\n                -5.3707e-02,  1.9515e-02,  1.1963e-03, -6.3856e-02,  2.1759e-02,\n                 2.5385e-02,  1.8733e-02,  2.2491e-02,  1.0297e-02,  2.8031e-02,\n                -4.6258e-02, -1.1799e-02, -1.3314e-02,  5.4411e-02, -7.6572e-03,\n                 3.3957e-02,  2.0984e-02,  4.9102e-02,  2.7995e-02,  1.6906e-02,\n                 5.7910e-02,  2.7746e-03,  3.5684e-02,  6.2370e-02,  4.0371e-02,\n                -5.3078e-02, -3.1356e-02, -4.6085e-03,  3.6526e-03,  1.1976e-02,\n                 9.9056e-03, -5.4176e-03,  1.4028e-02, -1.0813e-02,  4.7250e-03,\n                 6.9654e-03,  2.9732e-02,  4.2032e-02,  1.3129e-02, -7.6263e-04,\n                 7.1216e-03, -2.6940e-03,  3.1790e-02, -5.0250e-02, -2.9291e-02,\n                 3.7954e-02, -1.1737e-03, -1.4771e-02, -3.5895e-02, -1.2858e-02,\n                -3.1858e-02, -3.9706e-02,  2.7108e-02, -4.6092e-02,  3.0344e-02,\n                -2.1273e-03, -3.1049e-02, -2.6269e-02, -4.2712e-02,  1.9702e-02,\n                 3.0922e-02, -1.7837e-02,  8.8301e-03,  1.2058e-02, -3.4001e-02,\n                 3.7718e-03, -4.3639e-03, -3.3207e-02, -3.8104e-02, -3.8989e-03,\n                 2.5211e-02,  1.1433e-02,  1.7281e-02,  6.6179e-02, -1.9558e-02,\n                 3.2261e-02,  5.5485e-03,  5.5143e-03, -3.4240e-02,  2.9846e-02,\n                -5.7671e-02, -2.4302e-02, -9.1159e-02, -2.7065e-02,  5.6765e-02,\n                -2.5649e-02,  1.6046e-02,  2.2488e-02, -4.1221e-02,  1.8428e-03,\n                 2.1747e-02, -5.8593e-04,  1.4074e-02, -4.1632e-02, -2.3495e-02,\n                -3.6389e-02, -1.3972e-02,  2.2857e-02, -3.6728e-02, -3.6539e-02,\n                -2.7120e-02, -9.3963e-03,  3.1793e-02, -1.0216e-02,  8.6586e-03,\n                 8.5712e-03, -5.1087e-02,  2.7409e-02, -4.8170e-02, -1.7336e-02,\n                 9.4276e-03, -4.6747e-02,  4.0146e-02, -8.0714e-03, -1.0850e-02,\n                 3.8021e-02,  2.8783e-02, -4.1780e-02, -4.4764e-02,  1.0150e-02,\n                 3.1733e-02,  5.6638e-03, -3.2200e-02, -3.5356e-02, -4.9098e-03,\n                 1.5786e-02,  1.6913e-02,  9.3375e-04, -1.6813e-02, -4.7347e-02,\n                -5.3797e-02,  2.7017e-02,  1.9369e-02,  6.8145e-02,  3.4376e-02,\n                -3.3644e-02,  1.0300e-02,  1.5336e-02, -5.8112e-02,  1.2700e-02,\n                -5.2395e-02,  6.6041e-02, -3.5604e-02, -6.2075e-03,  3.0562e-03,\n                 2.8867e-03,  3.6813e-03,  1.9368e-02, -1.6911e-02, -8.9411e-03,\n                -7.3255e-03, -2.0217e-02,  2.4498e-02, -2.8234e-02,  5.8363e-02,\n                 3.0915e-02, -2.6612e-02, -2.4220e-02, -4.6821e-02,  1.3561e-02,\n                -1.2297e-02, -6.9661e-03,  8.1039e-02, -6.6069e-02, -2.9227e-02,\n                -9.3000e-03, -1.5099e-03,  1.8372e-02, -7.1845e-02, -2.9696e-03,\n                 1.3596e-02,  5.9722e-03, -5.6791e-02,  7.3685e-02,  2.1587e-02,\n                -2.7229e-02,  3.5058e-02, -2.4358e-04, -1.0303e-02,  4.4167e-04,\n                -2.9028e-02,  6.5807e-02, -1.3960e-02, -5.1367e-02,  2.0492e-03,\n                 1.8218e-02, -4.5860e-02,  5.9715e-02,  4.1166e-02, -4.8103e-02,\n                -6.2987e-02,  5.7109e-03,  6.3954e-03,  3.0676e-02, -1.9053e-02,\n                -2.4774e-02,  4.6259e-02,  3.3508e-02, -1.9918e-02, -2.2576e-02,\n                -3.3586e-02, -2.8239e-02, -9.9617e-03,  3.2111e-02, -1.2099e-02,\n                 7.4461e-02, -3.2992e-02, -2.1845e-02, -3.0599e-04,  4.2463e-02,\n                -2.0683e-02, -2.1466e-02,  1.2345e-02, -1.1023e-03, -2.1141e-03,\n                 1.9514e-02,  4.4010e-03,  2.7076e-02, -4.7475e-03,  3.8151e-03,\n                -7.2026e-03, -8.5600e-03, -4.4940e-02, -4.0163e-03, -2.8120e-03,\n                -3.5859e-02, -2.3312e-02,  3.3600e-02, -5.5326e-03,  1.9150e-02,\n                -7.6409e-03,  3.1974e-02,  9.2365e-02,  1.7242e-02,  1.2086e-02,\n                 1.3225e-02, -7.6593e-02,  9.3347e-03,  8.9114e-03,  4.7362e-02,\n                -3.6087e-02, -4.8107e-02,  3.4671e-02,  1.1496e-02,  1.8273e-02,\n                 5.7892e-02,  1.9433e-02, -1.4496e-02,  4.3964e-02, -1.1902e-02,\n                -5.8902e-02,  3.3793e-02, -5.3663e-02,  5.3660e-02, -2.6378e-03,\n                 4.0734e-02, -2.1927e-02, -3.3121e-02, -5.8356e-02, -6.5735e-03,\n                -2.1172e-02, -2.6448e-02,  5.7063e-02, -1.3847e-02, -1.1383e-02,\n                -1.1152e-02,  3.9916e-02, -2.4067e-02, -3.5583e-03,  1.6150e-02,\n                -4.8303e-02,  7.6347e-03,  5.2124e-02,  5.1108e-02,  1.0505e-02,\n                -2.1433e-03,  4.5570e-02,  1.6537e-02, -2.7248e-02, -2.0424e-02,\n                 3.4169e-02,  8.8645e-03, -6.9469e-03,  6.6093e-02,  1.5075e-02,\n                -6.2436e-02, -1.6179e-02,  1.5202e-02, -1.1665e-02, -9.0476e-02,\n                -3.9967e-02,  3.2079e-02,  8.8406e-03, -1.0988e-02, -1.6437e-02,\n                 4.3303e-02, -6.0091e-02, -4.0448e-02,  9.9452e-02,  7.6870e-03,\n                -2.0117e-02,  7.2669e-03,  4.4016e-02,  4.1188e-03, -1.5626e-02,\n                -3.1802e-02,  3.8418e-02,  1.6955e-03, -1.1464e-02,  3.7712e-02,\n                -2.9095e-02, -2.3389e-02, -3.4062e-02,  8.4698e-03,  6.2540e-02,\n                 3.1665e-02, -3.7114e-02,  3.0579e-02, -2.5675e-02,  3.6025e-02,\n                 1.1459e-02,  3.3990e-02,  2.0931e-02,  2.2204e-02, -5.3585e-03,\n                -5.8173e-03,  5.7126e-02, -2.4738e-02, -3.6851e-02,  1.4629e-02,\n                 5.7525e-02,  3.2383e-02,  2.9387e-02,  1.4128e-02, -1.1160e-02,\n                -2.6408e-02,  1.4037e-02,  1.1221e-02,  5.7211e-02, -4.0630e-03,\n                -3.5008e-02,  1.9165e-03,  2.6806e-02, -6.8892e-03, -5.7988e-03,\n                -3.5231e-02, -5.6477e-02,  2.8685e-02,  3.6639e-02,  1.1336e-02,\n                -2.1728e-02, -2.9820e-02, -2.6370e-02,  3.0039e-02,  3.9978e-02,\n                -3.8174e-02,  3.9942e-02, -9.8431e-03, -8.9874e-02, -2.3986e-02,\n                 1.8407e-02,  1.6176e-03,  1.2064e-02, -1.2629e-02,  5.8961e-02,\n                 6.2979e-02, -3.0741e-02,  1.6241e-02,  3.1535e-02,  2.6119e-02,\n                -1.1595e-01, -4.3697e-02, -2.2327e-02, -6.7190e-03, -4.4274e-02,\n                 1.0320e-02,  7.5603e-02,  2.8267e-02, -4.6105e-02, -1.4456e-02,\n                 1.4985e-05, -2.5619e-02,  4.4838e-02, -1.7455e-02,  3.5446e-02,\n                 3.1739e-02,  2.7146e-02, -6.8496e-02, -3.8019e-02, -3.5919e-03,\n                 2.7265e-02, -3.1673e-02, -2.7683e-02,  5.3333e-02,  4.8780e-02,\n                 5.2651e-03,  5.3125e-02,  2.0048e-02,  1.4710e-02,  4.0458e-02,\n                 3.5762e-02, -3.4465e-02, -3.7321e-04,  1.9930e-02,  7.0026e-03,\n                 4.7040e-02,  3.6577e-02,  6.9191e-02, -1.9002e-02, -1.6322e-02,\n                 2.3579e-02,  2.3792e-02,  2.6113e-02, -2.5827e-02, -3.4714e-02,\n                -4.4233e-02,  1.9079e-02, -1.0751e-01, -1.7399e-02, -4.3887e-02,\n                -2.0910e-02, -3.4699e-02, -2.2883e-02, -2.3967e-02, -3.5159e-02,\n                 2.4559e-02,  1.0848e-02,  1.5528e-03, -8.4141e-03, -2.3867e-02,\n                -1.0348e-03,  3.1944e-02,  6.3934e-03, -4.3433e-02, -2.6268e-02,\n                 1.6512e-02, -4.5066e-03, -3.2842e-02, -1.3575e-02, -1.0298e-02,\n                -8.4460e-03,  3.3635e-02,  3.5797e-03,  9.8480e-03, -2.9535e-02,\n                -5.1640e-02, -3.0982e-02,  4.7298e-02, -1.3954e-02, -2.9392e-02,\n                -3.3142e-02,  4.8207e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9175, 0.9212, 0.9236, 0.9193, 0.9189, 0.9183, 0.9190, 0.9147, 0.9167,\n                0.9155, 0.9154, 0.9209, 0.9159, 0.9177, 0.9146, 0.9160, 0.9174, 0.9156,\n                0.9209, 0.9140, 0.9138, 0.9274, 0.9120, 0.9156, 0.9148, 0.9225, 0.9138,\n                0.9140, 0.9132, 0.9197, 0.9204, 0.9148, 0.9167, 0.9130, 0.9180, 0.9170,\n                0.9148, 0.9184, 0.9119, 0.9188, 0.9199, 0.9124, 0.9144, 0.9159, 0.9154,\n                0.9148, 0.9152, 0.9158, 0.9147, 0.9156, 0.9145, 0.9144, 0.9138, 0.9169,\n                0.9115, 0.9164, 0.9181, 0.9275, 0.9164, 0.9175, 0.9215, 0.9104, 0.9138,\n                0.9182, 0.9158, 0.9138, 0.9242, 0.9156, 0.9142, 0.9163, 0.9189, 0.9189,\n                0.9325, 0.9124, 0.9215, 0.9185, 0.9123, 0.9167, 0.9191, 0.9135, 0.9162,\n                0.9179, 0.9167, 0.9146, 0.9239, 0.9150, 0.9161, 0.9152, 0.9170, 0.9205,\n                0.9183, 0.9210, 0.9135, 0.9226, 0.9163, 0.9126, 0.9124, 0.9128, 0.9156,\n                0.9158, 0.9226, 0.9136, 0.9176, 0.9147, 0.9190, 0.9171, 0.9106, 0.9109,\n                0.9158, 0.9112, 0.9161, 0.9151, 0.9147, 0.9121, 0.9146, 0.9164, 0.9180,\n                0.9154, 0.9162, 0.9128, 0.9178, 0.9173, 0.9152, 0.9221, 0.9219, 0.9178,\n                0.9141, 0.9160, 0.9167, 0.9136, 0.9111, 0.9148, 0.9169, 0.9176, 0.9165,\n                0.9181, 0.9212, 0.9281, 0.9172, 0.9166, 0.9214, 0.9161, 0.9300, 0.9143,\n                0.9129, 0.9142, 0.9155, 0.9105, 0.9259, 0.9140, 0.9138, 0.9177, 0.9161,\n                0.9150, 0.9167, 0.9155, 0.9166, 0.9148, 0.9167, 0.9203, 0.9157, 0.9119,\n                0.9152, 0.9157, 0.9170, 0.9171, 0.9140, 0.9124, 0.9119, 0.9144, 0.9174,\n                0.9153, 0.9185, 0.9138, 0.9151, 0.9247, 0.9147, 0.9201, 0.9170, 0.9199,\n                0.9134, 0.9136, 0.9182, 0.9166, 0.9132, 0.9196, 0.9131, 0.9153, 0.9170,\n                0.9189, 0.9154, 0.9156, 0.9196, 0.9176, 0.9174, 0.9195, 0.9151, 0.9146,\n                0.9186, 0.9166, 0.9205, 0.9163, 0.9163, 0.9151, 0.9215, 0.9161, 0.9149,\n                0.9127, 0.9235, 0.9128, 0.9190, 0.9112, 0.9154, 0.9159, 0.9188, 0.9131,\n                0.9192, 0.9208, 0.9221, 0.9145, 0.9128, 0.9150, 0.9167, 0.9151, 0.9191,\n                0.9124, 0.9133, 0.9109, 0.9202, 0.9232, 0.9158, 0.9197, 0.9140, 0.9183,\n                0.9162, 0.9163, 0.9135, 0.9133, 0.9164, 0.9147, 0.9127, 0.9157, 0.9184,\n                0.9154, 0.9167, 0.9141, 0.9145, 0.9121, 0.9190, 0.9198, 0.9163, 0.9155,\n                0.9127, 0.9133, 0.9202, 0.9168, 0.9202, 0.9166, 0.9130, 0.9152, 0.9108,\n                0.9149, 0.9143, 0.9121, 0.9173, 0.9282, 0.9120, 0.9229, 0.9123, 0.9111,\n                0.9160, 0.9147, 0.9138, 0.9173, 0.9101, 0.9146, 0.9127, 0.9139, 0.9122,\n                0.9329, 0.9190, 0.9152, 0.9165, 0.9166, 0.9199, 0.9147, 0.9254, 0.9190,\n                0.9113, 0.9180, 0.9157, 0.9195, 0.9239, 0.9151, 0.9115, 0.9177, 0.9192,\n                0.9141, 0.9169, 0.9172, 0.9206, 0.9201, 0.9202, 0.9167, 0.9167, 0.9197,\n                0.9224, 0.9144, 0.9178, 0.9157, 0.9153, 0.9229, 0.9197, 0.9114, 0.9185,\n                0.9148, 0.9140, 0.9122, 0.9127, 0.9242, 0.9180, 0.9123, 0.9141, 0.9117,\n                0.9107, 0.9122, 0.9149, 0.9133, 0.9139, 0.9135, 0.9148, 0.9147, 0.9127,\n                0.9136, 0.9137, 0.9113, 0.9158, 0.9213, 0.9175, 0.9108, 0.9146, 0.9125,\n                0.9190, 0.9213, 0.9138, 0.9182, 0.9145, 0.9144, 0.9136, 0.9182, 0.9195,\n                0.9126, 0.9160, 0.9154, 0.9181, 0.9183, 0.9163, 0.9183, 0.9142, 0.9130,\n                0.9156, 0.9201, 0.9099, 0.9178, 0.9100, 0.9130, 0.9176, 0.9187, 0.9150,\n                0.9208, 0.9140, 0.9143, 0.9170, 0.9111, 0.9185, 0.9150, 0.9184, 0.9289,\n                0.9119, 0.9120, 0.9139, 0.9156, 0.9197, 0.9207, 0.9108, 0.9129, 0.9309,\n                0.9191, 0.9122, 0.9155, 0.9270, 0.9196, 0.9177, 0.9205, 0.9177, 0.9131,\n                0.9151, 0.9150, 0.9279, 0.9182, 0.9127, 0.9195, 0.9138, 0.9159, 0.9129,\n                0.9176, 0.9140, 0.9133, 0.9131, 0.9145, 0.9196, 0.9167, 0.9122, 0.9161,\n                0.9188, 0.9218, 0.9151, 0.9147, 0.9163, 0.9121, 0.9160, 0.9231, 0.9172,\n                0.9114, 0.9156, 0.9165, 0.9231, 0.9137, 0.9221, 0.9179, 0.9240, 0.9151,\n                0.9215, 0.9195, 0.9141, 0.9157, 0.9127, 0.9144, 0.9239, 0.9152, 0.9153,\n                0.9138, 0.9172, 0.9153, 0.9125, 0.9267, 0.9228, 0.9178, 0.9117, 0.9143,\n                0.9186, 0.9178, 0.9225, 0.9153, 0.9190, 0.9208, 0.9152, 0.9109, 0.9144,\n                0.9133, 0.9102, 0.9130, 0.9145, 0.9142, 0.9121, 0.9221, 0.9125, 0.9303,\n                0.9147, 0.9115, 0.9126, 0.9159, 0.9131, 0.9172, 0.9173, 0.9276, 0.9152,\n                0.9161, 0.9251, 0.9153, 0.9142, 0.9149, 0.9151, 0.9167, 0.9151, 0.9138,\n                0.9178, 0.9192, 0.9168, 0.9197, 0.9179, 0.9200, 0.9151, 0.9190, 0.9147,\n                0.9282, 0.9134, 0.9188, 0.9130, 0.9168, 0.9158, 0.9189, 0.9194, 0.9155,\n                0.9129, 0.9194, 0.9153, 0.9127, 0.9155, 0.9159, 0.9137, 0.9190],\n               grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [512, 256, 3, 3]], ["0.bn1.weight", [512]], ["0.bn1.bias", [512]], ["0.conv2.weight", [512, 512, 3, 3]], ["0.bn2.weight", [512]], ["0.bn2.bias", [512]], ["0.downsample.0.weight", [512, 256, 1, 1]], ["0.downsample.1.weight", [512]], ["0.downsample.1.bias", [512]]], "output_shape": [[512, 512, 1, 1]], "num_parameters": [1179648, 512, 512, 2359296, 512, 512, 131072, 512, 512]}, {"name": "avgpool", "id": 140209539497600, "class_name": "AveragePool()", "parameters": [], "output_shape": [[512, 512]], "num_parameters": []}, {"name": "fc", "id": 140209539497504, "class_name": "Linear(in_features=512, out_features=10, bias=True)", "parameters": [["weight", [10, 512]], ["bias", [10]]], "output_shape": [[512, 10]], "num_parameters": [5120, 10]}], "edges": []}