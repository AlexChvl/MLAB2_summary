{"format": "torch", "nodes": [{"name": "conv1", "id": 140449530342032, "class_name": "Conv2d(\n  self.stride=2, self.padding=(3, 3), self.weight=Parameter containing:\n  tensor([[[[-7.7627e-02, -5.3868e-02, -9.9256e-03,  ..., -5.0522e-02,\n              6.4574e-02,  8.7419e-03],\n            [ 1.3751e-02,  2.6304e-02,  6.5045e-02,  ..., -1.0508e-02,\n             -1.7738e-02,  1.1426e-03],\n            [-3.5629e-02,  3.6209e-02,  6.5294e-02,  ...,  6.1906e-02,\n             -3.2011e-02,  1.0542e-02],\n            ...,\n            [-6.1279e-02, -6.8116e-02, -3.5117e-02,  ...,  1.0193e-02,\n              7.9562e-02,  4.4647e-02],\n            [-1.2170e-02, -7.2903e-02, -5.3617e-02,  ..., -4.8331e-02,\n              3.4956e-02, -8.0273e-02],\n            [ 7.7262e-02, -6.7773e-02,  4.6441e-02,  ..., -3.7231e-02,\n             -4.3310e-02, -4.1821e-02]],\n  \n           [[-1.7271e-02,  3.8704e-02,  3.0555e-02,  ...,  7.2271e-02,\n             -9.7250e-03, -7.0914e-02],\n            [ 3.0664e-02,  2.3001e-02, -4.7128e-02,  ...,  3.1186e-02,\n              5.6912e-02, -7.3420e-03],\n            [-1.0927e-02,  3.2752e-02,  4.2344e-02,  ..., -3.3322e-02,\n              4.4538e-02,  2.8809e-02],\n            ...,\n            [-4.2067e-02, -1.1556e-03, -5.7930e-02,  ...,  4.1213e-02,\n             -4.1139e-02, -2.5700e-02],\n            [ 1.1025e-02, -1.8457e-02,  1.8225e-02,  ..., -6.0200e-02,\n              5.3059e-03, -7.7481e-02],\n            [-7.6942e-02,  7.6085e-02,  4.9206e-02,  ..., -2.6731e-02,\n              6.8197e-02,  7.1667e-05]],\n  \n           [[ 1.3982e-02,  2.2553e-02, -7.1873e-02,  ...,  5.5316e-02,\n              5.4514e-02,  3.3711e-02],\n            [ 3.4225e-02,  6.3407e-02, -8.1392e-03,  ..., -3.7545e-02,\n              3.4961e-03, -1.7580e-02],\n            [-4.2157e-03,  2.3090e-02, -3.4328e-02,  ...,  3.9713e-02,\n             -3.7663e-02, -9.7511e-03],\n            ...,\n            [-6.7888e-02,  6.1090e-02, -8.0198e-02,  ...,  3.7439e-02,\n              1.2304e-02,  1.9572e-02],\n            [ 4.3076e-02,  3.3214e-02, -8.6623e-03,  ...,  3.8386e-02,\n              3.6290e-02,  8.1294e-02],\n            [-5.2843e-02,  2.9203e-02,  7.6519e-02,  ...,  7.3206e-02,\n              5.4409e-02,  2.8946e-03]]],\n  \n  \n          [[[ 6.0349e-02, -4.4256e-02, -1.2426e-02,  ..., -1.2511e-02,\n             -6.7072e-02,  5.9528e-02],\n            [-1.9556e-02,  7.0954e-02,  7.2924e-02,  ..., -1.2041e-02,\n             -4.4281e-03,  6.1725e-03],\n            [-6.0867e-02, -2.3513e-02, -1.5489e-02,  ...,  3.0791e-02,\n              2.6230e-02, -4.0725e-02],\n            ...,\n            [-2.1509e-02, -7.3994e-02,  2.4693e-02,  ..., -6.5994e-02,\n              4.4396e-02,  2.2087e-02],\n            [-4.0533e-02,  8.1437e-02,  3.5755e-02,  ...,  4.9162e-02,\n             -1.4144e-03,  6.8374e-02],\n            [-2.8660e-02,  2.8139e-02,  2.6233e-02,  ...,  4.1902e-03,\n              4.7406e-02, -7.8430e-02]],\n  \n           [[-1.3483e-02,  7.0367e-03, -2.4127e-02,  ...,  5.9348e-02,\n              6.1172e-02, -6.6260e-02],\n            [-6.8352e-02, -3.6914e-02,  5.3131e-02,  ..., -4.8745e-02,\n              1.0746e-02,  3.1208e-02],\n            [ 2.7989e-02, -7.2929e-02,  7.8932e-02,  ..., -6.3879e-02,\n              3.7929e-02, -5.2376e-02],\n            ...,\n            [-1.5019e-02,  7.7869e-02,  2.5522e-02,  ...,  5.1711e-02,\n              8.1189e-02, -1.2527e-02],\n            [-3.0573e-02, -1.4863e-02,  7.6979e-02,  ..., -1.3527e-02,\n              4.3943e-02, -2.1698e-02],\n            [ 4.2410e-02,  4.8215e-02,  2.3790e-02,  ..., -7.3539e-02,\n              3.6238e-02,  1.4884e-02]],\n  \n           [[-4.6213e-02, -3.7260e-02, -7.4835e-03,  ...,  2.1133e-02,\n              3.4893e-02,  6.0683e-02],\n            [ 4.8311e-02, -1.5829e-02, -3.3155e-02,  ...,  3.4919e-02,\n             -2.8314e-02,  7.2249e-02],\n            [ 1.7308e-03, -7.4921e-02, -1.8970e-02,  ...,  6.2707e-02,\n              6.2710e-03, -3.6588e-02],\n            ...,\n            [ 2.5993e-02,  4.1578e-02, -7.9787e-02,  ...,  2.6721e-02,\n              6.9331e-02,  2.9420e-03],\n            [-3.6184e-02,  6.7410e-02,  1.7021e-02,  ...,  1.0061e-03,\n             -3.7235e-02, -3.4600e-02],\n            [-7.3784e-02, -6.1459e-02, -4.8791e-02,  ..., -1.8138e-02,\n             -3.4512e-02,  7.7194e-03]]],\n  \n  \n          [[[-2.3689e-02, -6.6562e-03,  1.5895e-02,  ...,  5.4541e-02,\n              4.3748e-04,  1.6953e-02],\n            [-4.9089e-02,  7.8844e-02, -7.9572e-02,  ...,  2.4944e-02,\n             -4.1813e-02, -2.7621e-02],\n            [ 4.7737e-02, -7.5043e-02, -2.5418e-02,  ...,  6.3238e-02,\n              5.4717e-03, -5.7102e-02],\n            ...,\n            [-5.5204e-03, -5.5139e-03, -4.3242e-02,  ...,  3.0449e-02,\n             -4.9292e-02,  2.2269e-02],\n            [ 5.6420e-03,  1.9195e-02,  1.2358e-02,  ...,  2.9045e-02,\n              3.8209e-02, -1.3138e-02],\n            [ 7.4010e-02, -4.5880e-03, -2.2455e-02,  ...,  3.1385e-03,\n              1.4452e-02, -1.2990e-03]],\n  \n           [[ 4.6479e-02,  6.4187e-02, -6.5386e-02,  ..., -6.2601e-02,\n             -3.1598e-02, -6.5989e-02],\n            [ 2.3793e-02,  3.5892e-02,  6.0011e-02,  ...,  6.3540e-02,\n             -5.5353e-02,  6.7982e-02],\n            [-5.6067e-03, -4.2604e-02, -2.9529e-04,  ...,  8.1127e-03,\n              5.0604e-02,  5.6935e-02],\n            ...,\n            [-7.1992e-02,  7.5960e-02,  8.2032e-02,  ...,  2.6401e-02,\n             -1.9250e-02, -1.3481e-02],\n            [-4.8319e-02,  3.9079e-03,  5.0042e-03,  ...,  5.6024e-02,\n              6.6618e-02, -1.8663e-02],\n            [-5.6194e-02, -4.7137e-02, -7.1932e-02,  ...,  3.3108e-02,\n              2.4695e-02, -8.6326e-03]],\n  \n           [[ 6.8248e-02, -4.8054e-02,  7.8984e-02,  ..., -1.0122e-02,\n             -6.8652e-02,  6.0770e-03],\n            [ 1.1374e-02, -7.2837e-02, -4.3452e-02,  ...,  5.1926e-02,\n              7.2346e-02,  5.4431e-02],\n            [-5.6136e-02,  1.0836e-02, -6.6231e-02,  ..., -7.1421e-02,\n             -7.6497e-03,  2.3127e-02],\n            ...,\n            [-1.3451e-02,  1.5577e-03, -2.9479e-02,  ...,  6.5731e-02,\n              3.3363e-03,  5.6498e-02],\n            [-2.0525e-02, -7.3480e-03,  1.1414e-02,  ..., -7.6067e-02,\n              6.7007e-02,  4.2656e-02],\n            [ 2.8826e-02, -7.3194e-02, -8.0039e-02,  ..., -3.2264e-03,\n             -3.0403e-02,  2.1469e-02]]],\n  \n  \n          ...,\n  \n  \n          [[[ 1.4217e-03,  1.8798e-02,  7.2586e-02,  ...,  6.0824e-02,\n             -2.4041e-02, -7.6019e-02],\n            [-2.7052e-02,  2.0243e-02, -2.8941e-02,  ...,  6.8894e-02,\n             -2.6730e-02, -2.0261e-02],\n            [ 7.3263e-02, -7.7776e-02,  2.2298e-02,  ...,  7.6338e-02,\n             -8.3922e-03,  6.9650e-02],\n            ...,\n            [-7.1454e-02,  2.2351e-02, -5.4904e-02,  ..., -7.5833e-02,\n             -1.6971e-02, -5.7271e-02],\n            [ 9.6461e-03,  8.2390e-03,  4.1348e-02,  ..., -5.6254e-03,\n             -1.5337e-02, -4.2170e-02],\n            [ 1.5349e-02,  1.9941e-02,  4.0212e-02,  ..., -7.2430e-02,\n              6.8728e-03, -1.3314e-02]],\n  \n           [[ 8.0778e-02,  6.6481e-02, -4.6126e-02,  ..., -3.5051e-02,\n             -7.8407e-02,  6.7889e-02],\n            [-2.7367e-03, -3.1805e-03,  5.1003e-02,  ...,  7.3953e-02,\n             -3.3566e-03,  6.4767e-02],\n            [-4.2184e-02, -2.3159e-02,  4.6344e-02,  ...,  7.4095e-02,\n             -7.9194e-02, -1.6566e-03],\n            ...,\n            [-6.4006e-02, -6.5292e-02, -3.7045e-02,  ..., -2.5139e-02,\n              7.9469e-02,  5.5543e-03],\n            [-7.1191e-02, -9.1292e-03,  3.1379e-02,  ..., -5.1759e-02,\n              5.3802e-03,  3.1918e-02],\n            [-2.5550e-03,  6.1840e-03, -6.0527e-02,  ..., -4.5101e-03,\n             -8.1804e-02, -4.7959e-02]],\n  \n           [[-8.0801e-02, -2.8165e-02, -2.7923e-02,  ...,  4.7681e-03,\n              4.3181e-02, -3.4731e-02],\n            [-8.1905e-02, -2.4024e-02, -6.7191e-03,  ...,  6.9989e-02,\n              3.1469e-02,  3.1021e-02],\n            [ 7.0187e-02, -4.3741e-02, -4.5070e-03,  ..., -2.9325e-02,\n              4.7635e-02, -6.7103e-03],\n            ...,\n            [-7.3800e-02, -5.0215e-02,  3.3253e-02,  ..., -4.1549e-02,\n              6.4461e-02, -7.5735e-02],\n            [ 6.7199e-02,  1.9099e-02, -4.8012e-02,  ..., -1.3046e-02,\n             -1.5578e-04, -3.7460e-02],\n            [ 2.1306e-02, -6.6659e-02, -5.0366e-02,  ...,  3.2568e-02,\n              5.5876e-02, -4.4982e-02]]],\n  \n  \n          [[[-6.6468e-02, -1.2581e-02,  5.1961e-02,  ...,  6.9315e-02,\n              2.2666e-02,  2.0213e-02],\n            [ 1.1000e-02, -7.5784e-02,  2.9311e-02,  ...,  1.0470e-02,\n              7.1552e-02,  1.8329e-02],\n            [-6.6683e-02, -6.1149e-02,  5.1006e-02,  ...,  3.8313e-02,\n             -7.2523e-02,  1.5109e-02],\n            ...,\n            [-7.8370e-02,  2.7979e-02,  7.0464e-03,  ...,  3.9838e-02,\n             -5.9908e-02, -2.3920e-02],\n            [ 2.7877e-02, -1.8947e-02, -1.9611e-02,  ..., -1.1958e-02,\n              1.9190e-02,  1.7364e-02],\n            [ 3.4041e-02,  7.0658e-02,  5.3296e-02,  ..., -5.2047e-02,\n              8.0241e-02, -2.6415e-02]],\n  \n           [[ 8.2264e-02, -4.1318e-02, -5.0355e-02,  ..., -6.6770e-02,\n             -7.0093e-02, -1.9300e-03],\n            [-3.7706e-03, -1.3867e-02, -3.1939e-02,  ...,  3.1463e-02,\n             -5.5671e-02, -4.9156e-02],\n            [-1.8160e-02, -5.3446e-03, -3.5622e-02,  ...,  7.9130e-02,\n             -6.9087e-03, -7.7641e-02],\n            ...,\n            [ 4.9326e-02,  7.9222e-02, -6.1677e-02,  ...,  5.3389e-02,\n              6.6941e-02,  6.9177e-02],\n            [-2.9449e-02, -1.8124e-03, -3.9354e-02,  ...,  7.2344e-02,\n             -6.2608e-04,  8.0908e-02],\n            [-7.6369e-02,  2.7229e-02,  7.7972e-02,  ...,  3.0296e-02,\n             -7.3585e-02,  1.8361e-03]],\n  \n           [[ 7.2718e-02, -6.0314e-02,  1.3984e-02,  ...,  4.4447e-02,\n             -7.2926e-02, -8.8309e-03],\n            [-2.9523e-02,  5.3699e-02,  5.6184e-02,  ..., -2.9137e-02,\n             -4.2051e-02,  7.5451e-03],\n            [ 8.0263e-02, -1.5746e-02, -4.6281e-02,  ..., -4.8630e-02,\n             -7.2011e-04,  1.7179e-03],\n            ...,\n            [ 7.9282e-02,  1.2792e-02,  5.6012e-03,  ..., -1.2197e-02,\n             -2.2918e-02, -1.4661e-03],\n            [-6.1464e-02, -1.9142e-02,  4.5163e-02,  ..., -1.6866e-02,\n              2.0365e-02,  6.8885e-02],\n            [-3.2259e-02, -7.1886e-02,  7.3943e-03,  ...,  7.4887e-03,\n             -1.0950e-02,  7.3833e-03]]],\n  \n  \n          [[[-7.4223e-02, -3.7608e-03, -2.2406e-02,  ...,  4.9074e-02,\n              8.9254e-03, -5.7235e-02],\n            [-1.7479e-02, -6.5704e-02,  6.8135e-02,  ...,  2.8617e-02,\n             -6.9118e-02, -1.1395e-02],\n            [-3.8731e-02, -5.7708e-02,  1.5496e-02,  ...,  1.0937e-02,\n             -2.7898e-03, -2.3262e-02],\n            ...,\n            [-4.0771e-02,  3.8023e-02, -8.2132e-02,  ...,  1.0950e-02,\n             -5.7113e-02,  9.8299e-03],\n            [ 7.4161e-02, -5.8561e-02,  4.2537e-02,  ..., -7.8540e-02,\n              2.4740e-02, -4.8318e-02],\n            [-6.1802e-02, -2.7701e-02,  4.5248e-02,  ..., -3.1398e-03,\n             -5.0125e-02, -6.7725e-02]],\n  \n           [[-1.1205e-02,  5.9034e-02,  3.8113e-02,  ..., -1.5163e-02,\n              7.0887e-02,  5.6873e-02],\n            [-3.7774e-02, -1.5250e-02, -3.2279e-02,  ...,  1.4206e-02,\n             -7.7015e-02,  4.7094e-02],\n            [ 8.1657e-03,  1.6121e-02,  3.3926e-02,  ...,  8.0744e-02,\n             -1.7234e-02, -6.0822e-02],\n            ...,\n            [-6.8627e-02,  2.6506e-02,  7.0153e-02,  ..., -8.0083e-02,\n             -1.3115e-03, -1.9394e-02],\n            [-4.6547e-02,  2.9322e-02, -6.4546e-04,  ..., -5.7000e-02,\n             -2.3607e-02, -4.1973e-03],\n            [-6.7458e-02, -2.7056e-02, -6.3491e-02,  ..., -2.6838e-02,\n              3.4603e-02, -2.4805e-02]],\n  \n           [[-2.5721e-03, -3.2462e-02, -4.2863e-02,  ...,  1.2784e-02,\n              3.6219e-02,  6.8228e-02],\n            [-5.3267e-02, -3.5556e-02, -1.5022e-02,  ..., -5.1061e-02,\n              5.8105e-02, -2.5644e-03],\n            [ 6.8839e-02, -7.7530e-02, -3.0040e-02,  ..., -7.6782e-02,\n             -3.1353e-02, -4.7783e-02],\n            ...,\n            [-2.8730e-02, -4.2535e-02, -2.5201e-02,  ..., -2.1983e-02,\n              5.7114e-02, -7.9185e-02],\n            [-5.8470e-02, -6.9236e-03,  4.0818e-02,  ...,  7.7179e-02,\n              5.3547e-02,  5.7293e-02],\n            [ 5.6373e-02, -3.3006e-02, -5.0188e-02,  ...,  1.3654e-02,\n             -3.2471e-02,  4.2102e-02]]]], requires_grad=True)\n)", "parameters": [["weight", [64, 3, 7, 7]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [9408]}, {"name": "bn1", "id": 140449530342080, "class_name": "BatchNorm2d(\n  self.momentum=0.1, self.weight=Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 3.1823e-04,  1.7656e-03,  1.3635e-03, -7.6598e-04, -1.8678e-03,\n           1.7143e-03,  1.7144e-04, -1.3412e-03,  8.8612e-04,  1.1700e-03,\n           3.6221e-04, -1.2410e-04,  6.7610e-04, -1.1803e-03, -3.3658e-04,\n          -1.6688e-04,  1.8690e-03,  7.8760e-04,  2.3666e-04,  2.1606e-03,\n          -6.8809e-04,  1.4693e-03, -8.4415e-04, -1.3420e-03, -1.3244e-03,\n          -3.7287e-04,  1.0343e-03,  1.0697e-04, -8.2488e-04,  7.5377e-04,\n          -4.5295e-04, -4.3565e-04, -6.3646e-04, -1.1455e-04,  1.8763e-03,\n          -4.6862e-05,  2.5510e-04, -2.8001e-04,  1.0778e-03,  5.0471e-04,\n          -2.1193e-04,  1.1138e-03,  2.1730e-03,  1.3820e-05,  2.4133e-03,\n          -1.1557e-04,  1.0884e-03, -7.8188e-04, -3.0346e-04,  1.1509e-04,\n           1.6185e-04,  1.3253e-03, -8.2592e-04, -8.4907e-04,  7.7030e-05,\n          -4.3097e-04, -2.2202e-03,  6.7718e-05,  3.9121e-05, -1.6675e-03,\n           1.0265e-03, -4.7690e-04,  1.1476e-03, -2.7028e-03],\n         grad_fn=<AddBackward0>), self.running_var=tensor([0.9305, 0.9748, 0.9318, 0.9285, 0.9660, 0.9263, 0.9164, 0.9290, 0.9189,\n          0.9501, 0.9271, 0.9553, 0.9272, 1.0036, 0.9257, 0.9321, 0.9651, 0.9526,\n          0.9265, 1.0110, 0.9387, 1.0173, 0.9400, 0.9523, 0.9541, 0.9370, 0.9337,\n          0.9461, 0.9389, 0.9222, 0.9307, 0.9240, 0.9285, 0.9187, 0.9734, 0.9146,\n          0.9092, 0.9166, 0.9143, 0.9121, 0.9361, 0.9314, 1.1008, 0.9238, 0.9960,\n          0.9142, 0.9363, 0.9161, 0.9072, 0.9303, 0.9167, 1.0006, 0.9579, 0.9272,\n          0.9214, 0.9274, 0.9966, 0.9278, 0.9160, 1.0149, 0.9806, 0.9648, 0.9305,\n          1.0609], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n)", "parameters": [["weight", [64]], ["bias", [64]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [64, 64]}, {"name": "relu", "id": 140449530340784, "class_name": "ReLU()", "parameters": [], "output_shape": [[512, 64, 16, 16]], "num_parameters": []}, {"name": "pool", "id": 140449530342800, "class_name": "MaxPool2d(self.kernel_size=(3, 3), self.stride=(3, 3), self.padding=1)", "parameters": [], "output_shape": [[512, 64, 6, 6]], "num_parameters": []}, {"name": "layer1", "id": 140449530342224, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-0.0045,  0.0122, -0.0407],\n                [ 0.0009,  0.0402,  0.0058],\n                [-0.0062, -0.0394,  0.0007]],\n      \n               [[ 0.0083,  0.0192,  0.0034],\n                [ 0.0249, -0.0217,  0.0173],\n                [ 0.0296, -0.0303,  0.0364]],\n      \n               [[ 0.0398,  0.0267,  0.0337],\n                [-0.0235,  0.0275,  0.0129],\n                [-0.0235,  0.0185,  0.0139]],\n      \n               ...,\n      \n               [[-0.0244,  0.0313, -0.0396],\n                [-0.0123, -0.0066,  0.0062],\n                [ 0.0092, -0.0114, -0.0051]],\n      \n               [[-0.0167,  0.0403, -0.0325],\n                [-0.0306,  0.0104, -0.0167],\n                [-0.0262,  0.0223, -0.0393]],\n      \n               [[ 0.0034, -0.0229,  0.0260],\n                [ 0.0182, -0.0248,  0.0251],\n                [-0.0401,  0.0393, -0.0275]]],\n      \n      \n              [[[ 0.0357, -0.0044, -0.0009],\n                [ 0.0386, -0.0266,  0.0242],\n                [-0.0279, -0.0105,  0.0345]],\n      \n               [[-0.0271,  0.0094,  0.0138],\n                [ 0.0060,  0.0278, -0.0282],\n                [-0.0292, -0.0200,  0.0277]],\n      \n               [[-0.0166,  0.0214,  0.0044],\n                [ 0.0161,  0.0392,  0.0352],\n                [ 0.0338,  0.0300,  0.0373]],\n      \n               ...,\n      \n               [[ 0.0268,  0.0127, -0.0388],\n                [-0.0401,  0.0336,  0.0072],\n                [-0.0182, -0.0256, -0.0122]],\n      \n               [[-0.0412, -0.0035,  0.0066],\n                [-0.0161, -0.0111,  0.0309],\n                [ 0.0048, -0.0402,  0.0152]],\n      \n               [[-0.0212,  0.0416,  0.0398],\n                [ 0.0254,  0.0238, -0.0182],\n                [-0.0349,  0.0171, -0.0247]]],\n      \n      \n              [[[-0.0300,  0.0297, -0.0176],\n                [-0.0199,  0.0387, -0.0316],\n                [-0.0299, -0.0163,  0.0356]],\n      \n               [[ 0.0181,  0.0295,  0.0293],\n                [-0.0073, -0.0205, -0.0308],\n                [ 0.0337, -0.0369,  0.0409]],\n      \n               [[-0.0336, -0.0153,  0.0358],\n                [-0.0334,  0.0049, -0.0324],\n                [ 0.0275,  0.0286, -0.0397]],\n      \n               ...,\n      \n               [[ 0.0179, -0.0063,  0.0057],\n                [-0.0023, -0.0335, -0.0138],\n                [-0.0347,  0.0200,  0.0342]],\n      \n               [[-0.0156,  0.0151,  0.0116],\n                [ 0.0206, -0.0139,  0.0363],\n                [ 0.0400,  0.0113, -0.0073]],\n      \n               [[ 0.0354,  0.0083,  0.0138],\n                [-0.0100,  0.0032,  0.0281],\n                [-0.0210, -0.0116,  0.0304]]],\n      \n      \n              ...,\n      \n      \n              [[[-0.0081, -0.0167, -0.0089],\n                [ 0.0116,  0.0351, -0.0163],\n                [-0.0361, -0.0070, -0.0105]],\n      \n               [[-0.0153, -0.0165,  0.0126],\n                [-0.0086,  0.0279,  0.0161],\n                [-0.0195, -0.0297,  0.0228]],\n      \n               [[ 0.0347,  0.0407, -0.0261],\n                [ 0.0096, -0.0232, -0.0326],\n                [ 0.0256,  0.0390, -0.0246]],\n      \n               ...,\n      \n               [[-0.0359,  0.0095, -0.0346],\n                [-0.0308,  0.0213, -0.0416],\n                [ 0.0234, -0.0200, -0.0343]],\n      \n               [[-0.0221,  0.0119,  0.0069],\n                [-0.0178,  0.0046, -0.0314],\n                [-0.0296,  0.0151, -0.0129]],\n      \n               [[-0.0073, -0.0224,  0.0015],\n                [ 0.0021,  0.0301, -0.0367],\n                [ 0.0135, -0.0279,  0.0154]]],\n      \n      \n              [[[-0.0116, -0.0145,  0.0034],\n                [-0.0286,  0.0060, -0.0194],\n                [ 0.0084, -0.0349, -0.0259]],\n      \n               [[ 0.0390, -0.0017,  0.0256],\n                [ 0.0343,  0.0233, -0.0390],\n                [-0.0139,  0.0412, -0.0306]],\n      \n               [[-0.0228, -0.0336, -0.0404],\n                [-0.0025, -0.0303, -0.0275],\n                [ 0.0076,  0.0169, -0.0224]],\n      \n               ...,\n      \n               [[ 0.0411,  0.0173, -0.0291],\n                [-0.0196, -0.0243,  0.0234],\n                [ 0.0114, -0.0394,  0.0266]],\n      \n               [[-0.0050,  0.0073,  0.0280],\n                [ 0.0119, -0.0006,  0.0224],\n                [ 0.0028, -0.0155, -0.0181]],\n      \n               [[ 0.0167,  0.0017, -0.0062],\n                [ 0.0159, -0.0106,  0.0080],\n                [ 0.0094,  0.0034, -0.0038]]],\n      \n      \n              [[[ 0.0057, -0.0249, -0.0364],\n                [-0.0382, -0.0162,  0.0205],\n                [-0.0047, -0.0298, -0.0357]],\n      \n               [[-0.0388, -0.0286,  0.0365],\n                [-0.0288, -0.0122,  0.0049],\n                [ 0.0149, -0.0403, -0.0260]],\n      \n               [[-0.0210, -0.0235,  0.0097],\n                [-0.0044, -0.0130,  0.0326],\n                [-0.0376,  0.0343, -0.0293]],\n      \n               ...,\n      \n               [[ 0.0245,  0.0134, -0.0141],\n                [ 0.0156,  0.0378, -0.0174],\n                [-0.0287,  0.0178, -0.0257]],\n      \n               [[ 0.0207, -0.0113, -0.0137],\n                [ 0.0013,  0.0404, -0.0182],\n                [-0.0026, -0.0120, -0.0328]],\n      \n               [[ 0.0094,  0.0199, -0.0304],\n                [ 0.0007,  0.0074, -0.0187],\n                [-0.0404, -0.0250,  0.0177]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0663,  0.0497,  0.0358,  0.0421, -0.0379, -0.0268,  0.0303,  0.0245,\n              -0.0064, -0.0417,  0.0093, -0.0142, -0.0580, -0.0436, -0.0110, -0.0095,\n               0.0528,  0.0136,  0.0198, -0.0640, -0.0126,  0.0305, -0.0324, -0.0303,\n               0.0171, -0.0288, -0.0832, -0.0755,  0.0736, -0.0920,  0.0004, -0.0149,\n               0.0122, -0.0288,  0.0056,  0.0057,  0.0463, -0.0147, -0.0658, -0.0131,\n              -0.0297, -0.0209,  0.0474, -0.0599, -0.0178,  0.0396,  0.0534,  0.0406,\n               0.0205, -0.0090, -0.0196, -0.0649, -0.0352,  0.0363, -0.0108,  0.0395,\n              -0.0026,  0.0183,  0.0169, -0.0208, -0.0377, -0.0290,  0.0252, -0.0119],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9210, 0.9403, 0.9136, 0.9163, 0.9325, 0.9130, 0.9236, 0.9175, 0.9198,\n              0.9203, 0.9225, 0.9147, 0.9264, 0.9156, 0.9211, 0.9269, 0.9185, 0.9159,\n              0.9142, 0.9143, 0.9144, 0.9144, 0.9205, 0.9167, 0.9207, 0.9108, 0.9240,\n              0.9256, 0.9328, 0.9396, 0.9104, 0.9187, 0.9135, 0.9160, 0.9125, 0.9100,\n              0.9136, 0.9176, 0.9338, 0.9146, 0.9118, 0.9153, 0.9301, 0.9289, 0.9245,\n              0.9225, 0.9245, 0.9153, 0.9215, 0.9181, 0.9326, 0.9252, 0.9151, 0.9310,\n              0.9344, 0.9142, 0.9125, 0.9164, 0.9132, 0.9233, 0.9256, 0.9248, 0.9131,\n              0.9152], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 4.9922e-03,  1.2672e-02, -2.8283e-03],\n                [ 2.2923e-02, -1.7808e-02,  2.8009e-02],\n                [-3.1461e-02, -2.1055e-02, -1.6511e-02]],\n      \n               [[ 4.8140e-03,  2.0241e-02, -1.1884e-02],\n                [ 4.0394e-02,  3.8420e-02,  1.8447e-02],\n                [-1.0802e-02,  3.0213e-02,  1.5412e-02]],\n      \n               [[-2.4080e-02,  1.2837e-02,  3.3184e-03],\n                [-2.0499e-02,  3.5770e-02, -2.1163e-02],\n                [-2.1000e-02, -2.1410e-02, -8.1722e-03]],\n      \n               ...,\n      \n               [[-2.2775e-02, -3.1786e-02, -3.5729e-02],\n                [ 3.2027e-02, -2.5337e-02, -1.8875e-03],\n                [ 2.0951e-02, -1.3424e-02, -3.6673e-02]],\n      \n               [[ 1.8488e-02,  4.1118e-02,  2.5035e-02],\n                [-3.3103e-02,  3.5299e-02,  6.6316e-04],\n                [-1.4646e-02,  2.2093e-02,  3.3500e-02]],\n      \n               [[ 7.7289e-03,  2.6016e-02,  2.3400e-03],\n                [-1.4405e-02,  3.4557e-02,  3.4795e-03],\n                [-1.8957e-02,  9.5184e-03, -4.8461e-03]]],\n      \n      \n              [[[-3.9171e-02,  1.9754e-02, -8.7526e-03],\n                [ 2.7044e-02,  3.9090e-02, -5.1717e-04],\n                [ 8.7072e-03,  1.8721e-02, -1.5142e-02]],\n      \n               [[-5.1545e-03,  1.5582e-02, -4.6965e-03],\n                [-9.3791e-03, -2.6333e-02,  7.1188e-03],\n                [-4.8634e-03, -1.2537e-02,  1.8944e-02]],\n      \n               [[-1.9501e-02, -3.7764e-02,  8.7381e-03],\n                [-1.1686e-02, -3.6198e-02,  3.4243e-02],\n                [ 3.7042e-02, -5.6834e-03,  3.5603e-02]],\n      \n               ...,\n      \n               [[ 2.4866e-02, -3.1665e-02,  5.1049e-03],\n                [-9.3960e-03,  2.1644e-02,  1.5458e-02],\n                [ 2.2362e-05, -2.8456e-02, -3.7411e-03]],\n      \n               [[-6.8770e-03,  3.4265e-02, -2.5446e-02],\n                [ 1.8061e-02,  1.6682e-02, -3.3448e-02],\n                [ 2.4308e-02, -2.5610e-02, -3.9079e-02]],\n      \n               [[-3.0158e-02,  2.6088e-02, -3.2894e-02],\n                [-2.5605e-03, -2.0592e-02, -4.6895e-03],\n                [-1.3759e-02, -3.2720e-04, -3.4670e-02]]],\n      \n      \n              [[[-1.3969e-02, -3.9193e-02,  1.4309e-02],\n                [ 1.6562e-02, -1.3257e-02, -2.1115e-02],\n                [ 2.5126e-02,  2.5306e-02, -2.0076e-02]],\n      \n               [[ 3.0892e-02,  8.5118e-03,  2.2356e-02],\n                [ 3.0767e-02,  3.0013e-02,  2.7724e-02],\n                [-1.8401e-02, -1.7004e-02, -2.6189e-02]],\n      \n               [[ 3.3581e-02,  2.0189e-02,  3.0564e-02],\n                [-3.3473e-02,  3.2197e-02, -2.1289e-03],\n                [-3.6214e-03,  3.7494e-02,  3.4062e-02]],\n      \n               ...,\n      \n               [[ 9.0936e-03, -6.8057e-03, -6.6932e-03],\n                [-3.4326e-02,  1.4660e-02,  2.8477e-02],\n                [-3.8471e-02, -1.0674e-02,  6.3575e-03]],\n      \n               [[-2.9320e-02, -3.8750e-02,  3.6984e-02],\n                [-9.0194e-03, -2.9011e-02,  3.2797e-02],\n                [-7.7349e-03, -1.0252e-02,  2.3464e-02]],\n      \n               [[-2.8717e-02, -2.2179e-03,  4.0546e-02],\n                [ 2.4032e-02, -3.4958e-02,  3.5655e-02],\n                [-3.5480e-02, -2.7598e-02, -9.6916e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[-3.2419e-02,  1.0496e-02,  2.6622e-02],\n                [-2.3816e-02, -3.6574e-02,  2.6110e-02],\n                [ 2.5244e-02, -2.4075e-02,  1.7090e-02]],\n      \n               [[-8.8547e-03, -3.6793e-02,  7.8972e-04],\n                [-2.0581e-02, -1.7932e-02, -2.5901e-02],\n                [ 2.0227e-02,  2.1293e-02, -8.2087e-03]],\n      \n               [[ 2.0815e-02, -1.5521e-03, -2.8736e-02],\n                [ 1.5697e-02,  3.1665e-02, -2.2215e-02],\n                [-2.2348e-02, -2.8177e-02,  5.1582e-03]],\n      \n               ...,\n      \n               [[-9.6420e-03,  1.3487e-03, -3.3089e-02],\n                [ 3.5998e-02, -7.9623e-03, -4.8699e-03],\n                [-3.1266e-02,  8.0939e-03, -1.2528e-02]],\n      \n               [[-3.3981e-02,  1.4813e-02, -3.1804e-02],\n                [-2.8807e-02,  2.7645e-02,  2.1437e-02],\n                [-2.3100e-02, -3.5700e-02,  9.4382e-03]],\n      \n               [[-1.3372e-02, -3.0079e-02,  7.9312e-03],\n                [-1.5347e-02, -1.5610e-02, -4.0066e-02],\n                [ 3.8529e-02, -2.5438e-02,  2.0539e-02]]],\n      \n      \n              [[[-3.9421e-02, -2.3962e-02, -3.6200e-02],\n                [ 3.7701e-02,  3.6966e-02, -4.8167e-04],\n                [ 5.4542e-03,  2.6301e-02, -3.7620e-02]],\n      \n               [[ 1.3999e-02, -1.9306e-02, -1.1071e-02],\n                [-6.0551e-03, -2.6177e-02, -1.6089e-02],\n                [-2.3718e-02,  2.8744e-02, -2.6715e-02]],\n      \n               [[ 3.0443e-02,  2.6828e-02,  1.5421e-02],\n                [ 2.7378e-03, -3.8409e-03, -2.8058e-02],\n                [-1.3602e-02, -5.1889e-03,  3.7236e-02]],\n      \n               ...,\n      \n               [[-3.2003e-02, -2.4406e-02,  1.7525e-02],\n                [-1.1950e-02, -8.5897e-03, -1.7593e-03],\n                [-2.2135e-02, -8.5223e-04, -3.6638e-02]],\n      \n               [[ 7.0968e-03, -7.9724e-03, -3.3920e-03],\n                [ 2.9741e-02,  3.1464e-02,  1.1949e-02],\n                [-3.2072e-02, -2.8929e-02,  2.7466e-02]],\n      \n               [[ 7.5563e-03,  3.2259e-02, -3.0754e-04],\n                [ 3.8684e-02,  7.1399e-03, -4.4932e-03],\n                [-2.3226e-02, -2.5967e-02,  3.9677e-02]]],\n      \n      \n              [[[ 1.6371e-02, -4.2520e-03, -7.0749e-03],\n                [-3.7505e-03,  3.9908e-02,  2.9142e-02],\n                [ 3.6375e-02,  3.2567e-02, -2.0063e-02]],\n      \n               [[ 2.9074e-02, -3.8001e-02, -3.4714e-02],\n                [-2.6234e-02,  3.6583e-02, -1.2912e-02],\n                [ 2.8475e-02, -1.3433e-02,  1.5369e-02]],\n      \n               [[ 9.0946e-03, -4.1814e-03, -3.5485e-02],\n                [ 7.3279e-03,  1.9591e-02,  1.1678e-02],\n                [-2.6337e-03,  1.9040e-02, -4.1400e-02]],\n      \n               ...,\n      \n               [[ 3.9240e-02, -2.7955e-02,  1.2253e-02],\n                [-1.5576e-02, -3.9142e-02, -2.7591e-03],\n                [-2.3501e-02,  1.1813e-02,  2.8589e-02]],\n      \n               [[ 3.4011e-02, -3.0715e-02,  3.0342e-02],\n                [-2.3317e-02, -6.7702e-03, -1.7600e-02],\n                [ 5.4747e-03,  1.8193e-02, -3.2204e-02]],\n      \n               [[-3.9189e-02,  5.3714e-03, -9.8824e-03],\n                [-6.4185e-03, -6.6015e-03,  3.4960e-02],\n                [-3.5105e-02, -3.0428e-02,  5.3171e-03]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-6.8133e-03, -1.2666e-02,  1.1621e-02,  1.9601e-02, -1.0838e-02,\n              -2.7739e-02,  6.8356e-03,  4.5741e-02,  1.2955e-03, -2.8361e-02,\n               2.5762e-02, -7.6530e-04, -2.5097e-03, -1.0572e-02, -1.4993e-02,\n              -4.1498e-02,  1.3905e-02,  1.9249e-03, -9.5891e-03, -7.7564e-03,\n               1.8606e-02,  4.9159e-03, -1.1825e-03, -1.9151e-05, -1.0979e-02,\n              -3.5260e-02, -3.7935e-03,  8.6142e-03,  2.3159e-02, -1.0575e-02,\n              -2.8941e-03,  1.1525e-02, -1.2948e-02, -1.1779e-02,  2.0481e-02,\n               1.6551e-02,  4.5444e-02,  1.9218e-03, -4.8878e-02,  1.2176e-02,\n              -2.0714e-02,  9.5275e-03,  1.8954e-02,  1.8752e-02,  1.1416e-02,\n               2.5723e-02,  2.4304e-02,  8.2247e-03, -1.2995e-02, -1.3233e-02,\n               2.4262e-02, -7.6146e-03, -2.9348e-02, -2.5137e-02,  3.2615e-02,\n               2.0396e-02, -9.3709e-03, -3.0944e-02, -7.7827e-03,  2.0897e-02,\n              -6.2131e-03,  5.9839e-04, -2.8693e-02, -2.5034e-03],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9076, 0.9067, 0.9098, 0.9086, 0.9099, 0.9119, 0.9074, 0.9157, 0.9121,\n              0.9106, 0.9112, 0.9073, 0.9087, 0.9088, 0.9100, 0.9101, 0.9097, 0.9094,\n              0.9096, 0.9131, 0.9095, 0.9088, 0.9097, 0.9074, 0.9064, 0.9106, 0.9116,\n              0.9078, 0.9099, 0.9086, 0.9131, 0.9074, 0.9082, 0.9129, 0.9106, 0.9077,\n              0.9152, 0.9094, 0.9123, 0.9139, 0.9099, 0.9064, 0.9095, 0.9082, 0.9076,\n              0.9070, 0.9119, 0.9092, 0.9123, 0.9089, 0.9109, 0.9065, 0.9092, 0.9126,\n              0.9086, 0.9097, 0.9079, 0.9108, 0.9071, 0.9101, 0.9080, 0.9085, 0.9103,\n              0.9087], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n  )\n)", "parameters": [["0.conv1.weight", [64, 64, 3, 3]], ["0.bn1.weight", [64]], ["0.bn1.bias", [64]], ["0.conv2.weight", [64, 64, 3, 3]], ["0.bn2.weight", [64]], ["0.bn2.bias", [64]]], "output_shape": [[512, 64, 6, 6]], "num_parameters": [36864, 64, 64, 36864, 64, 64]}, {"name": "layer2", "id": 140449530341504, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-7.0012e-03, -3.2643e-02, -2.8021e-02],\n                [-2.4940e-02,  3.9259e-02, -1.1306e-02],\n                [ 3.5826e-02, -2.0516e-02, -2.7980e-02]],\n      \n               [[-7.6124e-03, -2.5551e-03,  1.4757e-03],\n                [-5.2967e-03, -3.4840e-02,  3.6159e-02],\n                [-3.3052e-02,  2.5820e-02,  2.2580e-02]],\n      \n               [[ 1.6890e-02,  2.3192e-03, -1.9734e-02],\n                [-1.3668e-02,  1.8492e-04,  3.3043e-02],\n                [ 1.7538e-02, -3.4614e-02,  4.2456e-03]],\n      \n               ...,\n      \n               [[ 1.0171e-02, -2.2026e-02,  2.4393e-02],\n                [ 1.1180e-02,  7.6420e-03, -2.6672e-02],\n                [-2.5444e-02,  1.2655e-02, -3.0120e-02]],\n      \n               [[ 8.0311e-03,  1.9117e-02, -3.8408e-02],\n                [ 3.9215e-03,  3.7637e-03, -1.0556e-02],\n                [ 5.9228e-03,  3.0270e-02, -1.0216e-02]],\n      \n               [[-3.5295e-02, -2.8825e-02,  3.6063e-02],\n                [-1.1247e-02, -3.3543e-02, -3.3336e-02],\n                [ 3.5783e-02, -2.4988e-03,  1.4447e-02]]],\n      \n      \n              [[[-4.3537e-03,  8.0830e-03, -3.4364e-02],\n                [-3.1018e-02,  1.0465e-02, -3.5014e-02],\n                [ 1.2804e-02, -1.9157e-02, -3.4769e-02]],\n      \n               [[ 2.8498e-02,  1.6928e-02,  4.0452e-02],\n                [ 4.0249e-02,  1.8040e-02, -4.1651e-02],\n                [-1.4383e-02, -1.7892e-02,  9.7780e-03]],\n      \n               [[-2.5063e-02,  7.0050e-03,  4.2503e-03],\n                [-2.5926e-02,  2.6107e-02, -3.7059e-02],\n                [-2.3050e-02,  3.5740e-02, -2.3623e-02]],\n      \n               ...,\n      \n               [[ 4.1267e-02, -6.4342e-04, -1.3075e-02],\n                [ 1.6119e-02,  1.7157e-02, -4.0684e-02],\n                [ 1.3535e-02,  2.5514e-02, -1.1586e-02]],\n      \n               [[-1.8629e-02, -2.7712e-02, -2.4576e-03],\n                [-8.0615e-04, -7.7329e-03,  2.0005e-02],\n                [-1.9413e-02, -1.2442e-02,  3.9593e-03]],\n      \n               [[-3.9236e-02, -3.4216e-02,  1.5083e-02],\n                [ 2.8679e-02, -3.3556e-02,  3.5584e-02],\n                [-1.5178e-02, -3.0922e-03, -1.2581e-02]]],\n      \n      \n              [[[ 1.1420e-02, -1.9104e-03, -8.1124e-03],\n                [ 3.2923e-02,  2.7526e-02,  1.3271e-02],\n                [ 1.4654e-02,  3.7205e-03,  1.5140e-02]],\n      \n               [[-1.7613e-02,  9.2415e-03,  7.8230e-03],\n                [-9.6440e-03, -9.1194e-03, -1.8906e-02],\n                [ 3.3567e-02, -2.4431e-02,  2.3859e-02]],\n      \n               [[ 3.4411e-02, -3.6274e-02, -2.3760e-02],\n                [-3.1458e-02,  1.8329e-02,  1.4481e-02],\n                [-3.8727e-02, -7.5038e-03,  7.5232e-03]],\n      \n               ...,\n      \n               [[ 4.0242e-02,  2.2532e-02,  3.3594e-02],\n                [-8.4524e-03, -1.4257e-02,  2.5112e-02],\n                [ 2.9517e-02, -9.3250e-03, -1.8403e-02]],\n      \n               [[-3.9602e-02,  2.4502e-02,  8.4958e-03],\n                [-1.1699e-02,  2.9669e-02,  4.0802e-02],\n                [ 3.1985e-02,  6.1004e-03,  3.3218e-03]],\n      \n               [[-6.5361e-03,  5.2720e-03,  4.0125e-03],\n                [-2.3156e-02,  4.5047e-03,  2.5069e-03],\n                [ 1.4103e-02,  6.2424e-04,  2.2143e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[-3.5728e-02, -2.2993e-02,  2.8958e-03],\n                [-2.5949e-02,  1.4073e-02,  3.2145e-02],\n                [ 2.7444e-02, -3.4677e-02, -6.7089e-03]],\n      \n               [[-3.6203e-02,  9.7012e-03, -5.0212e-03],\n                [-5.6627e-03,  1.4225e-02,  1.9331e-02],\n                [ 5.6675e-03, -3.9952e-02, -2.5671e-02]],\n      \n               [[-2.0969e-02,  2.7435e-02,  1.3557e-02],\n                [-2.9380e-02,  1.9556e-02,  1.2379e-02],\n                [ 1.9816e-02, -4.5634e-03, -1.9286e-02]],\n      \n               ...,\n      \n               [[ 7.9706e-03, -2.7440e-02, -1.9198e-03],\n                [ 2.0220e-02,  8.0927e-03,  3.1000e-04],\n                [ 1.7132e-03,  1.2209e-02,  4.9894e-03]],\n      \n               [[-5.7517e-03, -1.5345e-02,  1.0609e-03],\n                [ 3.1998e-02,  1.4600e-02, -2.6411e-02],\n                [-1.4243e-03, -9.4041e-03,  3.6222e-02]],\n      \n               [[-3.0245e-02,  7.5328e-03,  1.5991e-03],\n                [-3.2591e-02,  3.6165e-03,  1.3282e-02],\n                [-3.1040e-03,  1.2818e-02,  2.2256e-02]]],\n      \n      \n              [[[ 1.4295e-02, -1.3259e-02, -4.3917e-03],\n                [-2.3243e-02, -4.1525e-02,  1.6982e-02],\n                [ 2.7192e-02, -2.1126e-02, -2.6943e-02]],\n      \n               [[-1.4789e-02, -6.1097e-03, -2.8795e-02],\n                [-4.0662e-02, -7.5683e-03, -1.9583e-02],\n                [ 2.7582e-02,  3.5602e-02,  4.1226e-02]],\n      \n               [[ 8.8199e-03, -9.0013e-03,  2.7015e-02],\n                [-2.5995e-02,  3.7405e-02,  2.6745e-03],\n                [ 4.0394e-02, -2.0989e-02, -1.3288e-02]],\n      \n               ...,\n      \n               [[ 3.7969e-02,  4.1523e-02,  3.8364e-02],\n                [-6.2664e-03,  3.5006e-02, -3.0300e-02],\n                [-6.1959e-05, -1.4616e-02,  2.1555e-02]],\n      \n               [[-4.0782e-02,  1.6876e-02, -8.8952e-03],\n                [-2.6453e-02, -3.1682e-02,  4.1152e-02],\n                [ 2.7043e-02, -1.0203e-04, -3.1237e-03]],\n      \n               [[-6.6080e-03, -3.1448e-02, -1.6149e-02],\n                [ 3.4320e-02, -1.7190e-02,  2.5399e-02],\n                [-3.4371e-02,  2.0319e-02,  2.8203e-03]]],\n      \n      \n              [[[ 8.1372e-03,  2.4843e-02, -1.7684e-02],\n                [-1.3858e-02, -3.7909e-03, -1.0490e-02],\n                [ 3.7928e-02, -4.9816e-03, -2.2927e-02]],\n      \n               [[-1.6557e-02,  3.8648e-02,  2.5750e-02],\n                [ 7.0944e-03, -2.6037e-02,  3.0944e-02],\n                [ 3.2603e-02, -1.6070e-02, -3.0962e-02]],\n      \n               [[ 2.0463e-02,  1.8271e-03, -3.4183e-03],\n                [-1.1977e-02, -2.1407e-02,  1.3126e-02],\n                [-3.5073e-03,  1.6012e-02,  3.0023e-02]],\n      \n               ...,\n      \n               [[-2.8223e-02, -3.6524e-02, -2.4127e-02],\n                [ 3.4447e-02,  2.0507e-02,  1.3335e-02],\n                [ 1.8776e-02,  2.0718e-02,  2.9573e-02]],\n      \n               [[-6.5029e-03, -3.5436e-02, -4.0193e-02],\n                [ 9.9354e-03,  3.9294e-02, -2.8191e-02],\n                [ 2.7337e-02,  2.2771e-02,  4.3717e-03]],\n      \n               [[-2.2291e-03, -3.6832e-02, -4.5196e-03],\n                [ 1.2449e-02,  1.0432e-02, -1.3879e-02],\n                [-1.4115e-02, -1.9873e-02,  3.3247e-02]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0733,  0.0439, -0.0245,  0.0535,  0.0693, -0.0806, -0.0795,  0.0445,\n              -0.0790,  0.0284,  0.0191,  0.0432, -0.0062, -0.0027,  0.0730,  0.0485,\n              -0.0403,  0.0486,  0.0754,  0.0240, -0.0185, -0.0080,  0.0105,  0.0042,\n               0.0166, -0.0883, -0.0584,  0.0447, -0.0234,  0.0496,  0.0380, -0.0266,\n               0.0507,  0.0719,  0.0134,  0.0482,  0.0475,  0.0185,  0.0830,  0.0155,\n               0.0146,  0.0419,  0.0193, -0.0540,  0.0553, -0.0061, -0.0240, -0.0448,\n               0.0465,  0.0158, -0.0830, -0.0457, -0.0150,  0.0013,  0.0695, -0.0017,\n              -0.0320, -0.0517, -0.0522, -0.0329,  0.0389,  0.0288, -0.0296, -0.0078,\n              -0.0810, -0.0057, -0.0256, -0.0256, -0.0289, -0.0305, -0.0522, -0.0550,\n               0.0102,  0.0469, -0.0438, -0.0885, -0.0173, -0.0410, -0.0401,  0.0399,\n               0.0543, -0.0310, -0.0086, -0.0214,  0.0979,  0.0111,  0.0044, -0.0404,\n               0.0146, -0.0905,  0.0532, -0.0184, -0.0465, -0.0856,  0.0611,  0.0531,\n               0.0315,  0.0164, -0.0803,  0.0067, -0.0374, -0.0809, -0.0714,  0.0832,\n               0.0559, -0.0455,  0.1283, -0.0455,  0.0440, -0.0901, -0.0063, -0.0754,\n              -0.0069, -0.0178,  0.0623,  0.0339, -0.0693, -0.0065,  0.0095,  0.0751,\n               0.0174, -0.0316,  0.0065, -0.0494, -0.0104, -0.0245, -0.0184,  0.0224],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9404, 0.9211, 0.9291, 0.9315, 0.9462, 0.9497, 0.9478, 0.9326, 0.9232,\n              0.9204, 0.9809, 0.9557, 0.9346, 0.9318, 0.9369, 0.9283, 0.9450, 0.9242,\n              0.9468, 0.9333, 0.9325, 0.9313, 0.9356, 0.9259, 0.9267, 0.9301, 0.9632,\n              0.9322, 0.9231, 0.9482, 0.9252, 0.9375, 0.9213, 0.9280, 0.9291, 0.9335,\n              0.9368, 0.9318, 0.9326, 0.9406, 0.9207, 0.9223, 0.9313, 0.9236, 0.9410,\n              0.9480, 0.9337, 0.9293, 0.9219, 0.9337, 0.9392, 0.9384, 0.9330, 0.9217,\n              0.9268, 0.9261, 0.9309, 0.9348, 0.9255, 0.9234, 0.9307, 0.9278, 0.9334,\n              0.9353, 0.9370, 0.9266, 0.9211, 0.9303, 0.9225, 0.9321, 0.9287, 0.9216,\n              0.9265, 0.9375, 0.9267, 0.9429, 0.9235, 0.9362, 0.9230, 0.9298, 0.9277,\n              0.9327, 0.9256, 0.9366, 0.9604, 0.9382, 0.9285, 0.9372, 0.9328, 0.9299,\n              0.9294, 0.9375, 0.9641, 0.9397, 0.9457, 0.9277, 0.9381, 0.9291, 0.9266,\n              0.9341, 0.9373, 0.9552, 0.9325, 0.9322, 0.9272, 0.9366, 0.9391, 0.9507,\n              0.9294, 0.9441, 0.9319, 0.9388, 0.9234, 0.9366, 0.9239, 0.9286, 0.9370,\n              0.9237, 0.9373, 0.9625, 0.9444, 0.9195, 0.9221, 0.9480, 0.9245, 0.9284,\n              0.9333, 0.9342], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-0.0045,  0.0161,  0.0085],\n                [-0.0278,  0.0200, -0.0274],\n                [-0.0010, -0.0055,  0.0092]],\n      \n               [[-0.0020, -0.0132, -0.0273],\n                [-0.0192,  0.0040, -0.0006],\n                [ 0.0001, -0.0272,  0.0245]],\n      \n               [[-0.0045, -0.0271,  0.0106],\n                [ 0.0102, -0.0271,  0.0243],\n                [-0.0014, -0.0237,  0.0157]],\n      \n               ...,\n      \n               [[-0.0057,  0.0183,  0.0054],\n                [-0.0174, -0.0107, -0.0127],\n                [-0.0046,  0.0100,  0.0292]],\n      \n               [[ 0.0253,  0.0123, -0.0097],\n                [ 0.0237, -0.0182,  0.0023],\n                [ 0.0195,  0.0026,  0.0199]],\n      \n               [[ 0.0277, -0.0089, -0.0157],\n                [-0.0223,  0.0129,  0.0216],\n                [ 0.0044, -0.0161,  0.0007]]],\n      \n      \n              [[[-0.0294,  0.0148, -0.0111],\n                [-0.0252,  0.0190, -0.0101],\n                [ 0.0059, -0.0279, -0.0023]],\n      \n               [[ 0.0036,  0.0217, -0.0174],\n                [ 0.0022,  0.0117, -0.0183],\n                [-0.0159, -0.0200, -0.0286]],\n      \n               [[-0.0114,  0.0185,  0.0004],\n                [-0.0258,  0.0158,  0.0209],\n                [-0.0073,  0.0255, -0.0179]],\n      \n               ...,\n      \n               [[-0.0073,  0.0254,  0.0009],\n                [ 0.0232,  0.0162,  0.0060],\n                [-0.0192,  0.0197,  0.0240]],\n      \n               [[-0.0197, -0.0163,  0.0055],\n                [-0.0242, -0.0231, -0.0217],\n                [-0.0224, -0.0024,  0.0247]],\n      \n               [[-0.0071,  0.0161,  0.0162],\n                [ 0.0022,  0.0187,  0.0074],\n                [-0.0075, -0.0012, -0.0046]]],\n      \n      \n              [[[ 0.0204,  0.0141, -0.0252],\n                [-0.0049,  0.0099, -0.0227],\n                [-0.0048, -0.0157, -0.0186]],\n      \n               [[ 0.0101,  0.0164, -0.0124],\n                [ 0.0224, -0.0282, -0.0186],\n                [-0.0066,  0.0027, -0.0244]],\n      \n               [[-0.0210, -0.0194, -0.0245],\n                [ 0.0107, -0.0288,  0.0112],\n                [ 0.0281,  0.0194, -0.0039]],\n      \n               ...,\n      \n               [[ 0.0094, -0.0212, -0.0226],\n                [-0.0223,  0.0178,  0.0166],\n                [-0.0138, -0.0146,  0.0049]],\n      \n               [[ 0.0122, -0.0118, -0.0049],\n                [-0.0088,  0.0199, -0.0158],\n                [ 0.0233, -0.0102,  0.0144]],\n      \n               [[ 0.0062, -0.0186, -0.0241],\n                [ 0.0215, -0.0230,  0.0268],\n                [-0.0150,  0.0295, -0.0061]]],\n      \n      \n              ...,\n      \n      \n              [[[ 0.0102,  0.0262,  0.0226],\n                [ 0.0031, -0.0113, -0.0245],\n                [ 0.0011,  0.0100, -0.0150]],\n      \n               [[ 0.0282, -0.0105, -0.0169],\n                [ 0.0018, -0.0275, -0.0154],\n                [-0.0067, -0.0180,  0.0264]],\n      \n               [[ 0.0040, -0.0193, -0.0015],\n                [-0.0107,  0.0109, -0.0187],\n                [-0.0127,  0.0077,  0.0052]],\n      \n               ...,\n      \n               [[-0.0259,  0.0085, -0.0163],\n                [-0.0054,  0.0058, -0.0081],\n                [-0.0040, -0.0266,  0.0075]],\n      \n               [[-0.0048,  0.0253, -0.0008],\n                [ 0.0031, -0.0221, -0.0045],\n                [-0.0008, -0.0090,  0.0207]],\n      \n               [[ 0.0083,  0.0063, -0.0221],\n                [ 0.0111, -0.0290, -0.0252],\n                [ 0.0274,  0.0214, -0.0066]]],\n      \n      \n              [[[ 0.0110,  0.0140,  0.0050],\n                [-0.0010, -0.0027,  0.0078],\n                [-0.0036,  0.0094,  0.0040]],\n      \n               [[ 0.0027, -0.0115,  0.0028],\n                [ 0.0275,  0.0168, -0.0004],\n                [-0.0164, -0.0124, -0.0246]],\n      \n               [[-0.0074,  0.0024,  0.0196],\n                [ 0.0146, -0.0114,  0.0106],\n                [ 0.0176,  0.0144, -0.0093]],\n      \n               ...,\n      \n               [[ 0.0127, -0.0068,  0.0038],\n                [ 0.0098, -0.0203, -0.0136],\n                [ 0.0281,  0.0171, -0.0221]],\n      \n               [[-0.0087, -0.0145,  0.0282],\n                [-0.0055, -0.0290, -0.0073],\n                [ 0.0248,  0.0245,  0.0264]],\n      \n               [[-0.0260, -0.0174, -0.0139],\n                [-0.0157, -0.0012, -0.0200],\n                [ 0.0152, -0.0091, -0.0125]]],\n      \n      \n              [[[ 0.0162, -0.0192,  0.0075],\n                [-0.0064, -0.0090,  0.0125],\n                [ 0.0208,  0.0176,  0.0207]],\n      \n               [[ 0.0241,  0.0002, -0.0259],\n                [ 0.0275, -0.0117, -0.0240],\n                [-0.0089, -0.0182,  0.0070]],\n      \n               [[-0.0078,  0.0258, -0.0027],\n                [-0.0013, -0.0168,  0.0139],\n                [-0.0183, -0.0184, -0.0099]],\n      \n               ...,\n      \n               [[ 0.0259,  0.0034, -0.0133],\n                [ 0.0270,  0.0197,  0.0277],\n                [-0.0245,  0.0106, -0.0106]],\n      \n               [[-0.0065,  0.0234, -0.0076],\n                [-0.0069, -0.0144, -0.0061],\n                [ 0.0004, -0.0089, -0.0020]],\n      \n               [[ 0.0189, -0.0256, -0.0230],\n                [-0.0154, -0.0221,  0.0034],\n                [-0.0197,  0.0030, -0.0173]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 6.4229e-03,  7.9209e-03,  6.5199e-03, -1.3130e-02,  3.2157e-03,\n              -8.3851e-03,  3.4506e-02,  7.3129e-03, -1.3435e-03, -5.2854e-03,\n              -5.7543e-05,  2.6772e-02, -1.8797e-02,  1.6190e-02,  1.7782e-02,\n               2.4524e-02,  8.8876e-03,  2.7849e-02,  2.1064e-02, -1.7961e-02,\n               1.4114e-02,  2.7880e-03, -7.6704e-03,  2.7974e-03, -1.1213e-02,\n               3.2450e-03, -5.2737e-03, -2.1407e-03, -6.0599e-03, -1.9402e-02,\n              -1.1975e-02,  7.8417e-03, -1.9444e-02, -2.6476e-03,  2.0267e-02,\n              -1.1499e-02, -3.7033e-04,  8.5091e-03,  3.0184e-03,  3.7565e-03,\n               7.3150e-03,  1.5067e-02,  2.5276e-03, -5.7532e-03, -1.0500e-03,\n              -3.0145e-02,  1.8072e-02, -3.5235e-03,  6.8901e-03, -1.5080e-02,\n               9.0930e-03, -8.4133e-03,  1.1556e-02, -2.2111e-02,  1.1417e-02,\n               3.7437e-03,  2.1756e-02,  5.1229e-03,  1.5571e-02,  2.7439e-03,\n               6.6413e-03, -1.4756e-02, -1.2480e-02, -2.4595e-04,  6.3788e-03,\n               9.9508e-03,  9.6369e-03, -3.2908e-03, -1.2758e-03,  1.8595e-02,\n               8.4554e-03,  5.4910e-03,  1.7406e-02,  6.6935e-03,  1.6662e-02,\n               8.0308e-03,  8.0376e-03,  9.1833e-03, -7.2067e-03,  1.6936e-02,\n              -2.1348e-02, -4.0184e-04, -1.4020e-02, -1.4272e-02,  2.7489e-02,\n              -9.3222e-03, -1.0432e-03,  7.5029e-03,  5.2602e-03,  1.4693e-02,\n              -2.9144e-02,  9.5564e-03,  6.0572e-03,  7.8637e-03,  1.0438e-02,\n              -1.1100e-02,  4.1104e-03,  8.4852e-03, -9.5402e-03, -6.0845e-03,\n              -9.9036e-03,  2.0605e-02, -2.8828e-04,  5.9756e-03, -6.7811e-03,\n              -1.4156e-02, -1.7934e-02,  8.3652e-03,  2.6936e-03, -5.4237e-03,\n               1.0691e-03,  1.6931e-02, -5.8013e-03, -1.2881e-02, -1.1387e-02,\n               3.5573e-03, -1.1031e-02,  6.9107e-03, -1.1806e-02, -2.9876e-02,\n              -4.2085e-03, -2.0589e-02,  1.1769e-02, -1.2762e-03,  1.1210e-02,\n              -3.2627e-03, -1.5127e-03,  9.1589e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9062, 0.9087, 0.9056, 0.9115, 0.9093, 0.9073, 0.9085, 0.9077, 0.9089,\n              0.9094, 0.9074, 0.9080, 0.9086, 0.9115, 0.9072, 0.9055, 0.9078, 0.9077,\n              0.9067, 0.9073, 0.9067, 0.9112, 0.9070, 0.9064, 0.9079, 0.9077, 0.9066,\n              0.9080, 0.9087, 0.9073, 0.9065, 0.9063, 0.9068, 0.9075, 0.9060, 0.9070,\n              0.9062, 0.9076, 0.9067, 0.9094, 0.9064, 0.9071, 0.9064, 0.9066, 0.9066,\n              0.9096, 0.9065, 0.9068, 0.9070, 0.9070, 0.9071, 0.9082, 0.9106, 0.9058,\n              0.9098, 0.9093, 0.9091, 0.9084, 0.9120, 0.9068, 0.9074, 0.9089, 0.9086,\n              0.9095, 0.9069, 0.9063, 0.9090, 0.9073, 0.9093, 0.9086, 0.9068, 0.9055,\n              0.9101, 0.9072, 0.9082, 0.9076, 0.9111, 0.9060, 0.9065, 0.9088, 0.9106,\n              0.9086, 0.9061, 0.9123, 0.9110, 0.9076, 0.9067, 0.9134, 0.9071, 0.9084,\n              0.9080, 0.9072, 0.9068, 0.9088, 0.9084, 0.9112, 0.9094, 0.9127, 0.9096,\n              0.9063, 0.9064, 0.9103, 0.9060, 0.9070, 0.9081, 0.9074, 0.9072, 0.9063,\n              0.9082, 0.9078, 0.9066, 0.9095, 0.9055, 0.9070, 0.9086, 0.9086, 0.9070,\n              0.9064, 0.9070, 0.9088, 0.9103, 0.9077, 0.9076, 0.9091, 0.9076, 0.9080,\n              0.9073, 0.9062], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[ 0.0720]],\n        \n                 [[ 0.0373]],\n        \n                 [[ 0.0618]],\n        \n                 ...,\n        \n                 [[ 0.0522]],\n        \n                 [[-0.0149]],\n        \n                 [[ 0.1064]]],\n        \n        \n                [[[-0.1187]],\n        \n                 [[-0.1191]],\n        \n                 [[-0.0182]],\n        \n                 ...,\n        \n                 [[-0.1074]],\n        \n                 [[ 0.0627]],\n        \n                 [[-0.0134]]],\n        \n        \n                [[[-0.0393]],\n        \n                 [[ 0.0488]],\n        \n                 [[-0.0564]],\n        \n                 ...,\n        \n                 [[ 0.0442]],\n        \n                 [[ 0.0398]],\n        \n                 [[ 0.0997]]],\n        \n        \n                ...,\n        \n        \n                [[[ 0.1209]],\n        \n                 [[ 0.0036]],\n        \n                 [[ 0.0628]],\n        \n                 ...,\n        \n                 [[ 0.0323]],\n        \n                 [[-0.0079]],\n        \n                 [[ 0.0766]]],\n        \n        \n                [[[ 0.1097]],\n        \n                 [[-0.0124]],\n        \n                 [[ 0.1168]],\n        \n                 ...,\n        \n                 [[ 0.0651]],\n        \n                 [[-0.0873]],\n        \n                 [[-0.0723]]],\n        \n        \n                [[[-0.0720]],\n        \n                 [[ 0.0530]],\n        \n                 [[-0.0003]],\n        \n                 ...,\n        \n                 [[ 0.0577]],\n        \n                 [[ 0.0259]],\n        \n                 [[-0.0962]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0104, -0.0404, -0.0096, -0.0819,  0.0594,  0.0832,  0.0146, -0.0687,\n                 0.0214,  0.0472, -0.0718, -0.0481,  0.0284,  0.0821,  0.0405,  0.0536,\n                 0.0484, -0.0098, -0.0128, -0.0731,  0.0292, -0.0258,  0.0038,  0.0543,\n                -0.0163, -0.0602,  0.0494,  0.0811, -0.0177,  0.0596,  0.0148,  0.0070,\n                -0.1312, -0.0325,  0.0053, -0.0363, -0.0213, -0.1309,  0.0732, -0.0209,\n                 0.0249,  0.0297, -0.0165,  0.0332, -0.0800, -0.1060, -0.0670, -0.0112,\n                 0.0118, -0.0396, -0.0898,  0.0876, -0.0390, -0.0061, -0.0055, -0.0074,\n                -0.0410,  0.0807, -0.0375, -0.0259, -0.0015, -0.0076, -0.0179,  0.0480,\n                 0.1234,  0.1144, -0.0164, -0.0930,  0.1025,  0.0865,  0.0035, -0.0639,\n                -0.0859,  0.0569, -0.0385,  0.0603, -0.0297, -0.0120, -0.0371, -0.0825,\n                 0.0771, -0.0015, -0.0226,  0.0181, -0.0226, -0.0609,  0.0082, -0.0591,\n                -0.0699, -0.0981, -0.0352, -0.0006,  0.0175,  0.0537, -0.0106,  0.0734,\n                -0.0391, -0.1056, -0.0094, -0.0466, -0.0479, -0.1304, -0.0492, -0.0353,\n                -0.0031, -0.0037, -0.0307,  0.0017, -0.0106,  0.1052, -0.1021,  0.0111,\n                 0.0050,  0.0712, -0.0293,  0.0104,  0.0728,  0.0487, -0.1021, -0.0233,\n                 0.0090,  0.0330, -0.0267, -0.1054, -0.0114,  0.0688, -0.0332, -0.0422],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9380, 0.9355, 0.9349, 0.9296, 0.9817, 0.9246, 0.9365, 0.9250, 0.9219,\n                0.9273, 0.9575, 0.9228, 0.9377, 0.9611, 0.9285, 0.9317, 0.9665, 0.9431,\n                0.9276, 0.9297, 0.9676, 0.9369, 0.9274, 0.9680, 0.9398, 0.9613, 0.9325,\n                0.9502, 0.9529, 0.9258, 0.9328, 0.9256, 0.9438, 0.9206, 0.9250, 0.9459,\n                0.9424, 0.9408, 0.9759, 0.9247, 0.9270, 0.9242, 0.9192, 0.9397, 0.9556,\n                0.9401, 0.9473, 0.9181, 0.9327, 0.9557, 0.9367, 0.9322, 0.9343, 0.9352,\n                0.9224, 0.9345, 0.9384, 0.9521, 0.9414, 0.9506, 0.9320, 0.9426, 0.9322,\n                0.9299, 0.9421, 0.9544, 0.9202, 0.9329, 0.9326, 0.9391, 0.9482, 0.9364,\n                0.9301, 0.9355, 0.9312, 0.9289, 0.9266, 0.9443, 0.9334, 0.9250, 0.9403,\n                0.9232, 0.9261, 0.9256, 0.9264, 0.9509, 0.9275, 0.9393, 0.9531, 0.9795,\n                0.9401, 0.9276, 0.9372, 0.9193, 0.9281, 0.9352, 0.9277, 0.9470, 0.9339,\n                0.9255, 0.9263, 0.9386, 0.9318, 0.9322, 0.9489, 0.9309, 0.9291, 0.9287,\n                0.9256, 0.9476, 0.9403, 0.9327, 0.9340, 0.9365, 0.9315, 0.9249, 0.9419,\n                0.9670, 0.9355, 0.9202, 0.9273, 0.9223, 0.9353, 0.9663, 0.9354, 0.9440,\n                0.9287, 0.9280], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [128, 64, 3, 3]], ["0.bn1.weight", [128]], ["0.bn1.bias", [128]], ["0.conv2.weight", [128, 128, 3, 3]], ["0.bn2.weight", [128]], ["0.bn2.bias", [128]], ["0.downsample.0.weight", [128, 64, 1, 1]], ["0.downsample.1.weight", [128]], ["0.downsample.1.bias", [128]]], "output_shape": [[512, 128, 3, 3]], "num_parameters": [73728, 128, 128, 147456, 128, 128, 8192, 128, 128]}, {"name": "layer3", "id": 140449530340400, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 0.0099,  0.0176, -0.0290],\n                [-0.0118, -0.0208,  0.0133],\n                [-0.0131, -0.0251, -0.0063]],\n      \n               [[-0.0064,  0.0263,  0.0200],\n                [-0.0193, -0.0174, -0.0270],\n                [ 0.0132, -0.0078, -0.0290]],\n      \n               [[ 0.0278, -0.0257,  0.0252],\n                [-0.0070,  0.0053, -0.0259],\n                [-0.0061, -0.0093, -0.0095]],\n      \n               ...,\n      \n               [[-0.0227, -0.0064, -0.0135],\n                [-0.0241, -0.0001,  0.0108],\n                [ 0.0171, -0.0108, -0.0217]],\n      \n               [[-0.0048, -0.0121,  0.0186],\n                [-0.0176,  0.0017,  0.0196],\n                [-0.0179, -0.0217, -0.0246]],\n      \n               [[-0.0036, -0.0071,  0.0003],\n                [ 0.0294,  0.0175,  0.0016],\n                [-0.0026,  0.0287, -0.0058]]],\n      \n      \n              [[[ 0.0187, -0.0149, -0.0265],\n                [-0.0116, -0.0204, -0.0178],\n                [ 0.0211,  0.0024, -0.0133]],\n      \n               [[-0.0085,  0.0042, -0.0153],\n                [-0.0183,  0.0011, -0.0115],\n                [ 0.0255, -0.0216,  0.0046]],\n      \n               [[-0.0082, -0.0276, -0.0249],\n                [ 0.0257,  0.0137,  0.0263],\n                [ 0.0198, -0.0204,  0.0015]],\n      \n               ...,\n      \n               [[-0.0174,  0.0135,  0.0037],\n                [ 0.0136, -0.0201, -0.0210],\n                [-0.0095, -0.0064,  0.0287]],\n      \n               [[-0.0019, -0.0142,  0.0014],\n                [ 0.0142,  0.0246,  0.0051],\n                [ 0.0088,  0.0241, -0.0132]],\n      \n               [[-0.0146,  0.0037,  0.0114],\n                [-0.0233, -0.0159, -0.0088],\n                [ 0.0073, -0.0217,  0.0039]]],\n      \n      \n              [[[ 0.0185,  0.0188,  0.0227],\n                [-0.0147, -0.0228, -0.0219],\n                [ 0.0228, -0.0126, -0.0070]],\n      \n               [[ 0.0013,  0.0114, -0.0136],\n                [ 0.0194,  0.0201, -0.0194],\n                [ 0.0055, -0.0094,  0.0042]],\n      \n               [[ 0.0279, -0.0080, -0.0232],\n                [ 0.0096,  0.0216, -0.0057],\n                [-0.0150, -0.0224, -0.0172]],\n      \n               ...,\n      \n               [[-0.0091, -0.0059,  0.0108],\n                [-0.0048, -0.0290,  0.0165],\n                [ 0.0253, -0.0137, -0.0292]],\n      \n               [[ 0.0106, -0.0148, -0.0150],\n                [-0.0067, -0.0228,  0.0213],\n                [ 0.0264, -0.0114,  0.0116]],\n      \n               [[-0.0132, -0.0143, -0.0188],\n                [-0.0197, -0.0161, -0.0282],\n                [-0.0069,  0.0199,  0.0112]]],\n      \n      \n              ...,\n      \n      \n              [[[ 0.0270,  0.0009,  0.0286],\n                [-0.0010, -0.0194,  0.0227],\n                [-0.0294,  0.0007, -0.0249]],\n      \n               [[-0.0174, -0.0035,  0.0215],\n                [-0.0231, -0.0012, -0.0115],\n                [ 0.0002, -0.0150, -0.0200]],\n      \n               [[-0.0103,  0.0175,  0.0192],\n                [-0.0068, -0.0231,  0.0123],\n                [ 0.0291,  0.0139,  0.0149]],\n      \n               ...,\n      \n               [[ 0.0225, -0.0008,  0.0212],\n                [-0.0156, -0.0079,  0.0058],\n                [-0.0038,  0.0059, -0.0105]],\n      \n               [[ 0.0007, -0.0129, -0.0094],\n                [ 0.0247, -0.0254, -0.0150],\n                [ 0.0177,  0.0014,  0.0180]],\n      \n               [[-0.0162,  0.0294,  0.0253],\n                [-0.0087, -0.0162,  0.0093],\n                [-0.0081,  0.0159,  0.0027]]],\n      \n      \n              [[[-0.0022, -0.0172,  0.0017],\n                [ 0.0257, -0.0207,  0.0177],\n                [ 0.0217, -0.0188, -0.0196]],\n      \n               [[ 0.0254,  0.0104,  0.0091],\n                [-0.0082,  0.0276,  0.0274],\n                [ 0.0225, -0.0126,  0.0172]],\n      \n               [[-0.0021, -0.0012,  0.0197],\n                [ 0.0095, -0.0149, -0.0141],\n                [ 0.0018,  0.0294,  0.0294]],\n      \n               ...,\n      \n               [[ 0.0265, -0.0269, -0.0076],\n                [-0.0224,  0.0267,  0.0226],\n                [-0.0245, -0.0116,  0.0256]],\n      \n               [[-0.0095, -0.0257, -0.0270],\n                [ 0.0281,  0.0266, -0.0290],\n                [-0.0249,  0.0189, -0.0177]],\n      \n               [[-0.0224, -0.0005,  0.0073],\n                [ 0.0008, -0.0112,  0.0086],\n                [ 0.0119, -0.0211, -0.0137]]],\n      \n      \n              [[[-0.0045, -0.0114, -0.0114],\n                [ 0.0171, -0.0137, -0.0287],\n                [-0.0010, -0.0072, -0.0045]],\n      \n               [[-0.0277,  0.0141,  0.0233],\n                [ 0.0100, -0.0208, -0.0054],\n                [ 0.0003, -0.0212, -0.0202]],\n      \n               [[-0.0218,  0.0121, -0.0116],\n                [-0.0215,  0.0040,  0.0081],\n                [-0.0233, -0.0128,  0.0114]],\n      \n               ...,\n      \n               [[-0.0116,  0.0028, -0.0094],\n                [ 0.0209,  0.0157, -0.0034],\n                [-0.0156,  0.0141, -0.0245]],\n      \n               [[ 0.0060,  0.0040,  0.0108],\n                [ 0.0203,  0.0248, -0.0099],\n                [ 0.0230, -0.0143,  0.0056]],\n      \n               [[-0.0169, -0.0239, -0.0003],\n                [-0.0013, -0.0163, -0.0081],\n                [ 0.0026, -0.0098, -0.0066]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0123,  0.0027, -0.0223, -0.0301,  0.0246,  0.0014,  0.0137,  0.0002,\n               0.0136,  0.0057, -0.0379, -0.0156,  0.0124, -0.0284, -0.0112, -0.0253,\n               0.0065,  0.0024, -0.0130, -0.0097,  0.0095,  0.0253,  0.0289,  0.0181,\n               0.0113,  0.0084,  0.0007,  0.0128,  0.0069,  0.0130, -0.0108,  0.0148,\n               0.0004,  0.0106,  0.0149, -0.0021, -0.0181,  0.0192,  0.0296, -0.0080,\n              -0.0460, -0.0027,  0.0209,  0.0044,  0.0197, -0.0003, -0.0143,  0.0197,\n               0.0056,  0.0072,  0.0132,  0.0350, -0.0012, -0.0035, -0.0234, -0.0153,\n               0.0123, -0.0106, -0.0229,  0.0105,  0.0002, -0.0085, -0.0287, -0.0130,\n               0.0036, -0.0098, -0.0179,  0.0048, -0.0035, -0.0336, -0.0074, -0.0216,\n               0.0009, -0.0125, -0.0298, -0.0174,  0.0068,  0.0060,  0.0019,  0.0350,\n              -0.0098,  0.0241,  0.0311,  0.0152,  0.0016, -0.0272,  0.0038, -0.0156,\n               0.0260, -0.0216,  0.0126,  0.0129,  0.0015,  0.0052,  0.0059, -0.0040,\n              -0.0156,  0.0013,  0.0151, -0.0300, -0.0024, -0.0192, -0.0050, -0.0147,\n               0.0113, -0.0052,  0.0074,  0.0393, -0.0108, -0.0120, -0.0012, -0.0154,\n               0.0096, -0.0014,  0.0079,  0.0018, -0.0192,  0.0137,  0.0076, -0.0067,\n               0.0084,  0.0211, -0.0105, -0.0062,  0.0148,  0.0189, -0.0139, -0.0150,\n               0.0011,  0.0226,  0.0299,  0.0171, -0.0107, -0.0012, -0.0099,  0.0194,\n              -0.0066, -0.0110, -0.0010,  0.0327,  0.0252, -0.0094, -0.0132,  0.0046,\n               0.0144, -0.0224,  0.0021, -0.0277, -0.0245,  0.0001,  0.0008,  0.0048,\n              -0.0065,  0.0111,  0.0071, -0.0296,  0.0073,  0.0022,  0.0019,  0.0134,\n              -0.0127,  0.0167, -0.0153,  0.0150,  0.0044,  0.0145,  0.0095, -0.0230,\n               0.0241,  0.0238, -0.0175,  0.0347,  0.0136,  0.0024, -0.0052,  0.0015,\n              -0.0186, -0.0133,  0.0301, -0.0141, -0.0017, -0.0190, -0.0121,  0.0185,\n               0.0206, -0.0146,  0.0010,  0.0150, -0.0158, -0.0061,  0.0024,  0.0002,\n              -0.0166, -0.0085,  0.0187,  0.0095,  0.0153, -0.0140, -0.0037, -0.0287,\n              -0.0092, -0.0035, -0.0183,  0.0346, -0.0015,  0.0239, -0.0112, -0.0343,\n              -0.0223, -0.0020,  0.0134,  0.0035, -0.0322, -0.0131,  0.0389, -0.0174,\n               0.0050,  0.0076,  0.0066, -0.0120,  0.0142,  0.0021,  0.0080,  0.0374,\n              -0.0027, -0.0197,  0.0208,  0.0108, -0.0130, -0.0333, -0.0006, -0.0145,\n              -0.0110, -0.0197, -0.0266, -0.0101,  0.0033, -0.0104, -0.0354,  0.0130,\n              -0.0190, -0.0023,  0.0079, -0.0177,  0.0112, -0.0010,  0.0009, -0.0141,\n               0.0005, -0.0443,  0.0059,  0.0281,  0.0256, -0.0021,  0.0228,  0.0208],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9106, 0.9135, 0.9108, 0.9130, 0.9122, 0.9164, 0.9118, 0.9121, 0.9124,\n              0.9112, 0.9104, 0.9168, 0.9174, 0.9112, 0.9103, 0.9102, 0.9150, 0.9103,\n              0.9095, 0.9085, 0.9113, 0.9091, 0.9107, 0.9141, 0.9179, 0.9219, 0.9126,\n              0.9101, 0.9149, 0.9111, 0.9122, 0.9183, 0.9144, 0.9112, 0.9172, 0.9093,\n              0.9111, 0.9204, 0.9118, 0.9155, 0.9204, 0.9160, 0.9128, 0.9128, 0.9089,\n              0.9108, 0.9086, 0.9122, 0.9120, 0.9108, 0.9175, 0.9100, 0.9154, 0.9094,\n              0.9118, 0.9178, 0.9123, 0.9112, 0.9101, 0.9112, 0.9122, 0.9118, 0.9104,\n              0.9130, 0.9121, 0.9152, 0.9170, 0.9198, 0.9088, 0.9177, 0.9111, 0.9148,\n              0.9099, 0.9123, 0.9119, 0.9209, 0.9135, 0.9191, 0.9100, 0.9126, 0.9109,\n              0.9112, 0.9139, 0.9129, 0.9202, 0.9089, 0.9117, 0.9152, 0.9133, 0.9116,\n              0.9122, 0.9154, 0.9118, 0.9115, 0.9132, 0.9133, 0.9147, 0.9116, 0.9119,\n              0.9140, 0.9099, 0.9133, 0.9121, 0.9170, 0.9110, 0.9109, 0.9170, 0.9182,\n              0.9096, 0.9162, 0.9132, 0.9181, 0.9137, 0.9167, 0.9114, 0.9122, 0.9129,\n              0.9115, 0.9095, 0.9185, 0.9135, 0.9152, 0.9143, 0.9111, 0.9101, 0.9111,\n              0.9177, 0.9107, 0.9110, 0.9115, 0.9206, 0.9219, 0.9103, 0.9179, 0.9102,\n              0.9149, 0.9163, 0.9102, 0.9140, 0.9179, 0.9132, 0.9076, 0.9108, 0.9124,\n              0.9171, 0.9214, 0.9123, 0.9135, 0.9140, 0.9105, 0.9097, 0.9151, 0.9098,\n              0.9126, 0.9115, 0.9141, 0.9170, 0.9116, 0.9101, 0.9109, 0.9151, 0.9193,\n              0.9103, 0.9117, 0.9097, 0.9120, 0.9137, 0.9138, 0.9115, 0.9154, 0.9143,\n              0.9141, 0.9122, 0.9100, 0.9114, 0.9102, 0.9097, 0.9105, 0.9171, 0.9101,\n              0.9143, 0.9120, 0.9127, 0.9137, 0.9126, 0.9106, 0.9130, 0.9118, 0.9157,\n              0.9082, 0.9116, 0.9176, 0.9125, 0.9178, 0.9100, 0.9108, 0.9104, 0.9157,\n              0.9185, 0.9128, 0.9123, 0.9097, 0.9112, 0.9115, 0.9137, 0.9078, 0.9202,\n              0.9090, 0.9120, 0.9128, 0.9196, 0.9092, 0.9092, 0.9154, 0.9181, 0.9131,\n              0.9120, 0.9098, 0.9150, 0.9098, 0.9160, 0.9147, 0.9113, 0.9170, 0.9154,\n              0.9117, 0.9125, 0.9112, 0.9106, 0.9105, 0.9115, 0.9122, 0.9142, 0.9108,\n              0.9094, 0.9103, 0.9222, 0.9103, 0.9098, 0.9097, 0.9108, 0.9105, 0.9107,\n              0.9115, 0.9094, 0.9086, 0.9126, 0.9103, 0.9103, 0.9121, 0.9122, 0.9122,\n              0.9139, 0.9106, 0.9102, 0.9113], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-1.9575e-02,  1.4995e-02, -1.8487e-02],\n                [ 1.9757e-02, -1.0822e-02, -1.9559e-02],\n                [-1.1300e-05, -1.3101e-02,  1.3012e-02]],\n      \n               [[ 1.6887e-02, -1.5525e-02, -1.4021e-02],\n                [-5.0789e-03, -1.6562e-02,  1.4734e-02],\n                [-1.4729e-02, -1.0276e-02, -1.7814e-02]],\n      \n               [[ 7.6154e-03,  5.7723e-03,  2.3480e-03],\n                [ 4.6826e-03, -1.8535e-02, -1.3037e-02],\n                [-1.0207e-02, -8.5158e-04,  5.3255e-03]],\n      \n               ...,\n      \n               [[-1.7269e-02, -3.2116e-03, -2.0594e-02],\n                [-1.8560e-02,  1.0760e-02,  7.6387e-03],\n                [-1.3144e-02,  1.4487e-03, -1.9096e-02]],\n      \n               [[-6.4211e-03,  1.3759e-02, -8.1887e-03],\n                [ 1.8009e-02, -1.6580e-02, -1.8823e-02],\n                [-8.0875e-03,  1.3013e-02,  4.8022e-03]],\n      \n               [[-1.2775e-02,  1.3098e-02, -6.6472e-03],\n                [ 7.6833e-03, -2.0496e-02,  1.6534e-02],\n                [-1.5711e-02,  6.5838e-03,  8.1677e-04]]],\n      \n      \n              [[[-1.2093e-02, -1.8575e-02,  1.7376e-02],\n                [-1.0262e-02, -5.7577e-03, -1.4647e-02],\n                [-1.0609e-02, -1.5596e-02, -1.8425e-02]],\n      \n               [[-3.3961e-03, -1.7380e-02, -1.4385e-02],\n                [ 2.2280e-03,  2.0013e-02,  1.7332e-02],\n                [ 1.8786e-02, -1.0826e-02, -1.7676e-02]],\n      \n               [[ 5.2241e-04, -9.6568e-04,  1.3629e-02],\n                [ 1.3429e-02, -1.2309e-02,  9.8668e-03],\n                [-8.5400e-03, -1.7582e-02,  5.4622e-03]],\n      \n               ...,\n      \n               [[ 7.5740e-03, -7.1393e-03, -9.6364e-03],\n                [-3.4451e-03, -7.5548e-03,  8.7034e-03],\n                [-4.9608e-03, -1.8026e-02,  3.9342e-03]],\n      \n               [[ 3.1859e-03,  5.2139e-03, -1.6025e-02],\n                [ 2.0227e-02,  1.7052e-02,  1.4717e-02],\n                [ 4.2775e-03,  1.1615e-02, -8.7027e-03]],\n      \n               [[-1.4915e-02, -5.6843e-03, -2.0493e-03],\n                [ 1.4070e-02, -1.8476e-02, -9.8722e-04],\n                [-1.2718e-02,  1.9386e-02,  2.8772e-03]]],\n      \n      \n              [[[ 1.6069e-02,  8.3767e-03, -8.4678e-03],\n                [ 1.3735e-03,  1.5092e-02, -5.6164e-03],\n                [ 7.5091e-03,  1.7895e-02, -1.8060e-02]],\n      \n               [[-3.6811e-03, -6.9359e-03, -6.7041e-03],\n                [ 1.4081e-02, -1.3604e-03, -1.3334e-02],\n                [ 1.8008e-02,  1.3281e-02, -8.8653e-03]],\n      \n               [[-2.0341e-02,  5.5403e-04,  1.3642e-02],\n                [ 1.7032e-02, -1.3610e-02,  1.6974e-02],\n                [ 8.5624e-04,  1.4510e-03,  1.0687e-02]],\n      \n               ...,\n      \n               [[-1.6532e-03,  4.1399e-03,  1.8690e-02],\n                [-1.5864e-04,  1.0070e-02, -1.0492e-02],\n                [-1.1588e-02, -2.4500e-03,  1.7981e-02]],\n      \n               [[-1.4278e-02,  8.4419e-03,  5.4464e-03],\n                [-3.8078e-03,  1.6189e-02, -1.6707e-03],\n                [-1.0914e-02,  1.4249e-02, -1.6880e-02]],\n      \n               [[-1.5948e-02,  7.3832e-03,  4.6536e-03],\n                [ 5.6337e-03,  2.0503e-02, -1.3553e-02],\n                [-1.9081e-02, -9.7771e-05,  4.3858e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[ 8.8137e-03,  1.2880e-02,  9.9084e-03],\n                [-1.3819e-02, -8.5138e-03,  2.0718e-02],\n                [ 2.0012e-02,  2.0115e-02, -1.4226e-02]],\n      \n               [[ 1.8539e-02,  8.1959e-03, -1.1080e-02],\n                [ 2.0484e-02, -1.3438e-02, -1.6378e-02],\n                [ 6.3263e-03, -9.3912e-03,  4.2062e-03]],\n      \n               [[-7.6127e-03,  1.8088e-02, -1.3669e-02],\n                [ 7.4093e-03,  1.9655e-02, -6.0908e-03],\n                [ 9.4190e-03, -2.7456e-03,  1.8265e-02]],\n      \n               ...,\n      \n               [[ 1.5556e-02, -6.8754e-03, -3.9459e-03],\n                [ 1.3234e-02,  8.0563e-03,  2.2970e-03],\n                [-1.0622e-02, -1.1703e-02, -1.8937e-02]],\n      \n               [[-9.0312e-03, -1.6835e-03,  3.9874e-03],\n                [ 1.6921e-02, -8.5960e-03,  1.8741e-02],\n                [ 7.5702e-03, -1.5747e-03,  1.5929e-02]],\n      \n               [[ 1.4411e-02, -1.1745e-03,  1.9378e-02],\n                [-1.1296e-02,  1.6814e-02, -9.0802e-03],\n                [ 2.3597e-03,  1.9565e-02, -9.8384e-03]]],\n      \n      \n              [[[-1.8605e-02, -1.3413e-02,  1.2038e-02],\n                [ 1.3022e-02, -7.3660e-03, -4.3021e-03],\n                [-6.7237e-03, -4.8101e-04,  4.1352e-03]],\n      \n               [[-5.3566e-03,  1.5709e-02, -5.6728e-03],\n                [ 1.3709e-02,  1.7422e-03, -1.3525e-02],\n                [-1.2797e-02, -1.2742e-02,  7.0217e-03]],\n      \n               [[-2.0177e-02, -5.1107e-03,  1.2173e-02],\n                [ 5.2743e-03, -2.0700e-02,  2.7664e-03],\n                [-1.3867e-02, -2.5953e-03, -1.6931e-03]],\n      \n               ...,\n      \n               [[-9.6621e-03,  1.8140e-02, -1.1069e-02],\n                [ 3.3123e-03,  4.6104e-03,  1.5855e-02],\n                [-1.4215e-02, -1.2506e-03,  8.9953e-03]],\n      \n               [[ 1.0180e-02,  1.7895e-02, -1.2143e-02],\n                [-1.7104e-02, -1.7738e-02, -3.3713e-03],\n                [-3.3606e-04, -7.4513e-03, -4.8333e-03]],\n      \n               [[ 1.4207e-02,  9.5398e-03, -4.3586e-03],\n                [ 6.4276e-03, -5.6945e-03,  1.3180e-02],\n                [ 6.4193e-03, -5.0612e-03,  1.1405e-02]]],\n      \n      \n              [[[ 1.1065e-02, -7.2354e-03,  2.0833e-03],\n                [-8.1810e-03, -8.3018e-03, -1.6864e-02],\n                [-1.6296e-02, -1.4492e-02,  1.9119e-04]],\n      \n               [[-1.9572e-02,  6.9601e-03,  1.3536e-02],\n                [-2.0100e-02,  2.3444e-03,  5.7887e-03],\n                [-1.3467e-02,  1.7936e-02, -1.6911e-03]],\n      \n               [[ 2.3526e-03,  1.2020e-02, -1.4570e-02],\n                [-1.7292e-02,  1.3594e-02,  7.6345e-03],\n                [ 1.9175e-02, -3.1914e-03,  1.8137e-02]],\n      \n               ...,\n      \n               [[ 1.7487e-02, -2.9495e-03, -8.3376e-03],\n                [-2.9508e-03,  1.3192e-02,  5.0216e-03],\n                [ 9.2901e-03, -8.7827e-03,  2.8411e-03]],\n      \n               [[-1.4616e-02,  8.4393e-03,  5.9684e-04],\n                [-1.9264e-02,  1.1752e-02, -1.0601e-02],\n                [ 8.4071e-03,  1.6611e-02,  7.3949e-03]],\n      \n               [[-4.6541e-03,  1.7705e-03,  3.2644e-03],\n                [ 1.5602e-02, -1.4064e-02, -1.1212e-02],\n                [ 4.3660e-04,  2.0902e-03,  2.4649e-03]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 9.6197e-03, -1.4388e-02,  8.6170e-04, -2.0432e-02, -1.4309e-03,\n              -2.3429e-03, -2.9619e-02,  1.8005e-02, -1.6034e-04,  5.5726e-03,\n               7.7433e-03,  8.5256e-03,  1.4870e-02, -1.6849e-03,  1.6227e-02,\n              -1.6650e-02,  1.1758e-03, -2.1121e-02, -3.2688e-03,  1.9157e-02,\n              -1.2319e-02, -2.2260e-02,  1.8577e-02,  3.6267e-02, -5.7720e-03,\n               3.0291e-03,  7.9332e-03,  1.3202e-03,  3.3207e-03, -1.9868e-02,\n               1.6538e-02, -4.8186e-04,  7.2802e-03,  1.0786e-02, -8.6938e-03,\n              -1.7321e-02,  6.4222e-04,  4.4209e-03, -2.4064e-02,  4.2113e-03,\n              -5.4044e-03,  1.5721e-03,  1.3136e-03, -2.5415e-02, -1.4842e-03,\n               5.6990e-03,  8.1800e-03,  4.5194e-03, -1.3997e-02,  2.1419e-02,\n              -2.1518e-02,  6.9167e-03, -7.8114e-03, -5.2413e-03, -1.6193e-02,\n               8.3066e-04,  1.1256e-03,  1.2860e-02, -2.6901e-05,  1.0351e-02,\n               3.1060e-02, -4.2648e-03,  1.5724e-03, -1.0528e-02,  4.5892e-03,\n               2.1747e-04, -7.1193e-03, -1.6451e-02,  1.2508e-02,  1.2532e-02,\n              -2.2745e-02,  1.1148e-02, -1.4903e-02, -3.0725e-03, -3.3624e-03,\n              -5.3292e-03,  9.8679e-03, -4.7757e-03,  8.1853e-03,  9.2597e-04,\n              -7.1001e-03, -1.1435e-02,  1.1689e-02,  1.1812e-02, -9.9845e-04,\n               7.0459e-03,  1.9612e-02,  1.5990e-02,  2.8253e-03,  4.8381e-03,\n              -1.6018e-03, -1.0641e-02, -2.0917e-03, -1.1084e-02,  2.0451e-02,\n               1.3209e-02, -1.1530e-02,  1.1236e-02,  1.1065e-03,  6.6850e-03,\n              -1.2914e-03,  1.8099e-02,  3.3411e-02,  1.1106e-02,  4.5819e-03,\n               1.7166e-03, -1.7479e-02,  1.5787e-04, -1.2537e-02, -9.3390e-03,\n               1.8105e-02,  2.1425e-02, -5.9571e-03, -8.8624e-04,  1.0298e-02,\n               5.2482e-03,  2.5975e-03, -3.7156e-03, -4.7762e-04, -2.1370e-02,\n              -1.4279e-02, -6.7494e-03, -1.9389e-03,  5.1636e-03,  1.3983e-02,\n               4.9994e-03, -4.8133e-03,  1.0797e-02,  2.4554e-02,  4.7576e-03,\n              -9.4333e-03,  8.8940e-03, -1.2248e-03, -7.6379e-04,  9.5226e-03,\n               1.2649e-02, -1.5156e-03, -7.8697e-04,  2.0631e-02, -5.1770e-03,\n              -9.2527e-03, -7.5285e-03, -8.9864e-03,  1.7447e-02,  1.7957e-02,\n               3.0424e-03, -3.8201e-03,  5.7031e-03,  1.1505e-02,  9.8027e-03,\n               3.7214e-03, -1.2672e-02, -1.5680e-02, -5.3994e-03, -4.8079e-03,\n              -1.0996e-02,  1.5516e-02,  1.0956e-02,  2.7898e-02, -1.3413e-03,\n               1.5695e-02, -3.7671e-03,  1.7464e-02, -1.7234e-02, -4.3332e-03,\n               3.0595e-03, -2.6325e-03,  5.4979e-03,  2.3871e-02,  8.9532e-03,\n               1.8813e-02, -4.6133e-03, -1.2078e-02,  7.7985e-03, -2.7670e-03,\n              -1.1737e-02, -2.2827e-03, -2.3804e-03,  1.8193e-02,  8.9391e-03,\n              -3.2478e-03, -1.0675e-02,  8.4880e-03, -1.0067e-03,  6.0757e-03,\n              -2.8002e-03,  2.0142e-02,  4.0973e-03, -1.2282e-02,  5.6730e-03,\n               1.1036e-02, -1.3238e-02, -1.1635e-02, -3.1116e-03,  1.1647e-03,\n               2.1190e-03, -1.3701e-02, -5.8783e-03,  1.8556e-03,  1.9378e-02,\n              -9.3998e-03, -2.7445e-02, -4.4646e-03, -2.0177e-02, -6.1071e-03,\n               1.1999e-02, -8.1998e-03, -5.3720e-03,  9.8860e-04, -3.5675e-03,\n              -1.2511e-02, -6.5650e-03,  5.4433e-03,  7.9218e-03, -1.2868e-02,\n              -8.9521e-03, -2.0233e-03,  6.5618e-03,  1.0433e-02, -1.7264e-02,\n              -7.2752e-03,  1.6190e-02, -2.3409e-03, -1.1510e-02, -5.2503e-03,\n               2.2071e-02,  1.8844e-02,  1.4017e-02, -2.9248e-04, -5.2537e-03,\n               1.8216e-02,  5.9316e-03, -1.5559e-02,  9.6138e-03,  9.7790e-03,\n              -1.2420e-02, -7.0688e-03, -1.1842e-02, -2.6983e-02,  1.1648e-03,\n               8.0518e-03,  2.6696e-02, -3.1177e-03,  1.5136e-02,  5.4467e-04,\n               6.5475e-03,  2.3531e-02,  1.8422e-03,  1.7230e-02,  8.3859e-04,\n              -1.1836e-02, -1.2529e-02, -6.2394e-03, -3.0705e-03,  2.6044e-03,\n               1.3418e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9049, 0.9045, 0.9066, 0.9056, 0.9065, 0.9056, 0.9049, 0.9046, 0.9062,\n              0.9049, 0.9046, 0.9057, 0.9045, 0.9054, 0.9069, 0.9043, 0.9078, 0.9056,\n              0.9084, 0.9054, 0.9047, 0.9067, 0.9044, 0.9060, 0.9061, 0.9057, 0.9049,\n              0.9065, 0.9052, 0.9055, 0.9060, 0.9060, 0.9067, 0.9056, 0.9040, 0.9046,\n              0.9047, 0.9047, 0.9130, 0.9048, 0.9087, 0.9057, 0.9045, 0.9067, 0.9095,\n              0.9048, 0.9088, 0.9051, 0.9048, 0.9047, 0.9048, 0.9059, 0.9057, 0.9049,\n              0.9039, 0.9050, 0.9049, 0.9047, 0.9062, 0.9053, 0.9075, 0.9100, 0.9055,\n              0.9049, 0.9046, 0.9053, 0.9073, 0.9062, 0.9061, 0.9049, 0.9096, 0.9061,\n              0.9050, 0.9041, 0.9058, 0.9066, 0.9060, 0.9050, 0.9044, 0.9056, 0.9072,\n              0.9068, 0.9046, 0.9035, 0.9053, 0.9056, 0.9046, 0.9041, 0.9050, 0.9053,\n              0.9051, 0.9057, 0.9056, 0.9046, 0.9068, 0.9050, 0.9074, 0.9052, 0.9097,\n              0.9051, 0.9052, 0.9053, 0.9072, 0.9047, 0.9047, 0.9098, 0.9068, 0.9051,\n              0.9043, 0.9045, 0.9064, 0.9062, 0.9053, 0.9040, 0.9058, 0.9040, 0.9057,\n              0.9044, 0.9058, 0.9045, 0.9043, 0.9066, 0.9103, 0.9061, 0.9057, 0.9047,\n              0.9088, 0.9057, 0.9054, 0.9055, 0.9062, 0.9051, 0.9045, 0.9052, 0.9053,\n              0.9051, 0.9089, 0.9076, 0.9063, 0.9055, 0.9050, 0.9066, 0.9045, 0.9050,\n              0.9081, 0.9044, 0.9044, 0.9047, 0.9163, 0.9053, 0.9056, 0.9061, 0.9063,\n              0.9069, 0.9072, 0.9099, 0.9063, 0.9055, 0.9045, 0.9061, 0.9071, 0.9064,\n              0.9054, 0.9058, 0.9055, 0.9039, 0.9052, 0.9048, 0.9050, 0.9057, 0.9062,\n              0.9070, 0.9052, 0.9088, 0.9044, 0.9046, 0.9068, 0.9061, 0.9082, 0.9048,\n              0.9067, 0.9045, 0.9087, 0.9053, 0.9060, 0.9063, 0.9102, 0.9055, 0.9037,\n              0.9054, 0.9051, 0.9053, 0.9058, 0.9047, 0.9050, 0.9058, 0.9056, 0.9054,\n              0.9078, 0.9076, 0.9054, 0.9042, 0.9071, 0.9059, 0.9068, 0.9042, 0.9046,\n              0.9067, 0.9081, 0.9052, 0.9051, 0.9050, 0.9043, 0.9067, 0.9050, 0.9063,\n              0.9069, 0.9054, 0.9067, 0.9043, 0.9079, 0.9090, 0.9055, 0.9062, 0.9040,\n              0.9053, 0.9070, 0.9056, 0.9058, 0.9083, 0.9062, 0.9042, 0.9050, 0.9056,\n              0.9037, 0.9061, 0.9070, 0.9070, 0.9071, 0.9079, 0.9064, 0.9079, 0.9086,\n              0.9080, 0.9097, 0.9077, 0.9046, 0.9047, 0.9054, 0.9046, 0.9049, 0.9060,\n              0.9050, 0.9062, 0.9044, 0.9045], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[ 0.0024]],\n        \n                 [[ 0.0423]],\n        \n                 [[ 0.0694]],\n        \n                 ...,\n        \n                 [[ 0.0583]],\n        \n                 [[ 0.0629]],\n        \n                 [[ 0.0119]]],\n        \n        \n                [[[ 0.0009]],\n        \n                 [[-0.0855]],\n        \n                 [[ 0.0071]],\n        \n                 ...,\n        \n                 [[ 0.0580]],\n        \n                 [[-0.0005]],\n        \n                 [[-0.0271]]],\n        \n        \n                [[[-0.0076]],\n        \n                 [[ 0.0319]],\n        \n                 [[-0.0540]],\n        \n                 ...,\n        \n                 [[-0.0524]],\n        \n                 [[-0.0130]],\n        \n                 [[ 0.0767]]],\n        \n        \n                ...,\n        \n        \n                [[[-0.0333]],\n        \n                 [[-0.0841]],\n        \n                 [[ 0.0087]],\n        \n                 ...,\n        \n                 [[ 0.0745]],\n        \n                 [[ 0.0603]],\n        \n                 [[-0.0649]]],\n        \n        \n                [[[ 0.0854]],\n        \n                 [[-0.0773]],\n        \n                 [[-0.0854]],\n        \n                 ...,\n        \n                 [[ 0.0029]],\n        \n                 [[ 0.0421]],\n        \n                 [[ 0.0732]]],\n        \n        \n                [[[ 0.0502]],\n        \n                 [[ 0.0656]],\n        \n                 [[ 0.0310]],\n        \n                 ...,\n        \n                 [[-0.0660]],\n        \n                 [[-0.0780]],\n        \n                 [[ 0.0261]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n               requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 4.3582e-02, -4.8348e-03, -3.3708e-02,  3.3967e-03, -2.5444e-02,\n                 3.3944e-02,  3.4191e-02,  3.9240e-02, -5.0992e-02,  1.1332e-02,\n                -3.5450e-02, -2.9689e-02,  2.2834e-02, -4.6811e-02, -5.2883e-03,\n                 1.5328e-02, -9.7011e-03, -4.1994e-03, -3.9482e-02, -2.3239e-03,\n                -9.1979e-04,  3.3444e-02, -2.0184e-02, -4.6837e-02, -2.5068e-02,\n                 6.4982e-04,  5.4059e-02,  1.8050e-02, -3.1134e-02,  1.0045e-02,\n                -2.9173e-02,  1.3233e-02,  8.5314e-03, -2.5905e-02, -4.0439e-02,\n                 1.4669e-02,  2.4559e-02,  1.2967e-02,  1.9472e-02, -5.2335e-02,\n                -2.5393e-02,  3.3080e-02, -4.0855e-02, -1.0677e-02, -8.6702e-03,\n                 3.3921e-02, -4.9904e-02, -3.4474e-02,  5.1430e-03, -3.5749e-02,\n                -4.7684e-02,  6.1003e-02, -9.5305e-03,  4.0350e-02, -4.3571e-02,\n                 2.8138e-02,  2.2203e-02, -1.0154e-02, -2.2556e-02,  1.6714e-03,\n                 4.3338e-02, -2.9638e-02,  4.9682e-02, -5.9655e-02, -2.3463e-02,\n                -5.2605e-03, -1.9614e-02,  4.4808e-03, -2.5091e-02, -5.1343e-02,\n                 1.0609e-02, -4.9609e-02,  6.7107e-02,  2.0798e-02, -4.2474e-02,\n                -2.6022e-02, -1.3600e-02,  7.2343e-04, -1.8738e-03, -3.5628e-02,\n                -3.1893e-02,  2.7595e-02, -6.8539e-03, -2.1309e-02, -4.7765e-03,\n                 8.3174e-03,  8.4110e-02,  1.8754e-02,  8.3564e-03, -1.2784e-02,\n                 4.5358e-03,  2.5023e-02,  6.3117e-03,  2.9495e-02, -2.7638e-02,\n                 1.3783e-02,  1.8648e-02, -2.8388e-02, -1.8217e-02,  3.3195e-03,\n                 2.9067e-02,  3.7413e-02,  2.7591e-02,  1.9411e-03,  1.7398e-05,\n                -1.6315e-02, -1.4642e-03, -3.9583e-02,  2.1819e-02, -1.8775e-03,\n                -2.8403e-02,  3.7725e-02,  8.9658e-03,  1.2037e-02,  4.4366e-02,\n                -9.2786e-03, -3.2239e-02, -1.1225e-02, -2.0803e-02,  3.4237e-02,\n                -5.7310e-02, -9.9177e-05,  3.2520e-02, -1.5446e-02, -2.3967e-02,\n                -4.2947e-03,  1.3756e-02,  3.3022e-02, -3.4485e-03,  2.2980e-02,\n                -5.0583e-02,  1.6532e-02, -1.9582e-03, -3.8358e-02,  8.3422e-02,\n                 9.1907e-03, -2.3651e-02,  2.5679e-02,  9.2750e-03, -3.3092e-02,\n                 1.9925e-03, -1.6406e-02, -5.6582e-02,  4.0113e-03, -2.0139e-02,\n                -1.6398e-02, -5.0055e-02,  3.3687e-02, -9.5784e-03, -2.5294e-02,\n                -2.3804e-02, -1.1472e-02, -2.6417e-02,  5.6817e-02, -3.5227e-03,\n                 7.5787e-03, -2.1968e-02,  3.0059e-02,  2.8715e-02,  1.4195e-02,\n                 6.9129e-03,  1.6410e-02,  1.5802e-02,  3.4626e-02, -1.4846e-02,\n                -1.9439e-02, -3.5892e-02, -3.3072e-02, -8.5747e-03, -3.3377e-03,\n                 1.0971e-02,  9.5900e-04,  1.9367e-02, -1.6437e-02, -3.4255e-02,\n                 4.5757e-03, -3.5775e-02,  5.0399e-02, -3.7517e-03, -2.0018e-02,\n                 9.9771e-03, -5.8867e-03, -1.0009e-02,  1.7645e-02, -3.9639e-02,\n                 5.0117e-02, -1.1247e-02,  2.1519e-02,  6.5108e-02, -4.2248e-02,\n                -3.7662e-02, -3.8738e-02, -9.8827e-04, -1.3997e-02,  1.9059e-02,\n                 1.6388e-02,  1.2594e-03, -1.5954e-03,  5.1697e-02, -4.2627e-02,\n                 4.8378e-02,  4.3457e-03, -2.1707e-02,  5.8505e-02, -1.7303e-02,\n                -6.2556e-03, -8.0855e-03, -1.8954e-02,  4.6206e-02,  4.1684e-03,\n                 4.4173e-02, -1.7722e-03,  5.5954e-02,  3.1086e-02,  1.1375e-02,\n                 6.6372e-03,  1.4901e-02,  3.6540e-02,  1.5144e-02, -3.2664e-02,\n                -1.3746e-02, -2.9768e-02,  2.1284e-02, -4.7326e-02,  6.3893e-03,\n                -3.1619e-02, -1.8129e-02,  3.6009e-02, -5.8728e-02,  3.0206e-02,\n                -2.8868e-02, -2.1021e-02,  1.0079e-02,  1.6013e-02,  1.3943e-03,\n                 1.0276e-02,  4.2699e-03,  1.4676e-02,  2.5320e-02,  2.3119e-02,\n                -3.4808e-02, -3.5024e-02,  1.3866e-02,  1.0689e-02,  4.6459e-02,\n                -4.3619e-02, -1.9506e-02, -4.6436e-02, -2.0920e-02,  3.7911e-02,\n                 3.0246e-02,  1.6124e-02, -1.5162e-02, -3.0790e-02,  3.6239e-02,\n                -2.5261e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9282, 0.9164, 0.9211, 0.9112, 0.9149, 0.9189, 0.9121, 0.9186, 0.9197,\n                0.9152, 0.9211, 0.9168, 0.9177, 0.9232, 0.9174, 0.9257, 0.9279, 0.9241,\n                0.9176, 0.9185, 0.9138, 0.9166, 0.9185, 0.9122, 0.9356, 0.9137, 0.9156,\n                0.9197, 0.9192, 0.9257, 0.9105, 0.9140, 0.9149, 0.9133, 0.9244, 0.9188,\n                0.9215, 0.9161, 0.9196, 0.9202, 0.9227, 0.9216, 0.9193, 0.9172, 0.9170,\n                0.9222, 0.9198, 0.9255, 0.9320, 0.9165, 0.9155, 0.9182, 0.9223, 0.9179,\n                0.9199, 0.9164, 0.9296, 0.9196, 0.9188, 0.9162, 0.9167, 0.9293, 0.9198,\n                0.9152, 0.9196, 0.9130, 0.9152, 0.9224, 0.9231, 0.9145, 0.9168, 0.9199,\n                0.9130, 0.9136, 0.9185, 0.9218, 0.9154, 0.9153, 0.9197, 0.9306, 0.9230,\n                0.9193, 0.9223, 0.9180, 0.9215, 0.9134, 0.9206, 0.9178, 0.9208, 0.9187,\n                0.9152, 0.9135, 0.9121, 0.9135, 0.9150, 0.9160, 0.9193, 0.9204, 0.9217,\n                0.9151, 0.9205, 0.9152, 0.9209, 0.9275, 0.9251, 0.9217, 0.9167, 0.9229,\n                0.9195, 0.9139, 0.9155, 0.9193, 0.9241, 0.9134, 0.9195, 0.9178, 0.9235,\n                0.9158, 0.9195, 0.9192, 0.9193, 0.9238, 0.9144, 0.9205, 0.9189, 0.9195,\n                0.9254, 0.9138, 0.9216, 0.9226, 0.9273, 0.9123, 0.9121, 0.9130, 0.9156,\n                0.9173, 0.9215, 0.9182, 0.9203, 0.9182, 0.9168, 0.9132, 0.9242, 0.9155,\n                0.9241, 0.9187, 0.9180, 0.9137, 0.9182, 0.9174, 0.9146, 0.9195, 0.9196,\n                0.9184, 0.9359, 0.9172, 0.9192, 0.9204, 0.9173, 0.9202, 0.9228, 0.9153,\n                0.9140, 0.9189, 0.9139, 0.9163, 0.9359, 0.9183, 0.9180, 0.9147, 0.9154,\n                0.9145, 0.9177, 0.9176, 0.9134, 0.9223, 0.9259, 0.9219, 0.9201, 0.9156,\n                0.9173, 0.9205, 0.9215, 0.9209, 0.9163, 0.9206, 0.9173, 0.9233, 0.9274,\n                0.9193, 0.9215, 0.9206, 0.9201, 0.9196, 0.9149, 0.9164, 0.9335, 0.9198,\n                0.9182, 0.9211, 0.9164, 0.9143, 0.9178, 0.9217, 0.9195, 0.9176, 0.9178,\n                0.9176, 0.9321, 0.9170, 0.9160, 0.9144, 0.9138, 0.9303, 0.9239, 0.9173,\n                0.9228, 0.9240, 0.9252, 0.9168, 0.9244, 0.9209, 0.9151, 0.9176, 0.9165,\n                0.9237, 0.9219, 0.9145, 0.9165, 0.9142, 0.9150, 0.9154, 0.9154, 0.9143,\n                0.9192, 0.9208, 0.9174, 0.9159, 0.9241, 0.9185, 0.9127, 0.9123, 0.9260,\n                0.9173, 0.9200, 0.9232, 0.9330, 0.9194, 0.9208, 0.9225, 0.9120, 0.9219,\n                0.9154, 0.9185, 0.9156, 0.9130], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [256, 128, 3, 3]], ["0.bn1.weight", [256]], ["0.bn1.bias", [256]], ["0.conv2.weight", [256, 256, 3, 3]], ["0.bn2.weight", [256]], ["0.bn2.bias", [256]], ["0.downsample.0.weight", [256, 128, 1, 1]], ["0.downsample.1.weight", [256]], ["0.downsample.1.bias", [256]]], "output_shape": [[512, 256, 2, 2]], "num_parameters": [294912, 256, 256, 589824, 256, 256, 32768, 256, 256]}, {"name": "layer4", "id": 140449530341360, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 0.0176, -0.0162,  0.0149],\n                [-0.0026, -0.0150, -0.0045],\n                [ 0.0014,  0.0014, -0.0206]],\n      \n               [[-0.0079,  0.0096, -0.0044],\n                [-0.0147, -0.0172,  0.0031],\n                [ 0.0007, -0.0165,  0.0198]],\n      \n               [[ 0.0017,  0.0152, -0.0133],\n                [-0.0151, -0.0179,  0.0055],\n                [ 0.0006, -0.0031,  0.0181]],\n      \n               ...,\n      \n               [[-0.0165,  0.0180,  0.0057],\n                [ 0.0048, -0.0060, -0.0161],\n                [ 0.0186, -0.0104,  0.0184]],\n      \n               [[ 0.0179,  0.0122, -0.0173],\n                [-0.0071,  0.0004,  0.0177],\n                [-0.0165, -0.0203,  0.0073]],\n      \n               [[ 0.0197,  0.0112,  0.0152],\n                [ 0.0018,  0.0123, -0.0036],\n                [ 0.0203, -0.0039,  0.0163]]],\n      \n      \n              [[[-0.0208,  0.0205,  0.0108],\n                [-0.0133,  0.0162, -0.0119],\n                [ 0.0183, -0.0074, -0.0146]],\n      \n               [[ 0.0160, -0.0034, -0.0056],\n                [-0.0200, -0.0157, -0.0050],\n                [ 0.0121, -0.0115,  0.0115]],\n      \n               [[ 0.0110,  0.0132, -0.0197],\n                [-0.0127,  0.0167,  0.0049],\n                [-0.0171,  0.0173, -0.0160]],\n      \n               ...,\n      \n               [[-0.0099,  0.0059,  0.0145],\n                [ 0.0029, -0.0120, -0.0021],\n                [-0.0008,  0.0120, -0.0204]],\n      \n               [[-0.0032, -0.0067,  0.0162],\n                [-0.0007, -0.0197,  0.0159],\n                [-0.0165,  0.0200, -0.0111]],\n      \n               [[-0.0049, -0.0034, -0.0076],\n                [ 0.0018, -0.0026, -0.0159],\n                [-0.0034, -0.0143,  0.0170]]],\n      \n      \n              [[[ 0.0164, -0.0147,  0.0083],\n                [-0.0030, -0.0020,  0.0120],\n                [ 0.0149,  0.0205, -0.0151]],\n      \n               [[ 0.0047,  0.0192, -0.0004],\n                [-0.0103, -0.0049,  0.0179],\n                [-0.0161, -0.0030,  0.0193]],\n      \n               [[-0.0047,  0.0018, -0.0101],\n                [ 0.0203,  0.0078, -0.0114],\n                [-0.0039,  0.0060,  0.0135]],\n      \n               ...,\n      \n               [[-0.0168, -0.0029,  0.0196],\n                [-0.0192,  0.0041, -0.0119],\n                [ 0.0075,  0.0166, -0.0021]],\n      \n               [[-0.0129, -0.0075,  0.0067],\n                [-0.0093,  0.0033, -0.0126],\n                [ 0.0133, -0.0137,  0.0110]],\n      \n               [[ 0.0024,  0.0162, -0.0010],\n                [-0.0015,  0.0138,  0.0084],\n                [ 0.0154, -0.0101,  0.0142]]],\n      \n      \n              ...,\n      \n      \n              [[[ 0.0092,  0.0070,  0.0012],\n                [ 0.0050, -0.0012,  0.0003],\n                [ 0.0050, -0.0001, -0.0139]],\n      \n               [[-0.0165,  0.0128, -0.0044],\n                [-0.0141,  0.0031, -0.0014],\n                [-0.0177, -0.0056, -0.0046]],\n      \n               [[ 0.0139,  0.0025,  0.0004],\n                [ 0.0124,  0.0072, -0.0181],\n                [-0.0096, -0.0148, -0.0139]],\n      \n               ...,\n      \n               [[-0.0156, -0.0198, -0.0032],\n                [-0.0109, -0.0156, -0.0135],\n                [-0.0097,  0.0148, -0.0049]],\n      \n               [[ 0.0110, -0.0200, -0.0159],\n                [-0.0190, -0.0048,  0.0091],\n                [ 0.0154, -0.0195, -0.0054]],\n      \n               [[-0.0162,  0.0123,  0.0078],\n                [-0.0078,  0.0088,  0.0020],\n                [-0.0030, -0.0120, -0.0205]]],\n      \n      \n              [[[-0.0041, -0.0008,  0.0010],\n                [-0.0109,  0.0085, -0.0027],\n                [ 0.0075,  0.0145,  0.0060]],\n      \n               [[ 0.0201,  0.0182,  0.0194],\n                [ 0.0072,  0.0114, -0.0196],\n                [-0.0204, -0.0105,  0.0183]],\n      \n               [[-0.0037, -0.0187,  0.0182],\n                [-0.0160,  0.0061,  0.0025],\n                [-0.0203, -0.0010,  0.0157]],\n      \n               ...,\n      \n               [[-0.0124,  0.0068, -0.0163],\n                [-0.0012, -0.0048, -0.0083],\n                [-0.0170,  0.0186,  0.0052]],\n      \n               [[ 0.0090, -0.0059, -0.0197],\n                [ 0.0050,  0.0098,  0.0026],\n                [-0.0076,  0.0064, -0.0191]],\n      \n               [[ 0.0018, -0.0123,  0.0119],\n                [ 0.0182, -0.0058,  0.0119],\n                [ 0.0100, -0.0026, -0.0187]]],\n      \n      \n              [[[ 0.0192,  0.0172, -0.0076],\n                [ 0.0043,  0.0017, -0.0164],\n                [-0.0184,  0.0109,  0.0065]],\n      \n               [[ 0.0189,  0.0131, -0.0205],\n                [-0.0052, -0.0191, -0.0024],\n                [-0.0054, -0.0168,  0.0118]],\n      \n               [[-0.0063,  0.0181, -0.0097],\n                [ 0.0039, -0.0066,  0.0077],\n                [-0.0081,  0.0198,  0.0064]],\n      \n               ...,\n      \n               [[ 0.0170, -0.0078,  0.0070],\n                [-0.0070,  0.0051, -0.0128],\n                [ 0.0005,  0.0057,  0.0070]],\n      \n               [[-0.0195, -0.0138,  0.0119],\n                [ 0.0047, -0.0127, -0.0005],\n                [ 0.0080, -0.0064, -0.0053]],\n      \n               [[-0.0013,  0.0109, -0.0054],\n                [-0.0115,  0.0100, -0.0045],\n                [ 0.0182, -0.0121,  0.0051]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0066,  0.0140,  0.0574,  0.0178, -0.0105, -0.0394,  0.0033,  0.0013,\n              -0.0325, -0.0337, -0.0372, -0.0028, -0.0043,  0.0060, -0.0292,  0.0048,\n               0.0388,  0.0179,  0.0314, -0.0621,  0.0083,  0.0559,  0.0184,  0.0268,\n              -0.0070, -0.0534, -0.0062,  0.0099,  0.0121,  0.0074, -0.0051, -0.0161,\n              -0.0180,  0.0004, -0.0183,  0.0289,  0.0207,  0.0099,  0.0534, -0.0247,\n              -0.0114, -0.0455, -0.0307, -0.0327,  0.0061,  0.0222,  0.0202,  0.0099,\n               0.0080,  0.0176, -0.0092,  0.0001, -0.0097,  0.0058,  0.0034,  0.0251,\n              -0.0488,  0.0374, -0.0209, -0.0390, -0.0130, -0.0516, -0.0326,  0.0134,\n               0.0168,  0.0198,  0.0306, -0.0137,  0.0324,  0.0072, -0.0325, -0.0182,\n              -0.0182,  0.0054,  0.0340, -0.0071, -0.0174, -0.0049, -0.0154, -0.0114,\n              -0.0454,  0.0215, -0.0045, -0.0232, -0.0323,  0.0137, -0.0147, -0.0164,\n              -0.0029, -0.0035,  0.0519,  0.0419, -0.0151, -0.0078,  0.0166, -0.0363,\n               0.0121, -0.0033,  0.0195,  0.0153, -0.0004, -0.0202,  0.0521,  0.0088,\n              -0.0129, -0.0224, -0.0185, -0.0361,  0.0132,  0.0085, -0.0347, -0.0010,\n              -0.0192, -0.0283,  0.0269,  0.0205,  0.0077,  0.0304,  0.0543, -0.0128,\n               0.0521, -0.0174, -0.0037,  0.0032,  0.0306, -0.0363,  0.0087,  0.0258,\n               0.0233,  0.0048,  0.0355, -0.0104, -0.0074, -0.0053, -0.0153,  0.0011,\n              -0.0248, -0.0250, -0.0268,  0.0230,  0.0251,  0.0237, -0.0380,  0.0130,\n              -0.0293, -0.0186,  0.0299,  0.0329,  0.0227, -0.0200, -0.0224,  0.0190,\n               0.0013, -0.0039, -0.0286, -0.0047, -0.0029, -0.0163,  0.0493, -0.0128,\n              -0.0313, -0.0155, -0.0058,  0.0110,  0.0264, -0.0047,  0.0196, -0.0301,\n              -0.0020,  0.0212, -0.0057,  0.0178,  0.0398,  0.0548,  0.0150, -0.0148,\n               0.0367,  0.0185, -0.0039, -0.0376,  0.0197,  0.0103, -0.0427, -0.0043,\n              -0.0174,  0.0223, -0.0202, -0.0163, -0.0060, -0.0008,  0.0324, -0.0230,\n               0.0124, -0.0343, -0.0106,  0.0400, -0.0222,  0.0176, -0.0247,  0.0047,\n               0.0127, -0.0238, -0.0503,  0.0566,  0.0181, -0.0108, -0.0195,  0.0078,\n              -0.0217, -0.0203,  0.0667, -0.0211,  0.0030,  0.0471, -0.0202, -0.0065,\n              -0.0035,  0.0497, -0.0027,  0.0367,  0.0410,  0.0063,  0.0112,  0.0001,\n               0.0113, -0.0152, -0.0087,  0.0298,  0.0642, -0.0172, -0.0217, -0.0579,\n               0.0266, -0.0383,  0.0309, -0.0208, -0.0356,  0.0468, -0.0125,  0.0675,\n               0.0011, -0.0037, -0.0143, -0.0052,  0.0135,  0.0097,  0.0395, -0.0178,\n              -0.0129, -0.0323, -0.0417,  0.0167,  0.0415,  0.0515,  0.0247, -0.0259,\n               0.0496,  0.0363,  0.0082, -0.0016,  0.0196,  0.0206, -0.0071, -0.0773,\n               0.0368,  0.0458, -0.0062,  0.0326, -0.0325,  0.0269, -0.0186, -0.0265,\n              -0.0100,  0.0277, -0.0049, -0.0091,  0.0004,  0.0030, -0.0374,  0.0078,\n               0.0090,  0.0306,  0.0279,  0.0041, -0.0147,  0.0011,  0.0293, -0.0311,\n              -0.0092,  0.0200, -0.0074, -0.0206,  0.0030,  0.0046,  0.0150, -0.0189,\n              -0.0175, -0.0234,  0.0066,  0.0175,  0.0023, -0.0259, -0.0207,  0.0167,\n               0.0111,  0.0634,  0.0409, -0.0145, -0.0021, -0.0079,  0.0107,  0.0226,\n              -0.0189,  0.0299,  0.0062,  0.0033,  0.0289,  0.0400, -0.0413, -0.0022,\n               0.0072, -0.0344, -0.0457,  0.0223, -0.0282,  0.0354,  0.0274,  0.0022,\n              -0.0244,  0.0190, -0.0079, -0.0436, -0.0009,  0.0211, -0.0052, -0.0175,\n              -0.0283,  0.0094, -0.0123, -0.0109, -0.0242,  0.0122, -0.0106,  0.0107,\n              -0.0237,  0.0118, -0.0052, -0.0048,  0.0231, -0.0002,  0.0100, -0.0146,\n               0.0343,  0.0033, -0.0040,  0.0626,  0.0141, -0.0044, -0.0712, -0.0068,\n              -0.0025, -0.0126,  0.0339,  0.0054, -0.0361,  0.0204, -0.0287,  0.0480,\n               0.0232, -0.0180, -0.0013, -0.0051, -0.0008, -0.0257, -0.0239,  0.0030,\n               0.0232,  0.0107, -0.0033, -0.0098, -0.0334, -0.0099,  0.0209,  0.0078,\n               0.0059, -0.0205, -0.0281,  0.0655,  0.0007,  0.0240, -0.0144, -0.0040,\n               0.0059,  0.0079, -0.0328,  0.0095, -0.0273, -0.0232,  0.0100,  0.0451,\n              -0.0014,  0.0063,  0.0089,  0.0230,  0.0139,  0.0205, -0.0399, -0.0190,\n              -0.0342,  0.0188,  0.0023,  0.0230,  0.0406,  0.0029, -0.0172,  0.0354,\n               0.0185,  0.0181, -0.0138, -0.0508,  0.0124,  0.0153,  0.0024,  0.0487,\n               0.0163, -0.0257,  0.0499,  0.0290, -0.0212,  0.0021, -0.0467, -0.0276,\n              -0.0211,  0.0377,  0.0105,  0.0453,  0.0237, -0.0026,  0.0388, -0.0384,\n              -0.0140, -0.0003,  0.0292,  0.0347, -0.0762, -0.0361, -0.0018, -0.0229,\n              -0.0298, -0.0231,  0.0069, -0.0410, -0.0381,  0.0194, -0.0084,  0.0202,\n               0.0352, -0.0037, -0.0116,  0.0074,  0.0412, -0.0121,  0.0260,  0.0093,\n               0.0211, -0.0227,  0.0110,  0.0249, -0.0208,  0.0108,  0.0276, -0.0353,\n               0.0152, -0.0468, -0.0034, -0.0158,  0.0087,  0.0168, -0.0250, -0.0259,\n               0.0096,  0.0340,  0.0243,  0.0113,  0.0014, -0.0274,  0.0001, -0.0137,\n              -0.0595,  0.0022,  0.0321,  0.0046, -0.0162, -0.0066,  0.0583,  0.0086,\n              -0.0015, -0.0023,  0.0150,  0.0151, -0.0117,  0.0139, -0.0077, -0.0275,\n              -0.0308, -0.0085,  0.0211, -0.0482,  0.0250,  0.0110,  0.0313,  0.0115],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9082, 0.9073, 0.9142, 0.9068, 0.9078, 0.9115, 0.9076, 0.9077, 0.9102,\n              0.9078, 0.9113, 0.9093, 0.9085, 0.9097, 0.9110, 0.9077, 0.9080, 0.9094,\n              0.9082, 0.9066, 0.9079, 0.9094, 0.9074, 0.9106, 0.9069, 0.9088, 0.9074,\n              0.9092, 0.9076, 0.9073, 0.9089, 0.9089, 0.9079, 0.9077, 0.9077, 0.9108,\n              0.9064, 0.9099, 0.9109, 0.9087, 0.9087, 0.9071, 0.9083, 0.9096, 0.9105,\n              0.9072, 0.9101, 0.9123, 0.9082, 0.9090, 0.9100, 0.9094, 0.9075, 0.9085,\n              0.9071, 0.9091, 0.9094, 0.9100, 0.9093, 0.9079, 0.9075, 0.9099, 0.9090,\n              0.9085, 0.9072, 0.9067, 0.9083, 0.9098, 0.9085, 0.9077, 0.9080, 0.9089,\n              0.9075, 0.9130, 0.9084, 0.9083, 0.9072, 0.9085, 0.9077, 0.9079, 0.9088,\n              0.9123, 0.9086, 0.9080, 0.9066, 0.9145, 0.9091, 0.9075, 0.9077, 0.9074,\n              0.9098, 0.9138, 0.9072, 0.9089, 0.9094, 0.9084, 0.9072, 0.9094, 0.9107,\n              0.9106, 0.9069, 0.9095, 0.9095, 0.9082, 0.9078, 0.9095, 0.9067, 0.9069,\n              0.9078, 0.9087, 0.9084, 0.9136, 0.9077, 0.9114, 0.9098, 0.9084, 0.9078,\n              0.9074, 0.9124, 0.9110, 0.9092, 0.9075, 0.9083, 0.9075, 0.9086, 0.9076,\n              0.9089, 0.9068, 0.9099, 0.9074, 0.9079, 0.9088, 0.9076, 0.9087, 0.9076,\n              0.9093, 0.9083, 0.9084, 0.9085, 0.9083, 0.9077, 0.9088, 0.9101, 0.9073,\n              0.9082, 0.9094, 0.9072, 0.9060, 0.9078, 0.9091, 0.9097, 0.9111, 0.9059,\n              0.9077, 0.9070, 0.9070, 0.9078, 0.9072, 0.9081, 0.9089, 0.9064, 0.9079,\n              0.9079, 0.9067, 0.9102, 0.9069, 0.9093, 0.9091, 0.9084, 0.9093, 0.9078,\n              0.9104, 0.9092, 0.9090, 0.9065, 0.9113, 0.9079, 0.9102, 0.9090, 0.9080,\n              0.9069, 0.9099, 0.9116, 0.9088, 0.9079, 0.9094, 0.9086, 0.9070, 0.9073,\n              0.9069, 0.9150, 0.9102, 0.9068, 0.9116, 0.9075, 0.9068, 0.9077, 0.9080,\n              0.9075, 0.9101, 0.9093, 0.9062, 0.9066, 0.9151, 0.9082, 0.9072, 0.9096,\n              0.9070, 0.9066, 0.9079, 0.9109, 0.9095, 0.9089, 0.9087, 0.9094, 0.9085,\n              0.9086, 0.9121, 0.9085, 0.9137, 0.9086, 0.9086, 0.9096, 0.9063, 0.9103,\n              0.9082, 0.9089, 0.9077, 0.9111, 0.9075, 0.9081, 0.9135, 0.9091, 0.9082,\n              0.9076, 0.9091, 0.9091, 0.9096, 0.9077, 0.9122, 0.9075, 0.9076, 0.9097,\n              0.9105, 0.9076, 0.9076, 0.9099, 0.9075, 0.9082, 0.9089, 0.9078, 0.9094,\n              0.9086, 0.9074, 0.9086, 0.9072, 0.9097, 0.9085, 0.9079, 0.9124, 0.9080,\n              0.9078, 0.9076, 0.9092, 0.9099, 0.9095, 0.9077, 0.9090, 0.9107, 0.9086,\n              0.9097, 0.9100, 0.9103, 0.9086, 0.9089, 0.9068, 0.9096, 0.9103, 0.9079,\n              0.9072, 0.9072, 0.9142, 0.9077, 0.9075, 0.9094, 0.9091, 0.9066, 0.9077,\n              0.9118, 0.9082, 0.9082, 0.9085, 0.9085, 0.9091, 0.9063, 0.9080, 0.9071,\n              0.9103, 0.9109, 0.9096, 0.9134, 0.9090, 0.9108, 0.9071, 0.9063, 0.9129,\n              0.9081, 0.9077, 0.9076, 0.9085, 0.9082, 0.9086, 0.9088, 0.9099, 0.9092,\n              0.9097, 0.9102, 0.9068, 0.9118, 0.9087, 0.9074, 0.9086, 0.9085, 0.9069,\n              0.9085, 0.9073, 0.9072, 0.9106, 0.9088, 0.9074, 0.9085, 0.9092, 0.9099,\n              0.9099, 0.9090, 0.9112, 0.9070, 0.9094, 0.9081, 0.9073, 0.9100, 0.9090,\n              0.9078, 0.9089, 0.9082, 0.9078, 0.9087, 0.9084, 0.9082, 0.9093, 0.9074,\n              0.9079, 0.9083, 0.9068, 0.9077, 0.9109, 0.9077, 0.9089, 0.9141, 0.9086,\n              0.9080, 0.9080, 0.9073, 0.9078, 0.9076, 0.9112, 0.9071, 0.9095, 0.9073,\n              0.9086, 0.9088, 0.9076, 0.9085, 0.9078, 0.9077, 0.9065, 0.9067, 0.9078,\n              0.9080, 0.9090, 0.9080, 0.9090, 0.9086, 0.9085, 0.9063, 0.9080, 0.9112,\n              0.9128, 0.9084, 0.9108, 0.9119, 0.9070, 0.9073, 0.9076, 0.9091, 0.9122,\n              0.9098, 0.9069, 0.9110, 0.9075, 0.9066, 0.9079, 0.9116, 0.9088, 0.9081,\n              0.9059, 0.9100, 0.9087, 0.9098, 0.9088, 0.9068, 0.9071, 0.9100, 0.9108,\n              0.9085, 0.9067, 0.9091, 0.9059, 0.9070, 0.9092, 0.9090, 0.9079, 0.9071,\n              0.9107, 0.9097, 0.9099, 0.9090, 0.9076, 0.9081, 0.9090, 0.9076, 0.9088,\n              0.9097, 0.9088, 0.9064, 0.9093, 0.9091, 0.9092, 0.9148, 0.9078, 0.9096,\n              0.9069, 0.9084, 0.9081, 0.9078, 0.9086, 0.9075, 0.9074, 0.9064, 0.9088,\n              0.9085, 0.9110, 0.9081, 0.9073, 0.9124, 0.9084, 0.9071, 0.9078, 0.9100,\n              0.9084, 0.9081, 0.9101, 0.9078, 0.9082, 0.9111, 0.9093, 0.9073, 0.9078,\n              0.9064, 0.9081, 0.9107, 0.9088, 0.9095, 0.9106, 0.9081, 0.9081, 0.9090,\n              0.9088, 0.9071, 0.9073, 0.9113, 0.9084, 0.9074, 0.9074, 0.9093, 0.9081,\n              0.9087, 0.9075, 0.9077, 0.9104, 0.9109, 0.9071, 0.9081, 0.9082, 0.9082,\n              0.9082, 0.9087, 0.9094, 0.9065, 0.9064, 0.9067, 0.9082, 0.9064, 0.9068,\n              0.9082, 0.9086, 0.9058, 0.9137, 0.9113, 0.9097, 0.9064, 0.9112],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 4.4496e-03, -1.2868e-02,  9.0037e-03],\n                [-1.4222e-02,  6.7271e-03,  4.5439e-03],\n                [ 1.0655e-02,  1.0637e-02,  7.2976e-03]],\n      \n               [[-9.3100e-03,  8.8537e-03, -8.9050e-03],\n                [ 8.0996e-03,  9.7561e-03,  9.8110e-03],\n                [-1.4878e-03,  1.4548e-02,  1.3818e-02]],\n      \n               [[ 8.3193e-03, -1.5112e-03,  1.5257e-03],\n                [-1.2327e-03,  3.5601e-03,  3.0348e-03],\n                [-9.1143e-03, -1.3314e-02,  6.7600e-03]],\n      \n               ...,\n      \n               [[ 1.3213e-02,  1.2446e-02,  1.0665e-02],\n                [ 2.5239e-03, -1.7295e-03,  7.7891e-04],\n                [-8.8417e-03,  1.3250e-02,  1.0778e-02]],\n      \n               [[-2.9434e-03, -4.3521e-03,  4.2948e-03],\n                [ 1.6730e-03,  8.2161e-03,  7.2962e-03],\n                [ 5.1600e-03,  7.6218e-03, -1.0080e-02]],\n      \n               [[ 9.1892e-04, -5.9081e-03, -4.4605e-03],\n                [ 2.1299e-03,  1.1840e-02,  1.4104e-02],\n                [-1.2208e-03,  1.1720e-02,  2.2098e-03]]],\n      \n      \n              [[[-2.1629e-03, -8.0936e-03, -1.2361e-02],\n                [-2.7811e-03,  2.2381e-03,  3.2742e-03],\n                [-1.2796e-02,  1.0970e-02, -3.9817e-03]],\n      \n               [[-7.3783e-03,  5.7778e-04, -1.2821e-02],\n                [ 1.1074e-02,  8.4991e-03, -1.2575e-02],\n                [ 8.0103e-03, -1.3609e-02,  5.1237e-03]],\n      \n               [[-1.3420e-02, -3.0080e-03,  1.2209e-02],\n                [ 2.6260e-04, -1.1375e-02,  1.1474e-02],\n                [ 3.2144e-03, -8.4060e-03, -4.1558e-03]],\n      \n               ...,\n      \n               [[ 3.5131e-04,  1.4437e-02, -2.1061e-03],\n                [ 1.1180e-02, -1.0834e-02,  1.2466e-02],\n                [ 9.9495e-03, -7.4140e-03, -4.6402e-03]],\n      \n               [[ 2.0898e-03,  7.9639e-03,  1.0164e-02],\n                [ 9.9266e-03,  2.1664e-03,  1.2617e-02],\n                [-8.4123e-03,  8.9936e-03, -1.4116e-02]],\n      \n               [[ 1.2910e-02, -1.2513e-02, -4.9344e-03],\n                [ 9.1531e-03,  7.2105e-03, -1.3193e-02],\n                [-1.3679e-02, -1.3190e-02, -5.8797e-04]]],\n      \n      \n              [[[ 1.4413e-02, -7.4704e-03,  1.0084e-02],\n                [ 8.0868e-04,  1.1861e-02,  2.3616e-03],\n                [ 1.0036e-02,  4.9353e-03,  6.4638e-03]],\n      \n               [[-1.0625e-02, -9.3004e-03, -1.4604e-02],\n                [ 8.8378e-03, -3.1593e-03,  4.5973e-03],\n                [-1.3009e-02,  1.2452e-02, -6.7174e-03]],\n      \n               [[-1.3652e-02,  2.2950e-03, -4.3173e-03],\n                [ 9.1388e-03, -9.4873e-03, -4.4166e-03],\n                [ 8.5418e-03,  5.6651e-03, -6.9788e-03]],\n      \n               ...,\n      \n               [[ 1.4125e-02,  9.4716e-03, -1.0214e-03],\n                [-5.1902e-03,  2.4565e-04,  3.6354e-04],\n                [ 1.3403e-03,  1.5147e-03, -1.3102e-02]],\n      \n               [[ 1.2615e-02,  5.6266e-03, -1.2359e-02],\n                [ 2.4392e-06, -8.8208e-03, -8.1398e-03],\n                [-1.4476e-02, -6.3366e-03, -2.2628e-03]],\n      \n               [[-1.1036e-02,  4.8499e-03, -4.2958e-03],\n                [ 3.2353e-03, -6.3522e-03,  5.7621e-03],\n                [-7.8837e-04,  8.8729e-03, -1.3262e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[-4.5918e-03, -1.1187e-03, -2.0413e-03],\n                [-1.4461e-02, -9.6681e-03,  3.0943e-03],\n                [ 3.0694e-03,  7.0977e-03, -8.1941e-03]],\n      \n               [[ 8.4892e-03,  6.0325e-03, -8.9549e-03],\n                [-1.2307e-02,  1.4637e-04,  8.3302e-03],\n                [ 1.0497e-02, -1.3952e-02, -5.0669e-03]],\n      \n               [[-4.5579e-03,  9.5233e-03,  1.0703e-02],\n                [-1.4142e-02,  8.4838e-03,  3.8279e-03],\n                [-1.0415e-02,  2.9423e-03,  1.3692e-02]],\n      \n               ...,\n      \n               [[-1.1588e-03, -6.3575e-03,  1.1337e-03],\n                [ 7.7451e-03,  9.0737e-03,  1.1912e-02],\n                [ 1.8431e-03, -2.8674e-03, -1.4609e-02]],\n      \n               [[ 1.8013e-03, -4.8572e-04,  5.2309e-03],\n                [ 1.3525e-02,  1.3072e-02, -1.4105e-02],\n                [ 8.4442e-03,  1.0309e-02,  1.3684e-02]],\n      \n               [[ 1.4577e-02,  1.3429e-02, -6.7257e-03],\n                [ 1.1729e-02, -1.1195e-02,  2.7361e-03],\n                [ 1.2044e-02,  1.0849e-02,  3.2662e-03]]],\n      \n      \n              [[[-7.7870e-03, -1.2305e-02,  1.3777e-02],\n                [ 5.2876e-03,  2.9648e-04,  4.7979e-03],\n                [ 8.1797e-03, -7.3199e-03, -7.3764e-03]],\n      \n               [[-6.0014e-04, -3.8679e-03,  7.2507e-03],\n                [-5.2461e-03, -1.6248e-03,  1.0839e-02],\n                [ 1.3120e-02, -5.7571e-03,  1.2277e-02]],\n      \n               [[-2.0643e-03,  1.3034e-02, -1.0011e-02],\n                [ 5.2128e-03,  3.0912e-03,  1.0405e-02],\n                [ 1.3847e-02, -1.1761e-02,  9.6574e-03]],\n      \n               ...,\n      \n               [[-5.0070e-03,  6.0069e-03,  7.0145e-03],\n                [-1.1538e-02,  9.8139e-03, -2.1567e-03],\n                [ 1.1790e-02, -1.2659e-02,  1.4452e-03]],\n      \n               [[-8.7213e-03, -4.5561e-03, -3.8952e-04],\n                [-7.1953e-03,  1.1768e-02, -1.0360e-02],\n                [-1.3872e-02,  1.1721e-02, -1.5099e-03]],\n      \n               [[-3.5071e-03,  1.4310e-02, -7.2840e-03],\n                [ 3.7511e-03,  4.4073e-04, -1.4093e-02],\n                [ 1.0493e-02, -1.1273e-02, -1.0509e-02]]],\n      \n      \n              [[[ 5.9241e-03,  3.8500e-03,  6.8222e-04],\n                [ 1.1396e-02,  4.9585e-04, -7.2699e-03],\n                [-1.1050e-02, -1.3523e-02,  1.7088e-03]],\n      \n               [[ 3.3669e-03,  1.3375e-03,  7.3368e-03],\n                [-7.4790e-03, -1.1752e-02, -9.9687e-04],\n                [-5.4690e-03,  2.6468e-03, -1.2640e-02]],\n      \n               [[ 1.2318e-02,  3.1025e-03,  1.2310e-02],\n                [ 2.4463e-03, -5.8190e-03,  7.6000e-05],\n                [-7.0045e-03,  7.5007e-03, -9.0908e-03]],\n      \n               ...,\n      \n               [[-1.0755e-02,  1.4184e-02, -1.0513e-02],\n                [-1.4016e-02, -1.4016e-02, -3.2075e-03],\n                [ 8.6690e-03, -1.0712e-02, -4.4874e-04]],\n      \n               [[-6.5359e-03,  1.3323e-02,  1.7378e-03],\n                [ 9.0394e-03, -1.4677e-02,  1.4579e-02],\n                [-2.3091e-03, -5.7394e-03,  1.4388e-02]],\n      \n               [[-1.3836e-02, -1.0761e-02, -1.8843e-03],\n                [ 3.2895e-03,  8.1135e-03,  1.1807e-02],\n                [-1.3381e-02, -1.2378e-02,  1.0943e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-7.6895e-03,  1.1757e-02,  3.1021e-05,  6.9690e-03, -6.3488e-04,\n               7.8325e-03,  7.8578e-03,  3.5019e-03,  3.4956e-03, -8.6020e-03,\n              -6.1257e-03, -1.2262e-02, -3.1577e-03,  5.8023e-04, -1.8963e-03,\n               2.7673e-03, -4.4097e-03,  1.0247e-02,  5.2847e-04, -1.4824e-02,\n              -1.2115e-03,  1.7251e-03,  2.7253e-03,  9.7945e-04,  1.1310e-03,\n              -6.8313e-03, -1.1537e-02,  4.8261e-03, -4.5462e-03,  2.1188e-03,\n              -4.4192e-03, -3.5544e-03,  1.3468e-02,  3.0945e-04,  3.0299e-03,\n              -2.6548e-03,  2.9223e-03, -5.1702e-03, -3.5324e-03, -2.6902e-03,\n              -1.8998e-03, -1.5301e-03, -8.6162e-05,  7.9698e-03, -1.6532e-02,\n              -3.0588e-03,  8.1560e-05,  1.1892e-03,  1.0882e-03, -4.0378e-03,\n               1.3453e-02, -7.2329e-03, -3.3346e-03, -1.3757e-02, -4.6009e-03,\n               1.6108e-02, -2.4249e-03,  9.6522e-03, -1.1722e-02,  3.8892e-03,\n              -7.1481e-03,  7.2561e-03, -4.7374e-03, -1.0240e-02, -3.1217e-04,\n              -4.0232e-05,  8.5631e-03,  7.8766e-03,  8.9663e-03,  5.9199e-03,\n               7.4085e-03,  6.8074e-03,  2.2852e-03, -8.7478e-03,  1.5597e-02,\n              -1.7746e-02, -8.0053e-03,  3.3220e-03,  6.0085e-04, -2.5061e-02,\n               6.9880e-03,  5.5735e-03,  4.4949e-03, -2.3962e-03, -6.5850e-03,\n               9.4910e-04,  1.7209e-02, -1.0017e-03,  7.6089e-04, -1.2305e-04,\n               4.6325e-03, -8.6326e-03, -1.3193e-02,  9.3357e-03,  3.4812e-03,\n              -5.6483e-03, -5.0459e-03,  4.2779e-03,  6.5712e-03, -1.3821e-03,\n              -8.5985e-03,  8.9665e-04, -8.5186e-03, -5.2670e-03,  1.3304e-02,\n               6.7804e-03,  1.1452e-02,  5.3628e-03, -1.1005e-02, -7.0621e-03,\n              -6.6577e-03,  5.2646e-04,  4.5626e-03, -5.6417e-03,  5.9522e-04,\n               2.1027e-03, -8.0930e-03, -4.3641e-03,  2.1039e-04, -1.9700e-03,\n               1.2912e-02, -9.0001e-03, -1.0589e-02, -9.2563e-03,  1.7943e-02,\n               2.9999e-04,  1.0576e-02,  1.7747e-03,  3.9693e-03,  9.8664e-03,\n              -3.8016e-03,  4.8884e-04, -1.5498e-02, -5.5737e-03, -3.1959e-03,\n              -1.0162e-03, -5.3240e-04,  8.5075e-03,  6.7594e-03,  6.2100e-03,\n              -4.0193e-03,  5.1923e-03,  4.4782e-03, -9.3521e-04, -1.3562e-03,\n              -6.0587e-04, -9.8258e-03, -8.9421e-04, -1.7861e-03,  1.9224e-03,\n              -8.9582e-04,  1.4305e-05, -9.2900e-03, -8.8723e-03, -8.5830e-03,\n               1.8370e-03,  8.7941e-03,  5.1547e-03,  2.8190e-03,  9.5324e-03,\n              -3.2455e-03,  2.5469e-03,  8.3554e-03, -2.6552e-03,  2.7539e-03,\n               2.2077e-03,  2.0335e-03,  8.5828e-03, -1.3043e-02, -4.2687e-03,\n              -3.6948e-03, -6.4743e-03,  1.4702e-03, -4.3507e-03, -5.2101e-03,\n              -1.7164e-02, -2.7131e-03,  7.9037e-03,  1.1504e-03, -9.4656e-03,\n               3.5312e-03,  1.0482e-02,  1.8608e-03, -9.8494e-03, -9.8329e-04,\n              -8.0127e-03, -4.8232e-03, -6.4274e-04, -6.0861e-03, -1.2697e-02,\n              -4.0008e-03, -4.5515e-04, -1.0283e-02, -1.2784e-02,  2.9917e-03,\n               3.9893e-03,  5.4692e-03,  3.0916e-03, -1.1011e-02, -1.1687e-03,\n              -1.7796e-02,  4.2193e-03, -4.5055e-03,  1.1370e-02, -1.3309e-03,\n               1.3864e-02, -6.3556e-03,  1.5311e-02,  4.1574e-03,  5.3439e-03,\n               8.0479e-03,  6.3630e-03, -1.3883e-02,  7.8967e-04,  9.6707e-03,\n               4.4441e-03, -9.4612e-03,  3.6396e-03,  1.7300e-02, -5.1901e-03,\n               4.0745e-03,  5.4121e-03, -3.1378e-03,  4.9016e-03, -4.4171e-03,\n              -7.8858e-03,  5.7601e-03,  1.1735e-02, -9.8599e-03, -1.9535e-03,\n              -7.2612e-03, -1.8126e-03,  2.2085e-03,  5.0382e-03,  6.9828e-03,\n               1.2389e-02, -4.2488e-03, -1.6708e-03, -1.1492e-03, -3.7563e-03,\n               1.4369e-02,  5.0214e-03,  8.9831e-03,  5.9616e-03,  3.6447e-03,\n              -9.3704e-03,  1.4777e-03, -6.4848e-05, -8.2777e-03,  1.3193e-03,\n               4.8255e-04, -1.0680e-02, -4.8708e-03, -1.7165e-02, -1.1448e-02,\n               5.3093e-03,  9.9409e-03, -6.6377e-03,  3.6830e-03,  1.2451e-02,\n              -8.0036e-03,  1.6328e-03,  2.9353e-03,  5.0100e-03, -5.0899e-03,\n              -2.2368e-03, -1.1135e-02, -5.6483e-05, -2.1406e-03, -7.6404e-03,\n               2.8170e-03, -4.4260e-03,  1.4271e-02, -5.4218e-04,  4.3936e-03,\n              -3.1066e-03,  1.1970e-02, -4.9248e-03, -3.6213e-03,  2.7181e-03,\n              -3.5027e-03, -1.8252e-03, -3.4635e-03, -4.0909e-04, -7.3834e-03,\n               5.0844e-03,  1.7783e-03, -5.8438e-03,  7.0292e-03, -8.1033e-03,\n              -6.2559e-03,  6.1938e-04,  5.6051e-03, -9.1050e-03,  3.9220e-04,\n              -3.7295e-03, -1.2777e-02,  1.0130e-02,  3.9938e-03, -1.0834e-02,\n              -1.5552e-02, -1.1870e-02, -7.9836e-03,  7.9967e-03,  1.0944e-02,\n              -5.7790e-03, -1.0697e-02, -8.5881e-03, -4.8791e-03, -5.1166e-03,\n               1.1716e-02, -3.2946e-03,  1.7158e-02,  4.3998e-03, -1.2706e-02,\n              -2.0692e-03, -5.4009e-03, -4.0317e-04, -9.5348e-03, -1.8795e-02,\n              -1.3445e-02, -1.0159e-03, -2.3767e-03, -1.3270e-02,  7.0576e-03,\n               1.8551e-02,  1.5483e-02,  4.6987e-03, -1.4280e-02,  8.0087e-03,\n               4.2803e-03, -1.1877e-02, -6.4902e-03,  4.3218e-03, -2.2978e-03,\n               1.2611e-03,  2.0502e-03, -1.3573e-03,  3.2505e-03,  1.1180e-03,\n              -8.7264e-03, -1.5257e-03, -8.9396e-03,  2.3098e-03, -8.4544e-03,\n               2.6764e-03,  1.7455e-03, -3.1023e-03,  3.9845e-03, -9.6943e-03,\n               3.4583e-03, -4.5956e-04,  1.6119e-02, -2.9254e-03, -8.5461e-03,\n              -5.9113e-03,  6.2834e-03,  1.4744e-03,  6.6422e-03, -6.7209e-03,\n              -6.3399e-03,  4.5162e-03, -3.9279e-03,  1.2319e-03,  2.7105e-03,\n              -1.0778e-02, -7.2975e-03, -1.3796e-02, -1.8696e-03, -2.0258e-03,\n               3.5503e-03, -4.5916e-03,  2.4404e-03,  1.1300e-03,  1.2203e-02,\n              -9.8744e-04, -1.8912e-04,  1.4614e-02,  1.9793e-03, -1.5566e-02,\n               4.6938e-04, -8.9819e-03, -8.6921e-04,  1.3688e-02,  4.2896e-03,\n              -7.0377e-04, -1.4283e-02,  1.4584e-02, -2.9864e-04,  1.8851e-03,\n               2.3995e-03, -1.1424e-02,  1.1444e-03, -7.9043e-04,  4.7350e-04,\n               4.7993e-03, -8.5767e-07, -2.2940e-03,  1.7816e-03,  9.4938e-03,\n              -1.2326e-03,  1.5515e-03, -7.3979e-03,  1.7205e-02,  3.6380e-03,\n              -8.9196e-03, -6.4013e-03,  1.1378e-02, -3.6164e-03,  4.9014e-04,\n              -4.0739e-04,  9.6853e-03, -8.9263e-03, -8.7535e-04,  5.6755e-03,\n              -1.4730e-03, -6.3088e-03, -8.8791e-03, -9.7136e-03, -2.5021e-04,\n              -9.3406e-03,  1.0097e-02, -3.8624e-03, -3.1033e-03, -1.1474e-03,\n              -4.7129e-03,  1.3232e-04,  6.1604e-03,  7.1986e-04,  5.7892e-03,\n               1.4188e-02, -5.9046e-03, -2.2285e-03, -5.3645e-03,  2.8351e-03,\n               8.6432e-03,  5.5841e-03, -7.9834e-03, -1.0649e-02,  1.2828e-02,\n               1.2581e-03,  7.0296e-03,  1.4392e-02, -2.0730e-03,  4.9201e-03,\n              -3.7425e-03,  7.3349e-03, -4.7150e-03,  1.0960e-02, -8.5090e-03,\n               3.9103e-03, -7.6083e-03, -6.6373e-03,  9.9551e-04, -2.1334e-03,\n              -4.8523e-03,  8.5498e-03,  2.1961e-04, -2.2000e-02, -5.2186e-03,\n               8.4716e-04, -1.3745e-02,  4.1993e-03,  6.3657e-03,  4.6467e-03,\n               9.5264e-03, -1.5163e-02,  6.9622e-03, -4.5253e-03,  4.4843e-03,\n              -2.5431e-03, -4.9708e-03,  7.1695e-04,  4.7210e-03, -1.0994e-02,\n               3.8008e-03,  7.7946e-03,  1.3746e-02, -2.9699e-03,  2.4917e-04,\n               8.3404e-04, -8.8223e-03,  1.5195e-02, -5.3685e-03,  1.0973e-02,\n              -2.0813e-03,  1.1842e-02,  2.2498e-03,  4.3419e-03,  4.5129e-03,\n               1.2563e-03, -1.7851e-03, -1.7682e-03, -2.0951e-03, -3.3630e-03,\n              -6.3750e-03,  6.0342e-03, -8.2398e-04,  1.4645e-02, -7.4150e-03,\n               6.8893e-03, -1.0387e-02, -6.6427e-03,  6.9806e-03,  6.0344e-04,\n               8.1705e-03,  3.9384e-03,  3.8942e-03,  9.2668e-03,  6.6551e-03,\n              -1.0008e-02,  7.3240e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9010, 0.9014, 0.9012, 0.9010, 0.9012, 0.9012, 0.9014, 0.9015, 0.9010,\n              0.9011, 0.9014, 0.9014, 0.9011, 0.9018, 0.9011, 0.9013, 0.9015, 0.9013,\n              0.9013, 0.9016, 0.9014, 0.9013, 0.9009, 0.9013, 0.9014, 0.9013, 0.9010,\n              0.9012, 0.9012, 0.9014, 0.9011, 0.9013, 0.9010, 0.9011, 0.9016, 0.9013,\n              0.9011, 0.9012, 0.9012, 0.9010, 0.9014, 0.9012, 0.9012, 0.9014, 0.9013,\n              0.9012, 0.9014, 0.9013, 0.9011, 0.9015, 0.9014, 0.9013, 0.9014, 0.9014,\n              0.9011, 0.9011, 0.9014, 0.9014, 0.9012, 0.9012, 0.9012, 0.9012, 0.9013,\n              0.9014, 0.9011, 0.9011, 0.9012, 0.9013, 0.9015, 0.9015, 0.9016, 0.9010,\n              0.9011, 0.9014, 0.9012, 0.9018, 0.9010, 0.9010, 0.9013, 0.9012, 0.9010,\n              0.9011, 0.9014, 0.9014, 0.9013, 0.9014, 0.9014, 0.9015, 0.9015, 0.9014,\n              0.9014, 0.9012, 0.9011, 0.9014, 0.9010, 0.9015, 0.9013, 0.9017, 0.9012,\n              0.9013, 0.9013, 0.9010, 0.9011, 0.9016, 0.9012, 0.9010, 0.9013, 0.9012,\n              0.9014, 0.9012, 0.9013, 0.9011, 0.9013, 0.9015, 0.9012, 0.9010, 0.9010,\n              0.9012, 0.9012, 0.9013, 0.9011, 0.9015, 0.9015, 0.9015, 0.9016, 0.9014,\n              0.9014, 0.9012, 0.9016, 0.9012, 0.9011, 0.9010, 0.9011, 0.9012, 0.9014,\n              0.9013, 0.9012, 0.9015, 0.9017, 0.9013, 0.9013, 0.9015, 0.9012, 0.9013,\n              0.9011, 0.9014, 0.9015, 0.9016, 0.9012, 0.9010, 0.9013, 0.9014, 0.9012,\n              0.9014, 0.9011, 0.9013, 0.9010, 0.9012, 0.9014, 0.9012, 0.9016, 0.9010,\n              0.9012, 0.9010, 0.9014, 0.9012, 0.9014, 0.9016, 0.9014, 0.9011, 0.9013,\n              0.9013, 0.9013, 0.9010, 0.9015, 0.9014, 0.9010, 0.9012, 0.9022, 0.9012,\n              0.9011, 0.9013, 0.9015, 0.9014, 0.9014, 0.9013, 0.9013, 0.9013, 0.9012,\n              0.9013, 0.9012, 0.9012, 0.9014, 0.9011, 0.9013, 0.9012, 0.9017, 0.9013,\n              0.9009, 0.9011, 0.9012, 0.9016, 0.9015, 0.9011, 0.9012, 0.9016, 0.9011,\n              0.9014, 0.9014, 0.9014, 0.9011, 0.9012, 0.9014, 0.9012, 0.9011, 0.9010,\n              0.9014, 0.9020, 0.9014, 0.9015, 0.9014, 0.9012, 0.9011, 0.9011, 0.9011,\n              0.9014, 0.9010, 0.9013, 0.9016, 0.9012, 0.9015, 0.9012, 0.9009, 0.9011,\n              0.9013, 0.9012, 0.9013, 0.9014, 0.9013, 0.9017, 0.9013, 0.9014, 0.9016,\n              0.9011, 0.9011, 0.9011, 0.9013, 0.9015, 0.9014, 0.9011, 0.9012, 0.9014,\n              0.9013, 0.9013, 0.9014, 0.9009, 0.9012, 0.9010, 0.9014, 0.9019, 0.9010,\n              0.9010, 0.9012, 0.9012, 0.9017, 0.9010, 0.9013, 0.9012, 0.9016, 0.9012,\n              0.9011, 0.9013, 0.9014, 0.9011, 0.9013, 0.9010, 0.9013, 0.9014, 0.9013,\n              0.9011, 0.9012, 0.9014, 0.9011, 0.9012, 0.9012, 0.9014, 0.9013, 0.9011,\n              0.9014, 0.9015, 0.9018, 0.9011, 0.9011, 0.9014, 0.9013, 0.9012, 0.9012,\n              0.9012, 0.9013, 0.9012, 0.9014, 0.9017, 0.9012, 0.9012, 0.9013, 0.9013,\n              0.9011, 0.9014, 0.9011, 0.9015, 0.9016, 0.9012, 0.9015, 0.9013, 0.9015,\n              0.9013, 0.9012, 0.9010, 0.9012, 0.9017, 0.9011, 0.9012, 0.9013, 0.9012,\n              0.9010, 0.9011, 0.9011, 0.9014, 0.9013, 0.9013, 0.9017, 0.9012, 0.9011,\n              0.9010, 0.9011, 0.9012, 0.9012, 0.9013, 0.9010, 0.9014, 0.9014, 0.9013,\n              0.9011, 0.9013, 0.9014, 0.9019, 0.9012, 0.9012, 0.9013, 0.9015, 0.9019,\n              0.9013, 0.9011, 0.9013, 0.9013, 0.9010, 0.9012, 0.9011, 0.9016, 0.9015,\n              0.9013, 0.9012, 0.9016, 0.9013, 0.9012, 0.9011, 0.9009, 0.9013, 0.9016,\n              0.9013, 0.9018, 0.9012, 0.9013, 0.9013, 0.9013, 0.9012, 0.9011, 0.9014,\n              0.9011, 0.9015, 0.9011, 0.9011, 0.9013, 0.9023, 0.9011, 0.9011, 0.9014,\n              0.9013, 0.9011, 0.9012, 0.9011, 0.9012, 0.9011, 0.9015, 0.9011, 0.9011,\n              0.9014, 0.9014, 0.9011, 0.9011, 0.9014, 0.9014, 0.9013, 0.9016, 0.9013,\n              0.9015, 0.9011, 0.9011, 0.9010, 0.9014, 0.9014, 0.9012, 0.9014, 0.9011,\n              0.9012, 0.9013, 0.9011, 0.9011, 0.9015, 0.9012, 0.9012, 0.9011, 0.9012,\n              0.9013, 0.9014, 0.9011, 0.9010, 0.9009, 0.9013, 0.9016, 0.9012, 0.9012,\n              0.9011, 0.9011, 0.9012, 0.9012, 0.9015, 0.9013, 0.9012, 0.9011, 0.9011,\n              0.9014, 0.9013, 0.9012, 0.9012, 0.9016, 0.9011, 0.9014, 0.9013, 0.9014,\n              0.9010, 0.9021, 0.9014, 0.9011, 0.9011, 0.9017, 0.9012, 0.9013, 0.9016,\n              0.9017, 0.9013, 0.9014, 0.9011, 0.9013, 0.9016, 0.9011, 0.9015, 0.9013,\n              0.9015, 0.9014, 0.9016, 0.9016, 0.9014, 0.9012, 0.9011, 0.9011, 0.9014,\n              0.9011, 0.9011, 0.9014, 0.9015, 0.9013, 0.9014, 0.9013, 0.9014, 0.9018,\n              0.9013, 0.9014, 0.9011, 0.9014, 0.9012, 0.9013, 0.9016, 0.9013, 0.9012,\n              0.9016, 0.9014, 0.9011, 0.9012, 0.9014, 0.9012, 0.9016, 0.9020, 0.9012,\n              0.9012, 0.9012, 0.9013, 0.9013, 0.9012, 0.9012, 0.9013, 0.9013],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[-0.0056]],\n        \n                 [[ 0.0566]],\n        \n                 [[-0.0030]],\n        \n                 ...,\n        \n                 [[ 0.0230]],\n        \n                 [[ 0.0487]],\n        \n                 [[ 0.0059]]],\n        \n        \n                [[[-0.0517]],\n        \n                 [[-0.0532]],\n        \n                 [[ 0.0445]],\n        \n                 ...,\n        \n                 [[ 0.0094]],\n        \n                 [[-0.0161]],\n        \n                 [[-0.0346]]],\n        \n        \n                [[[ 0.0208]],\n        \n                 [[-0.0478]],\n        \n                 [[ 0.0317]],\n        \n                 ...,\n        \n                 [[-0.0238]],\n        \n                 [[-0.0393]],\n        \n                 [[ 0.0455]]],\n        \n        \n                ...,\n        \n        \n                [[[-0.0122]],\n        \n                 [[ 0.0134]],\n        \n                 [[-0.0228]],\n        \n                 ...,\n        \n                 [[ 0.0019]],\n        \n                 [[-0.0093]],\n        \n                 [[-0.0383]]],\n        \n        \n                [[[ 0.0378]],\n        \n                 [[ 0.0151]],\n        \n                 [[-0.0451]],\n        \n                 ...,\n        \n                 [[-0.0501]],\n        \n                 [[ 0.0023]],\n        \n                 [[-0.0316]]],\n        \n        \n                [[[ 0.0259]],\n        \n                 [[-0.0422]],\n        \n                 [[ 0.0587]],\n        \n                 ...,\n        \n                 [[ 0.0524]],\n        \n                 [[ 0.0593]],\n        \n                 [[-0.0161]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-0.0152,  0.0455, -0.0249,  0.0054,  0.0338,  0.0492,  0.0593, -0.0342,\n                -0.0084,  0.0006, -0.0492, -0.0037, -0.0357, -0.0063,  0.0657, -0.0352,\n                 0.0713, -0.0558,  0.0191, -0.0206, -0.0325, -0.0546, -0.0081, -0.0596,\n                 0.0373, -0.0132,  0.0407, -0.0189,  0.0340, -0.0782,  0.0122, -0.0020,\n                -0.0047, -0.0625,  0.0203,  0.0474,  0.0350,  0.0104,  0.0135,  0.0653,\n                -0.0129,  0.0482,  0.0033, -0.0262,  0.0033,  0.0258,  0.0379,  0.0271,\n                -0.0095, -0.0362, -0.0580, -0.0801, -0.0746,  0.0761, -0.0900, -0.0689,\n                -0.0282, -0.0033, -0.0335,  0.0179, -0.0042,  0.0178, -0.0375, -0.0391,\n                 0.0028,  0.0348,  0.0434, -0.0233, -0.0335, -0.0615, -0.0618,  0.0546,\n                -0.0191, -0.0503, -0.0320,  0.0201, -0.0310, -0.0559, -0.0714,  0.0053,\n                 0.0434, -0.0496,  0.0137,  0.0267,  0.0754, -0.0029, -0.0471, -0.0048,\n                -0.0211,  0.0074, -0.0328, -0.0361, -0.0003,  0.0170,  0.0132, -0.0260,\n                -0.0102, -0.0948,  0.0085,  0.0067, -0.0311,  0.0340,  0.0545,  0.0306,\n                -0.0495,  0.0822, -0.0446, -0.0145, -0.0407,  0.0214, -0.0571,  0.0073,\n                 0.0182, -0.0123, -0.0338, -0.0676, -0.0187, -0.0597, -0.0256, -0.0400,\n                -0.0671,  0.0113,  0.0295,  0.0093, -0.0075, -0.0201, -0.0267,  0.0546,\n                -0.0176, -0.0111, -0.0102, -0.0276,  0.0420,  0.0031,  0.0387,  0.0083,\n                -0.0451, -0.0219,  0.0292, -0.0228, -0.0127,  0.0685,  0.0209,  0.0221,\n                 0.0188, -0.0646, -0.0741,  0.0038,  0.0458,  0.0524, -0.0265,  0.0646,\n                 0.0262,  0.0172,  0.0329, -0.0222, -0.0164, -0.0083,  0.0280,  0.0307,\n                 0.0337, -0.0554, -0.0600, -0.0350, -0.0344, -0.0355, -0.0146, -0.0110,\n                 0.0007, -0.0066, -0.0487, -0.0239, -0.0041, -0.0201, -0.0116, -0.0165,\n                -0.0161,  0.0205, -0.0048,  0.0028, -0.0378,  0.0913,  0.0233, -0.0077,\n                -0.0509, -0.0790,  0.0556, -0.0418,  0.0391, -0.0576,  0.0165,  0.0034,\n                -0.0188, -0.0685, -0.0389,  0.0225, -0.0151, -0.0490,  0.0173, -0.0193,\n                -0.0323,  0.0535, -0.0016,  0.0456,  0.0303, -0.0170,  0.0415, -0.0149,\n                -0.0096, -0.0531,  0.0140, -0.0310,  0.0206, -0.0051, -0.0391, -0.0068,\n                -0.1006,  0.0282,  0.0190, -0.0169,  0.0169,  0.0243,  0.0514, -0.0655,\n                -0.0057, -0.0043,  0.0153, -0.0326,  0.0400,  0.0112, -0.0341,  0.0188,\n                -0.0219, -0.0347,  0.0369, -0.0517,  0.0418, -0.0222, -0.0013,  0.0109,\n                 0.0528, -0.0021,  0.0058,  0.0327,  0.0262,  0.0773,  0.0430, -0.0203,\n                 0.0107,  0.0668, -0.0284, -0.0434, -0.0025,  0.0201,  0.0465,  0.0267,\n                -0.0479, -0.0158,  0.0023, -0.0111, -0.0435,  0.0022,  0.0368, -0.0351,\n                 0.0060, -0.0111, -0.0400, -0.0736, -0.0112, -0.0291,  0.0354,  0.0363,\n                -0.0358, -0.0815, -0.0691, -0.0144, -0.0142,  0.0108, -0.0489,  0.0057,\n                -0.0059,  0.0331, -0.0104,  0.0654,  0.0268, -0.0135,  0.0687,  0.0128,\n                 0.0052,  0.0620,  0.0265, -0.0067,  0.0395, -0.0163,  0.0154,  0.0090,\n                 0.0041,  0.0073, -0.0165, -0.0555, -0.0173,  0.0156, -0.0422, -0.0036,\n                 0.0204,  0.0010,  0.0546,  0.0025, -0.0247, -0.0614,  0.0184,  0.0012,\n                -0.0304,  0.0450,  0.0066,  0.0046, -0.0452, -0.0124,  0.0507, -0.0050,\n                 0.0619, -0.0184, -0.0138, -0.0011, -0.0460, -0.0049,  0.0300, -0.0691,\n                -0.0175, -0.0207, -0.0205, -0.0042, -0.0500, -0.0404,  0.0290, -0.0148,\n                -0.0476,  0.0106,  0.0343, -0.0099, -0.0594,  0.0287,  0.0559, -0.0204,\n                 0.0186, -0.0181,  0.0003,  0.0471, -0.0425, -0.0372,  0.0409, -0.0139,\n                -0.0102,  0.0250,  0.0551, -0.0076,  0.0418, -0.0122, -0.0219,  0.0500,\n                -0.0181,  0.0399,  0.0334, -0.0243, -0.0114,  0.0107, -0.0298,  0.0046,\n                 0.0262, -0.0185, -0.0066, -0.0163,  0.0431, -0.0140,  0.0004,  0.0331,\n                -0.0035, -0.0007,  0.0186,  0.0244,  0.0319, -0.0386, -0.0191,  0.0138,\n                 0.0111, -0.0203, -0.0559, -0.0318,  0.0153, -0.0088,  0.0550, -0.0443,\n                 0.0103, -0.0370, -0.0569, -0.0056,  0.0317,  0.0134, -0.0320,  0.0016,\n                 0.0188, -0.0182,  0.0074,  0.0030,  0.0548,  0.0605, -0.0141,  0.0032,\n                 0.0287,  0.0307,  0.0264,  0.0052,  0.0454, -0.0241,  0.0531, -0.0374,\n                -0.0608,  0.0352,  0.0088,  0.0111,  0.0339,  0.0092, -0.0306,  0.0208,\n                -0.0485, -0.0098,  0.0230,  0.0288,  0.0350,  0.0132,  0.0012,  0.0024,\n                -0.0152,  0.0207,  0.0443,  0.0044,  0.0218,  0.0371, -0.0335, -0.0405,\n                -0.0100,  0.0073, -0.0527, -0.0172, -0.0577, -0.0045,  0.0202,  0.0068,\n                 0.0355, -0.0107, -0.0134, -0.0579,  0.0798,  0.0065,  0.0035, -0.0566,\n                 0.0188,  0.0389,  0.0388, -0.0234,  0.0222,  0.0084,  0.0411,  0.0625,\n                 0.0388, -0.0151, -0.0439,  0.0193,  0.0065, -0.0101,  0.0124,  0.0233,\n                -0.0439,  0.0039,  0.0126,  0.0319,  0.0377,  0.0103,  0.0406, -0.0501,\n                 0.0004,  0.0542, -0.0059,  0.0373,  0.0489, -0.0203, -0.0422,  0.0188,\n                 0.0870,  0.0353,  0.0021, -0.0275, -0.0050,  0.0861, -0.0477, -0.0250,\n                 0.0038, -0.0223, -0.0052,  0.0283, -0.0809, -0.0222, -0.0149,  0.0177,\n                -0.0091, -0.0172,  0.0508,  0.0360,  0.0325,  0.0447, -0.0192,  0.0416],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9121, 0.9147, 0.9150, 0.9136, 0.9189, 0.9144, 0.9271, 0.9144, 0.9142,\n                0.9131, 0.9141, 0.9146, 0.9147, 0.9191, 0.9149, 0.9184, 0.9187, 0.9168,\n                0.9170, 0.9168, 0.9177, 0.9202, 0.9124, 0.9209, 0.9154, 0.9126, 0.9192,\n                0.9164, 0.9178, 0.9151, 0.9285, 0.9250, 0.9154, 0.9128, 0.9141, 0.9269,\n                0.9153, 0.9176, 0.9129, 0.9152, 0.9240, 0.9190, 0.9305, 0.9199, 0.9146,\n                0.9154, 0.9170, 0.9154, 0.9134, 0.9145, 0.9205, 0.9171, 0.9136, 0.9154,\n                0.9255, 0.9144, 0.9126, 0.9188, 0.9153, 0.9292, 0.9389, 0.9185, 0.9153,\n                0.9152, 0.9141, 0.9152, 0.9184, 0.9194, 0.9179, 0.9161, 0.9146, 0.9148,\n                0.9280, 0.9153, 0.9183, 0.9159, 0.9211, 0.9235, 0.9158, 0.9148, 0.9169,\n                0.9185, 0.9117, 0.9189, 0.9211, 0.9213, 0.9191, 0.9175, 0.9146, 0.9106,\n                0.9183, 0.9167, 0.9160, 0.9176, 0.9142, 0.9136, 0.9161, 0.9166, 0.9147,\n                0.9223, 0.9123, 0.9146, 0.9148, 0.9193, 0.9123, 0.9146, 0.9152, 0.9277,\n                0.9229, 0.9173, 0.9165, 0.9166, 0.9204, 0.9172, 0.9174, 0.9187, 0.9173,\n                0.9197, 0.9157, 0.9136, 0.9206, 0.9158, 0.9139, 0.9131, 0.9156, 0.9156,\n                0.9164, 0.9186, 0.9182, 0.9163, 0.9118, 0.9139, 0.9098, 0.9156, 0.9174,\n                0.9213, 0.9218, 0.9179, 0.9169, 0.9188, 0.9172, 0.9155, 0.9134, 0.9186,\n                0.9158, 0.9205, 0.9130, 0.9133, 0.9214, 0.9230, 0.9129, 0.9185, 0.9173,\n                0.9126, 0.9196, 0.9144, 0.9204, 0.9154, 0.9147, 0.9146, 0.9174, 0.9157,\n                0.9208, 0.9209, 0.9191, 0.9156, 0.9178, 0.9116, 0.9176, 0.9114, 0.9130,\n                0.9130, 0.9151, 0.9244, 0.9158, 0.9167, 0.9155, 0.9160, 0.9140, 0.9148,\n                0.9166, 0.9275, 0.9228, 0.9147, 0.9149, 0.9176, 0.9142, 0.9161, 0.9221,\n                0.9160, 0.9147, 0.9169, 0.9138, 0.9322, 0.9152, 0.9193, 0.9172, 0.9244,\n                0.9181, 0.9168, 0.9123, 0.9165, 0.9165, 0.9177, 0.9258, 0.9225, 0.9187,\n                0.9174, 0.9243, 0.9148, 0.9188, 0.9217, 0.9177, 0.9187, 0.9136, 0.9132,\n                0.9418, 0.9129, 0.9097, 0.9139, 0.9334, 0.9237, 0.9117, 0.9232, 0.9186,\n                0.9176, 0.9146, 0.9135, 0.9173, 0.9147, 0.9283, 0.9183, 0.9160, 0.9170,\n                0.9148, 0.9295, 0.9131, 0.9127, 0.9150, 0.9208, 0.9196, 0.9106, 0.9157,\n                0.9161, 0.9126, 0.9155, 0.9183, 0.9138, 0.9171, 0.9156, 0.9194, 0.9210,\n                0.9183, 0.9156, 0.9248, 0.9167, 0.9319, 0.9121, 0.9191, 0.9176, 0.9194,\n                0.9182, 0.9132, 0.9157, 0.9166, 0.9144, 0.9223, 0.9240, 0.9165, 0.9223,\n                0.9202, 0.9159, 0.9130, 0.9144, 0.9276, 0.9187, 0.9215, 0.9119, 0.9140,\n                0.9134, 0.9136, 0.9149, 0.9162, 0.9188, 0.9171, 0.9121, 0.9168, 0.9198,\n                0.9141, 0.9135, 0.9158, 0.9145, 0.9159, 0.9247, 0.9189, 0.9170, 0.9202,\n                0.9128, 0.9149, 0.9196, 0.9172, 0.9139, 0.9181, 0.9176, 0.9168, 0.9128,\n                0.9160, 0.9152, 0.9133, 0.9118, 0.9117, 0.9157, 0.9146, 0.9163, 0.9134,\n                0.9161, 0.9199, 0.9183, 0.9170, 0.9165, 0.9135, 0.9164, 0.9145, 0.9144,\n                0.9141, 0.9124, 0.9163, 0.9381, 0.9129, 0.9177, 0.9135, 0.9142, 0.9147,\n                0.9155, 0.9115, 0.9173, 0.9127, 0.9187, 0.9133, 0.9126, 0.9156, 0.9194,\n                0.9178, 0.9135, 0.9168, 0.9175, 0.9142, 0.9142, 0.9149, 0.9296, 0.9158,\n                0.9182, 0.9142, 0.9212, 0.9132, 0.9178, 0.9160, 0.9135, 0.9172, 0.9253,\n                0.9151, 0.9129, 0.9147, 0.9160, 0.9123, 0.9146, 0.9251, 0.9152, 0.9184,\n                0.9182, 0.9160, 0.9154, 0.9198, 0.9231, 0.9134, 0.9175, 0.9151, 0.9153,\n                0.9146, 0.9157, 0.9192, 0.9176, 0.9150, 0.9159, 0.9187, 0.9165, 0.9182,\n                0.9141, 0.9170, 0.9127, 0.9188, 0.9173, 0.9169, 0.9226, 0.9244, 0.9248,\n                0.9283, 0.9157, 0.9198, 0.9142, 0.9145, 0.9218, 0.9227, 0.9314, 0.9140,\n                0.9179, 0.9125, 0.9326, 0.9225, 0.9163, 0.9174, 0.9161, 0.9191, 0.9128,\n                0.9186, 0.9213, 0.9203, 0.9132, 0.9215, 0.9156, 0.9161, 0.9129, 0.9165,\n                0.9157, 0.9220, 0.9151, 0.9192, 0.9180, 0.9197, 0.9160, 0.9125, 0.9268,\n                0.9190, 0.9187, 0.9146, 0.9168, 0.9148, 0.9230, 0.9147, 0.9179, 0.9158,\n                0.9164, 0.9168, 0.9148, 0.9138, 0.9278, 0.9143, 0.9135, 0.9161, 0.9143,\n                0.9164, 0.9268, 0.9269, 0.9134, 0.9154, 0.9120, 0.9175, 0.9136, 0.9150,\n                0.9145, 0.9162, 0.9143, 0.9192, 0.9244, 0.9138, 0.9150, 0.9135, 0.9139,\n                0.9139, 0.9134, 0.9150, 0.9200, 0.9156, 0.9127, 0.9148, 0.9132, 0.9229,\n                0.9187, 0.9127, 0.9175, 0.9128, 0.9159, 0.9156, 0.9209, 0.9162, 0.9216,\n                0.9238, 0.9154, 0.9160, 0.9211, 0.9136, 0.9161, 0.9143, 0.9185, 0.9124,\n                0.9140, 0.9189, 0.9115, 0.9154, 0.9123, 0.9173, 0.9189, 0.9164, 0.9143,\n                0.9119, 0.9202, 0.9188, 0.9194, 0.9153, 0.9149, 0.9232, 0.9166],\n               grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [512, 256, 3, 3]], ["0.bn1.weight", [512]], ["0.bn1.bias", [512]], ["0.conv2.weight", [512, 512, 3, 3]], ["0.bn2.weight", [512]], ["0.bn2.bias", [512]], ["0.downsample.0.weight", [512, 256, 1, 1]], ["0.downsample.1.weight", [512]], ["0.downsample.1.bias", [512]]], "output_shape": [[512, 512, 1, 1]], "num_parameters": [1179648, 512, 512, 2359296, 512, 512, 131072, 512, 512]}, {"name": "avgpool", "id": 140449530342944, "class_name": "AveragePool()", "parameters": [], "output_shape": [[512, 512]], "num_parameters": []}, {"name": "fc", "id": 140449530342992, "class_name": "Linear(in_features=512, out_features=10, bias=True)", "parameters": [["weight", [10, 512]], ["bias", [10]]], "output_shape": [[512, 10]], "num_parameters": [5120, 10]}], "edges": []}