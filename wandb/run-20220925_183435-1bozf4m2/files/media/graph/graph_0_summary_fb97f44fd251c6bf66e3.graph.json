{"format": "torch", "nodes": [{"name": "conv1", "id": 140621193487984, "class_name": "Conv2d(\n  self.stride=2, self.padding=(3, 3), self.weight=Parameter containing:\n  tensor([[[[-2.3147e-02, -6.7390e-02, -8.0443e-02,  ...,  6.3063e-02,\n              3.4829e-02,  3.9100e-02],\n            [-6.8796e-02, -4.9369e-03, -1.1261e-02,  ..., -6.2447e-02,\n              5.6937e-02,  6.3866e-02],\n            [ 7.3694e-03, -4.4599e-02, -1.3066e-02,  ..., -1.0523e-02,\n             -5.0080e-02, -1.4944e-03],\n            ...,\n            [-3.8193e-02, -7.4862e-05,  7.6232e-02,  ..., -6.8088e-02,\n             -7.8024e-02, -4.3070e-02],\n            [-1.4393e-02, -7.6863e-02,  6.2710e-02,  ...,  3.6758e-02,\n              3.2152e-02, -5.7817e-02],\n            [ 7.3711e-02,  5.9497e-03,  5.0685e-02,  ...,  7.6747e-02,\n              3.5685e-02, -5.4462e-02]],\n  \n           [[-5.2686e-02, -5.8371e-03,  5.3160e-02,  ...,  2.2941e-03,\n             -6.9605e-02,  1.8897e-02],\n            [-5.8357e-02,  2.5308e-02,  3.3970e-02,  ..., -7.3898e-02,\n              3.6445e-02,  2.8739e-02],\n            [-3.9071e-02,  7.7391e-02,  5.5976e-03,  ..., -8.0278e-02,\n              4.9752e-02, -1.0321e-02],\n            ...,\n            [-7.9465e-02,  6.2715e-02,  4.5068e-02,  ...,  1.8365e-02,\n             -8.4961e-03,  5.1355e-02],\n            [-4.9033e-03, -4.9705e-02, -7.8562e-02,  ..., -2.8306e-02,\n             -1.7136e-02, -2.2104e-03],\n            [ 3.9633e-02,  2.9260e-03, -1.0857e-02,  ..., -4.7426e-02,\n             -1.0541e-02,  4.8018e-02]],\n  \n           [[ 7.5324e-02, -4.3536e-02, -3.6909e-02,  ..., -6.5533e-03,\n              2.0682e-02, -3.2065e-02],\n            [-3.5618e-02, -5.2579e-04, -5.8425e-02,  ...,  3.1783e-02,\n              4.4609e-02,  3.2762e-02],\n            [ 6.7254e-02,  4.8668e-02,  1.8513e-02,  ..., -5.2353e-02,\n             -5.8630e-02,  7.1769e-02],\n            ...,\n            [-8.1006e-02, -6.0075e-02, -2.0949e-02,  ...,  1.5511e-02,\n             -6.4922e-02, -1.4514e-02],\n            [ 3.5469e-03, -1.9976e-03,  4.6020e-03,  ..., -6.9193e-02,\n             -1.1568e-03, -6.3038e-02],\n            [-5.4762e-02, -3.6202e-02,  5.1713e-02,  ..., -4.9108e-02,\n             -3.8620e-02,  2.0589e-03]]],\n  \n  \n          [[[-4.5708e-02,  5.4240e-02,  3.8934e-02,  ...,  3.5175e-02,\n              7.3764e-02,  3.8058e-02],\n            [ 6.9740e-02,  1.1799e-02,  2.9687e-03,  ..., -4.3501e-02,\n             -3.9402e-02,  7.6115e-03],\n            [-8.0496e-02,  3.4170e-02, -3.6958e-02,  ..., -2.3598e-02,\n             -4.1184e-02, -7.6833e-02],\n            ...,\n            [ 5.8312e-02, -5.0129e-02, -3.8851e-03,  ...,  3.9074e-02,\n              5.0753e-02, -2.6322e-02],\n            [-3.9069e-02, -1.7548e-02, -1.0246e-02,  ..., -2.8120e-02,\n             -2.3270e-02, -1.2457e-02],\n            [-3.2352e-03,  5.4587e-02,  4.5513e-02,  ...,  4.3165e-02,\n             -6.9207e-02, -1.2656e-02]],\n  \n           [[-6.2582e-02,  5.8924e-02, -7.9011e-02,  ..., -7.7064e-02,\n             -3.6367e-02, -1.3573e-02],\n            [-6.5803e-02, -4.7706e-02,  4.7886e-02,  ...,  3.7930e-02,\n              5.0927e-02,  4.3837e-02],\n            [-6.4142e-02,  8.0916e-02, -7.0436e-02,  ..., -1.2327e-04,\n             -7.1742e-02,  4.8835e-02],\n            ...,\n            [-1.8460e-02, -3.9476e-02, -2.6213e-02,  ..., -1.3295e-02,\n             -6.3161e-02, -4.0105e-02],\n            [ 3.1282e-02, -8.4664e-03,  4.9703e-02,  ...,  5.4139e-02,\n              6.6482e-03,  1.0763e-03],\n            [-9.3515e-03,  2.4759e-02,  3.3983e-02,  ...,  1.8663e-02,\n              6.5643e-03, -1.6651e-02]],\n  \n           [[ 1.9797e-02,  1.4507e-02, -6.3371e-02,  ...,  5.5958e-02,\n              2.9941e-02,  1.2418e-02],\n            [-7.2173e-03, -6.0127e-02, -1.6205e-02,  ..., -1.9737e-02,\n              2.6884e-02,  7.5527e-02],\n            [ 4.3581e-02,  6.4528e-02,  5.7489e-02,  ...,  1.2484e-03,\n             -6.5504e-02, -7.8384e-02],\n            ...,\n            [ 5.4361e-02,  1.3263e-02,  8.1227e-02,  ..., -6.6250e-02,\n             -1.5452e-02, -1.3466e-02],\n            [-2.1252e-02, -5.1926e-03, -1.8289e-03,  ...,  4.5761e-02,\n             -5.1286e-02, -1.7426e-02],\n            [ 2.6337e-02, -3.2678e-02,  5.7095e-02,  ..., -4.9326e-02,\n              5.5979e-02,  4.2910e-02]]],\n  \n  \n          [[[ 3.0859e-03,  3.8440e-02, -4.7661e-02,  ..., -3.4362e-02,\n             -1.0687e-02,  5.0986e-02],\n            [-2.9437e-02, -6.9351e-02, -2.1948e-02,  ...,  3.6749e-02,\n             -3.9705e-03, -6.3999e-02],\n            [-1.1968e-02, -1.1926e-02, -1.5338e-02,  ..., -5.2326e-02,\n             -6.0440e-02, -2.9276e-02],\n            ...,\n            [-5.8896e-03, -1.9707e-02,  4.4490e-02,  ...,  5.7274e-02,\n              6.5395e-02,  6.2359e-02],\n            [-1.6108e-03, -4.3709e-02, -6.7738e-02,  ...,  6.3207e-02,\n              7.6874e-02,  9.5901e-03],\n            [ 2.2254e-02, -5.9125e-02,  2.4773e-02,  ..., -1.6738e-02,\n             -5.7808e-02,  8.0081e-02]],\n  \n           [[ 5.2483e-02,  4.9630e-02,  7.2234e-02,  ...,  7.2391e-02,\n              3.4994e-02, -2.2633e-02],\n            [-2.8103e-02, -4.7556e-02,  5.2610e-02,  ..., -1.3372e-02,\n              7.5147e-03, -8.0430e-02],\n            [-2.7660e-03, -3.4149e-02, -4.2149e-02,  ...,  1.8972e-02,\n              3.8613e-02,  2.6256e-02],\n            ...,\n            [-3.3576e-02, -3.7028e-05, -3.2736e-02,  ..., -5.0260e-02,\n              7.9628e-02,  1.8064e-02],\n            [-4.9712e-02,  5.4603e-02,  9.1179e-04,  ...,  1.3068e-02,\n              7.1239e-02,  4.9966e-02],\n            [ 1.6127e-02,  2.6142e-02, -6.7701e-02,  ..., -7.2690e-02,\n              7.8705e-02, -3.9697e-02]],\n  \n           [[-2.8837e-02, -6.5919e-02,  3.1314e-02,  ..., -1.7867e-02,\n             -5.8225e-02,  9.0760e-03],\n            [-4.0351e-02,  4.4550e-02, -6.2006e-02,  ..., -7.9207e-03,\n             -4.3024e-02, -2.2625e-02],\n            [-5.3499e-02, -7.3346e-02, -4.0692e-02,  ...,  7.1978e-02,\n             -5.1329e-02,  5.8090e-02],\n            ...,\n            [-3.0778e-02, -5.5774e-02, -3.2461e-02,  ..., -1.0034e-02,\n              5.5416e-02, -1.3379e-02],\n            [ 5.4858e-02, -5.2415e-02, -1.1247e-02,  ..., -2.2906e-02,\n             -2.7904e-02,  1.6129e-02],\n            [ 1.2836e-02, -2.5180e-02,  5.3671e-02,  ...,  3.1142e-03,\n              4.6956e-02,  1.3587e-02]]],\n  \n  \n          ...,\n  \n  \n          [[[-3.8124e-02,  5.0290e-02,  4.9120e-02,  ..., -3.4261e-02,\n              5.3448e-03, -3.1664e-02],\n            [ 6.7644e-02, -8.0838e-03,  4.9161e-02,  ...,  7.9475e-03,\n             -2.4787e-02,  5.7068e-02],\n            [ 2.6331e-02,  5.3831e-04,  4.2921e-02,  ...,  7.2650e-02,\n             -1.7731e-02, -4.2596e-02],\n            ...,\n            [ 6.9816e-02,  1.6826e-02, -2.8230e-02,  ..., -1.0270e-02,\n             -2.6238e-02,  2.8978e-02],\n            [ 4.1861e-02,  1.8951e-02, -7.5883e-02,  ..., -3.2112e-02,\n             -5.0324e-02, -6.0792e-02],\n            [-8.0832e-02,  5.7544e-03,  8.1474e-02,  ...,  7.9509e-02,\n             -1.2705e-02,  1.2585e-02]],\n  \n           [[ 7.8486e-02,  5.5180e-02, -2.0453e-02,  ..., -8.0290e-02,\n             -5.2991e-02,  4.7409e-02],\n            [ 1.1901e-03, -9.4204e-03,  7.9256e-02,  ...,  3.1656e-02,\n             -1.6044e-02, -6.6840e-02],\n            [ 1.7204e-02,  6.8159e-02, -3.8076e-02,  ...,  2.1032e-02,\n             -1.6173e-02, -5.9599e-02],\n            ...,\n            [-5.5521e-02,  7.4760e-02, -4.4635e-02,  ..., -7.3754e-02,\n              6.0633e-02, -1.4400e-02],\n            [ 7.1225e-03, -4.2226e-02,  6.7029e-02,  ...,  7.3332e-02,\n             -5.6981e-03,  2.2740e-02],\n            [-2.1024e-02, -5.2310e-02, -5.3298e-02,  ..., -3.1363e-02,\n             -3.2760e-03, -3.0048e-02]],\n  \n           [[ 1.7618e-02,  7.8062e-04,  8.2125e-02,  ..., -4.3929e-02,\n             -2.6772e-02, -3.1474e-03],\n            [ 1.3385e-03, -7.3218e-02,  4.7477e-02,  ..., -9.5780e-03,\n              5.8715e-02, -4.2364e-02],\n            [ 2.1814e-02,  3.9615e-02,  5.3396e-02,  ..., -7.5363e-02,\n             -4.0547e-03,  6.8946e-02],\n            ...,\n            [ 1.4459e-02,  7.0675e-02,  1.2909e-02,  ..., -4.2313e-03,\n             -4.9042e-02, -2.3240e-02],\n            [-7.6552e-02,  2.5046e-02,  2.8049e-02,  ..., -5.7018e-02,\n             -5.6616e-02,  1.8209e-02],\n            [-2.1823e-02, -1.5907e-02,  7.3705e-02,  ...,  7.8429e-02,\n             -2.7871e-02, -2.5556e-02]]],\n  \n  \n          [[[-2.1593e-02, -4.6072e-02,  3.0241e-02,  ...,  3.7561e-02,\n              7.6700e-02, -5.3306e-02],\n            [ 2.1680e-02, -4.6852e-02,  3.6308e-02,  ...,  3.1280e-02,\n              3.4032e-02, -7.4991e-02],\n            [ 3.7331e-02, -5.9944e-02, -4.6810e-02,  ..., -2.8606e-02,\n              2.1185e-02, -1.1777e-02],\n            ...,\n            [-7.1228e-02,  2.0298e-02, -7.7320e-02,  ...,  8.1904e-02,\n             -3.0599e-02,  8.9421e-03],\n            [-5.0704e-02,  2.7115e-02, -8.1924e-02,  ...,  5.3398e-02,\n             -4.0726e-02,  2.5183e-02],\n            [ 1.5245e-02,  7.3618e-02,  2.3429e-02,  ...,  4.8500e-02,\n             -6.9234e-02, -3.5737e-02]],\n  \n           [[ 4.4423e-02, -2.0953e-02,  2.8530e-02,  ..., -1.0653e-02,\n             -4.2549e-02, -1.9479e-02],\n            [ 4.6582e-02, -3.1973e-02, -7.4491e-02,  ..., -5.9723e-02,\n              8.2059e-02, -3.7941e-03],\n            [-5.6192e-02,  7.7968e-02, -7.3999e-02,  ..., -7.5918e-02,\n             -1.1559e-02,  8.7309e-03],\n            ...,\n            [-2.4410e-02,  8.1503e-02,  6.0130e-02,  ..., -6.3710e-02,\n              9.9076e-03, -1.3121e-03],\n            [-3.7528e-02,  4.7661e-02,  3.1249e-02,  ...,  2.5161e-02,\n              7.5651e-02,  4.7112e-02],\n            [ 1.6521e-02,  6.9486e-02,  6.5567e-02,  ..., -2.5428e-02,\n              8.2461e-02, -3.2037e-02]],\n  \n           [[ 2.6054e-02,  1.9609e-02,  1.4384e-03,  ..., -6.4697e-02,\n             -6.2796e-02, -3.7501e-02],\n            [-6.2957e-02,  2.1075e-02, -8.0505e-02,  ..., -1.5032e-02,\n             -4.0759e-02, -6.1857e-02],\n            [ 1.8026e-02, -6.0732e-02, -4.5379e-02,  ..., -5.9874e-02,\n             -6.0644e-02,  7.6561e-03],\n            ...,\n            [-7.6683e-02, -2.9194e-02,  3.4643e-02,  ...,  2.8419e-02,\n             -4.4386e-02,  4.9692e-02],\n            [-3.0022e-03, -2.6656e-02, -2.4319e-03,  ...,  3.1112e-02,\n              6.8292e-02, -5.8589e-02],\n            [-2.9720e-02,  7.1074e-02, -8.1562e-02,  ...,  7.1724e-02,\n              5.9465e-02,  2.5388e-02]]],\n  \n  \n          [[[ 1.8574e-02,  1.8720e-02,  1.3932e-02,  ...,  7.5164e-02,\n             -6.6079e-02,  2.2899e-02],\n            [ 3.4142e-02,  7.6649e-02,  1.0673e-03,  ..., -1.0437e-02,\n              1.3184e-02,  5.1705e-02],\n            [-4.0264e-02,  5.5465e-02, -7.9422e-02,  ...,  1.1812e-02,\n             -7.2939e-02,  7.8813e-02],\n            ...,\n            [-8.0808e-02,  3.5289e-02, -7.4104e-02,  ...,  6.6192e-02,\n              2.6433e-02, -7.0725e-03],\n            [ 7.7553e-03, -6.0164e-02,  6.4488e-02,  ...,  1.0493e-02,\n              7.5083e-02,  4.6892e-02],\n            [-4.4861e-02, -2.6221e-02, -3.3248e-02,  ..., -4.5930e-02,\n             -2.0060e-02,  2.1506e-02]],\n  \n           [[-6.7244e-02, -6.6991e-02,  3.6591e-03,  ..., -7.6205e-02,\n             -4.9503e-02, -5.8778e-02],\n            [-1.1239e-02,  7.5844e-03, -3.3997e-02,  ...,  3.2515e-02,\n             -5.6826e-02, -1.3621e-02],\n            [-6.5008e-02,  6.5563e-02, -3.6627e-02,  ..., -3.1984e-02,\n              2.1685e-02, -2.0890e-02],\n            ...,\n            [-4.7194e-02, -6.6915e-02,  4.6524e-02,  ...,  4.7385e-03,\n             -3.2143e-02,  6.7135e-02],\n            [-7.0919e-02, -8.2844e-03,  4.8195e-02,  ..., -2.8379e-02,\n             -7.9027e-02, -6.3003e-02],\n            [ 2.0982e-02,  2.8737e-02, -4.7233e-03,  ..., -2.2013e-02,\n              6.5388e-02, -6.6077e-02]],\n  \n           [[ 5.4343e-02,  3.2959e-02, -4.1546e-02,  ...,  5.0299e-02,\n              6.2987e-02, -3.6121e-02],\n            [-6.0878e-02, -6.7336e-02,  5.2449e-03,  ..., -3.0856e-04,\n             -5.2957e-03, -1.2447e-02],\n            [-5.9248e-02, -1.9418e-02, -2.8853e-02,  ..., -3.0065e-02,\n             -5.7408e-02,  7.4110e-02],\n            ...,\n            [-7.0938e-02, -1.8187e-02,  5.7305e-02,  ...,  2.3110e-02,\n             -5.7437e-02, -3.1829e-03],\n            [ 8.2083e-02,  7.7053e-02, -1.1716e-02,  ..., -2.4785e-02,\n             -4.1189e-03, -8.0575e-02],\n            [ 4.2338e-02,  8.0200e-02, -7.3796e-02,  ...,  5.3685e-02,\n             -6.0535e-02,  4.6479e-03]]]], requires_grad=True)\n)", "parameters": [["weight", [64, 3, 7, 7]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [9408]}, {"name": "bn1", "id": 140621193488032, "class_name": "BatchNorm2d(\n  self.momentum=0.1, self.weight=Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-2.8786e-04, -4.3709e-04, -3.7217e-04, -9.1637e-04,  1.1760e-03,\n           1.0195e-03, -1.1639e-03,  4.7598e-04,  5.9551e-04,  1.5939e-03,\n          -4.1885e-04,  8.3841e-04,  2.5121e-04,  9.6459e-04,  7.1937e-04,\n          -6.1190e-04,  6.6139e-04, -5.4426e-04, -1.7031e-04, -3.8126e-04,\n          -5.7404e-04, -1.7722e-03,  1.1621e-03,  6.9780e-04, -7.4249e-04,\n          -3.5811e-04, -1.4096e-03, -7.9837e-04, -4.7387e-04,  4.3260e-04,\n           4.2008e-04, -4.8021e-05,  1.3205e-03,  4.3409e-04, -1.4962e-03,\n          -1.4120e-03, -2.4441e-04, -4.6731e-04, -1.9203e-04,  3.6009e-04,\n           6.6072e-04,  1.3534e-03, -9.3614e-05, -8.1118e-04, -4.0396e-04,\n          -7.1993e-04,  8.2212e-04, -1.2470e-03, -1.3131e-03, -5.8012e-04,\n          -1.4861e-04,  1.6113e-03,  9.6870e-04,  7.0194e-04, -9.8111e-04,\n          -3.7077e-04,  7.4598e-04, -4.3532e-04,  2.1078e-03, -3.3515e-05,\n          -3.4427e-04, -6.6745e-04,  3.4134e-04,  3.0730e-04],\n         grad_fn=<AddBackward0>), self.running_var=tensor([0.9597, 0.9114, 0.9615, 0.9341, 0.9680, 0.9823, 0.9478, 0.9226, 0.9274,\n          0.9210, 0.9361, 0.9330, 0.9733, 0.9190, 0.9168, 0.9114, 0.9365, 0.9740,\n          0.9313, 0.9334, 0.9533, 0.9272, 0.9645, 0.9363, 0.9347, 0.9075, 0.9390,\n          0.9402, 0.9285, 1.0102, 0.9437, 0.9686, 0.9738, 0.9177, 0.9218, 0.9201,\n          0.9556, 0.9185, 0.9310, 0.9746, 0.9291, 0.9691, 0.9125, 1.0249, 0.9397,\n          0.9484, 0.9298, 0.9386, 0.9764, 0.9290, 0.9238, 0.9355, 0.9182, 0.9475,\n          0.9162, 0.9400, 0.9346, 0.9283, 0.9406, 0.9429, 0.9191, 0.9305, 0.9446,\n          0.9411], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n)", "parameters": [["weight", [64]], ["bias", [64]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [64, 64]}, {"name": "relu", "id": 140621193487936, "class_name": "ReLU()", "parameters": [], "output_shape": [[512, 64, 16, 16]], "num_parameters": []}, {"name": "pool", "id": 140621193487888, "class_name": "MaxPool2d(self.kernel_size=(3, 3), self.stride=(3, 3), self.padding=1)", "parameters": [], "output_shape": [[512, 64, 6, 6]], "num_parameters": []}, {"name": "layer1", "id": 140621193487312, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-4.1385e-02, -3.1040e-02,  6.4650e-03],\n                [-3.0060e-02, -2.5402e-02, -4.0486e-02],\n                [-3.8063e-02, -4.1629e-02, -5.7503e-03]],\n      \n               [[ 2.4033e-02,  2.0380e-02, -2.8624e-02],\n                [ 2.6203e-04, -1.1332e-02, -1.6046e-03],\n                [ 4.2910e-03, -6.7651e-03, -3.0065e-02]],\n      \n               [[-3.8606e-02, -1.1447e-02, -3.4138e-02],\n                [-3.9690e-02, -1.2163e-02,  1.6980e-02],\n                [-3.3853e-02, -1.3604e-02,  4.2598e-03]],\n      \n               ...,\n      \n               [[-3.2959e-02,  1.6695e-02, -6.7126e-03],\n                [ 6.7818e-03, -4.5424e-03, -2.2659e-02],\n                [ 1.0508e-02, -1.1831e-02, -3.5612e-02]],\n      \n               [[-2.6260e-02,  3.8964e-02, -1.5367e-03],\n                [-1.7674e-02,  6.2694e-05, -3.8549e-02],\n                [ 1.2181e-02,  3.9478e-02,  3.2287e-03]],\n      \n               [[ 1.5904e-02, -2.6628e-02,  2.3437e-02],\n                [-3.7278e-02,  3.6844e-02, -1.5845e-03],\n                [-9.1918e-03,  1.1623e-02,  3.7384e-02]]],\n      \n      \n              [[[ 2.0134e-02,  2.7658e-02,  3.7170e-03],\n                [-3.4897e-02,  3.8930e-02,  1.9342e-02],\n                [-3.7102e-02, -2.0559e-03,  3.8919e-02]],\n      \n               [[-1.8549e-02, -1.8473e-02,  2.4967e-02],\n                [-3.7764e-02, -5.5021e-04, -1.9344e-02],\n                [ 4.0396e-03,  2.6878e-02,  1.6149e-02]],\n      \n               [[-1.3840e-02,  3.0752e-02,  1.6718e-02],\n                [-1.3862e-02, -3.5192e-02, -2.8041e-02],\n                [-3.3309e-03,  1.4524e-02, -8.0185e-03]],\n      \n               ...,\n      \n               [[-2.5742e-02, -3.7809e-02,  2.5518e-02],\n                [ 8.7697e-04,  2.8767e-02, -2.5634e-02],\n                [ 3.3897e-02,  2.0058e-02, -3.9892e-03]],\n      \n               [[-2.5854e-02,  1.3111e-02,  4.0390e-02],\n                [ 3.9457e-02,  1.0425e-02, -3.6702e-02],\n                [-1.1013e-02, -2.5769e-02,  3.3357e-02]],\n      \n               [[ 1.5314e-02, -9.4441e-03,  1.0513e-02],\n                [ 1.2252e-02, -1.5572e-02,  2.4530e-02],\n                [-3.2789e-02, -2.9026e-02, -5.6635e-03]]],\n      \n      \n              [[[ 8.0341e-03,  2.3240e-02,  1.0112e-02],\n                [-3.0766e-02, -3.8758e-02,  3.7118e-02],\n                [-2.4347e-02, -4.9625e-03,  3.9463e-03]],\n      \n               [[-1.8051e-03, -5.2258e-03,  3.0631e-02],\n                [-2.7697e-02,  1.5750e-03, -1.7966e-02],\n                [ 2.0765e-02,  1.3617e-02,  1.4058e-03]],\n      \n               [[-1.0073e-02,  2.7676e-02, -3.9271e-02],\n                [-3.3179e-02,  1.7592e-02, -1.0525e-02],\n                [-3.8979e-02,  3.1095e-02, -1.3192e-02]],\n      \n               ...,\n      \n               [[ 1.9667e-02, -1.9958e-02,  2.5944e-02],\n                [ 3.0721e-02, -2.3826e-02,  2.1588e-02],\n                [-3.6987e-02,  2.5708e-02, -2.0880e-02]],\n      \n               [[-8.1758e-04,  1.8140e-02,  4.2666e-03],\n                [ 2.1123e-02, -7.0147e-03,  3.1147e-02],\n                [-3.4392e-02, -9.6027e-03,  2.7291e-02]],\n      \n               [[ 9.0333e-04,  1.6631e-02,  3.1082e-02],\n                [-6.8956e-03,  1.9016e-02, -1.0552e-02],\n                [-3.5521e-02,  2.4779e-02,  3.4322e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[ 1.8589e-02, -2.9397e-02,  1.1067e-02],\n                [ 1.9530e-02, -1.4032e-02,  2.4743e-02],\n                [ 1.5054e-02,  3.9180e-02, -8.3942e-04]],\n      \n               [[ 1.5345e-02, -3.5704e-02, -3.0795e-02],\n                [-4.8562e-03,  2.8489e-02, -8.9986e-03],\n                [ 1.2871e-02,  3.3965e-02, -3.4500e-02]],\n      \n               [[-1.6702e-02,  6.9676e-04, -2.7858e-02],\n                [-1.9002e-02, -1.9547e-02,  7.4208e-03],\n                [-2.7728e-03, -2.5341e-02,  1.8592e-02]],\n      \n               ...,\n      \n               [[-4.0068e-02,  1.9417e-02,  3.6107e-03],\n                [ 2.7228e-02,  3.2435e-02,  1.0341e-02],\n                [ 3.5772e-02, -3.5374e-02,  6.9020e-03]],\n      \n               [[-1.7734e-02, -1.0979e-02, -2.9419e-02],\n                [ 1.9707e-02,  5.9920e-04,  3.0532e-02],\n                [ 6.2916e-03,  3.6912e-02,  2.7111e-02]],\n      \n               [[-8.2555e-03,  2.3337e-02,  3.2000e-02],\n                [ 1.7606e-03,  4.0300e-02, -2.6980e-02],\n                [-4.0601e-02,  4.0586e-02, -4.9954e-03]]],\n      \n      \n              [[[-2.8090e-02,  2.5684e-02,  1.1478e-02],\n                [ 3.8067e-02,  1.6568e-02,  9.7861e-03],\n                [ 2.8939e-02, -3.2440e-02, -3.7477e-02]],\n      \n               [[ 2.8933e-02,  2.5764e-02,  5.6730e-04],\n                [ 2.8343e-02,  1.4926e-02, -1.1832e-02],\n                [ 2.1334e-02, -3.1811e-02, -4.0594e-02]],\n      \n               [[-3.6174e-02,  8.2691e-03,  9.1459e-03],\n                [-2.4468e-02,  6.9930e-03,  1.4265e-02],\n                [-2.9172e-02, -2.4521e-03,  3.8973e-03]],\n      \n               ...,\n      \n               [[ 3.6258e-02,  3.2850e-02, -3.0819e-02],\n                [ 3.7417e-02,  2.7247e-02,  4.0363e-02],\n                [-3.1912e-02,  2.9652e-02,  3.9345e-02]],\n      \n               [[-1.6941e-02,  3.8678e-02,  1.7079e-02],\n                [ 2.9073e-04,  2.7385e-02,  4.8184e-03],\n                [-3.3861e-02,  1.7056e-02, -2.8792e-02]],\n      \n               [[ 2.9473e-02,  9.3083e-04,  3.2891e-02],\n                [-1.9609e-02,  3.3068e-02,  9.9219e-03],\n                [ 1.6170e-02, -1.5593e-02,  1.4819e-02]]],\n      \n      \n              [[[-3.2409e-02, -2.1836e-02, -6.8083e-03],\n                [-2.7344e-02,  8.8893e-03, -1.8423e-02],\n                [ 3.2255e-02, -2.9329e-02, -2.3137e-02]],\n      \n               [[ 2.7336e-02,  5.8633e-03,  1.8422e-02],\n                [-2.6350e-02,  1.2718e-02, -3.8051e-02],\n                [ 3.5881e-02, -1.9494e-02, -1.5672e-02]],\n      \n               [[ 5.8232e-03, -3.7424e-02,  6.3432e-03],\n                [ 3.1311e-02, -1.6444e-02,  2.9978e-02],\n                [-2.3612e-03, -3.1937e-03, -1.4056e-03]],\n      \n               ...,\n      \n               [[-2.0537e-02, -3.7703e-03,  8.0347e-03],\n                [ 2.8021e-02,  1.5049e-02,  3.7689e-02],\n                [-1.3897e-02,  3.0911e-02, -6.5907e-03]],\n      \n               [[ 2.7161e-02, -1.9937e-03, -5.1771e-03],\n                [ 3.9347e-02, -2.0286e-02,  2.5862e-03],\n                [ 1.9064e-02, -4.0284e-02, -3.6223e-02]],\n      \n               [[-2.7854e-02,  3.7019e-02, -2.2099e-02],\n                [-2.4908e-02, -1.1679e-02,  1.2517e-02],\n                [-1.7167e-02,  4.0377e-02,  1.2390e-02]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-3.7450e-02,  6.0100e-02, -3.5708e-02, -5.2963e-02, -2.7185e-02,\n               5.3412e-02,  4.3770e-02, -7.0934e-02, -1.5652e-02, -9.6567e-03,\n              -9.3554e-04, -2.0320e-02, -5.8599e-02,  6.9159e-02, -4.8205e-02,\n               4.9925e-02, -3.9396e-02,  3.7560e-02, -5.4595e-02, -1.5650e-02,\n               4.2851e-02,  1.5683e-02,  1.5085e-02,  2.1848e-02, -8.9251e-05,\n               8.3858e-02, -3.7573e-02,  1.0889e-02,  1.4688e-02, -3.1233e-02,\n              -1.0869e-02, -2.4987e-02, -6.6464e-03,  7.1998e-02, -5.1931e-02,\n              -2.6824e-02, -4.2684e-02,  3.4356e-05,  6.1022e-02,  5.6068e-02,\n               2.2703e-02, -2.9199e-02, -2.4860e-02,  1.0358e-02,  2.4827e-02,\n               1.6078e-02,  2.1024e-02,  4.3161e-02, -2.7279e-02,  3.3201e-02,\n              -2.9351e-02, -2.7688e-02, -7.8779e-03, -9.5655e-03, -1.0787e-01,\n              -5.8795e-02, -1.4730e-02, -3.5763e-02, -4.0577e-02,  7.7212e-02,\n              -3.7110e-03,  9.7275e-02,  1.3808e-02, -8.6149e-02],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9180, 0.9157, 0.9153, 0.9159, 0.9146, 0.9301, 0.9330, 0.9238, 0.9178,\n              0.9203, 0.9140, 0.9100, 0.9179, 0.9212, 0.9263, 0.9138, 0.9199, 0.9148,\n              0.9254, 0.9225, 0.9192, 0.9103, 0.9109, 0.9153, 0.9089, 0.9192, 0.9127,\n              0.9132, 0.9159, 0.9136, 0.9176, 0.9246, 0.9128, 0.9224, 0.9171, 0.9186,\n              0.9246, 0.9126, 0.9165, 0.9275, 0.9193, 0.9140, 0.9134, 0.9158, 0.9135,\n              0.9116, 0.9197, 0.9256, 0.9212, 0.9225, 0.9327, 0.9159, 0.9164, 0.9161,\n              0.9318, 0.9253, 0.9100, 0.9352, 0.9208, 0.9237, 0.9287, 0.9301, 0.9186,\n              0.9268], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-0.0052,  0.0020, -0.0271],\n                [ 0.0385, -0.0303,  0.0413],\n                [ 0.0107,  0.0325, -0.0288]],\n      \n               [[ 0.0253, -0.0201,  0.0281],\n                [ 0.0219, -0.0143, -0.0113],\n                [ 0.0409, -0.0090,  0.0155]],\n      \n               [[-0.0148, -0.0253, -0.0088],\n                [-0.0375,  0.0074,  0.0063],\n                [-0.0352, -0.0324, -0.0008]],\n      \n               ...,\n      \n               [[-0.0009,  0.0008,  0.0416],\n                [-0.0285, -0.0317,  0.0367],\n                [ 0.0292, -0.0095,  0.0306]],\n      \n               [[-0.0015,  0.0128,  0.0406],\n                [-0.0397, -0.0070, -0.0026],\n                [ 0.0037, -0.0366, -0.0214]],\n      \n               [[ 0.0324, -0.0415,  0.0345],\n                [-0.0157, -0.0342,  0.0107],\n                [ 0.0296, -0.0254,  0.0375]]],\n      \n      \n              [[[-0.0318,  0.0076,  0.0166],\n                [-0.0351,  0.0235, -0.0388],\n                [-0.0219,  0.0006, -0.0184]],\n      \n               [[-0.0072,  0.0370, -0.0048],\n                [-0.0233, -0.0041, -0.0207],\n                [-0.0275, -0.0080, -0.0232]],\n      \n               [[-0.0263, -0.0417, -0.0137],\n                [ 0.0163, -0.0201, -0.0220],\n                [-0.0161, -0.0233, -0.0380]],\n      \n               ...,\n      \n               [[ 0.0094, -0.0054, -0.0344],\n                [-0.0223,  0.0011,  0.0109],\n                [-0.0363,  0.0154,  0.0293]],\n      \n               [[ 0.0023,  0.0256,  0.0326],\n                [-0.0401,  0.0282,  0.0258],\n                [-0.0209, -0.0014,  0.0349]],\n      \n               [[ 0.0011,  0.0323,  0.0237],\n                [-0.0204, -0.0204, -0.0174],\n                [-0.0053, -0.0068,  0.0356]]],\n      \n      \n              [[[ 0.0380,  0.0286, -0.0085],\n                [-0.0059,  0.0157,  0.0376],\n                [ 0.0058,  0.0069,  0.0379]],\n      \n               [[-0.0240, -0.0399, -0.0131],\n                [ 0.0266, -0.0109, -0.0299],\n                [ 0.0309, -0.0091,  0.0157]],\n      \n               [[ 0.0234,  0.0129,  0.0403],\n                [-0.0087, -0.0161, -0.0307],\n                [-0.0294,  0.0363,  0.0232]],\n      \n               ...,\n      \n               [[-0.0315, -0.0383, -0.0328],\n                [-0.0184, -0.0194, -0.0266],\n                [ 0.0325,  0.0277,  0.0086]],\n      \n               [[-0.0190, -0.0354,  0.0154],\n                [-0.0255, -0.0198, -0.0136],\n                [ 0.0315, -0.0358,  0.0355]],\n      \n               [[-0.0376,  0.0080,  0.0011],\n                [-0.0249,  0.0133, -0.0356],\n                [ 0.0010, -0.0322, -0.0187]]],\n      \n      \n              ...,\n      \n      \n              [[[ 0.0312, -0.0372,  0.0131],\n                [ 0.0286,  0.0310, -0.0216],\n                [ 0.0203,  0.0316, -0.0087]],\n      \n               [[ 0.0039, -0.0330, -0.0013],\n                [-0.0219, -0.0267, -0.0047],\n                [ 0.0324,  0.0166,  0.0124]],\n      \n               [[-0.0266, -0.0211, -0.0241],\n                [ 0.0182, -0.0148,  0.0398],\n                [-0.0410,  0.0105,  0.0147]],\n      \n               ...,\n      \n               [[-0.0068, -0.0035,  0.0101],\n                [ 0.0363,  0.0202,  0.0072],\n                [-0.0061, -0.0195, -0.0139]],\n      \n               [[-0.0244, -0.0278, -0.0253],\n                [-0.0283,  0.0284, -0.0345],\n                [ 0.0063,  0.0131, -0.0223]],\n      \n               [[-0.0221,  0.0311, -0.0175],\n                [-0.0172,  0.0154,  0.0261],\n                [ 0.0374, -0.0209,  0.0032]]],\n      \n      \n              [[[-0.0049, -0.0051, -0.0237],\n                [-0.0361,  0.0138,  0.0260],\n                [-0.0270,  0.0061,  0.0094]],\n      \n               [[ 0.0412,  0.0297, -0.0343],\n                [-0.0204, -0.0372,  0.0159],\n                [-0.0255,  0.0344,  0.0040]],\n      \n               [[-0.0293,  0.0284,  0.0113],\n                [-0.0021,  0.0306, -0.0365],\n                [-0.0132, -0.0178,  0.0169]],\n      \n               ...,\n      \n               [[-0.0272, -0.0039, -0.0324],\n                [-0.0191,  0.0033,  0.0253],\n                [ 0.0298,  0.0405,  0.0119]],\n      \n               [[ 0.0173,  0.0091,  0.0134],\n                [-0.0413, -0.0296, -0.0404],\n                [ 0.0091, -0.0051,  0.0311]],\n      \n               [[-0.0344, -0.0066,  0.0071],\n                [-0.0335, -0.0081, -0.0086],\n                [ 0.0055, -0.0263, -0.0310]]],\n      \n      \n              [[[ 0.0107, -0.0050, -0.0383],\n                [ 0.0385,  0.0173, -0.0039],\n                [ 0.0282, -0.0011,  0.0141]],\n      \n               [[-0.0324,  0.0139, -0.0005],\n                [-0.0099,  0.0192, -0.0336],\n                [-0.0359,  0.0120,  0.0219]],\n      \n               [[-0.0096, -0.0034, -0.0216],\n                [-0.0231,  0.0380,  0.0007],\n                [-0.0001, -0.0026, -0.0319]],\n      \n               ...,\n      \n               [[-0.0316, -0.0324,  0.0255],\n                [-0.0186,  0.0021, -0.0094],\n                [-0.0125,  0.0288,  0.0101]],\n      \n               [[-0.0391, -0.0056, -0.0415],\n                [-0.0403, -0.0147, -0.0337],\n                [ 0.0174,  0.0345, -0.0389]],\n      \n               [[ 0.0396,  0.0177,  0.0110],\n                [-0.0399, -0.0015,  0.0111],\n                [ 0.0054, -0.0211,  0.0219]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0273, -0.0219, -0.0171, -0.0001,  0.0148, -0.0317, -0.0122,  0.0044,\n              -0.0117,  0.0214,  0.0039, -0.0095,  0.0153,  0.0349, -0.0116,  0.0037,\n              -0.0162,  0.0357,  0.0167,  0.0066, -0.0241, -0.0296,  0.0013, -0.0062,\n              -0.0067,  0.0137,  0.0166, -0.0062, -0.0062,  0.0146, -0.0414, -0.0120,\n              -0.0133, -0.0035,  0.0156, -0.0097,  0.0207, -0.0295,  0.0068,  0.0124,\n              -0.0007, -0.0102, -0.0042,  0.0035, -0.0058,  0.0012,  0.0048, -0.0330,\n               0.0102,  0.0221, -0.0018, -0.0348, -0.0264, -0.0137, -0.0154, -0.0426,\n              -0.0009,  0.0439,  0.0231,  0.0006, -0.0121, -0.0046, -0.0196, -0.0048],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9152, 0.9073, 0.9122, 0.9089, 0.9084, 0.9096, 0.9083, 0.9072, 0.9054,\n              0.9159, 0.9117, 0.9100, 0.9109, 0.9133, 0.9100, 0.9104, 0.9093, 0.9102,\n              0.9089, 0.9077, 0.9095, 0.9082, 0.9106, 0.9123, 0.9109, 0.9096, 0.9104,\n              0.9100, 0.9100, 0.9059, 0.9120, 0.9067, 0.9077, 0.9129, 0.9089, 0.9086,\n              0.9103, 0.9115, 0.9093, 0.9103, 0.9092, 0.9113, 0.9084, 0.9086, 0.9101,\n              0.9104, 0.9094, 0.9096, 0.9083, 0.9155, 0.9079, 0.9102, 0.9081, 0.9088,\n              0.9105, 0.9104, 0.9111, 0.9123, 0.9106, 0.9079, 0.9071, 0.9081, 0.9089,\n              0.9078], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n  )\n)", "parameters": [["0.conv1.weight", [64, 64, 3, 3]], ["0.bn1.weight", [64]], ["0.bn1.bias", [64]], ["0.conv2.weight", [64, 64, 3, 3]], ["0.bn2.weight", [64]], ["0.bn2.bias", [64]]], "output_shape": [[512, 64, 6, 6]], "num_parameters": [36864, 64, 64, 36864, 64, 64]}, {"name": "layer2", "id": 140621193486400, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 0.0127,  0.0244,  0.0413],\n                [ 0.0256,  0.0328,  0.0040],\n                [-0.0144,  0.0216,  0.0129]],\n      \n               [[-0.0142, -0.0124,  0.0084],\n                [ 0.0008, -0.0190, -0.0328],\n                [ 0.0304, -0.0038,  0.0296]],\n      \n               [[-0.0212,  0.0339,  0.0071],\n                [-0.0118, -0.0190,  0.0029],\n                [ 0.0362,  0.0143, -0.0359]],\n      \n               ...,\n      \n               [[ 0.0334, -0.0324, -0.0129],\n                [ 0.0039,  0.0287, -0.0034],\n                [ 0.0115,  0.0291,  0.0399]],\n      \n               [[-0.0372,  0.0227, -0.0408],\n                [ 0.0117, -0.0250,  0.0032],\n                [ 0.0211,  0.0182,  0.0302]],\n      \n               [[-0.0155,  0.0237,  0.0330],\n                [-0.0053,  0.0027, -0.0326],\n                [-0.0252, -0.0083, -0.0097]]],\n      \n      \n              [[[ 0.0368,  0.0021,  0.0234],\n                [ 0.0101,  0.0189,  0.0304],\n                [-0.0272,  0.0317,  0.0373]],\n      \n               [[-0.0139, -0.0140,  0.0094],\n                [-0.0354,  0.0085, -0.0323],\n                [-0.0009, -0.0368, -0.0244]],\n      \n               [[ 0.0255,  0.0335,  0.0056],\n                [ 0.0141, -0.0355, -0.0248],\n                [-0.0350, -0.0238,  0.0184]],\n      \n               ...,\n      \n               [[-0.0252, -0.0043,  0.0346],\n                [-0.0270,  0.0110,  0.0374],\n                [-0.0309,  0.0062, -0.0340]],\n      \n               [[-0.0077,  0.0232,  0.0026],\n                [-0.0249, -0.0132, -0.0304],\n                [ 0.0276, -0.0290, -0.0015]],\n      \n               [[ 0.0060, -0.0056, -0.0085],\n                [ 0.0039,  0.0066, -0.0166],\n                [-0.0316, -0.0396, -0.0261]]],\n      \n      \n              [[[-0.0178, -0.0393,  0.0305],\n                [-0.0398, -0.0267,  0.0152],\n                [-0.0412, -0.0242,  0.0137]],\n      \n               [[ 0.0268,  0.0359,  0.0089],\n                [ 0.0261, -0.0394,  0.0154],\n                [-0.0370,  0.0268, -0.0351]],\n      \n               [[ 0.0058, -0.0242,  0.0313],\n                [-0.0281,  0.0006, -0.0163],\n                [-0.0126,  0.0136, -0.0298]],\n      \n               ...,\n      \n               [[ 0.0223, -0.0154, -0.0391],\n                [-0.0203, -0.0330, -0.0072],\n                [-0.0323,  0.0124,  0.0136]],\n      \n               [[ 0.0219, -0.0042, -0.0170],\n                [ 0.0203,  0.0030,  0.0269],\n                [-0.0189, -0.0029,  0.0050]],\n      \n               [[ 0.0261,  0.0307, -0.0388],\n                [ 0.0340,  0.0299, -0.0224],\n                [ 0.0235,  0.0237, -0.0271]]],\n      \n      \n              ...,\n      \n      \n              [[[-0.0389, -0.0297, -0.0203],\n                [ 0.0004,  0.0181, -0.0126],\n                [ 0.0377,  0.0215, -0.0049]],\n      \n               [[ 0.0049, -0.0086,  0.0023],\n                [ 0.0389, -0.0347,  0.0191],\n                [-0.0318,  0.0256,  0.0300]],\n      \n               [[ 0.0033,  0.0085, -0.0384],\n                [ 0.0081, -0.0033, -0.0110],\n                [-0.0324,  0.0272, -0.0409]],\n      \n               ...,\n      \n               [[-0.0078, -0.0137,  0.0051],\n                [ 0.0384, -0.0152,  0.0078],\n                [ 0.0337, -0.0148,  0.0343]],\n      \n               [[-0.0302, -0.0160,  0.0350],\n                [ 0.0045, -0.0012, -0.0261],\n                [-0.0160,  0.0367,  0.0018]],\n      \n               [[ 0.0218,  0.0276, -0.0221],\n                [ 0.0038,  0.0264,  0.0198],\n                [ 0.0065, -0.0260,  0.0135]]],\n      \n      \n              [[[-0.0240, -0.0036, -0.0088],\n                [ 0.0395, -0.0101, -0.0172],\n                [ 0.0363,  0.0379,  0.0041]],\n      \n               [[-0.0273, -0.0269, -0.0352],\n                [ 0.0229, -0.0235,  0.0254],\n                [ 0.0345,  0.0261,  0.0415]],\n      \n               [[ 0.0284, -0.0384,  0.0318],\n                [ 0.0248,  0.0325,  0.0040],\n                [-0.0208,  0.0226,  0.0134]],\n      \n               ...,\n      \n               [[-0.0298, -0.0246, -0.0325],\n                [-0.0248, -0.0382, -0.0391],\n                [ 0.0258, -0.0233,  0.0065]],\n      \n               [[ 0.0301, -0.0141,  0.0151],\n                [ 0.0296,  0.0237,  0.0101],\n                [-0.0203,  0.0323, -0.0364]],\n      \n               [[ 0.0060, -0.0273,  0.0407],\n                [ 0.0284,  0.0240,  0.0379],\n                [ 0.0240,  0.0138, -0.0342]]],\n      \n      \n              [[[ 0.0349, -0.0074,  0.0085],\n                [ 0.0309,  0.0344, -0.0011],\n                [-0.0193,  0.0166, -0.0199]],\n      \n               [[ 0.0272,  0.0084, -0.0345],\n                [-0.0054,  0.0032,  0.0313],\n                [ 0.0203, -0.0369,  0.0092]],\n      \n               [[ 0.0272, -0.0325,  0.0272],\n                [ 0.0301,  0.0254,  0.0217],\n                [ 0.0020,  0.0329,  0.0318]],\n      \n               ...,\n      \n               [[-0.0175, -0.0370, -0.0141],\n                [-0.0024,  0.0126,  0.0039],\n                [-0.0366, -0.0262, -0.0331]],\n      \n               [[ 0.0385,  0.0299, -0.0331],\n                [-0.0342, -0.0041, -0.0031],\n                [-0.0135, -0.0068,  0.0205]],\n      \n               [[ 0.0315,  0.0199,  0.0026],\n                [-0.0027, -0.0408, -0.0107],\n                [ 0.0402, -0.0332, -0.0093]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0399, -0.0019, -0.0252, -0.0166, -0.0541, -0.0307, -0.0243, -0.0439,\n              -0.0025,  0.0107,  0.0331, -0.0150,  0.0067,  0.0135, -0.0612,  0.0447,\n              -0.0052,  0.0637, -0.0403, -0.0239, -0.0088, -0.0236,  0.0484, -0.0173,\n               0.0676, -0.0034, -0.0633, -0.0060, -0.0174,  0.0489, -0.0608,  0.0046,\n              -0.0253, -0.0592, -0.0098, -0.0064,  0.0251, -0.0242, -0.0173, -0.0059,\n              -0.0193,  0.0299,  0.0314,  0.0322,  0.0505, -0.0108,  0.0496, -0.0232,\n              -0.0001,  0.0643, -0.0093, -0.0485, -0.0132, -0.0479, -0.0126,  0.0837,\n              -0.0272,  0.1257,  0.0775, -0.0448,  0.0495, -0.0473, -0.0158,  0.0203,\n              -0.0131, -0.0043, -0.1214, -0.0808,  0.0180, -0.0791,  0.0783,  0.0650,\n              -0.0976, -0.0410, -0.0192, -0.0126,  0.0791, -0.0347,  0.0281, -0.0114,\n              -0.0340, -0.0686, -0.0224, -0.0160, -0.0684,  0.0602,  0.0632,  0.0529,\n              -0.0451,  0.0057, -0.0361, -0.0359, -0.0417,  0.0545, -0.0900,  0.0075,\n              -0.0565, -0.0188,  0.0122, -0.0315, -0.0273,  0.0436, -0.0617,  0.0317,\n              -0.0154,  0.0820, -0.0982,  0.0613, -0.0270,  0.0266,  0.0630, -0.0660,\n               0.0476,  0.0087,  0.0033, -0.0551, -0.0425,  0.0394, -0.0335,  0.0169,\n               0.0581, -0.0385, -0.0088,  0.0351, -0.0108, -0.0148, -0.0063,  0.0032],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9374, 0.9335, 0.9182, 0.9348, 0.9349, 0.9275, 0.9337, 0.9451, 0.9221,\n              0.9389, 0.9305, 0.9290, 0.9259, 0.9313, 0.9706, 0.9420, 0.9238, 0.9302,\n              0.9378, 0.9261, 0.9296, 0.9301, 0.9225, 0.9222, 0.9361, 0.9320, 0.9407,\n              0.9272, 0.9295, 0.9292, 0.9328, 0.9253, 0.9284, 0.9251, 0.9346, 0.9342,\n              0.9355, 0.9273, 0.9303, 0.9275, 0.9281, 0.9314, 0.9289, 0.9320, 0.9242,\n              0.9287, 0.9565, 0.9298, 0.9335, 0.9292, 0.9468, 0.9279, 0.9394, 0.9506,\n              0.9262, 0.9390, 0.9277, 0.9715, 0.9819, 0.9510, 0.9332, 0.9318, 0.9408,\n              0.9474, 0.9559, 0.9276, 0.9378, 0.9421, 0.9316, 0.9303, 0.9407, 0.9328,\n              0.9444, 0.9344, 0.9361, 0.9469, 0.9492, 0.9315, 0.9379, 0.9216, 0.9235,\n              0.9657, 0.9245, 0.9257, 0.9319, 0.9396, 0.9277, 0.9466, 0.9392, 0.9297,\n              0.9266, 0.9373, 0.9223, 0.9360, 0.9277, 0.9487, 0.9350, 0.9321, 0.9299,\n              0.9409, 0.9380, 0.9305, 0.9434, 0.9227, 0.9320, 0.9317, 0.9477, 0.9299,\n              0.9235, 0.9281, 0.9305, 0.9666, 0.9337, 0.9289, 0.9281, 0.9285, 0.9556,\n              0.9432, 0.9370, 0.9408, 0.9275, 0.9779, 0.9223, 0.9434, 0.9383, 0.9256,\n              0.9450, 0.9282], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-2.0442e-02, -2.7919e-02, -7.1195e-03],\n                [-2.5038e-02,  2.2044e-04, -2.4113e-02],\n                [ 7.9078e-05,  1.8130e-02,  1.3879e-02]],\n      \n               [[-1.0175e-02, -1.6625e-02,  7.7044e-03],\n                [ 2.8542e-02,  2.7393e-02, -7.8169e-03],\n                [ 2.0353e-02, -2.7034e-02, -2.5291e-02]],\n      \n               [[-1.5634e-02, -7.7418e-03,  2.2289e-02],\n                [ 2.4797e-02,  2.3206e-02, -4.4008e-03],\n                [-8.0870e-03,  1.0791e-02, -3.3433e-03]],\n      \n               ...,\n      \n               [[-2.6807e-02, -2.1882e-03,  1.9625e-02],\n                [-2.4898e-02,  8.2197e-03, -2.6288e-02],\n                [-5.8687e-03,  2.8626e-02, -1.2672e-02]],\n      \n               [[ 1.8827e-02,  2.0609e-02,  1.7559e-02],\n                [-2.9000e-02,  2.3610e-02, -2.3970e-02],\n                [-2.9991e-03, -1.0534e-03, -1.8968e-02]],\n      \n               [[-1.5626e-02, -1.0628e-02, -2.1620e-02],\n                [-2.4552e-02,  2.4026e-02, -1.3278e-02],\n                [ 2.1877e-02,  8.4252e-03,  2.5893e-02]]],\n      \n      \n              [[[ 1.2591e-02, -4.2545e-03,  6.6649e-03],\n                [ 1.0598e-02, -1.4100e-02,  1.4148e-03],\n                [ 1.1218e-02,  2.4410e-02,  9.9179e-03]],\n      \n               [[ 2.7064e-02,  1.4117e-03, -4.2601e-03],\n                [-9.8792e-03, -1.2665e-02, -1.5564e-02],\n                [-3.5388e-03, -2.1799e-02, -1.9763e-02]],\n      \n               [[ 5.2722e-03,  9.2644e-03,  2.7873e-02],\n                [-1.1234e-02, -1.1542e-02,  8.9011e-03],\n                [-1.0814e-02, -2.6058e-02, -1.3342e-03]],\n      \n               ...,\n      \n               [[ 1.1606e-02,  6.4163e-03,  6.6279e-03],\n                [-8.1541e-04,  2.7923e-02, -2.2051e-02],\n                [-1.2347e-02, -2.5429e-02, -2.5852e-03]],\n      \n               [[-1.9993e-02, -1.2297e-02,  9.1154e-03],\n                [ 1.2579e-02, -1.3792e-02,  7.8394e-03],\n                [ 1.2084e-02, -1.3129e-02, -1.6850e-02]],\n      \n               [[ 2.6019e-03, -2.1347e-02,  2.9704e-03],\n                [-1.0078e-02,  1.4339e-02,  2.3733e-02],\n                [-1.5121e-02,  2.8608e-03, -2.5219e-02]]],\n      \n      \n              [[[-2.8696e-03, -3.5495e-03, -2.0508e-02],\n                [ 2.5360e-02, -6.8333e-03, -2.5396e-02],\n                [ 1.9871e-02, -1.0775e-02, -5.4275e-03]],\n      \n               [[ 2.9065e-02,  2.8567e-02, -2.5014e-02],\n                [-2.4934e-04,  2.8852e-02,  7.3783e-03],\n                [-1.3583e-03, -1.4834e-03,  1.2206e-02]],\n      \n               [[-1.3068e-02, -6.2572e-03,  1.0581e-02],\n                [-1.2148e-02,  3.4205e-03,  9.3983e-03],\n                [ 1.7520e-02,  2.6646e-03, -3.4056e-03]],\n      \n               ...,\n      \n               [[ 6.5152e-03,  2.6604e-02,  2.1965e-02],\n                [-1.4985e-02, -2.0868e-02,  2.4828e-02],\n                [ 1.2542e-02, -4.2538e-03, -2.5919e-02]],\n      \n               [[-2.7878e-02,  2.6914e-03,  1.7721e-03],\n                [ 3.8730e-03, -2.6294e-02, -2.0968e-02],\n                [ 1.8961e-02, -2.2928e-02,  7.4449e-03]],\n      \n               [[-1.7468e-02, -1.4256e-02, -2.4215e-02],\n                [ 4.3969e-04, -1.2776e-02,  1.2444e-02],\n                [-2.5670e-02, -2.2016e-02, -2.2732e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[-1.1171e-02,  1.7439e-02, -2.2881e-02],\n                [-1.6147e-03,  1.5333e-02,  2.3645e-02],\n                [-2.7212e-02, -1.2528e-02,  7.1716e-03]],\n      \n               [[-1.1362e-02,  1.2041e-02, -3.2060e-03],\n                [-2.4407e-02, -1.1043e-02, -1.5410e-03],\n                [-2.2555e-02, -2.2652e-02,  9.9108e-04]],\n      \n               [[ 3.5046e-03,  4.3886e-03,  2.7141e-02],\n                [ 2.2525e-03, -2.7795e-02,  4.8595e-03],\n                [ 5.8804e-03, -3.2783e-03, -2.2589e-02]],\n      \n               ...,\n      \n               [[-1.7263e-02, -1.8898e-02, -2.6067e-02],\n                [-3.8964e-03, -3.0658e-03,  2.9249e-02],\n                [-6.6275e-03, -2.4015e-02,  6.8912e-03]],\n      \n               [[ 2.6402e-02,  2.6481e-02,  4.0784e-04],\n                [ 1.9714e-03,  1.1444e-02,  2.0563e-02],\n                [-1.6747e-02, -2.0215e-03,  9.5679e-03]],\n      \n               [[-2.7532e-02, -1.3952e-02, -1.2102e-02],\n                [ 1.6140e-02,  1.0589e-02,  2.2208e-04],\n                [-1.6865e-02, -1.1418e-02, -8.1334e-03]]],\n      \n      \n              [[[-1.6046e-02, -2.2993e-02,  1.8483e-02],\n                [-2.4067e-02, -4.1482e-03, -1.0606e-02],\n                [ 2.7033e-02,  2.6340e-02,  6.1200e-03]],\n      \n               [[-1.7802e-04,  1.8240e-02,  1.3853e-02],\n                [ 1.7928e-02,  7.3631e-03,  2.4084e-02],\n                [ 8.9253e-03,  2.2080e-02,  3.9287e-03]],\n      \n               [[ 2.0650e-02, -5.9287e-04,  2.3708e-03],\n                [-2.6447e-02, -2.9361e-04,  1.6390e-02],\n                [-5.1442e-03, -4.8753e-03,  2.0557e-02]],\n      \n               ...,\n      \n               [[-2.3372e-03, -1.1855e-02,  2.5409e-02],\n                [ 2.8641e-02, -2.6200e-03, -1.2509e-02],\n                [-1.6703e-02,  2.3930e-02,  2.7219e-02]],\n      \n               [[ 1.9579e-02, -2.4601e-02,  1.9050e-02],\n                [-1.8075e-02, -2.1395e-02, -2.7934e-02],\n                [ 2.0318e-02, -2.1892e-02, -1.4681e-02]],\n      \n               [[ 2.1578e-02,  2.0518e-02,  1.4792e-02],\n                [ 2.3094e-02,  2.6351e-02,  1.5536e-04],\n                [-2.3319e-02, -2.7846e-02,  2.3123e-02]]],\n      \n      \n              [[[ 2.3437e-02,  1.0606e-02,  1.2216e-03],\n                [-2.5650e-02,  1.6712e-02, -1.8199e-02],\n                [-1.3637e-02,  2.4901e-02, -4.1883e-03]],\n      \n               [[ 1.3784e-02, -7.6890e-03,  2.2254e-02],\n                [ 2.3003e-02, -4.0034e-03, -4.2455e-03],\n                [-2.8002e-02, -4.6400e-03, -1.5713e-02]],\n      \n               [[-2.1634e-02,  1.0138e-02, -1.9233e-02],\n                [-8.7551e-03,  7.2576e-03,  2.5830e-02],\n                [ 9.6519e-03, -1.3769e-02,  1.0538e-02]],\n      \n               ...,\n      \n               [[-2.8789e-02, -1.6725e-03, -2.0366e-02],\n                [ 6.5343e-03,  5.0207e-04, -2.5696e-02],\n                [-9.5666e-03, -1.1256e-03, -2.0421e-02]],\n      \n               [[-2.0778e-02, -1.0719e-02, -2.3135e-02],\n                [-2.7572e-02, -2.5592e-03,  2.0509e-02],\n                [ 9.7491e-03,  2.5508e-02, -1.9043e-02]],\n      \n               [[-2.5045e-02,  9.3424e-03, -1.7310e-03],\n                [ 2.3572e-02, -2.7836e-02, -2.1068e-02],\n                [-1.8139e-02, -1.1361e-02, -1.1983e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-5.1362e-03,  2.2051e-02, -1.2180e-02,  1.4493e-02,  1.4900e-02,\n              -3.0446e-02, -1.1415e-02, -2.0971e-02, -3.5047e-03, -2.4456e-02,\n               2.5872e-04, -2.3474e-02,  5.8061e-03, -9.5445e-03,  1.2667e-02,\n               2.4708e-02,  2.2576e-03,  7.7395e-03,  5.4501e-03, -7.1380e-03,\n              -3.9599e-03, -7.9523e-03,  6.5376e-03,  1.3112e-02,  3.1139e-03,\n               1.5878e-03, -1.0706e-02,  5.0571e-03,  2.1339e-02, -1.3635e-02,\n               1.2937e-02, -4.0879e-03, -1.8368e-02,  1.3755e-03, -8.3288e-03,\n              -1.2159e-02,  2.9876e-02,  1.4189e-02,  3.8503e-03,  2.3117e-03,\n               1.0857e-02, -8.2666e-03,  3.8856e-03,  7.2797e-03, -5.0590e-04,\n              -2.3610e-02,  2.7566e-04, -4.0416e-03, -1.3606e-02, -5.7510e-03,\n              -7.5986e-03, -2.0313e-02, -3.4826e-03,  6.5611e-03, -2.2652e-03,\n               9.6045e-03,  2.5026e-02, -2.2805e-02,  8.0926e-03, -2.0150e-02,\n               2.2867e-03, -1.6698e-02, -1.0465e-03, -1.6107e-02, -2.2170e-03,\n               1.1392e-02,  1.6675e-02, -1.3396e-03,  4.4288e-04,  6.7394e-04,\n              -1.5640e-02,  1.2509e-02,  2.3976e-03, -8.1051e-03,  1.2807e-02,\n              -1.8502e-03, -6.5449e-03,  1.5474e-02,  1.8100e-02, -2.1902e-02,\n              -3.8542e-03,  1.4466e-02, -1.3552e-02, -9.7534e-03,  1.8761e-02,\n               7.4289e-03, -2.1136e-02,  2.2525e-02, -2.4955e-02, -7.5120e-03,\n               6.1848e-03,  2.2781e-02, -3.7945e-03,  1.8017e-02,  1.8451e-02,\n              -1.6489e-02, -2.6241e-02, -1.5545e-02, -7.3122e-03, -2.0009e-02,\n              -1.7675e-03,  8.8793e-03,  2.3025e-02,  1.7044e-02, -8.4824e-03,\n              -1.0756e-02,  1.2355e-03, -5.7519e-03, -1.3665e-02, -9.8210e-03,\n               9.7200e-03, -2.7420e-03,  3.5150e-03,  1.2936e-02,  2.2574e-02,\n              -2.7756e-02, -4.4295e-03,  1.2101e-02,  1.8479e-02, -6.2572e-04,\n               8.4732e-03, -1.6905e-02,  5.2396e-03,  2.2594e-02, -1.4074e-02,\n               9.2424e-03,  2.1644e-02, -1.2560e-05], grad_fn=<AddBackward0>), self.running_var=tensor([0.9056, 0.9065, 0.9094, 0.9089, 0.9085, 0.9115, 0.9073, 0.9088, 0.9072,\n              0.9092, 0.9084, 0.9105, 0.9081, 0.9083, 0.9106, 0.9096, 0.9083, 0.9142,\n              0.9073, 0.9107, 0.9081, 0.9081, 0.9072, 0.9089, 0.9058, 0.9084, 0.9073,\n              0.9071, 0.9082, 0.9081, 0.9063, 0.9077, 0.9065, 0.9092, 0.9074, 0.9073,\n              0.9105, 0.9106, 0.9102, 0.9100, 0.9078, 0.9088, 0.9086, 0.9087, 0.9138,\n              0.9073, 0.9054, 0.9086, 0.9091, 0.9074, 0.9091, 0.9095, 0.9092, 0.9070,\n              0.9067, 0.9069, 0.9112, 0.9073, 0.9082, 0.9113, 0.9073, 0.9075, 0.9067,\n              0.9070, 0.9065, 0.9102, 0.9057, 0.9114, 0.9076, 0.9144, 0.9079, 0.9079,\n              0.9084, 0.9097, 0.9063, 0.9102, 0.9061, 0.9073, 0.9060, 0.9110, 0.9073,\n              0.9104, 0.9078, 0.9131, 0.9068, 0.9075, 0.9068, 0.9081, 0.9137, 0.9065,\n              0.9066, 0.9075, 0.9087, 0.9061, 0.9129, 0.9059, 0.9089, 0.9099, 0.9083,\n              0.9091, 0.9094, 0.9067, 0.9111, 0.9098, 0.9105, 0.9084, 0.9085, 0.9096,\n              0.9102, 0.9064, 0.9119, 0.9067, 0.9082, 0.9075, 0.9099, 0.9080, 0.9070,\n              0.9085, 0.9083, 0.9061, 0.9060, 0.9091, 0.9067, 0.9138, 0.9061, 0.9084,\n              0.9073, 0.9079], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[-0.0445]],\n        \n                 [[ 0.0039]],\n        \n                 [[ 0.1125]],\n        \n                 ...,\n        \n                 [[-0.0873]],\n        \n                 [[-0.0886]],\n        \n                 [[ 0.0645]]],\n        \n        \n                [[[-0.0178]],\n        \n                 [[ 0.1172]],\n        \n                 [[ 0.0273]],\n        \n                 ...,\n        \n                 [[ 0.0868]],\n        \n                 [[-0.0790]],\n        \n                 [[ 0.1015]]],\n        \n        \n                [[[-0.0170]],\n        \n                 [[ 0.0589]],\n        \n                 [[ 0.0969]],\n        \n                 ...,\n        \n                 [[-0.0967]],\n        \n                 [[ 0.0755]],\n        \n                 [[ 0.0699]]],\n        \n        \n                ...,\n        \n        \n                [[[ 0.1164]],\n        \n                 [[-0.0466]],\n        \n                 [[-0.0307]],\n        \n                 ...,\n        \n                 [[-0.0941]],\n        \n                 [[-0.0258]],\n        \n                 [[ 0.0819]]],\n        \n        \n                [[[ 0.0776]],\n        \n                 [[ 0.0796]],\n        \n                 [[ 0.0788]],\n        \n                 ...,\n        \n                 [[ 0.0694]],\n        \n                 [[ 0.0933]],\n        \n                 [[-0.0387]]],\n        \n        \n                [[[-0.0966]],\n        \n                 [[-0.0832]],\n        \n                 [[-0.0663]],\n        \n                 ...,\n        \n                 [[-0.0663]],\n        \n                 [[-0.0809]],\n        \n                 [[-0.0806]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0192,  0.1736,  0.0316, -0.0472,  0.0991,  0.0109,  0.0240,  0.0293,\n                -0.0646,  0.0455,  0.0157, -0.1089, -0.0078,  0.0172, -0.0177,  0.0770,\n                -0.0072,  0.0940,  0.0541, -0.0113, -0.0168,  0.0023,  0.0746, -0.0223,\n                 0.0822,  0.1240, -0.0859, -0.0947,  0.0078,  0.0297,  0.0375, -0.0713,\n                -0.0711,  0.0721, -0.0560,  0.0399,  0.0397, -0.0643,  0.0027, -0.0285,\n                 0.0688, -0.0301, -0.0579,  0.1646, -0.0346, -0.0906, -0.1185,  0.0280,\n                -0.0237, -0.0930,  0.0830, -0.0362, -0.0171,  0.0312,  0.0983, -0.0762,\n                -0.0327, -0.0229, -0.1375,  0.0066, -0.0238,  0.0785, -0.0476,  0.0319,\n                 0.1094, -0.0280,  0.0463, -0.0194,  0.0116,  0.0187, -0.0436, -0.0006,\n                -0.0006,  0.0172, -0.0892,  0.0502,  0.0435,  0.0596, -0.0964,  0.0222,\n                 0.0198,  0.0017, -0.1038, -0.0208,  0.1302,  0.0078, -0.0285,  0.0141,\n                 0.0963, -0.0199,  0.0920, -0.0477, -0.0350, -0.0531,  0.0012, -0.1161,\n                 0.0555,  0.1157, -0.0316,  0.0133, -0.0402,  0.0274, -0.0481, -0.0606,\n                 0.1076, -0.0122, -0.0247,  0.0247, -0.0268, -0.0866, -0.0641,  0.0538,\n                 0.0156, -0.0128, -0.0253, -0.0625, -0.0292, -0.0576,  0.0318, -0.0076,\n                -0.0560, -0.0121, -0.0848,  0.0333,  0.0315,  0.0698,  0.0676, -0.0941],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9226, 0.9625, 0.9209, 0.9308, 0.9247, 0.9349, 0.9426, 0.9438, 0.9353,\n                0.9229, 0.9401, 0.9428, 0.9274, 0.9329, 0.9240, 0.9389, 0.9206, 0.9389,\n                0.9308, 0.9244, 0.9373, 0.9230, 0.9395, 0.9270, 0.9713, 0.9505, 0.9329,\n                0.9380, 0.9299, 0.9408, 0.9445, 0.9350, 0.9417, 0.9441, 0.9280, 0.9452,\n                0.9529, 0.9189, 0.9541, 0.9349, 0.9451, 0.9257, 0.9402, 0.9758, 0.9237,\n                0.9586, 0.9775, 0.9278, 0.9299, 0.9370, 0.9294, 0.9237, 0.9310, 0.9351,\n                0.9314, 0.9267, 0.9301, 0.9369, 0.9530, 0.9318, 0.9637, 0.9338, 0.9266,\n                0.9268, 0.9552, 0.9268, 0.9273, 0.9359, 0.9282, 0.9363, 0.9353, 0.9175,\n                0.9302, 0.9272, 0.9384, 0.9301, 0.9340, 0.9306, 0.9292, 0.9284, 0.9227,\n                0.9347, 0.9422, 0.9231, 0.9389, 0.9288, 0.9219, 0.9428, 0.9367, 0.9404,\n                0.9458, 0.9385, 0.9278, 0.9390, 0.9366, 0.9375, 0.9332, 0.9423, 0.9176,\n                0.9361, 0.9308, 0.9374, 0.9389, 0.9361, 0.9348, 0.9230, 0.9316, 0.9437,\n                0.9231, 0.9614, 0.9581, 0.9452, 0.9297, 0.9366, 0.9336, 0.9253, 0.9419,\n                0.9281, 0.9673, 0.9423, 0.9442, 0.9273, 0.9345, 0.9253, 0.9322, 0.9321,\n                0.9494, 0.9784], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [128, 64, 3, 3]], ["0.bn1.weight", [128]], ["0.bn1.bias", [128]], ["0.conv2.weight", [128, 128, 3, 3]], ["0.bn2.weight", [128]], ["0.bn2.bias", [128]], ["0.downsample.0.weight", [128, 64, 1, 1]], ["0.downsample.1.weight", [128]], ["0.downsample.1.bias", [128]]], "output_shape": [[512, 128, 3, 3]], "num_parameters": [73728, 128, 128, 147456, 128, 128, 8192, 128, 128]}, {"name": "layer3", "id": 140621165299744, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 1.2607e-02, -1.8644e-02, -1.0945e-02],\n                [-2.9052e-02, -1.0863e-02,  1.2096e-02],\n                [ 7.8641e-04, -1.4029e-02,  2.1219e-02]],\n      \n               [[-2.5879e-02,  2.2146e-02, -3.2496e-04],\n                [-2.6172e-02, -9.4504e-03,  2.0643e-03],\n                [ 1.8273e-02,  1.0583e-02,  2.8417e-02]],\n      \n               [[ 1.2125e-02,  1.2220e-03, -2.6736e-02],\n                [ 2.7832e-02,  1.0906e-02, -2.5649e-02],\n                [ 2.3791e-02, -1.0401e-02,  2.1927e-02]],\n      \n               ...,\n      \n               [[-2.0846e-02,  2.1508e-02,  1.9979e-02],\n                [ 4.9416e-03,  1.0119e-02, -2.0115e-02],\n                [-2.8787e-02, -5.7274e-03,  1.6998e-02]],\n      \n               [[-8.0862e-03,  4.1295e-03, -6.2474e-03],\n                [ 2.1733e-02,  1.9647e-02,  2.7605e-02],\n                [ 2.5899e-02, -2.1342e-03,  4.4716e-03]],\n      \n               [[ 7.2861e-03,  1.0586e-02,  2.5355e-02],\n                [ 2.7493e-02,  2.5983e-02, -1.5847e-02],\n                [-1.9768e-03,  2.5701e-02,  5.5666e-03]]],\n      \n      \n              [[[ 2.0584e-02, -6.1840e-03, -3.8012e-03],\n                [ 1.8544e-02,  1.8381e-02,  6.3601e-03],\n                [-1.0217e-02, -2.7423e-02, -2.0480e-02]],\n      \n               [[-1.4470e-02,  2.7200e-02, -5.0190e-04],\n                [-2.3614e-02,  5.6807e-03, -2.2940e-02],\n                [-2.0709e-02, -2.8079e-02,  1.9179e-02]],\n      \n               [[ 2.2262e-02, -2.5417e-02,  1.9820e-02],\n                [-2.9242e-03,  2.5415e-02,  9.0615e-03],\n                [ 1.7170e-02,  2.8831e-02, -8.9606e-03]],\n      \n               ...,\n      \n               [[-8.7717e-03, -1.6543e-02,  3.2894e-03],\n                [-8.3970e-03,  2.4391e-03,  9.3349e-03],\n                [ 1.7427e-02,  7.8847e-04, -6.1784e-03]],\n      \n               [[-5.7452e-03, -9.1671e-03,  1.5507e-02],\n                [-2.3676e-02,  3.3993e-03, -9.6840e-03],\n                [ 6.5606e-03, -3.0824e-03, -2.1530e-02]],\n      \n               [[-7.8359e-04, -2.4329e-02,  1.8488e-02],\n                [-1.0887e-02, -2.5926e-02,  1.7571e-02],\n                [-2.0527e-02, -6.6844e-03,  4.4879e-03]]],\n      \n      \n              [[[-1.6249e-02, -1.0382e-02, -5.5813e-03],\n                [ 1.2168e-02,  1.1280e-02,  1.5798e-02],\n                [ 2.6198e-02, -2.1825e-02,  1.6023e-02]],\n      \n               [[ 9.4257e-03,  5.8331e-03,  1.1835e-02],\n                [ 7.5831e-04, -2.6984e-02, -2.1581e-02],\n                [ 5.6812e-03, -1.2893e-02,  1.9279e-02]],\n      \n               [[-1.2168e-02,  2.0909e-02, -9.6443e-03],\n                [-1.9771e-03, -1.7254e-02,  2.3902e-02],\n                [-2.2734e-02,  4.9810e-03, -1.2169e-02]],\n      \n               ...,\n      \n               [[-1.6090e-02,  7.9911e-04,  4.8131e-04],\n                [ 1.2839e-02, -2.2212e-02,  1.4570e-02],\n                [ 1.1216e-02,  2.6747e-03,  5.9958e-03]],\n      \n               [[-2.7378e-02, -1.6785e-02,  2.9423e-03],\n                [-1.6174e-02,  9.4687e-03,  2.0954e-02],\n                [-1.8321e-02, -2.7761e-02,  1.0014e-02]],\n      \n               [[-1.2644e-02, -2.6732e-02,  1.9426e-02],\n                [ 1.5133e-02,  1.7980e-02, -2.1467e-02],\n                [ 8.3646e-03, -2.0032e-02, -2.7538e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[-2.2241e-02,  2.2574e-02,  1.8864e-02],\n                [-2.6740e-02,  1.3234e-02,  1.0070e-05],\n                [-8.2973e-03,  2.8431e-02,  1.4889e-02]],\n      \n               [[ 6.4433e-03,  1.9217e-02, -4.4675e-03],\n                [ 2.4176e-02, -1.3717e-02,  2.1534e-02],\n                [-1.6045e-02,  1.1421e-02,  5.5129e-03]],\n      \n               [[-2.5026e-02, -1.7501e-02, -2.6048e-02],\n                [-2.3051e-02,  1.3015e-02,  1.4826e-02],\n                [-2.5016e-02,  3.1624e-03, -2.5895e-02]],\n      \n               ...,\n      \n               [[-9.2795e-03, -9.8422e-03, -2.0647e-02],\n                [ 9.1534e-03, -1.7706e-02,  1.8598e-02],\n                [-2.6626e-02, -2.0853e-02,  5.3208e-03]],\n      \n               [[ 1.5242e-02,  2.2383e-02,  2.2502e-02],\n                [ 1.4129e-02,  2.1380e-02, -1.8631e-02],\n                [-9.1650e-03, -2.1172e-02,  1.4604e-03]],\n      \n               [[-1.9937e-02, -2.2753e-02,  2.8333e-02],\n                [ 1.6173e-02, -2.7004e-02, -1.3878e-02],\n                [-9.2876e-03,  1.0669e-02, -6.2555e-03]]],\n      \n      \n              [[[-2.9209e-02,  1.2321e-02,  5.6916e-03],\n                [ 2.3138e-02, -1.8220e-02, -1.4713e-02],\n                [ 1.5361e-02,  8.6322e-03,  1.4144e-02]],\n      \n               [[ 8.7882e-03, -7.3173e-03, -2.7013e-03],\n                [-4.2142e-03, -2.7564e-02, -3.3680e-03],\n                [ 2.6717e-02,  1.0694e-02, -2.1865e-02]],\n      \n               [[ 2.9218e-03, -7.0162e-03, -2.0960e-02],\n                [ 3.0083e-03,  1.8926e-02,  4.3202e-03],\n                [-2.9262e-02,  2.1104e-02, -2.6339e-02]],\n      \n               ...,\n      \n               [[-2.0508e-02, -1.7857e-02,  1.5628e-02],\n                [-2.8047e-02,  1.1513e-04,  9.0720e-03],\n                [ 2.0401e-02, -1.1110e-02, -1.7453e-03]],\n      \n               [[-6.4134e-03,  1.2358e-02, -1.3671e-02],\n                [-1.5825e-02,  7.2655e-03, -8.7292e-03],\n                [-1.1454e-02,  2.0914e-03,  1.1741e-02]],\n      \n               [[ 1.7388e-02, -1.8514e-02,  1.0699e-03],\n                [-3.7022e-03,  2.0747e-02,  1.0568e-02],\n                [-1.4019e-02,  2.6667e-03, -4.1847e-03]]],\n      \n      \n              [[[-1.3746e-02,  1.7097e-02, -7.4502e-03],\n                [-2.0482e-02,  1.1518e-02, -6.4425e-03],\n                [ 2.5994e-02,  2.7020e-02,  1.8322e-02]],\n      \n               [[ 6.9228e-03,  2.2080e-03, -2.5979e-02],\n                [ 2.6378e-02,  1.1189e-02,  1.7433e-02],\n                [-1.0802e-02,  2.1958e-02, -1.3177e-02]],\n      \n               [[-1.7616e-02,  2.2486e-02, -1.7406e-02],\n                [ 8.8794e-03,  2.4659e-02,  1.7828e-02],\n                [ 2.9419e-02,  9.6527e-03,  5.2511e-03]],\n      \n               ...,\n      \n               [[ 1.7111e-02, -2.8425e-02, -1.3967e-02],\n                [ 2.2737e-02, -2.0942e-02,  2.7213e-02],\n                [-1.4932e-02,  6.3444e-03,  9.1618e-03]],\n      \n               [[ 1.1004e-02,  6.7197e-03,  1.9106e-02],\n                [ 2.2739e-02, -2.5625e-03,  2.3170e-02],\n                [ 7.1203e-03,  2.8921e-02,  1.0510e-02]],\n      \n               [[-2.0692e-02,  2.2972e-02, -4.3016e-03],\n                [-4.2068e-03, -2.8002e-02,  2.1471e-02],\n                [-8.7120e-03,  9.1780e-03, -1.3785e-03]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 1.5475e-02,  1.7709e-02, -3.4637e-03, -2.8450e-02,  8.6361e-03,\n               3.4423e-02, -6.7780e-03, -4.0363e-02, -2.1908e-02, -1.8534e-02,\n               1.0036e-02, -9.8019e-03, -2.1077e-03, -7.8963e-03,  1.0838e-03,\n               1.1385e-02,  1.3767e-02, -7.2266e-03, -7.1156e-03, -6.2126e-03,\n              -3.0010e-02,  2.4633e-02,  3.7986e-04,  1.4644e-02, -3.9917e-03,\n               7.9743e-03, -2.4808e-03, -1.8524e-02, -2.6564e-02,  2.0198e-02,\n              -1.5071e-02,  7.4757e-03, -4.5369e-02, -2.5169e-02, -6.3403e-03,\n              -3.6859e-02,  4.5846e-03,  9.1130e-04,  1.2127e-02, -7.6592e-03,\n               1.7685e-02, -8.8055e-03, -2.1584e-02, -6.4166e-03, -1.6879e-02,\n               1.2267e-02, -1.1517e-02, -1.5295e-02, -7.6770e-03,  2.3984e-02,\n               2.0426e-02, -1.5099e-02,  3.0567e-02,  6.2610e-03, -2.1670e-03,\n               3.0503e-02,  1.8926e-02,  3.3630e-03, -4.6925e-03,  1.5585e-02,\n               6.4156e-03,  1.4911e-02,  8.3518e-04,  1.0194e-03,  1.6010e-03,\n              -1.6135e-02, -5.4017e-04,  1.1250e-02,  1.4281e-03,  1.8979e-02,\n               8.1618e-03,  3.0344e-03, -2.8122e-02, -2.0415e-03, -3.0910e-02,\n               3.1053e-02,  3.8856e-02, -2.3108e-02,  2.4181e-02, -1.1267e-02,\n               1.4654e-02, -9.8978e-03, -3.9897e-03, -1.9020e-02, -2.5483e-02,\n              -2.6132e-02, -7.2714e-03, -5.9021e-03, -2.8560e-02, -2.7123e-02,\n               8.8679e-03, -1.7625e-02, -7.1573e-03,  5.6477e-03,  2.2351e-03,\n               6.2980e-03,  1.3067e-02,  1.0929e-02,  5.1229e-03, -9.1944e-03,\n              -1.3662e-02,  1.1309e-02, -2.2030e-02, -5.1429e-03, -4.9491e-03,\n               1.7255e-02,  1.3236e-02,  1.3334e-02, -1.2727e-02,  9.7571e-04,\n              -8.4404e-03,  7.1426e-03,  1.4045e-02, -1.3338e-02,  2.5554e-03,\n              -3.5328e-03,  6.9354e-03,  1.5000e-02,  1.5513e-02,  7.0542e-03,\n               4.6345e-03,  8.6097e-03, -4.2815e-03,  2.4900e-02, -1.3423e-02,\n              -7.8280e-04,  1.8550e-03,  8.1453e-03,  8.0930e-03, -8.6225e-03,\n              -7.7365e-03,  2.5983e-02,  3.2596e-04, -8.9864e-03,  1.7766e-02,\n               3.4788e-02,  1.9215e-02,  2.8590e-02, -3.0598e-03, -8.0358e-03,\n               9.4847e-03,  3.5225e-03, -1.3780e-02, -1.8476e-02,  6.9915e-03,\n              -2.8140e-02,  5.5670e-03,  5.5086e-03, -8.1870e-03,  8.9446e-04,\n              -1.0283e-02,  1.3717e-02,  2.0865e-02,  9.0633e-03,  9.3020e-03,\n              -5.8971e-03, -1.7025e-02,  5.7691e-03,  2.1538e-02,  1.1306e-02,\n               1.1266e-02, -9.2368e-03,  2.8398e-02,  2.7341e-02, -3.0763e-02,\n              -2.6465e-03, -2.6652e-02,  2.4762e-03, -7.9237e-03,  4.3061e-02,\n              -8.8347e-03, -1.3236e-02,  7.1481e-04,  1.3498e-02,  1.3319e-02,\n               2.2332e-03,  3.5163e-02, -7.4633e-03, -2.1781e-02, -1.9390e-02,\n              -2.0185e-02, -4.1707e-03, -2.1022e-02, -4.9306e-03,  2.1201e-02,\n               1.4221e-02, -2.2877e-03,  3.0966e-03,  2.5848e-02, -2.5112e-02,\n              -7.5335e-03, -8.3429e-04, -8.5231e-03,  4.6095e-03,  1.8071e-02,\n               1.9347e-02, -2.5936e-02,  1.5498e-02, -3.8394e-03, -1.1434e-02,\n              -1.1717e-02, -4.4916e-03,  3.2822e-02,  4.7982e-03,  2.7821e-03,\n               1.2284e-02,  8.8234e-05,  1.8286e-02,  1.2848e-02, -1.9684e-02,\n               7.9375e-03, -2.1957e-02, -1.2459e-03, -7.1759e-03,  1.8044e-02,\n               1.6152e-02,  2.2163e-02,  7.2941e-03, -2.3840e-02,  3.2984e-02,\n               1.2495e-02,  2.5235e-02,  1.1839e-02, -1.6437e-03, -9.6604e-03,\n              -2.1901e-03,  4.0536e-02,  8.4949e-05,  1.3986e-02, -2.5018e-02,\n               8.6087e-03,  3.0531e-02, -2.5024e-02,  2.9996e-02, -1.0591e-02,\n              -5.1889e-02,  2.1482e-02,  2.7185e-03, -3.7415e-03, -1.1988e-02,\n              -2.2543e-02,  1.2124e-02, -1.9822e-02,  1.1359e-02,  5.6893e-03,\n              -4.5891e-02, -1.4991e-02,  2.9405e-02, -1.0178e-02,  2.1519e-02,\n               1.3284e-03,  7.2742e-03, -1.1264e-02, -6.9254e-03, -7.1039e-03,\n              -1.1942e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9148, 0.9113, 0.9142, 0.9150, 0.9131, 0.9117, 0.9110, 0.9159, 0.9127,\n              0.9115, 0.9133, 0.9155, 0.9180, 0.9144, 0.9163, 0.9098, 0.9132, 0.9183,\n              0.9150, 0.9120, 0.9120, 0.9088, 0.9129, 0.9168, 0.9125, 0.9180, 0.9139,\n              0.9134, 0.9131, 0.9110, 0.9124, 0.9133, 0.9148, 0.9196, 0.9159, 0.9139,\n              0.9144, 0.9087, 0.9164, 0.9136, 0.9142, 0.9129, 0.9164, 0.9172, 0.9147,\n              0.9184, 0.9135, 0.9236, 0.9098, 0.9153, 0.9222, 0.9134, 0.9139, 0.9180,\n              0.9107, 0.9100, 0.9152, 0.9105, 0.9123, 0.9152, 0.9101, 0.9124, 0.9146,\n              0.9170, 0.9112, 0.9104, 0.9125, 0.9139, 0.9155, 0.9105, 0.9154, 0.9122,\n              0.9096, 0.9119, 0.9172, 0.9154, 0.9201, 0.9184, 0.9099, 0.9112, 0.9161,\n              0.9119, 0.9183, 0.9156, 0.9116, 0.9156, 0.9144, 0.9204, 0.9136, 0.9152,\n              0.9147, 0.9125, 0.9108, 0.9122, 0.9180, 0.9090, 0.9152, 0.9100, 0.9125,\n              0.9174, 0.9095, 0.9120, 0.9099, 0.9112, 0.9193, 0.9145, 0.9114, 0.9117,\n              0.9195, 0.9116, 0.9173, 0.9124, 0.9192, 0.9129, 0.9129, 0.9097, 0.9106,\n              0.9142, 0.9181, 0.9125, 0.9126, 0.9081, 0.9184, 0.9122, 0.9157, 0.9113,\n              0.9131, 0.9211, 0.9098, 0.9105, 0.9113, 0.9168, 0.9140, 0.9149, 0.9112,\n              0.9165, 0.9130, 0.9128, 0.9211, 0.9171, 0.9096, 0.9109, 0.9127, 0.9120,\n              0.9118, 0.9165, 0.9117, 0.9121, 0.9108, 0.9148, 0.9151, 0.9112, 0.9104,\n              0.9159, 0.9152, 0.9153, 0.9124, 0.9142, 0.9158, 0.9105, 0.9121, 0.9153,\n              0.9210, 0.9115, 0.9161, 0.9150, 0.9189, 0.9086, 0.9132, 0.9119, 0.9164,\n              0.9122, 0.9099, 0.9121, 0.9184, 0.9162, 0.9210, 0.9123, 0.9166, 0.9169,\n              0.9122, 0.9106, 0.9102, 0.9150, 0.9112, 0.9126, 0.9143, 0.9153, 0.9091,\n              0.9160, 0.9267, 0.9120, 0.9107, 0.9261, 0.9210, 0.9134, 0.9194, 0.9156,\n              0.9139, 0.9189, 0.9131, 0.9090, 0.9150, 0.9118, 0.9162, 0.9105, 0.9134,\n              0.9135, 0.9098, 0.9139, 0.9143, 0.9114, 0.9124, 0.9097, 0.9194, 0.9136,\n              0.9107, 0.9265, 0.9200, 0.9172, 0.9156, 0.9132, 0.9181, 0.9106, 0.9137,\n              0.9122, 0.9233, 0.9091, 0.9147, 0.9189, 0.9127, 0.9157, 0.9106, 0.9131,\n              0.9095, 0.9153, 0.9115, 0.9147, 0.9163, 0.9130, 0.9106, 0.9122, 0.9140,\n              0.9242, 0.9152, 0.9093, 0.9152, 0.9177, 0.9111, 0.9127, 0.9095, 0.9120,\n              0.9101, 0.9157, 0.9117, 0.9204], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-0.0180, -0.0016,  0.0194],\n                [-0.0081, -0.0130, -0.0045],\n                [ 0.0055, -0.0177, -0.0023]],\n      \n               [[-0.0142,  0.0086,  0.0070],\n                [ 0.0055, -0.0093,  0.0036],\n                [-0.0080,  0.0030,  0.0107]],\n      \n               [[-0.0036,  0.0101,  0.0204],\n                [-0.0040, -0.0066,  0.0125],\n                [ 0.0200,  0.0023, -0.0179]],\n      \n               ...,\n      \n               [[ 0.0183,  0.0081,  0.0024],\n                [-0.0079,  0.0016, -0.0095],\n                [ 0.0078, -0.0189, -0.0117]],\n      \n               [[ 0.0011, -0.0199,  0.0187],\n                [ 0.0012, -0.0151, -0.0134],\n                [-0.0074,  0.0048,  0.0026]],\n      \n               [[-0.0123, -0.0087, -0.0074],\n                [ 0.0200, -0.0167, -0.0006],\n                [-0.0141, -0.0160,  0.0035]]],\n      \n      \n              [[[-0.0180, -0.0072,  0.0132],\n                [ 0.0090, -0.0060,  0.0193],\n                [ 0.0175, -0.0101,  0.0117]],\n      \n               [[-0.0049,  0.0178,  0.0115],\n                [-0.0168, -0.0183,  0.0151],\n                [-0.0189,  0.0131, -0.0048]],\n      \n               [[-0.0009, -0.0008,  0.0202],\n                [-0.0159,  0.0061,  0.0177],\n                [ 0.0150, -0.0199,  0.0008]],\n      \n               ...,\n      \n               [[-0.0166, -0.0206, -0.0070],\n                [-0.0004, -0.0006,  0.0174],\n                [-0.0183,  0.0119,  0.0106]],\n      \n               [[ 0.0141, -0.0113, -0.0118],\n                [-0.0029, -0.0022, -0.0075],\n                [ 0.0153, -0.0082,  0.0082]],\n      \n               [[-0.0173, -0.0117,  0.0056],\n                [-0.0011,  0.0160,  0.0076],\n                [-0.0065, -0.0194, -0.0203]]],\n      \n      \n              [[[ 0.0139,  0.0119,  0.0074],\n                [-0.0045,  0.0098,  0.0040],\n                [-0.0110, -0.0099, -0.0082]],\n      \n               [[ 0.0077,  0.0167, -0.0120],\n                [ 0.0066, -0.0172,  0.0156],\n                [-0.0180,  0.0132,  0.0036]],\n      \n               [[-0.0070, -0.0160,  0.0196],\n                [ 0.0128,  0.0014,  0.0167],\n                [ 0.0143,  0.0198,  0.0074]],\n      \n               ...,\n      \n               [[ 0.0013, -0.0087, -0.0179],\n                [-0.0156,  0.0193, -0.0113],\n                [ 0.0159,  0.0030,  0.0143]],\n      \n               [[ 0.0138,  0.0011, -0.0156],\n                [ 0.0011, -0.0182, -0.0002],\n                [-0.0136, -0.0082, -0.0125]],\n      \n               [[-0.0203,  0.0086, -0.0060],\n                [ 0.0177,  0.0093,  0.0044],\n                [ 0.0056, -0.0018,  0.0154]]],\n      \n      \n              ...,\n      \n      \n              [[[-0.0196, -0.0087, -0.0047],\n                [ 0.0150, -0.0083, -0.0174],\n                [-0.0107, -0.0114,  0.0047]],\n      \n               [[ 0.0108,  0.0154, -0.0105],\n                [ 0.0119, -0.0134,  0.0166],\n                [-0.0104, -0.0019,  0.0081]],\n      \n               [[-0.0079,  0.0013, -0.0134],\n                [ 0.0159,  0.0022, -0.0103],\n                [-0.0023,  0.0125,  0.0132]],\n      \n               ...,\n      \n               [[ 0.0093,  0.0018, -0.0015],\n                [ 0.0195, -0.0152,  0.0187],\n                [ 0.0038, -0.0195,  0.0072]],\n      \n               [[-0.0135, -0.0099, -0.0062],\n                [-0.0195,  0.0084,  0.0098],\n                [-0.0093,  0.0007,  0.0003]],\n      \n               [[-0.0008,  0.0067, -0.0037],\n                [ 0.0176, -0.0172,  0.0142],\n                [ 0.0081, -0.0206,  0.0010]]],\n      \n      \n              [[[ 0.0183,  0.0180, -0.0069],\n                [ 0.0011,  0.0003, -0.0006],\n                [ 0.0187,  0.0098, -0.0098]],\n      \n               [[-0.0037, -0.0205, -0.0034],\n                [-0.0052, -0.0042, -0.0155],\n                [-0.0040, -0.0037,  0.0196]],\n      \n               [[-0.0002,  0.0193, -0.0193],\n                [-0.0183, -0.0016,  0.0158],\n                [-0.0135,  0.0044, -0.0153]],\n      \n               ...,\n      \n               [[-0.0079, -0.0057, -0.0045],\n                [ 0.0021,  0.0159,  0.0121],\n                [-0.0063,  0.0116,  0.0119]],\n      \n               [[ 0.0159, -0.0104, -0.0161],\n                [ 0.0126, -0.0105, -0.0058],\n                [ 0.0026, -0.0055,  0.0141]],\n      \n               [[ 0.0178,  0.0047, -0.0074],\n                [-0.0189,  0.0193, -0.0147],\n                [ 0.0183, -0.0130, -0.0126]]],\n      \n      \n              [[[-0.0030, -0.0137,  0.0117],\n                [-0.0063, -0.0151, -0.0037],\n                [ 0.0102, -0.0040,  0.0047]],\n      \n               [[-0.0023, -0.0015, -0.0053],\n                [ 0.0154, -0.0194,  0.0069],\n                [ 0.0080,  0.0035, -0.0171]],\n      \n               [[-0.0011,  0.0206,  0.0095],\n                [ 0.0199,  0.0123,  0.0103],\n                [ 0.0089,  0.0168, -0.0083]],\n      \n               ...,\n      \n               [[ 0.0024,  0.0109, -0.0002],\n                [ 0.0151, -0.0196,  0.0087],\n                [-0.0024,  0.0129,  0.0161]],\n      \n               [[-0.0123, -0.0145, -0.0022],\n                [-0.0061,  0.0163, -0.0008],\n                [-0.0096,  0.0163,  0.0181]],\n      \n               [[ 0.0170, -0.0178,  0.0162],\n                [ 0.0039,  0.0121,  0.0094],\n                [-0.0104, -0.0192,  0.0072]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 1.6196e-02,  1.1086e-02,  2.6403e-02, -2.5705e-02, -5.8884e-03,\n               1.2968e-02, -1.8881e-03, -1.6204e-03, -2.0838e-02, -1.6715e-02,\n               6.8080e-03, -1.1751e-02, -7.4895e-03,  6.3905e-04,  1.7389e-02,\n              -1.1391e-02,  6.2901e-03,  3.4264e-03, -9.4076e-03, -3.6902e-03,\n              -1.0455e-02,  2.1406e-02,  7.1382e-04, -1.3999e-02,  3.9085e-03,\n              -3.5775e-02, -2.5849e-03, -1.5712e-02, -1.9795e-02,  1.5775e-02,\n               3.0815e-03,  1.4806e-02, -6.4106e-03,  1.5000e-02,  3.5812e-03,\n              -3.6259e-02,  1.0898e-02,  6.9226e-03, -9.9213e-03, -9.0309e-03,\n               4.7887e-03, -1.5428e-02,  2.3283e-03, -9.0269e-03,  8.0665e-03,\n              -1.7523e-02,  2.3560e-03, -2.5092e-02, -1.6628e-02,  9.1795e-03,\n              -6.9217e-04,  3.3303e-02,  5.6566e-03, -8.5200e-03,  1.1991e-02,\n              -1.0151e-02, -2.0826e-02, -1.2948e-02, -1.4430e-02,  7.0635e-05,\n              -1.5328e-02, -1.4231e-02, -1.2732e-02,  5.2534e-03, -8.2127e-03,\n               1.2359e-02, -1.3960e-03,  2.3204e-02, -1.2122e-02,  1.7929e-02,\n               2.6094e-03,  1.1887e-02,  1.3313e-02,  4.9334e-03,  2.0697e-02,\n               1.9706e-03, -9.8815e-03, -6.1002e-04, -6.1277e-03, -1.9945e-02,\n              -1.0503e-02,  3.3314e-03,  4.6967e-03,  2.9334e-02, -2.0140e-03,\n               2.4791e-04,  1.0423e-03, -1.6865e-02,  4.9728e-03,  7.9173e-03,\n              -2.1366e-02, -6.6515e-03, -2.1308e-03, -2.6327e-02,  2.8602e-02,\n              -6.1695e-03, -2.0480e-04, -1.4469e-02,  1.4029e-02,  1.2105e-02,\n              -8.6847e-03,  2.7638e-02, -5.3744e-03,  5.1777e-03, -1.4402e-02,\n              -3.9889e-03,  2.7260e-02,  8.5036e-03, -1.3349e-02, -1.4127e-02,\n              -1.3262e-02, -1.3136e-02,  5.6038e-03,  4.8963e-03,  1.1873e-02,\n               1.3036e-02, -3.9966e-03, -1.3376e-03, -8.7585e-04, -1.0048e-02,\n              -1.1497e-02, -7.5953e-05, -1.4949e-02, -2.0794e-02, -9.7128e-03,\n               1.9982e-02,  1.5602e-03, -1.3097e-02,  6.2976e-03, -6.1560e-04,\n               1.0244e-02,  1.1801e-02, -9.9171e-03,  1.9731e-02, -1.0417e-04,\n              -7.1090e-03,  4.6750e-04,  4.1369e-03, -1.1224e-02,  8.8801e-03,\n               1.6951e-02, -1.4715e-02, -6.5162e-03,  2.4040e-02, -2.8829e-02,\n              -8.3132e-03,  1.0123e-02,  1.4592e-02, -1.7853e-02, -3.1033e-03,\n               9.6279e-03,  1.4680e-02, -7.8540e-03,  3.7193e-03,  2.5834e-02,\n               2.0349e-02, -9.8067e-03,  2.1522e-03,  2.8918e-03, -1.3645e-03,\n               4.8448e-03, -5.4102e-03,  8.2841e-03,  4.4071e-03, -3.5458e-02,\n              -1.1570e-02,  1.7225e-02,  1.1732e-02,  8.9363e-03,  2.3759e-02,\n               4.1907e-03, -1.9217e-02,  2.4897e-02,  9.4802e-03,  1.6136e-02,\n               4.4693e-03,  2.6348e-02, -4.1051e-03,  1.7031e-02,  3.2268e-03,\n              -1.1403e-03,  1.7531e-03, -1.3367e-03,  8.2190e-03, -3.2361e-03,\n              -1.7001e-02,  1.7040e-02,  1.1621e-03,  7.6347e-03, -2.6151e-02,\n               7.5377e-03,  2.0273e-02, -1.2665e-03, -2.3479e-02,  2.2204e-02,\n               5.9105e-03, -1.3272e-02,  1.4636e-03, -2.4850e-03, -2.3954e-03,\n               1.3651e-02,  1.9294e-02, -8.3064e-03, -1.1814e-02,  1.3342e-02,\n              -9.0264e-04,  9.7907e-03, -4.9872e-03,  1.8184e-02, -6.6003e-03,\n               1.1233e-02,  2.8594e-03,  3.8482e-03, -1.1414e-03,  4.2088e-02,\n               1.0459e-02, -1.5694e-02,  3.3282e-03, -1.3550e-02, -7.5757e-03,\n               2.9717e-03,  9.7027e-03,  1.6772e-02,  3.2025e-04,  4.3978e-03,\n               2.5675e-04, -2.9801e-03, -1.2527e-02,  1.2830e-02, -1.2054e-03,\n              -3.5715e-03,  1.1497e-02,  4.8357e-03, -2.7792e-02, -2.2290e-02,\n              -8.4420e-03, -3.8929e-04, -2.7574e-02, -4.9092e-04, -1.0398e-02,\n               1.2524e-02,  2.7058e-03, -5.8231e-03,  1.5310e-02,  1.1248e-02,\n              -1.5813e-02,  4.9180e-03, -3.5991e-03,  2.7528e-03, -1.2060e-02,\n               2.8027e-02, -1.5684e-02,  5.3421e-03, -1.3769e-02,  7.0455e-03,\n               4.3626e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9058, 0.9049, 0.9064, 0.9053, 0.9066, 0.9048, 0.9056, 0.9073, 0.9090,\n              0.9044, 0.9062, 0.9064, 0.9093, 0.9052, 0.9046, 0.9051, 0.9087, 0.9063,\n              0.9054, 0.9063, 0.9056, 0.9051, 0.9056, 0.9062, 0.9075, 0.9049, 0.9059,\n              0.9052, 0.9045, 0.9051, 0.9060, 0.9047, 0.9042, 0.9050, 0.9065, 0.9110,\n              0.9051, 0.9056, 0.9046, 0.9045, 0.9047, 0.9048, 0.9062, 0.9045, 0.9045,\n              0.9084, 0.9089, 0.9057, 0.9046, 0.9105, 0.9057, 0.9046, 0.9042, 0.9064,\n              0.9073, 0.9043, 0.9050, 0.9048, 0.9043, 0.9046, 0.9053, 0.9061, 0.9079,\n              0.9040, 0.9040, 0.9091, 0.9071, 0.9057, 0.9060, 0.9040, 0.9042, 0.9101,\n              0.9054, 0.9044, 0.9057, 0.9076, 0.9076, 0.9071, 0.9055, 0.9062, 0.9063,\n              0.9065, 0.9042, 0.9046, 0.9053, 0.9062, 0.9054, 0.9069, 0.9044, 0.9049,\n              0.9069, 0.9058, 0.9051, 0.9088, 0.9060, 0.9053, 0.9040, 0.9046, 0.9057,\n              0.9043, 0.9056, 0.9059, 0.9056, 0.9047, 0.9054, 0.9073, 0.9048, 0.9054,\n              0.9075, 0.9042, 0.9066, 0.9092, 0.9058, 0.9064, 0.9061, 0.9045, 0.9064,\n              0.9073, 0.9051, 0.9067, 0.9042, 0.9050, 0.9050, 0.9044, 0.9048, 0.9041,\n              0.9055, 0.9047, 0.9047, 0.9055, 0.9095, 0.9062, 0.9049, 0.9041, 0.9047,\n              0.9076, 0.9065, 0.9061, 0.9053, 0.9062, 0.9059, 0.9067, 0.9043, 0.9105,\n              0.9042, 0.9037, 0.9045, 0.9046, 0.9075, 0.9077, 0.9064, 0.9062, 0.9047,\n              0.9042, 0.9076, 0.9068, 0.9055, 0.9041, 0.9056, 0.9048, 0.9055, 0.9043,\n              0.9052, 0.9105, 0.9081, 0.9056, 0.9047, 0.9057, 0.9045, 0.9054, 0.9085,\n              0.9080, 0.9045, 0.9117, 0.9053, 0.9041, 0.9067, 0.9059, 0.9044, 0.9053,\n              0.9042, 0.9062, 0.9049, 0.9073, 0.9062, 0.9074, 0.9051, 0.9049, 0.9036,\n              0.9056, 0.9076, 0.9066, 0.9053, 0.9053, 0.9048, 0.9046, 0.9057, 0.9047,\n              0.9071, 0.9040, 0.9041, 0.9054, 0.9081, 0.9057, 0.9059, 0.9036, 0.9048,\n              0.9046, 0.9054, 0.9050, 0.9055, 0.9061, 0.9068, 0.9047, 0.9046, 0.9049,\n              0.9054, 0.9048, 0.9063, 0.9050, 0.9053, 0.9044, 0.9043, 0.9059, 0.9059,\n              0.9062, 0.9050, 0.9057, 0.9056, 0.9048, 0.9067, 0.9050, 0.9093, 0.9107,\n              0.9048, 0.9045, 0.9041, 0.9058, 0.9099, 0.9045, 0.9061, 0.9060, 0.9040,\n              0.9050, 0.9058, 0.9055, 0.9053, 0.9047, 0.9058, 0.9040, 0.9051, 0.9050,\n              0.9066, 0.9053, 0.9057, 0.9084], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[ 7.7544e-02]],\n        \n                 [[ 6.2740e-02]],\n        \n                 [[-5.6846e-02]],\n        \n                 ...,\n        \n                 [[ 2.9812e-02]],\n        \n                 [[ 5.6238e-02]],\n        \n                 [[-5.0683e-02]]],\n        \n        \n                [[[ 8.0795e-03]],\n        \n                 [[-7.7622e-02]],\n        \n                 [[-6.9294e-03]],\n        \n                 ...,\n        \n                 [[ 5.2309e-02]],\n        \n                 [[-5.1227e-02]],\n        \n                 [[ 6.6321e-02]]],\n        \n        \n                [[[-4.1051e-02]],\n        \n                 [[-2.5102e-02]],\n        \n                 [[-3.8548e-02]],\n        \n                 ...,\n        \n                 [[-1.6578e-02]],\n        \n                 [[-2.0742e-02]],\n        \n                 [[-3.0574e-02]]],\n        \n        \n                ...,\n        \n        \n                [[[-8.3777e-02]],\n        \n                 [[ 6.9050e-02]],\n        \n                 [[ 4.0801e-02]],\n        \n                 ...,\n        \n                 [[ 2.8828e-05]],\n        \n                 [[ 3.0945e-02]],\n        \n                 [[ 3.4133e-02]]],\n        \n        \n                [[[-2.5862e-03]],\n        \n                 [[ 8.3472e-02]],\n        \n                 [[ 2.6604e-02]],\n        \n                 ...,\n        \n                 [[ 3.9907e-02]],\n        \n                 [[-3.4751e-02]],\n        \n                 [[-8.6586e-02]]],\n        \n        \n                [[[-3.9085e-02]],\n        \n                 [[ 3.5738e-02]],\n        \n                 [[ 3.0562e-02]],\n        \n                 ...,\n        \n                 [[-1.6395e-02]],\n        \n                 [[ 2.3906e-02]],\n        \n                 [[ 1.7999e-02]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n               requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0604,  0.0122,  0.0413, -0.0062, -0.0201, -0.0391, -0.0202,  0.0222,\n                -0.0016,  0.0418, -0.0181,  0.0183, -0.0427,  0.0002,  0.1073,  0.0083,\n                 0.0038, -0.0042,  0.0104,  0.0210, -0.0398,  0.0201,  0.0165,  0.0353,\n                 0.0328, -0.0170,  0.0088,  0.0420,  0.0321, -0.0317, -0.0549,  0.0021,\n                -0.0017, -0.0102,  0.0081,  0.0225,  0.0979,  0.0116, -0.0351, -0.0642,\n                 0.0215,  0.0147, -0.0020, -0.0180, -0.0012, -0.0062,  0.0130, -0.0337,\n                -0.0602, -0.0354,  0.0192,  0.0302,  0.0221,  0.0220, -0.0065,  0.0401,\n                 0.0043, -0.0189, -0.0297,  0.0352,  0.0186, -0.0164,  0.0023, -0.0208,\n                 0.0075, -0.0548, -0.0107,  0.0429,  0.0321,  0.0207, -0.0157, -0.0267,\n                 0.0126, -0.0314,  0.0073,  0.0654, -0.0143, -0.0832,  0.0341,  0.0196,\n                -0.0074,  0.0315,  0.0340, -0.0100, -0.0303, -0.0416, -0.0307, -0.0585,\n                 0.0111, -0.0149,  0.0073, -0.0247,  0.0092, -0.0383,  0.0518,  0.0387,\n                -0.0063, -0.0127, -0.0140,  0.0103,  0.0386,  0.0265,  0.0490,  0.0275,\n                 0.0252, -0.0776,  0.0384, -0.0289, -0.0047, -0.0286, -0.0060,  0.0105,\n                 0.0100,  0.0210,  0.0497, -0.0421, -0.0219,  0.0068,  0.0122,  0.0031,\n                -0.0155, -0.0247, -0.0100,  0.0019,  0.0191, -0.0243,  0.0350, -0.0177,\n                -0.0126, -0.0564,  0.0045, -0.0160, -0.0009, -0.0181, -0.0290,  0.0155,\n                -0.0201,  0.0106,  0.0543, -0.0238,  0.0462, -0.0016,  0.0388, -0.0231,\n                -0.0464, -0.0012,  0.0202, -0.0005, -0.0174, -0.0159,  0.0023, -0.0059,\n                 0.0354, -0.0458, -0.0106, -0.0106, -0.0048,  0.0066,  0.0143, -0.0319,\n                 0.0044, -0.0468,  0.0248, -0.0500,  0.0123,  0.0106,  0.0042, -0.0498,\n                 0.0341,  0.0106, -0.0337,  0.0230, -0.0059,  0.0135,  0.0090, -0.0264,\n                 0.0178,  0.0116, -0.0568, -0.0197,  0.0116,  0.0466,  0.0247, -0.0495,\n                 0.0307, -0.0110,  0.0107, -0.0279, -0.0259,  0.0102,  0.0502, -0.0146,\n                 0.0921,  0.0181,  0.0267, -0.0155,  0.0598,  0.0040,  0.0132, -0.0385,\n                -0.0342,  0.0078,  0.0098, -0.0149,  0.0575,  0.0200, -0.0172,  0.0305,\n                -0.0284,  0.0258,  0.0145,  0.0331,  0.0036,  0.0210, -0.0160, -0.0174,\n                 0.0109,  0.0079,  0.0230,  0.0280, -0.0071,  0.0432,  0.0044, -0.0740,\n                -0.0297,  0.0294,  0.0001,  0.0024,  0.0070, -0.0015, -0.0195,  0.0466,\n                -0.0226,  0.0139, -0.0125,  0.0252,  0.0121, -0.0074, -0.0039, -0.0415,\n                 0.0030, -0.0372, -0.0266,  0.0145,  0.0070,  0.0118,  0.0273,  0.0393,\n                -0.0423,  0.0010, -0.0069,  0.0122, -0.0558,  0.0073, -0.0263,  0.0324],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9158, 0.9145, 0.9287, 0.9182, 0.9225, 0.9141, 0.9159, 0.9176, 0.9201,\n                0.9202, 0.9245, 0.9188, 0.9230, 0.9155, 0.9163, 0.9150, 0.9125, 0.9182,\n                0.9213, 0.9129, 0.9148, 0.9199, 0.9138, 0.9167, 0.9152, 0.9145, 0.9185,\n                0.9156, 0.9199, 0.9176, 0.9234, 0.9225, 0.9175, 0.9213, 0.9191, 0.9210,\n                0.9233, 0.9182, 0.9237, 0.9172, 0.9174, 0.9173, 0.9297, 0.9149, 0.9184,\n                0.9223, 0.9178, 0.9164, 0.9125, 0.9224, 0.9150, 0.9177, 0.9138, 0.9165,\n                0.9191, 0.9155, 0.9148, 0.9157, 0.9166, 0.9227, 0.9180, 0.9174, 0.9206,\n                0.9147, 0.9139, 0.9179, 0.9204, 0.9190, 0.9135, 0.9153, 0.9166, 0.9170,\n                0.9207, 0.9203, 0.9142, 0.9147, 0.9285, 0.9203, 0.9160, 0.9155, 0.9202,\n                0.9181, 0.9151, 0.9153, 0.9138, 0.9199, 0.9242, 0.9214, 0.9137, 0.9184,\n                0.9171, 0.9227, 0.9285, 0.9192, 0.9249, 0.9136, 0.9154, 0.9199, 0.9115,\n                0.9174, 0.9125, 0.9187, 0.9142, 0.9236, 0.9136, 0.9207, 0.9238, 0.9123,\n                0.9141, 0.9175, 0.9135, 0.9151, 0.9164, 0.9136, 0.9152, 0.9188, 0.9144,\n                0.9174, 0.9127, 0.9195, 0.9125, 0.9192, 0.9167, 0.9196, 0.9237, 0.9179,\n                0.9181, 0.9201, 0.9183, 0.9139, 0.9130, 0.9141, 0.9170, 0.9167, 0.9163,\n                0.9164, 0.9296, 0.9174, 0.9139, 0.9185, 0.9208, 0.9221, 0.9149, 0.9151,\n                0.9188, 0.9257, 0.9200, 0.9186, 0.9199, 0.9227, 0.9159, 0.9246, 0.9133,\n                0.9182, 0.9158, 0.9189, 0.9195, 0.9230, 0.9269, 0.9233, 0.9148, 0.9178,\n                0.9136, 0.9185, 0.9237, 0.9195, 0.9255, 0.9201, 0.9194, 0.9134, 0.9166,\n                0.9147, 0.9145, 0.9191, 0.9171, 0.9131, 0.9263, 0.9176, 0.9120, 0.9171,\n                0.9187, 0.9235, 0.9198, 0.9226, 0.9307, 0.9149, 0.9166, 0.9257, 0.9208,\n                0.9155, 0.9203, 0.9230, 0.9242, 0.9220, 0.9179, 0.9150, 0.9211, 0.9169,\n                0.9226, 0.9119, 0.9179, 0.9153, 0.9162, 0.9178, 0.9206, 0.9143, 0.9143,\n                0.9126, 0.9165, 0.9266, 0.9145, 0.9144, 0.9148, 0.9157, 0.9181, 0.9164,\n                0.9186, 0.9185, 0.9155, 0.9178, 0.9152, 0.9225, 0.9142, 0.9264, 0.9127,\n                0.9240, 0.9150, 0.9127, 0.9148, 0.9179, 0.9162, 0.9225, 0.9137, 0.9150,\n                0.9172, 0.9174, 0.9197, 0.9139, 0.9169, 0.9232, 0.9149, 0.9098, 0.9223,\n                0.9165, 0.9154, 0.9183, 0.9207, 0.9239, 0.9147, 0.9190, 0.9194, 0.9197,\n                0.9229, 0.9140, 0.9129, 0.9225], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [256, 128, 3, 3]], ["0.bn1.weight", [256]], ["0.bn1.bias", [256]], ["0.conv2.weight", [256, 256, 3, 3]], ["0.bn2.weight", [256]], ["0.bn2.bias", [256]], ["0.downsample.0.weight", [256, 128, 1, 1]], ["0.downsample.1.weight", [256]], ["0.downsample.1.bias", [256]]], "output_shape": [[512, 256, 2, 2]], "num_parameters": [294912, 256, 256, 589824, 256, 256, 32768, 256, 256]}, {"name": "layer4", "id": 140621193411168, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-7.9830e-03,  9.4254e-03,  1.7550e-02],\n                [ 1.8580e-02,  4.7889e-03, -1.7453e-02],\n                [-1.4901e-02,  1.6050e-02,  6.7854e-03]],\n      \n               [[ 1.4046e-02,  1.5047e-02,  5.4418e-03],\n                [ 2.0208e-02,  2.1430e-03,  9.7475e-03],\n                [-2.0174e-02, -1.2819e-02, -6.1324e-03]],\n      \n               [[-3.2162e-04, -4.8104e-04, -1.7273e-02],\n                [ 2.0079e-02,  5.3352e-03, -1.6482e-02],\n                [ 1.0499e-02, -1.6151e-02,  3.4511e-03]],\n      \n               ...,\n      \n               [[ 5.6530e-03,  9.3402e-04,  1.7136e-03],\n                [ 2.9165e-03,  5.3862e-03,  6.0669e-03],\n                [-1.5006e-02,  7.6925e-03,  1.0892e-02]],\n      \n               [[ 1.4165e-02,  1.0757e-02, -2.0432e-02],\n                [-1.0162e-02,  1.6852e-02,  1.6191e-02],\n                [ 4.2183e-04, -1.8380e-02, -3.5253e-03]],\n      \n               [[ 8.8863e-03,  4.4988e-03,  2.6874e-03],\n                [-2.0305e-03,  2.0631e-02,  1.9621e-02],\n                [ 2.4366e-03, -1.5999e-02,  2.0055e-02]]],\n      \n      \n              [[[ 1.4737e-02, -7.3697e-03,  1.8181e-02],\n                [-4.3137e-03,  2.2951e-03,  3.7343e-03],\n                [ 1.4694e-02,  1.5490e-02, -8.9857e-03]],\n      \n               [[ 6.7381e-05, -3.3933e-03,  1.9494e-02],\n                [ 1.0040e-02,  3.7413e-04, -2.0185e-02],\n                [ 1.6883e-02,  1.7976e-02,  1.0072e-02]],\n      \n               [[ 1.6349e-02, -2.0738e-02, -2.5502e-03],\n                [-1.6098e-02, -4.5207e-03, -1.9872e-02],\n                [ 9.1061e-05, -1.0755e-02, -1.2529e-03]],\n      \n               ...,\n      \n               [[ 8.9629e-03,  9.0392e-03, -1.3459e-02],\n                [-4.4499e-03,  5.6334e-04, -1.7098e-02],\n                [ 1.0082e-02,  1.9817e-02,  1.2434e-03]],\n      \n               [[-4.9874e-03,  8.6868e-03, -7.8013e-03],\n                [ 1.8845e-02,  9.5525e-03, -1.2788e-02],\n                [ 1.9001e-02,  7.1436e-03,  1.3475e-03]],\n      \n               [[-2.2183e-04,  4.5725e-03,  1.0070e-02],\n                [ 2.0562e-02, -1.1022e-02, -1.1450e-02],\n                [-7.3253e-04,  1.7169e-02, -1.6267e-03]]],\n      \n      \n              [[[ 5.3039e-03, -4.2674e-03, -9.7347e-03],\n                [ 2.5569e-03, -1.0907e-02, -7.3640e-03],\n                [ 1.5605e-02,  1.7124e-02, -8.2244e-03]],\n      \n               [[-7.0735e-03, -1.1720e-02, -7.4378e-03],\n                [ 4.6129e-03,  7.9640e-03, -4.4429e-03],\n                [-1.3508e-02, -1.7025e-02,  1.3259e-02]],\n      \n               [[-6.4863e-03,  8.7667e-03, -1.5193e-02],\n                [ 6.5500e-03,  8.2009e-03, -1.7586e-04],\n                [ 1.6162e-02, -1.2310e-02, -1.8525e-03]],\n      \n               ...,\n      \n               [[ 1.2750e-02, -1.5123e-02, -9.7656e-03],\n                [-1.5880e-02, -1.1320e-02, -1.0088e-02],\n                [ 1.5365e-02,  8.3929e-03,  5.6847e-04]],\n      \n               [[ 1.2480e-02,  5.4210e-03,  5.9555e-03],\n                [ 1.0163e-02, -6.5549e-03,  1.1153e-02],\n                [-4.7906e-04, -1.8083e-02, -1.1817e-02]],\n      \n               [[ 1.1322e-02, -1.3929e-02, -1.9135e-02],\n                [ 7.6315e-03,  1.7810e-02, -1.4542e-02],\n                [-1.5802e-02, -6.6520e-03, -3.4619e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[-1.3271e-02,  1.8200e-02,  3.5123e-03],\n                [ 1.4893e-02,  1.0226e-02, -1.9124e-02],\n                [-1.4011e-02, -1.8414e-02,  1.3867e-02]],\n      \n               [[-1.0455e-02, -1.7234e-02, -6.0462e-03],\n                [-1.2118e-02, -1.2165e-02, -7.5210e-03],\n                [ 4.6746e-03, -1.8985e-02,  4.6340e-03]],\n      \n               [[ 1.9525e-02, -3.1737e-03, -1.7274e-02],\n                [ 1.6640e-02,  4.3879e-04,  5.0959e-03],\n                [-1.9212e-03, -1.0090e-02, -6.7726e-04]],\n      \n               ...,\n      \n               [[ 1.9398e-02, -5.2019e-03, -1.8768e-02],\n                [-1.8681e-02,  5.8616e-03,  2.2899e-03],\n                [ 7.6380e-03,  8.8021e-05, -5.8781e-03]],\n      \n               [[ 1.6271e-02, -1.5379e-02,  1.7671e-02],\n                [ 1.6214e-02,  1.9356e-02,  1.5546e-03],\n                [-9.1552e-03, -2.4913e-03, -9.6321e-03]],\n      \n               [[ 1.2869e-02,  1.2119e-02, -1.1766e-02],\n                [ 1.1904e-02,  6.7975e-03, -1.8959e-02],\n                [ 1.2140e-02, -1.8290e-02,  3.9810e-03]]],\n      \n      \n              [[[ 7.6242e-03,  1.1392e-02, -6.7130e-03],\n                [-1.4925e-03, -1.1231e-02,  8.4101e-03],\n                [ 3.3438e-03, -5.6982e-03,  1.0720e-02]],\n      \n               [[ 1.3957e-02, -1.4749e-02, -1.0143e-02],\n                [-1.5895e-06, -6.7043e-03, -1.6178e-02],\n                [ 4.8494e-03, -1.5623e-02, -1.5732e-02]],\n      \n               [[-1.9648e-02,  1.1877e-02, -1.3518e-02],\n                [ 2.0773e-02,  5.4243e-03,  4.9283e-03],\n                [-1.0795e-03,  1.2977e-02, -8.7442e-03]],\n      \n               ...,\n      \n               [[ 3.6113e-03,  9.0068e-03,  1.3989e-02],\n                [ 1.7698e-02,  8.7160e-03,  1.3027e-02],\n                [ 1.2817e-02,  1.9314e-02, -1.2005e-03]],\n      \n               [[ 1.8990e-02, -5.1855e-03, -1.0683e-02],\n                [ 1.8039e-02, -1.2093e-02,  1.2113e-02],\n                [-4.8715e-03, -1.1930e-02,  1.9151e-03]],\n      \n               [[-1.8653e-02,  2.5701e-03, -8.3923e-03],\n                [-1.5292e-02, -8.8989e-04, -8.6591e-03],\n                [ 1.8557e-02, -9.6247e-03, -2.0610e-02]]],\n      \n      \n              [[[ 4.2480e-03,  1.3865e-02, -2.0497e-02],\n                [-1.0327e-02,  1.9662e-02,  1.1193e-02],\n                [ 1.9183e-02,  8.3207e-03, -1.0816e-02]],\n      \n               [[-1.6619e-02,  8.8894e-03,  1.6418e-02],\n                [-7.6118e-03,  1.7281e-02,  1.2961e-02],\n                [ 2.5141e-03, -1.9954e-02, -2.0189e-02]],\n      \n               [[-1.1988e-02,  7.0197e-03,  7.0282e-03],\n                [ 5.8158e-03, -1.3440e-02,  1.8821e-02],\n                [-1.6634e-02, -1.2747e-02,  1.0019e-02]],\n      \n               ...,\n      \n               [[-8.7937e-03, -1.6317e-02, -9.0926e-04],\n                [-1.6420e-02, -1.2425e-02, -4.7747e-03],\n                [-3.8363e-03,  1.0840e-03,  8.9703e-03]],\n      \n               [[ 1.8796e-02,  1.9100e-04, -1.2218e-02],\n                [ 5.5092e-03,  1.5444e-02, -8.3092e-03],\n                [ 1.9403e-02, -1.6051e-02,  1.2050e-02]],\n      \n               [[ 1.4508e-02, -1.4415e-02, -1.3033e-03],\n                [-4.8104e-03, -2.0793e-02,  1.5896e-02],\n                [ 5.2406e-03, -1.9316e-02,  1.0552e-02]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0080,  0.0152, -0.0342,  0.0114, -0.0344, -0.0409, -0.0019, -0.0069,\n               0.0157, -0.0207,  0.0349, -0.0312, -0.0314, -0.0303,  0.0043,  0.0149,\n               0.0222,  0.0142,  0.0370, -0.0195, -0.0254,  0.0255,  0.0343, -0.0016,\n              -0.0006, -0.0395,  0.0100,  0.0331,  0.0023, -0.0207, -0.0127,  0.0215,\n               0.0151, -0.0485,  0.0059, -0.0167,  0.0436,  0.0065,  0.0236,  0.0169,\n               0.0023,  0.0478,  0.0080,  0.0192, -0.0024,  0.0049, -0.0197,  0.0263,\n              -0.0283, -0.0232,  0.0312, -0.0044,  0.0094,  0.0182, -0.0052, -0.0278,\n              -0.0002, -0.0115, -0.0162,  0.0032, -0.0364, -0.0060,  0.0020,  0.0160,\n              -0.0168,  0.0122,  0.0485,  0.0076, -0.0128, -0.0233,  0.0286,  0.0036,\n              -0.0035,  0.0170,  0.0046, -0.0775, -0.0174, -0.0220,  0.0040, -0.0077,\n              -0.0257, -0.0323, -0.0001, -0.0035, -0.0121,  0.0456, -0.0074, -0.0260,\n              -0.0439,  0.0018, -0.0443, -0.0154, -0.0482,  0.0358, -0.0023,  0.0011,\n               0.0177, -0.0261,  0.0232, -0.0117, -0.0231, -0.0167, -0.0174,  0.0175,\n               0.0100, -0.0221, -0.0083,  0.0045, -0.0043,  0.0053, -0.0283, -0.0263,\n               0.0580, -0.0066, -0.0011, -0.0277, -0.0042,  0.0235,  0.0545,  0.0059,\n              -0.0290,  0.0166,  0.0007, -0.0088,  0.0086, -0.0116, -0.0133,  0.0102,\n              -0.0026, -0.0119,  0.0177, -0.0115,  0.0269, -0.0015, -0.0189, -0.0142,\n               0.0193,  0.0150,  0.0266, -0.0347, -0.0152,  0.0026,  0.0064,  0.0288,\n               0.0164, -0.0487,  0.0109,  0.0259, -0.0107, -0.0143,  0.0115, -0.0105,\n               0.0178, -0.0085, -0.0466,  0.0101, -0.0259,  0.0281, -0.0172, -0.0184,\n               0.0269,  0.0222, -0.0480,  0.0127, -0.0038, -0.0220,  0.0327,  0.0046,\n               0.0027,  0.0231, -0.0303, -0.0348, -0.0166, -0.0442,  0.0452, -0.0044,\n              -0.0289, -0.0205,  0.0218, -0.0384,  0.0060,  0.0257,  0.0447, -0.0482,\n              -0.0016,  0.0387, -0.0385, -0.0102,  0.0222, -0.0229, -0.0024,  0.0147,\n               0.0026, -0.0087, -0.0128, -0.0284,  0.0031,  0.0065, -0.0248, -0.0022,\n              -0.0404,  0.0154, -0.0302, -0.0071, -0.0864, -0.0126, -0.0042, -0.0026,\n               0.0169,  0.0079, -0.0043,  0.0152, -0.0104, -0.0121,  0.0357, -0.0280,\n              -0.0092, -0.0326,  0.0077,  0.0356, -0.0024,  0.0173,  0.0346,  0.0409,\n               0.0415, -0.0222, -0.0544,  0.0390, -0.0031,  0.0285, -0.0453, -0.0077,\n               0.0074,  0.0064,  0.0369,  0.0045,  0.0241, -0.0330,  0.0224,  0.0434,\n              -0.0238,  0.0251,  0.0161,  0.0153, -0.0105,  0.0230,  0.0093,  0.0201,\n               0.0098,  0.0523, -0.0183,  0.0276,  0.0002, -0.0284,  0.0264, -0.0050,\n               0.0173,  0.0180, -0.0452,  0.0136,  0.0069,  0.0060,  0.0157, -0.0156,\n              -0.0034,  0.0115,  0.0350, -0.0378,  0.0027,  0.0072, -0.0296, -0.0196,\n              -0.0072, -0.0098,  0.0690, -0.0454, -0.0185, -0.0023,  0.0299,  0.0124,\n               0.0228,  0.0075,  0.0235, -0.0627, -0.0095,  0.0059, -0.0810, -0.0172,\n               0.0347,  0.0352, -0.0106,  0.0066,  0.0026,  0.0021,  0.0454,  0.0109,\n              -0.0187, -0.0175, -0.0277, -0.0102,  0.0616, -0.0156,  0.0044,  0.0215,\n              -0.0140,  0.0126, -0.0067,  0.0250,  0.0400,  0.0062,  0.0551, -0.0304,\n              -0.0460, -0.0145,  0.0256,  0.0472, -0.0088, -0.0151, -0.0095, -0.0137,\n               0.0277,  0.0240, -0.0240, -0.0022, -0.0312,  0.0284, -0.0137, -0.0009,\n              -0.0198, -0.0165,  0.0142, -0.0043,  0.0127,  0.0078,  0.0265, -0.0140,\n              -0.0281,  0.0142, -0.0239,  0.0012,  0.0236,  0.0010, -0.0002, -0.0319,\n               0.0079,  0.0190,  0.0321,  0.0566,  0.0345,  0.0305,  0.0194, -0.0253,\n               0.0367,  0.0225, -0.0107, -0.0305, -0.0469, -0.0462,  0.0011,  0.0417,\n               0.0322, -0.0020,  0.0208, -0.0052,  0.0013,  0.0181, -0.0131, -0.0487,\n              -0.0231,  0.0067,  0.0093,  0.0286, -0.0428,  0.0016, -0.0073,  0.0189,\n               0.0283, -0.0190, -0.0273,  0.0337,  0.0400, -0.0357, -0.0139,  0.0113,\n               0.0346, -0.0353,  0.0139,  0.0102,  0.0101,  0.0310, -0.0528,  0.0259,\n              -0.0283, -0.0069,  0.0536, -0.0215,  0.0292,  0.0310,  0.0064, -0.0636,\n              -0.0072, -0.0162,  0.0237,  0.0093, -0.0243,  0.0321, -0.0047, -0.0137,\n               0.0106, -0.0218,  0.0088, -0.0345, -0.0249,  0.0319,  0.0354, -0.0039,\n              -0.0547, -0.0048, -0.0258,  0.0191, -0.0105,  0.0218, -0.0265,  0.0404,\n              -0.0094,  0.0184,  0.0020, -0.0208, -0.0478,  0.0143, -0.0031, -0.0227,\n              -0.0078, -0.0516, -0.0116,  0.0105, -0.0249, -0.0200, -0.0315, -0.0239,\n              -0.0024,  0.0196, -0.0223, -0.0068,  0.0376, -0.0097, -0.0266, -0.0057,\n              -0.0159,  0.0268, -0.0113, -0.0072,  0.0289,  0.0221, -0.0074,  0.0160,\n              -0.0146, -0.0465, -0.0256,  0.0242, -0.0107,  0.0079, -0.0135, -0.0174,\n               0.0507, -0.0051,  0.0228,  0.0131,  0.0149, -0.0266, -0.0252, -0.0128,\n              -0.0255,  0.0204,  0.0233, -0.0474,  0.0018, -0.0353,  0.0165,  0.0026,\n              -0.0241,  0.0280, -0.0045,  0.0124,  0.0539, -0.0073, -0.0460,  0.0206,\n               0.0206,  0.0134,  0.0235,  0.0351, -0.0060, -0.0101,  0.0311, -0.0028,\n              -0.0049,  0.0269,  0.0006, -0.0365,  0.0165, -0.0211, -0.0114, -0.0237,\n              -0.0145, -0.0029, -0.0296, -0.0072,  0.0321, -0.0090,  0.0223,  0.0264],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9073, 0.9053, 0.9074, 0.9076, 0.9094, 0.9079, 0.9090, 0.9079, 0.9086,\n              0.9082, 0.9105, 0.9076, 0.9088, 0.9137, 0.9095, 0.9104, 0.9097, 0.9094,\n              0.9084, 0.9070, 0.9080, 0.9092, 0.9100, 0.9072, 0.9076, 0.9068, 0.9086,\n              0.9084, 0.9076, 0.9108, 0.9098, 0.9113, 0.9110, 0.9067, 0.9088, 0.9093,\n              0.9084, 0.9064, 0.9082, 0.9096, 0.9090, 0.9104, 0.9077, 0.9112, 0.9070,\n              0.9134, 0.9101, 0.9121, 0.9091, 0.9082, 0.9077, 0.9076, 0.9087, 0.9096,\n              0.9081, 0.9077, 0.9073, 0.9077, 0.9069, 0.9074, 0.9080, 0.9070, 0.9097,\n              0.9075, 0.9078, 0.9086, 0.9101, 0.9076, 0.9088, 0.9085, 0.9087, 0.9094,\n              0.9081, 0.9129, 0.9097, 0.9096, 0.9063, 0.9066, 0.9077, 0.9072, 0.9115,\n              0.9103, 0.9102, 0.9082, 0.9089, 0.9091, 0.9077, 0.9095, 0.9123, 0.9117,\n              0.9091, 0.9076, 0.9082, 0.9093, 0.9097, 0.9069, 0.9067, 0.9104, 0.9100,\n              0.9121, 0.9082, 0.9071, 0.9092, 0.9107, 0.9089, 0.9069, 0.9075, 0.9081,\n              0.9112, 0.9072, 0.9070, 0.9092, 0.9084, 0.9072, 0.9081, 0.9086, 0.9078,\n              0.9094, 0.9106, 0.9065, 0.9077, 0.9100, 0.9097, 0.9094, 0.9102, 0.9086,\n              0.9097, 0.9097, 0.9071, 0.9082, 0.9095, 0.9089, 0.9085, 0.9080, 0.9099,\n              0.9075, 0.9074, 0.9090, 0.9080, 0.9089, 0.9080, 0.9082, 0.9079, 0.9073,\n              0.9063, 0.9074, 0.9088, 0.9078, 0.9073, 0.9102, 0.9080, 0.9091, 0.9117,\n              0.9075, 0.9133, 0.9067, 0.9085, 0.9061, 0.9085, 0.9074, 0.9113, 0.9077,\n              0.9134, 0.9074, 0.9103, 0.9114, 0.9076, 0.9082, 0.9095, 0.9069, 0.9077,\n              0.9104, 0.9083, 0.9084, 0.9131, 0.9076, 0.9072, 0.9093, 0.9102, 0.9082,\n              0.9097, 0.9095, 0.9092, 0.9116, 0.9080, 0.9089, 0.9105, 0.9090, 0.9106,\n              0.9074, 0.9092, 0.9075, 0.9076, 0.9066, 0.9092, 0.9094, 0.9073, 0.9098,\n              0.9080, 0.9071, 0.9082, 0.9125, 0.9069, 0.9096, 0.9129, 0.9090, 0.9061,\n              0.9088, 0.9094, 0.9085, 0.9077, 0.9081, 0.9086, 0.9117, 0.9086, 0.9110,\n              0.9122, 0.9064, 0.9091, 0.9104, 0.9105, 0.9065, 0.9085, 0.9106, 0.9100,\n              0.9088, 0.9077, 0.9088, 0.9100, 0.9094, 0.9104, 0.9076, 0.9075, 0.9112,\n              0.9088, 0.9095, 0.9086, 0.9079, 0.9096, 0.9076, 0.9101, 0.9086, 0.9097,\n              0.9060, 0.9069, 0.9085, 0.9071, 0.9063, 0.9090, 0.9091, 0.9074, 0.9075,\n              0.9085, 0.9091, 0.9066, 0.9083, 0.9095, 0.9073, 0.9115, 0.9085, 0.9113,\n              0.9077, 0.9080, 0.9079, 0.9075, 0.9082, 0.9098, 0.9117, 0.9071, 0.9099,\n              0.9090, 0.9092, 0.9076, 0.9089, 0.9083, 0.9095, 0.9060, 0.9098, 0.9088,\n              0.9072, 0.9079, 0.9082, 0.9083, 0.9082, 0.9074, 0.9089, 0.9120, 0.9100,\n              0.9091, 0.9107, 0.9087, 0.9113, 0.9066, 0.9069, 0.9088, 0.9070, 0.9073,\n              0.9090, 0.9140, 0.9093, 0.9073, 0.9078, 0.9088, 0.9108, 0.9084, 0.9090,\n              0.9095, 0.9085, 0.9114, 0.9114, 0.9083, 0.9075, 0.9081, 0.9081, 0.9090,\n              0.9087, 0.9069, 0.9090, 0.9089, 0.9088, 0.9083, 0.9081, 0.9065, 0.9083,\n              0.9069, 0.9096, 0.9110, 0.9079, 0.9081, 0.9110, 0.9081, 0.9078, 0.9079,\n              0.9066, 0.9073, 0.9074, 0.9062, 0.9081, 0.9076, 0.9092, 0.9091, 0.9080,\n              0.9087, 0.9109, 0.9090, 0.9102, 0.9073, 0.9104, 0.9085, 0.9126, 0.9070,\n              0.9088, 0.9114, 0.9067, 0.9091, 0.9090, 0.9065, 0.9096, 0.9086, 0.9095,\n              0.9072, 0.9088, 0.9111, 0.9064, 0.9079, 0.9092, 0.9089, 0.9107, 0.9076,\n              0.9086, 0.9077, 0.9121, 0.9066, 0.9086, 0.9091, 0.9111, 0.9120, 0.9099,\n              0.9061, 0.9082, 0.9086, 0.9087, 0.9074, 0.9068, 0.9080, 0.9076, 0.9084,\n              0.9102, 0.9067, 0.9109, 0.9123, 0.9088, 0.9116, 0.9078, 0.9083, 0.9081,\n              0.9074, 0.9074, 0.9071, 0.9086, 0.9103, 0.9076, 0.9087, 0.9067, 0.9082,\n              0.9098, 0.9081, 0.9094, 0.9068, 0.9073, 0.9083, 0.9086, 0.9106, 0.9065,\n              0.9098, 0.9083, 0.9077, 0.9086, 0.9087, 0.9067, 0.9091, 0.9065, 0.9079,\n              0.9116, 0.9086, 0.9099, 0.9068, 0.9077, 0.9074, 0.9083, 0.9070, 0.9091,\n              0.9092, 0.9106, 0.9085, 0.9082, 0.9077, 0.9080, 0.9084, 0.9089, 0.9076,\n              0.9083, 0.9092, 0.9128, 0.9106, 0.9087, 0.9086, 0.9106, 0.9087, 0.9080,\n              0.9086, 0.9106, 0.9098, 0.9084, 0.9074, 0.9091, 0.9085, 0.9117, 0.9094,\n              0.9109, 0.9087, 0.9073, 0.9084, 0.9090, 0.9105, 0.9082, 0.9090, 0.9097,\n              0.9071, 0.9098, 0.9061, 0.9076, 0.9071, 0.9100, 0.9067, 0.9079, 0.9089,\n              0.9098, 0.9076, 0.9077, 0.9094, 0.9081, 0.9083, 0.9072, 0.9106, 0.9081,\n              0.9114, 0.9075, 0.9086, 0.9084, 0.9086, 0.9070, 0.9093, 0.9066, 0.9085,\n              0.9074, 0.9093, 0.9086, 0.9086, 0.9087, 0.9086, 0.9074, 0.9069, 0.9065,\n              0.9088, 0.9080, 0.9080, 0.9086, 0.9063, 0.9082, 0.9081, 0.9090],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 1.1694e-03, -3.3357e-03, -1.6208e-03],\n                [-9.3269e-03,  8.3880e-03, -7.9280e-05],\n                [ 1.0900e-02,  7.6803e-03, -9.5244e-03]],\n      \n               [[ 4.8334e-03,  2.2947e-03,  4.7559e-03],\n                [ 1.3068e-02, -9.5558e-03,  1.3263e-02],\n                [ 1.2208e-02, -5.8599e-03, -1.2895e-02]],\n      \n               [[ 6.5555e-03,  1.1654e-02,  1.2020e-02],\n                [-1.4582e-02,  1.3054e-02,  3.4358e-03],\n                [-7.9534e-03,  6.5335e-03, -1.0868e-02]],\n      \n               ...,\n      \n               [[ 5.6513e-03,  1.4705e-02,  1.2571e-02],\n                [ 3.7043e-03, -1.0033e-03,  1.4450e-02],\n                [ 5.4925e-03, -3.8694e-03,  5.0172e-05]],\n      \n               [[ 1.4484e-02,  1.0725e-02,  1.4618e-03],\n                [ 7.0071e-03, -7.3243e-03, -3.5311e-03],\n                [-1.0595e-02, -1.4288e-02, -4.6611e-03]],\n      \n               [[ 1.2698e-03, -2.6604e-03, -1.3307e-02],\n                [-1.3994e-02,  1.0327e-02,  1.1110e-02],\n                [ 1.0148e-02,  1.3837e-02, -5.0527e-03]]],\n      \n      \n              [[[ 4.7407e-03, -2.7625e-04, -8.4170e-03],\n                [ 2.4882e-03,  6.8942e-03, -4.7096e-05],\n                [ 6.7636e-03, -7.8420e-03,  1.0368e-02]],\n      \n               [[-6.5088e-04, -6.7375e-03, -4.0132e-03],\n                [ 8.0329e-03, -1.0551e-02, -7.9535e-03],\n                [ 1.4138e-02, -7.5744e-03, -4.4363e-04]],\n      \n               [[-1.7494e-03,  1.0741e-02,  3.3987e-03],\n                [-1.8792e-03, -1.0893e-02,  1.0344e-02],\n                [-8.1549e-03,  1.1036e-02, -9.1619e-03]],\n      \n               ...,\n      \n               [[ 1.9904e-03,  4.9530e-03,  5.7858e-03],\n                [ 1.4103e-02, -2.1490e-03,  1.2915e-02],\n                [ 1.2539e-02, -5.8217e-04, -7.3799e-03]],\n      \n               [[-1.7852e-03, -1.7236e-03, -5.2588e-03],\n                [ 8.0601e-03, -6.4963e-03, -4.6851e-03],\n                [-1.0553e-02, -4.3083e-03,  8.5573e-03]],\n      \n               [[ 7.9114e-03,  9.7331e-03,  1.4241e-02],\n                [ 6.1121e-03,  4.5483e-03,  6.3394e-03],\n                [-1.4722e-02, -2.5664e-03,  3.5324e-03]]],\n      \n      \n              [[[ 3.1841e-04, -1.8314e-03, -1.2209e-02],\n                [-8.6305e-03,  8.8884e-03,  7.8772e-04],\n                [ 2.1875e-03, -3.7528e-03,  1.1226e-03]],\n      \n               [[-8.5318e-03, -8.0290e-03, -6.5290e-03],\n                [ 6.0459e-03, -9.8691e-03, -9.3201e-03],\n                [-3.4586e-03,  4.2423e-03,  6.9568e-03]],\n      \n               [[ 1.2075e-02, -3.6084e-03, -2.4858e-04],\n                [-8.2475e-03,  7.2374e-03,  3.8065e-03],\n                [ 5.6773e-03, -9.3491e-03, -2.7975e-03]],\n      \n               ...,\n      \n               [[-1.0088e-03, -1.3770e-02, -8.2329e-03],\n                [ 4.2692e-03,  1.3222e-02,  9.8041e-03],\n                [-3.6683e-03,  3.4155e-03,  3.7448e-03]],\n      \n               [[ 7.9647e-03,  1.4010e-02,  1.1623e-03],\n                [-1.4019e-02, -1.1209e-02, -7.0174e-03],\n                [-1.3536e-02,  2.6771e-03,  8.2727e-03]],\n      \n               [[-6.3807e-03,  1.2154e-02,  3.4308e-03],\n                [ 2.2726e-03,  9.1518e-03,  1.0622e-02],\n                [-1.1156e-02, -1.2954e-02,  1.9352e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[-2.4362e-03,  3.0633e-03,  8.6999e-03],\n                [-1.2272e-02,  8.7038e-03, -8.1429e-03],\n                [-1.1287e-02, -5.3035e-03, -1.1056e-02]],\n      \n               [[ 1.3565e-02,  3.4403e-03,  3.3363e-03],\n                [ 1.1513e-02,  1.2498e-02, -4.1851e-03],\n                [ 1.0982e-03, -1.1749e-02,  7.2575e-03]],\n      \n               [[ 1.3720e-02, -4.2012e-03, -1.2331e-02],\n                [ 4.5953e-03, -1.0076e-02, -1.1349e-02],\n                [-8.0349e-04,  9.6254e-03, -9.2948e-03]],\n      \n               ...,\n      \n               [[ 1.3608e-02,  6.4040e-03,  9.4739e-04],\n                [ 3.8266e-03, -1.2629e-02,  2.2010e-04],\n                [ 4.0430e-04,  4.0952e-03, -9.7347e-03]],\n      \n               [[-9.7286e-03, -1.4406e-02, -5.5954e-03],\n                [ 3.6782e-05, -5.3871e-03,  2.6051e-03],\n                [-8.7250e-03, -1.2190e-02, -7.2746e-03]],\n      \n               [[ 1.1676e-02,  3.8592e-03,  9.5111e-04],\n                [ 1.2320e-02,  4.9815e-03, -1.4396e-03],\n                [ 1.0684e-02,  1.4345e-02,  8.8435e-03]]],\n      \n      \n              [[[-1.1431e-02,  7.7123e-03, -4.8143e-03],\n                [ 1.4700e-02, -1.6416e-03, -1.1880e-02],\n                [-8.0336e-03,  1.2658e-02, -4.9879e-03]],\n      \n               [[-5.4541e-04,  7.8871e-03, -1.1316e-02],\n                [-9.1525e-03,  1.0883e-02, -7.2916e-03],\n                [ 3.4575e-03,  1.1393e-02,  5.4240e-03]],\n      \n               [[-1.0759e-02,  1.0352e-02,  7.8715e-04],\n                [ 1.3903e-02, -9.8807e-03, -1.2476e-02],\n                [-9.8784e-03,  1.0250e-02,  8.3643e-03]],\n      \n               ...,\n      \n               [[-6.9419e-03,  2.8074e-03,  2.4431e-04],\n                [-6.8125e-03,  8.7846e-03,  1.2154e-02],\n                [ 1.4650e-02,  6.7540e-03,  5.1939e-03]],\n      \n               [[ 5.6454e-03,  1.1031e-02,  3.1233e-03],\n                [-1.0543e-02,  2.1264e-03, -1.3239e-02],\n                [-2.4728e-03,  8.8805e-03,  8.4862e-03]],\n      \n               [[-7.9364e-03, -1.7232e-03, -5.5035e-03],\n                [ 1.5166e-03,  3.9903e-03,  6.0183e-03],\n                [-1.0330e-02,  6.7479e-03, -2.9612e-03]]],\n      \n      \n              [[[ 7.7266e-03, -1.0333e-02, -9.3505e-03],\n                [ 9.9942e-03,  4.1722e-03,  2.8618e-03],\n                [ 3.1029e-03, -9.9287e-03, -9.6593e-03]],\n      \n               [[ 1.3599e-02, -9.5701e-03, -2.6693e-03],\n                [-2.9313e-03, -4.8676e-03,  9.2637e-05],\n                [ 1.1587e-02, -2.6762e-03, -2.1152e-03]],\n      \n               [[-1.3119e-02,  6.1370e-03,  1.2888e-02],\n                [ 4.5439e-03,  1.1267e-02, -3.9547e-03],\n                [ 1.6817e-03,  1.3990e-02, -1.1201e-05]],\n      \n               ...,\n      \n               [[-2.9971e-03, -8.9347e-03, -5.1285e-03],\n                [ 7.6174e-03, -1.2210e-02, -1.3955e-02],\n                [-9.0249e-04, -2.0165e-03, -5.5921e-04]],\n      \n               [[ 6.0168e-03,  1.7261e-03, -9.1811e-03],\n                [-1.3852e-02,  6.7181e-03,  4.9742e-03],\n                [ 1.0315e-02,  1.2359e-02,  1.0159e-02]],\n      \n               [[-1.4691e-02,  1.2756e-02, -3.2088e-03],\n                [-1.1521e-02,  9.2177e-03, -1.0828e-02],\n                [ 5.1347e-03, -7.9452e-03,  3.1989e-03]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-3.8256e-03, -1.4047e-02,  4.0569e-03, -6.1297e-03,  8.5712e-03,\n               1.8587e-02,  3.2879e-03, -3.3623e-03, -9.1302e-03, -3.4656e-03,\n               5.9506e-03, -6.8134e-03,  1.3079e-02, -3.8215e-03, -1.1301e-02,\n              -1.0168e-02,  1.8142e-03, -1.3449e-02,  1.0632e-02, -2.9192e-04,\n              -1.4230e-02, -7.3155e-03, -1.3987e-02,  6.0221e-03, -1.8439e-03,\n               1.4148e-02,  4.0485e-03, -2.6075e-03, -7.8797e-04, -1.3620e-03,\n               6.1190e-03,  3.0756e-03,  8.2568e-03,  1.4831e-02,  5.4074e-04,\n               3.5731e-03, -2.5469e-03,  5.9131e-03, -5.7525e-03,  4.9988e-03,\n               4.6641e-03, -4.2235e-03,  6.1116e-03, -2.8348e-03, -7.2432e-03,\n               6.5805e-03, -4.4075e-03, -5.1007e-03,  1.3903e-02,  4.1851e-04,\n              -5.5994e-03, -6.9455e-03,  4.2418e-04,  7.1103e-03,  1.3772e-03,\n               1.9576e-03, -1.7034e-03,  1.0385e-02, -4.6525e-04, -1.1963e-02,\n              -1.3654e-03,  7.5812e-04,  1.9564e-04,  9.5137e-03, -1.0733e-02,\n              -5.2027e-03, -7.8255e-03,  1.1973e-03, -1.7211e-02, -2.8438e-04,\n              -7.5978e-03, -1.6241e-03, -7.8598e-03,  4.7883e-03,  3.8868e-03,\n              -4.3419e-05,  1.2158e-03,  6.1415e-03,  1.0825e-03, -1.5118e-02,\n               8.3280e-03, -1.0078e-03,  7.9577e-03,  7.4536e-03, -3.0684e-03,\n               6.4047e-04, -1.9952e-03, -1.0376e-02, -1.4683e-02,  1.5330e-03,\n              -3.4039e-03,  4.4622e-03, -8.0997e-03,  7.6723e-03, -3.8446e-03,\n               1.4667e-02,  7.5753e-04, -3.0338e-03, -8.0147e-03, -6.2537e-03,\n              -8.4738e-03, -1.3275e-03, -8.9154e-03,  1.6493e-02, -6.1786e-03,\n               6.9583e-03, -6.3283e-03, -1.0383e-02, -1.7209e-03, -1.2526e-02,\n               2.8702e-03,  5.3927e-03,  6.7367e-03, -2.8510e-03,  8.6378e-03,\n              -5.3961e-04,  1.0020e-02, -5.7289e-03, -7.5154e-03, -8.8610e-03,\n              -4.3646e-03, -3.6612e-03, -5.4273e-03, -2.5595e-03, -1.2710e-02,\n               5.7774e-03, -6.6829e-03,  4.7601e-03,  9.3073e-03, -4.1481e-03,\n               8.0002e-03,  2.1290e-02,  1.1236e-02, -5.9676e-03, -7.4944e-04,\n              -1.5530e-02, -7.6670e-03,  1.3377e-02, -4.6174e-03, -2.4321e-03,\n               1.0294e-02,  4.1920e-03, -9.0503e-04, -7.1964e-05, -3.5623e-03,\n              -2.4024e-03,  2.5144e-03, -1.1087e-02, -3.0263e-03, -1.3909e-02,\n               5.6840e-03,  1.2520e-03, -1.1549e-02,  7.6357e-03,  4.0810e-03,\n              -1.5754e-03, -1.8290e-03,  1.3391e-03, -3.6810e-03,  2.0515e-03,\n              -5.7327e-03,  1.0365e-02,  5.3739e-03, -1.1081e-02,  8.8308e-03,\n               2.1257e-03, -7.1118e-03, -6.2715e-03,  3.3342e-05, -5.3826e-03,\n               3.0840e-03,  1.1543e-02,  9.9717e-04,  1.4020e-03, -8.7761e-03,\n              -6.7340e-03,  4.5730e-03,  6.0689e-03, -2.2138e-03,  1.5058e-03,\n               6.4565e-03,  3.3478e-03,  4.3163e-04, -6.0510e-03,  1.0623e-02,\n              -9.8313e-03, -3.3184e-03,  8.1239e-03,  9.9948e-03, -5.9582e-03,\n              -7.3701e-03,  2.8825e-03,  1.4470e-03, -1.0024e-02, -1.3685e-03,\n              -5.7387e-03, -3.0839e-03,  6.2625e-03, -1.1480e-02, -1.2238e-03,\n              -1.3866e-05, -1.5875e-03, -8.5899e-03,  1.9886e-03,  5.1876e-03,\n              -6.4841e-03,  3.3714e-03, -2.3927e-03, -5.1125e-03,  1.3581e-02,\n              -2.5542e-03, -1.4538e-02, -8.2577e-03,  9.8482e-03, -3.8612e-03,\n              -1.2995e-03,  7.0068e-03,  1.9860e-03, -3.4073e-03, -7.5184e-03,\n               3.8416e-03,  1.0922e-02, -1.0704e-02, -2.9133e-03,  3.0905e-05,\n              -1.6120e-02,  2.2849e-04, -4.3028e-03, -4.9124e-03, -5.4094e-03,\n              -1.6600e-04, -5.3124e-03, -3.3252e-04, -2.0744e-03, -1.2829e-03,\n               2.5373e-03, -7.7494e-04, -4.1544e-03, -1.5989e-03, -1.9969e-03,\n               2.0511e-03,  5.2268e-03, -3.3026e-03,  5.0304e-03,  3.0968e-03,\n               6.7963e-03,  3.2632e-03,  4.3477e-03,  8.3345e-04,  1.4759e-02,\n              -3.8941e-03, -9.3085e-03,  2.4601e-03, -1.9746e-02, -2.5889e-04,\n               2.8096e-03, -6.1419e-03, -1.4664e-02,  2.7456e-03, -4.2348e-03,\n              -1.3170e-02, -5.8447e-03, -8.2392e-03, -3.9831e-03,  1.6479e-03,\n               7.5735e-03, -8.5132e-03,  8.1678e-04,  5.6725e-03,  1.6623e-02,\n              -5.8786e-03,  4.6676e-03, -4.6190e-03, -2.2624e-03,  5.7411e-03,\n              -4.7878e-03,  7.2980e-03,  8.3886e-04, -8.3413e-03,  2.6575e-03,\n               2.7568e-03,  8.4396e-03, -9.8805e-03,  2.2267e-03,  7.3391e-03,\n              -5.6550e-03, -5.0959e-03, -1.9796e-03, -5.8095e-03,  1.5059e-02,\n               1.1111e-03,  6.3081e-04, -1.4890e-02,  1.2973e-03, -1.2611e-03,\n              -9.4562e-03, -7.4900e-05, -6.0538e-03, -1.6204e-02,  1.2436e-02,\n               9.2606e-03,  5.5713e-03, -2.1416e-03,  1.0371e-02,  5.6909e-03,\n               1.6895e-03, -1.5934e-04,  5.5238e-03, -1.0026e-02,  1.3672e-02,\n               3.5173e-03, -1.9648e-03, -8.1979e-03,  1.0642e-03, -1.3795e-02,\n              -5.0100e-03,  2.4896e-03, -1.0991e-02, -6.6950e-03, -4.4190e-03,\n               1.4379e-02,  1.0282e-03,  1.0585e-02, -9.5523e-03, -7.8032e-03,\n               1.5097e-02, -6.2593e-03,  1.1295e-02, -6.6170e-03, -6.3026e-03,\n               1.4659e-02,  3.5961e-03, -5.4881e-03, -4.4067e-04, -4.6726e-03,\n              -1.5401e-03, -5.3296e-04,  3.9135e-03, -3.7131e-03, -1.3465e-03,\n               1.2562e-02,  2.6695e-04, -6.8014e-03, -4.9742e-03, -5.1799e-03,\n               1.0200e-02, -1.8824e-02,  1.0046e-03, -2.7556e-03,  2.5733e-03,\n               3.6579e-03,  3.1933e-03,  2.7051e-03,  1.2690e-02, -6.2051e-03,\n              -4.2562e-03, -3.8895e-03,  7.6313e-03, -1.9753e-03,  8.7621e-03,\n               1.2063e-02,  5.0359e-03,  1.4345e-03,  7.0065e-03,  5.2978e-03,\n               2.4466e-03, -1.2878e-03, -3.4629e-03,  9.2884e-03,  8.3471e-03,\n              -8.7287e-03,  1.2105e-02,  8.8866e-03, -1.1549e-05,  1.0896e-02,\n              -1.0401e-02,  1.2184e-02, -3.2591e-03,  8.9627e-04, -1.0820e-03,\n               1.7234e-04, -6.9105e-03, -8.1989e-04,  7.7696e-03, -1.2896e-03,\n               8.0458e-03, -6.1483e-03, -5.6630e-03,  2.7165e-03, -6.8455e-03,\n              -3.8288e-03, -5.6711e-03, -1.1099e-02,  4.3853e-03,  2.4958e-03,\n              -6.5169e-03,  1.4456e-02,  1.0732e-04,  8.4313e-03, -8.1745e-03,\n               1.7038e-02, -1.1471e-02, -5.4484e-03,  6.3387e-03, -4.7628e-03,\n               1.0834e-02, -6.5229e-03,  7.5637e-03, -1.4263e-02, -7.7321e-03,\n               1.6505e-03,  3.5491e-04,  3.8272e-03, -2.9789e-03, -1.5440e-04,\n               8.6376e-04,  7.4226e-03, -2.4100e-03,  8.1225e-03,  1.6842e-03,\n               1.5525e-02,  4.1589e-03, -9.1883e-03, -1.5192e-02, -9.1106e-04,\n               7.3390e-03, -3.8099e-03,  1.1896e-03,  4.3338e-03,  5.2972e-03,\n               9.0496e-03,  9.7339e-03, -1.8580e-03, -1.3124e-02,  5.0225e-03,\n              -9.9864e-03, -1.0366e-02,  1.0732e-02, -9.0147e-03, -3.3104e-03,\n               5.7370e-03,  1.2810e-02,  6.6145e-03,  3.7765e-03,  6.4686e-03,\n               7.4854e-04, -6.4215e-03,  7.6079e-03,  6.4888e-03, -1.3747e-02,\n              -8.7761e-04, -6.8510e-03, -8.4090e-04, -9.0012e-03,  7.4159e-03,\n              -6.4797e-03, -7.6395e-03,  6.6048e-05,  1.0434e-03, -8.2807e-04,\n              -1.6183e-02, -2.9183e-03, -2.8076e-04,  8.0383e-03, -7.3498e-03,\n               3.2183e-03,  5.3177e-03, -5.8649e-03, -1.6127e-02, -1.1047e-02,\n               8.5331e-03, -3.1745e-03,  6.7023e-04,  1.2242e-02, -4.8020e-05,\n               2.3964e-03,  9.4514e-03,  9.7995e-05, -1.3442e-02, -9.5764e-03,\n               1.0707e-03,  1.1284e-02, -5.8928e-03,  6.7230e-03,  9.4321e-03,\n               1.8222e-02,  1.2226e-03,  4.7751e-04, -3.3182e-03, -2.7290e-03,\n               9.1602e-03, -9.6307e-03, -1.2832e-02,  8.8299e-03,  4.2851e-03,\n               4.2486e-03, -9.8128e-03,  1.2675e-02,  1.2149e-02, -3.8731e-04,\n              -1.8730e-04,  5.6303e-03, -9.2608e-03, -1.4718e-02,  3.8524e-04,\n               1.2102e-02, -4.3037e-03, -4.6630e-03,  5.2095e-03, -3.2355e-03,\n               2.7637e-03,  9.0309e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9013, 0.9017, 0.9012, 0.9011, 0.9012, 0.9012, 0.9014, 0.9011, 0.9017,\n              0.9014, 0.9016, 0.9012, 0.9013, 0.9011, 0.9015, 0.9016, 0.9011, 0.9012,\n              0.9014, 0.9012, 0.9011, 0.9013, 0.9013, 0.9012, 0.9018, 0.9012, 0.9015,\n              0.9010, 0.9013, 0.9011, 0.9014, 0.9013, 0.9013, 0.9012, 0.9015, 0.9012,\n              0.9012, 0.9012, 0.9011, 0.9016, 0.9015, 0.9013, 0.9011, 0.9014, 0.9015,\n              0.9017, 0.9012, 0.9014, 0.9010, 0.9014, 0.9014, 0.9012, 0.9015, 0.9011,\n              0.9013, 0.9015, 0.9011, 0.9016, 0.9012, 0.9012, 0.9015, 0.9014, 0.9016,\n              0.9015, 0.9012, 0.9013, 0.9012, 0.9012, 0.9016, 0.9014, 0.9012, 0.9014,\n              0.9017, 0.9013, 0.9012, 0.9011, 0.9011, 0.9011, 0.9015, 0.9011, 0.9014,\n              0.9011, 0.9011, 0.9015, 0.9014, 0.9010, 0.9016, 0.9013, 0.9015, 0.9013,\n              0.9012, 0.9013, 0.9012, 0.9012, 0.9013, 0.9011, 0.9015, 0.9013, 0.9012,\n              0.9013, 0.9014, 0.9011, 0.9011, 0.9012, 0.9013, 0.9011, 0.9013, 0.9013,\n              0.9013, 0.9011, 0.9014, 0.9014, 0.9013, 0.9014, 0.9011, 0.9014, 0.9012,\n              0.9013, 0.9015, 0.9012, 0.9011, 0.9013, 0.9013, 0.9011, 0.9017, 0.9014,\n              0.9013, 0.9012, 0.9012, 0.9011, 0.9015, 0.9014, 0.9016, 0.9015, 0.9012,\n              0.9020, 0.9011, 0.9015, 0.9015, 0.9009, 0.9017, 0.9012, 0.9012, 0.9013,\n              0.9014, 0.9010, 0.9011, 0.9012, 0.9014, 0.9012, 0.9011, 0.9014, 0.9012,\n              0.9013, 0.9011, 0.9011, 0.9011, 0.9012, 0.9013, 0.9011, 0.9014, 0.9010,\n              0.9014, 0.9016, 0.9013, 0.9012, 0.9012, 0.9009, 0.9013, 0.9013, 0.9012,\n              0.9014, 0.9010, 0.9012, 0.9011, 0.9014, 0.9011, 0.9013, 0.9014, 0.9012,\n              0.9014, 0.9011, 0.9011, 0.9010, 0.9011, 0.9015, 0.9013, 0.9013, 0.9012,\n              0.9011, 0.9015, 0.9011, 0.9013, 0.9013, 0.9011, 0.9016, 0.9009, 0.9014,\n              0.9014, 0.9017, 0.9011, 0.9014, 0.9014, 0.9013, 0.9016, 0.9015, 0.9012,\n              0.9014, 0.9015, 0.9011, 0.9015, 0.9011, 0.9012, 0.9013, 0.9013, 0.9010,\n              0.9012, 0.9012, 0.9011, 0.9013, 0.9013, 0.9016, 0.9010, 0.9011, 0.9011,\n              0.9015, 0.9010, 0.9013, 0.9009, 0.9012, 0.9010, 0.9015, 0.9012, 0.9012,\n              0.9011, 0.9011, 0.9015, 0.9013, 0.9015, 0.9014, 0.9011, 0.9011, 0.9012,\n              0.9012, 0.9009, 0.9012, 0.9010, 0.9015, 0.9012, 0.9017, 0.9011, 0.9016,\n              0.9016, 0.9013, 0.9012, 0.9012, 0.9012, 0.9011, 0.9013, 0.9011, 0.9011,\n              0.9014, 0.9011, 0.9011, 0.9013, 0.9015, 0.9012, 0.9010, 0.9015, 0.9013,\n              0.9012, 0.9011, 0.9011, 0.9012, 0.9011, 0.9012, 0.9011, 0.9011, 0.9012,\n              0.9015, 0.9012, 0.9011, 0.9013, 0.9011, 0.9014, 0.9011, 0.9010, 0.9016,\n              0.9014, 0.9016, 0.9011, 0.9012, 0.9013, 0.9012, 0.9012, 0.9011, 0.9014,\n              0.9012, 0.9013, 0.9016, 0.9012, 0.9014, 0.9013, 0.9014, 0.9013, 0.9012,\n              0.9015, 0.9014, 0.9011, 0.9016, 0.9010, 0.9012, 0.9016, 0.9011, 0.9011,\n              0.9020, 0.9014, 0.9013, 0.9013, 0.9012, 0.9011, 0.9012, 0.9018, 0.9018,\n              0.9011, 0.9010, 0.9016, 0.9013, 0.9014, 0.9015, 0.9013, 0.9012, 0.9012,\n              0.9013, 0.9010, 0.9010, 0.9015, 0.9011, 0.9013, 0.9013, 0.9012, 0.9014,\n              0.9015, 0.9016, 0.9012, 0.9014, 0.9012, 0.9011, 0.9015, 0.9013, 0.9013,\n              0.9011, 0.9013, 0.9014, 0.9012, 0.9012, 0.9014, 0.9011, 0.9015, 0.9016,\n              0.9017, 0.9011, 0.9007, 0.9010, 0.9017, 0.9012, 0.9011, 0.9014, 0.9013,\n              0.9011, 0.9011, 0.9012, 0.9013, 0.9011, 0.9011, 0.9015, 0.9011, 0.9012,\n              0.9012, 0.9011, 0.9013, 0.9015, 0.9014, 0.9013, 0.9012, 0.9013, 0.9013,\n              0.9012, 0.9012, 0.9011, 0.9016, 0.9012, 0.9013, 0.9011, 0.9013, 0.9015,\n              0.9011, 0.9013, 0.9012, 0.9013, 0.9016, 0.9012, 0.9013, 0.9011, 0.9013,\n              0.9010, 0.9012, 0.9013, 0.9013, 0.9012, 0.9016, 0.9013, 0.9013, 0.9010,\n              0.9012, 0.9015, 0.9011, 0.9010, 0.9011, 0.9015, 0.9013, 0.9013, 0.9015,\n              0.9012, 0.9013, 0.9011, 0.9012, 0.9012, 0.9012, 0.9012, 0.9011, 0.9012,\n              0.9013, 0.9016, 0.9015, 0.9013, 0.9012, 0.9013, 0.9014, 0.9014, 0.9012,\n              0.9013, 0.9014, 0.9011, 0.9012, 0.9013, 0.9012, 0.9016, 0.9009, 0.9015,\n              0.9014, 0.9011, 0.9013, 0.9014, 0.9013, 0.9013, 0.9015, 0.9012, 0.9011,\n              0.9016, 0.9014, 0.9016, 0.9011, 0.9014, 0.9014, 0.9012, 0.9013, 0.9013,\n              0.9020, 0.9012, 0.9013, 0.9011, 0.9012, 0.9011, 0.9012, 0.9011, 0.9014,\n              0.9013, 0.9014, 0.9015, 0.9016, 0.9010, 0.9009, 0.9012, 0.9017, 0.9012,\n              0.9013, 0.9011, 0.9014, 0.9011, 0.9014, 0.9011, 0.9012, 0.9011, 0.9016,\n              0.9013, 0.9015, 0.9012, 0.9013, 0.9011, 0.9012, 0.9014, 0.9012, 0.9015,\n              0.9011, 0.9022, 0.9016, 0.9012, 0.9011, 0.9011, 0.9014, 0.9013],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[-0.0400]],\n        \n                 [[ 0.0083]],\n        \n                 [[-0.0421]],\n        \n                 ...,\n        \n                 [[-0.0060]],\n        \n                 [[-0.0403]],\n        \n                 [[-0.0247]]],\n        \n        \n                [[[ 0.0076]],\n        \n                 [[-0.0594]],\n        \n                 [[-0.0221]],\n        \n                 ...,\n        \n                 [[-0.0424]],\n        \n                 [[ 0.0332]],\n        \n                 [[-0.0562]]],\n        \n        \n                [[[-0.0177]],\n        \n                 [[ 0.0151]],\n        \n                 [[ 0.0522]],\n        \n                 ...,\n        \n                 [[-0.0248]],\n        \n                 [[ 0.0595]],\n        \n                 [[ 0.0506]]],\n        \n        \n                ...,\n        \n        \n                [[[-0.0154]],\n        \n                 [[ 0.0129]],\n        \n                 [[-0.0322]],\n        \n                 ...,\n        \n                 [[-0.0358]],\n        \n                 [[ 0.0022]],\n        \n                 [[-0.0130]]],\n        \n        \n                [[[-0.0576]],\n        \n                 [[-0.0245]],\n        \n                 [[-0.0058]],\n        \n                 ...,\n        \n                 [[ 0.0334]],\n        \n                 [[ 0.0256]],\n        \n                 [[ 0.0617]]],\n        \n        \n                [[[ 0.0564]],\n        \n                 [[ 0.0051]],\n        \n                 [[-0.0235]],\n        \n                 ...,\n        \n                 [[ 0.0237]],\n        \n                 [[ 0.0207]],\n        \n                 [[-0.0434]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0443, -0.0823, -0.0205, -0.0298,  0.1059,  0.0158,  0.0271,  0.0368,\n                 0.0071,  0.0386,  0.0519, -0.0804,  0.0191, -0.0059,  0.0163,  0.0139,\n                -0.0203, -0.0330, -0.0118,  0.0688,  0.0156, -0.0020,  0.0048,  0.0097,\n                 0.0087,  0.0119, -0.0044,  0.0235, -0.0419, -0.0165, -0.0002,  0.0359,\n                 0.0320, -0.0627,  0.0220,  0.0056,  0.0209, -0.0242, -0.0099,  0.0927,\n                -0.0140, -0.1072, -0.0522,  0.0521,  0.0638,  0.0031,  0.0381, -0.0200,\n                 0.0459, -0.0290, -0.0573,  0.0325, -0.0446,  0.0174,  0.0442, -0.0541,\n                -0.0144, -0.0371,  0.0062,  0.0630, -0.0499, -0.0211,  0.0395,  0.0121,\n                 0.0528, -0.0133,  0.0149, -0.0183,  0.0128, -0.0033,  0.0286, -0.0140,\n                 0.0063, -0.0089, -0.0597, -0.0267,  0.0007, -0.0239,  0.0180, -0.0440,\n                -0.0433,  0.0631, -0.0017,  0.0586, -0.0122, -0.0144,  0.0321, -0.0575,\n                -0.0617, -0.0519,  0.0024,  0.0224, -0.0546,  0.0213,  0.0143, -0.0156,\n                -0.0264, -0.0106, -0.0293, -0.0074, -0.0312, -0.0040, -0.0152, -0.0392,\n                 0.0178, -0.0521,  0.0066,  0.0022, -0.0512, -0.0103, -0.0023,  0.0307,\n                -0.0053,  0.0327,  0.0502, -0.0135, -0.0182, -0.0421, -0.0092, -0.0142,\n                 0.0584,  0.0652, -0.0834,  0.0644, -0.0032,  0.0269, -0.0621, -0.0822,\n                 0.0383, -0.0169,  0.0628,  0.0658, -0.0297, -0.0197, -0.0396, -0.0026,\n                -0.0457,  0.0237, -0.0397, -0.0388, -0.0537, -0.0193,  0.0109,  0.0481,\n                 0.0090,  0.0230, -0.0009,  0.0299,  0.0213, -0.0124,  0.0082, -0.0380,\n                 0.0243, -0.0579,  0.0037, -0.0054,  0.0124,  0.0354, -0.0271, -0.0141,\n                 0.0103,  0.0344,  0.0280, -0.0131,  0.0007, -0.0048,  0.0107, -0.0460,\n                -0.0304,  0.0682, -0.0125, -0.0064,  0.0163, -0.0561,  0.0578,  0.0105,\n                -0.0187,  0.0981,  0.0511,  0.0419,  0.0176,  0.0273,  0.0050, -0.0095,\n                -0.0376,  0.0367, -0.0556, -0.0232, -0.0340, -0.0029,  0.0047, -0.0079,\n                -0.0234, -0.0130,  0.0110,  0.0035,  0.0535,  0.0251, -0.0417,  0.0014,\n                 0.0528, -0.0071, -0.0506, -0.0318,  0.0365,  0.0352, -0.0629, -0.0610,\n                -0.0662,  0.0171,  0.0281, -0.0338,  0.0380,  0.0565, -0.0071,  0.0430,\n                -0.0282,  0.0014, -0.0074, -0.0322,  0.0639, -0.0187, -0.0591,  0.0216,\n                 0.0607, -0.0021, -0.0574,  0.0079,  0.0284,  0.0311, -0.0252,  0.0214,\n                 0.0055,  0.0465,  0.0421,  0.0272,  0.0219,  0.0058,  0.0642,  0.0354,\n                -0.0079,  0.0307,  0.0191, -0.0428,  0.0138,  0.0708, -0.0311,  0.0014,\n                -0.0272, -0.0334, -0.0664, -0.1053, -0.0159,  0.0503,  0.0792,  0.0091,\n                -0.0174,  0.0040, -0.0692,  0.0045, -0.0478,  0.0220,  0.0424,  0.0275,\n                 0.0122,  0.0249, -0.0088,  0.0271,  0.0493, -0.0208, -0.0106, -0.0081,\n                -0.0191, -0.0042,  0.0312,  0.0155, -0.0891, -0.0053, -0.0250, -0.0275,\n                -0.0412,  0.0155, -0.0312,  0.0126, -0.0164, -0.0329, -0.0150, -0.0467,\n                -0.0070, -0.0152,  0.0162,  0.0560,  0.0644, -0.0112,  0.0496,  0.0166,\n                 0.1293,  0.0053,  0.0663,  0.0270, -0.0356,  0.0058, -0.0124,  0.0009,\n                 0.0422, -0.0043,  0.0604, -0.0053, -0.0377,  0.0382,  0.0052,  0.0458,\n                -0.0255, -0.0447,  0.0676,  0.0409,  0.0170, -0.0068, -0.0444,  0.0128,\n                 0.0192,  0.0093,  0.0525,  0.0467, -0.0540, -0.0428,  0.0234,  0.0727,\n                 0.0125,  0.0088,  0.0435, -0.0341, -0.0638,  0.0402, -0.0072,  0.0317,\n                -0.0903, -0.0310, -0.0200, -0.0841, -0.0192, -0.0413,  0.0110, -0.0462,\n                 0.0485, -0.0235, -0.0278,  0.0039, -0.0443, -0.0174, -0.0101,  0.0207,\n                 0.0065,  0.0602, -0.0083, -0.1311,  0.0651, -0.0258,  0.0136,  0.0070,\n                -0.0789, -0.0414,  0.0006, -0.0091,  0.0221, -0.0267, -0.0045,  0.0158,\n                 0.0925,  0.0038,  0.0096, -0.0173,  0.0138,  0.0334,  0.0207, -0.0135,\n                 0.0178, -0.0311, -0.0247,  0.0509, -0.0094,  0.0289,  0.0692, -0.0349,\n                 0.0620, -0.0021,  0.0645,  0.0542,  0.0385, -0.0124,  0.0288, -0.0316,\n                 0.0487, -0.0070, -0.0764,  0.0522,  0.0045, -0.0120,  0.0025,  0.0043,\n                -0.0130,  0.0175, -0.0244, -0.0145,  0.0015, -0.0650,  0.0857,  0.0234,\n                 0.0012, -0.0388, -0.0145, -0.0342, -0.0591, -0.0208,  0.0614, -0.0487,\n                 0.0408, -0.0378,  0.0705,  0.0253, -0.0201, -0.0145, -0.0831,  0.0422,\n                -0.0521, -0.0669,  0.0057, -0.0464, -0.0038, -0.0095, -0.0582,  0.0398,\n                 0.0337,  0.0481, -0.0466, -0.0292, -0.0249, -0.0004, -0.0274,  0.0058,\n                 0.0072, -0.0523,  0.0617, -0.0343, -0.0298, -0.0329,  0.0364,  0.0202,\n                -0.0029,  0.0837,  0.0270, -0.0120, -0.0950, -0.0337, -0.0260,  0.0430,\n                 0.0553,  0.0345, -0.0257, -0.0407, -0.0047,  0.0131,  0.0217,  0.0160,\n                 0.0157, -0.0201, -0.0070, -0.0249, -0.0574, -0.0611, -0.0060,  0.0387,\n                -0.0575, -0.1146, -0.0169, -0.0517,  0.0393, -0.0686, -0.0245,  0.0465,\n                 0.0104, -0.0309,  0.0412,  0.0074,  0.0959, -0.0334, -0.0387, -0.0123,\n                -0.0023,  0.0087, -0.0901, -0.0455, -0.0041, -0.0140, -0.0117,  0.0735,\n                 0.0117, -0.0020, -0.0087, -0.0645,  0.0110, -0.0008,  0.0432,  0.0017,\n                 0.0474, -0.0171, -0.0131,  0.0055, -0.0024, -0.0073, -0.0388,  0.0353],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9111, 0.9289, 0.9215, 0.9133, 0.9255, 0.9157, 0.9164, 0.9162, 0.9148,\n                0.9158, 0.9127, 0.9250, 0.9169, 0.9194, 0.9138, 0.9175, 0.9174, 0.9180,\n                0.9131, 0.9249, 0.9173, 0.9175, 0.9256, 0.9170, 0.9177, 0.9148, 0.9141,\n                0.9179, 0.9129, 0.9181, 0.9165, 0.9112, 0.9171, 0.9197, 0.9175, 0.9166,\n                0.9155, 0.9170, 0.9238, 0.9165, 0.9162, 0.9269, 0.9238, 0.9248, 0.9144,\n                0.9152, 0.9192, 0.9155, 0.9161, 0.9264, 0.9127, 0.9140, 0.9130, 0.9196,\n                0.9151, 0.9167, 0.9190, 0.9172, 0.9228, 0.9135, 0.9172, 0.9229, 0.9181,\n                0.9152, 0.9171, 0.9270, 0.9125, 0.9140, 0.9144, 0.9178, 0.9187, 0.9216,\n                0.9164, 0.9180, 0.9216, 0.9405, 0.9141, 0.9137, 0.9125, 0.9191, 0.9199,\n                0.9265, 0.9135, 0.9202, 0.9162, 0.9138, 0.9200, 0.9198, 0.9180, 0.9158,\n                0.9175, 0.9205, 0.9179, 0.9159, 0.9175, 0.9157, 0.9211, 0.9158, 0.9220,\n                0.9137, 0.9174, 0.9158, 0.9207, 0.9168, 0.9138, 0.9170, 0.9154, 0.9137,\n                0.9282, 0.9146, 0.9304, 0.9137, 0.9156, 0.9147, 0.9148, 0.9203, 0.9131,\n                0.9141, 0.9166, 0.9189, 0.9252, 0.9216, 0.9165, 0.9161, 0.9155, 0.9185,\n                0.9197, 0.9244, 0.9229, 0.9244, 0.9230, 0.9173, 0.9193, 0.9196, 0.9177,\n                0.9220, 0.9231, 0.9172, 0.9152, 0.9181, 0.9322, 0.9156, 0.9186, 0.9165,\n                0.9124, 0.9198, 0.9229, 0.9131, 0.9169, 0.9166, 0.9141, 0.9134, 0.9286,\n                0.9156, 0.9158, 0.9160, 0.9174, 0.9133, 0.9139, 0.9202, 0.9125, 0.9177,\n                0.9181, 0.9152, 0.9163, 0.9114, 0.9196, 0.9149, 0.9137, 0.9192, 0.9159,\n                0.9185, 0.9185, 0.9183, 0.9150, 0.9222, 0.9147, 0.9189, 0.9146, 0.9179,\n                0.9140, 0.9166, 0.9126, 0.9138, 0.9188, 0.9188, 0.9191, 0.9154, 0.9224,\n                0.9181, 0.9195, 0.9179, 0.9135, 0.9177, 0.9134, 0.9171, 0.9143, 0.9177,\n                0.9172, 0.9144, 0.9114, 0.9181, 0.9211, 0.9216, 0.9200, 0.9155, 0.9210,\n                0.9216, 0.9174, 0.9201, 0.9123, 0.9223, 0.9130, 0.9138, 0.9151, 0.9205,\n                0.9226, 0.9142, 0.9177, 0.9187, 0.9210, 0.9113, 0.9145, 0.9153, 0.9157,\n                0.9163, 0.9173, 0.9161, 0.9200, 0.9188, 0.9182, 0.9134, 0.9215, 0.9224,\n                0.9181, 0.9322, 0.9177, 0.9162, 0.9151, 0.9146, 0.9160, 0.9147, 0.9156,\n                0.9167, 0.9205, 0.9200, 0.9138, 0.9156, 0.9131, 0.9214, 0.9172, 0.9244,\n                0.9225, 0.9126, 0.9133, 0.9172, 0.9142, 0.9209, 0.9218, 0.9193, 0.9174,\n                0.9136, 0.9215, 0.9282, 0.9166, 0.9194, 0.9163, 0.9176, 0.9151, 0.9171,\n                0.9158, 0.9158, 0.9151, 0.9146, 0.9247, 0.9221, 0.9194, 0.9139, 0.9148,\n                0.9175, 0.9198, 0.9138, 0.9163, 0.9137, 0.9260, 0.9232, 0.9113, 0.9146,\n                0.9186, 0.9185, 0.9175, 0.9147, 0.9170, 0.9174, 0.9173, 0.9156, 0.9272,\n                0.9196, 0.9150, 0.9208, 0.9173, 0.9295, 0.9163, 0.9183, 0.9130, 0.9158,\n                0.9254, 0.9190, 0.9111, 0.9185, 0.9168, 0.9124, 0.9169, 0.9128, 0.9179,\n                0.9202, 0.9163, 0.9188, 0.9191, 0.9155, 0.9160, 0.9156, 0.9173, 0.9216,\n                0.9268, 0.9211, 0.9260, 0.9181, 0.9150, 0.9171, 0.9140, 0.9136, 0.9142,\n                0.9174, 0.9144, 0.9250, 0.9129, 0.9144, 0.9152, 0.9251, 0.9200, 0.9247,\n                0.9147, 0.9147, 0.9177, 0.9178, 0.9238, 0.9181, 0.9242, 0.9157, 0.9153,\n                0.9150, 0.9110, 0.9175, 0.9152, 0.9188, 0.9163, 0.9180, 0.9174, 0.9272,\n                0.9286, 0.9279, 0.9166, 0.9172, 0.9139, 0.9149, 0.9153, 0.9143, 0.9179,\n                0.9177, 0.9131, 0.9121, 0.9227, 0.9155, 0.9181, 0.9158, 0.9195, 0.9110,\n                0.9134, 0.9129, 0.9143, 0.9219, 0.9196, 0.9123, 0.9183, 0.9114, 0.9202,\n                0.9149, 0.9149, 0.9168, 0.9199, 0.9171, 0.9138, 0.9181, 0.9162, 0.9186,\n                0.9136, 0.9327, 0.9115, 0.9181, 0.9198, 0.9128, 0.9185, 0.9146, 0.9185,\n                0.9190, 0.9166, 0.9150, 0.9212, 0.9159, 0.9122, 0.9170, 0.9192, 0.9146,\n                0.9147, 0.9185, 0.9167, 0.9151, 0.9137, 0.9163, 0.9170, 0.9141, 0.9123,\n                0.9163, 0.9165, 0.9144, 0.9135, 0.9155, 0.9156, 0.9165, 0.9136, 0.9129,\n                0.9197, 0.9170, 0.9183, 0.9162, 0.9161, 0.9114, 0.9184, 0.9157, 0.9197,\n                0.9153, 0.9161, 0.9173, 0.9206, 0.9150, 0.9169, 0.9397, 0.9192, 0.9197,\n                0.9122, 0.9193, 0.9148, 0.9161, 0.9149, 0.9135, 0.9179, 0.9114, 0.9150,\n                0.9139, 0.9136, 0.9178, 0.9185, 0.9162, 0.9157, 0.9178, 0.9175, 0.9173,\n                0.9239, 0.9160, 0.9247, 0.9180, 0.9178, 0.9164, 0.9287, 0.9229, 0.9220,\n                0.9161, 0.9179, 0.9291, 0.9216, 0.9177, 0.9236, 0.9158, 0.9314, 0.9177,\n                0.9151, 0.9217, 0.9260, 0.9158, 0.9172, 0.9169, 0.9129, 0.9145, 0.9118,\n                0.9218, 0.9117, 0.9117, 0.9131, 0.9235, 0.9159, 0.9207, 0.9222, 0.9194,\n                0.9172, 0.9158, 0.9152, 0.9254, 0.9146, 0.9255, 0.9180, 0.9157],\n               grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [512, 256, 3, 3]], ["0.bn1.weight", [512]], ["0.bn1.bias", [512]], ["0.conv2.weight", [512, 512, 3, 3]], ["0.bn2.weight", [512]], ["0.bn2.bias", [512]], ["0.downsample.0.weight", [512, 256, 1, 1]], ["0.downsample.1.weight", [512]], ["0.downsample.1.bias", [512]]], "output_shape": [[512, 512, 1, 1]], "num_parameters": [1179648, 512, 512, 2359296, 512, 512, 131072, 512, 512]}, {"name": "avgpool", "id": 140621193487792, "class_name": "AveragePool()", "parameters": [], "output_shape": [[512, 512]], "num_parameters": []}, {"name": "fc", "id": 140621193486736, "class_name": "Linear(in_features=512, out_features=10, bias=True)", "parameters": [["weight", [10, 512]], ["bias", [10]]], "output_shape": [[512, 10]], "num_parameters": [5120, 10]}], "edges": []}