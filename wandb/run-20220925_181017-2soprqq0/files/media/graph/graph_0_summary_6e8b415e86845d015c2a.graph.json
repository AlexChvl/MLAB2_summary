{"format": "torch", "nodes": [{"name": "conv1", "id": 140584624434960, "class_name": "Conv2d(\n  self.stride=2, self.padding=(3, 3), self.weight=Parameter containing:\n  tensor([[[[-0.0403,  0.0216, -0.0759,  ..., -0.0326, -0.0009,  0.0430],\n            [-0.0364, -0.0112,  0.0742,  ...,  0.0753, -0.0778,  0.0681],\n            [-0.0105, -0.0091,  0.0095,  ...,  0.0650, -0.0205, -0.0601],\n            ...,\n            [-0.0606,  0.0741,  0.0719,  ..., -0.0675, -0.0624, -0.0646],\n            [-0.0042, -0.0334, -0.0540,  ..., -0.0283, -0.0190, -0.0066],\n            [-0.0682,  0.0182, -0.0819,  ..., -0.0652,  0.0823,  0.0480]],\n  \n           [[-0.0589,  0.0706, -0.0264,  ...,  0.0791,  0.0726, -0.0253],\n            [-0.0770,  0.0484, -0.0235,  ..., -0.0133,  0.0492, -0.0688],\n            [-0.0109, -0.0376, -0.0560,  ..., -0.0442,  0.0438,  0.0059],\n            ...,\n            [ 0.0519, -0.0727, -0.0718,  ..., -0.0147, -0.0127,  0.0081],\n            [-0.0274,  0.0805, -0.0391,  ...,  0.0036, -0.0659, -0.0734],\n            [-0.0108, -0.0377, -0.0196,  ...,  0.0482,  0.0318, -0.0590]],\n  \n           [[ 0.0193,  0.0508,  0.0125,  ..., -0.0271,  0.0369, -0.0042],\n            [-0.0287, -0.0728,  0.0179,  ...,  0.0117, -0.0367, -0.0453],\n            [-0.0382,  0.0213,  0.0551,  ...,  0.0665,  0.0614, -0.0733],\n            ...,\n            [ 0.0319,  0.0368,  0.0582,  ..., -0.0537, -0.0728,  0.0537],\n            [ 0.0745, -0.0568, -0.0786,  ...,  0.0540, -0.0531, -0.0594],\n            [ 0.0705, -0.0279, -0.0068,  ..., -0.0693, -0.0775, -0.0265]]],\n  \n  \n          [[[ 0.0094, -0.0657, -0.0100,  ...,  0.0541, -0.0532, -0.0092],\n            [-0.0395, -0.0695,  0.0361,  ..., -0.0609,  0.0735, -0.0022],\n            [ 0.0594,  0.0034,  0.0023,  ...,  0.0553,  0.0597, -0.0567],\n            ...,\n            [ 0.0369, -0.0016, -0.0176,  ..., -0.0268, -0.0391, -0.0581],\n            [-0.0461,  0.0646,  0.0249,  ...,  0.0578, -0.0729, -0.0171],\n            [ 0.0645, -0.0190,  0.0552,  ...,  0.0590, -0.0274, -0.0081]],\n  \n           [[-0.0072,  0.0437,  0.0274,  ...,  0.0622,  0.0771, -0.0528],\n            [-0.0272, -0.0703,  0.0669,  ...,  0.0496, -0.0591, -0.0735],\n            [-0.0732, -0.0283,  0.0466,  ..., -0.0289,  0.0236,  0.0346],\n            ...,\n            [-0.0579, -0.0458,  0.0113,  ...,  0.0635, -0.0791,  0.0571],\n            [ 0.0720,  0.0193,  0.0316,  ...,  0.0584,  0.0255, -0.0316],\n            [-0.0234, -0.0384, -0.0120,  ...,  0.0192,  0.0631, -0.0686]],\n  \n           [[-0.0328,  0.0123,  0.0717,  ...,  0.0336,  0.0087,  0.0278],\n            [ 0.0205,  0.0762, -0.0732,  ..., -0.0279,  0.0327, -0.0734],\n            [ 0.0777, -0.0603, -0.0538,  ...,  0.0690, -0.0203,  0.0589],\n            ...,\n            [-0.0517,  0.0529,  0.0530,  ...,  0.0409,  0.0643,  0.0033],\n            [ 0.0788, -0.0029,  0.0370,  ..., -0.0231,  0.0055, -0.0528],\n            [ 0.0189, -0.0573,  0.0818,  ...,  0.0264,  0.0159, -0.0430]]],\n  \n  \n          [[[ 0.0084,  0.0525, -0.0306,  ..., -0.0661, -0.0391, -0.0266],\n            [-0.0585, -0.0011,  0.0314,  ...,  0.0095,  0.0792,  0.0786],\n            [-0.0532, -0.0052,  0.0100,  ...,  0.0762,  0.0158, -0.0403],\n            ...,\n            [-0.0241,  0.0169,  0.0015,  ..., -0.0750,  0.0807,  0.0726],\n            [-0.0526,  0.0589,  0.0724,  ..., -0.0426, -0.0321, -0.0348],\n            [ 0.0729, -0.0754,  0.0760,  ...,  0.0409, -0.0794, -0.0626]],\n  \n           [[ 0.0383, -0.0075, -0.0452,  ...,  0.0290,  0.0442, -0.0652],\n            [ 0.0239, -0.0538, -0.0048,  ...,  0.0282, -0.0651, -0.0120],\n            [ 0.0290,  0.0347,  0.0111,  ...,  0.0121,  0.0012, -0.0671],\n            ...,\n            [ 0.0431,  0.0667, -0.0596,  ...,  0.0727,  0.0221,  0.0089],\n            [-0.0824,  0.0127, -0.0172,  ..., -0.0386, -0.0560,  0.0284],\n            [-0.0516, -0.0220,  0.0764,  ...,  0.0745, -0.0363, -0.0390]],\n  \n           [[-0.0682, -0.0190,  0.0313,  ...,  0.0481, -0.0552,  0.0034],\n            [ 0.0793,  0.0022, -0.0704,  ..., -0.0529, -0.0330,  0.0097],\n            [ 0.0535,  0.0066,  0.0610,  ..., -0.0548, -0.0538, -0.0074],\n            ...,\n            [-0.0741,  0.0457,  0.0621,  ...,  0.0142,  0.0099,  0.0749],\n            [-0.0439, -0.0775, -0.0611,  ...,  0.0347,  0.0665,  0.0242],\n            [ 0.0018,  0.0406,  0.0344,  ...,  0.0681, -0.0254, -0.0788]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0752, -0.0057, -0.0528,  ..., -0.0468, -0.0238, -0.0274],\n            [ 0.0780, -0.0726,  0.0350,  ..., -0.0332,  0.0130,  0.0030],\n            [ 0.0760, -0.0538,  0.0083,  ...,  0.0277, -0.0078,  0.0491],\n            ...,\n            [ 0.0780,  0.0725,  0.0180,  ..., -0.0510, -0.0417,  0.0112],\n            [ 0.0380, -0.0504,  0.0033,  ..., -0.0257, -0.0436,  0.0611],\n            [-0.0351, -0.0335,  0.0287,  ...,  0.0669,  0.0725,  0.0168]],\n  \n           [[ 0.0603, -0.0058,  0.0278,  ..., -0.0552,  0.0611, -0.0210],\n            [ 0.0267, -0.0239,  0.0446,  ..., -0.0334,  0.0234,  0.0262],\n            [ 0.0743,  0.0036, -0.0054,  ..., -0.0551,  0.0292, -0.0361],\n            ...,\n            [ 0.0474,  0.0712, -0.0739,  ..., -0.0641, -0.0005, -0.0165],\n            [-0.0043,  0.0431,  0.0671,  ...,  0.0059,  0.0761,  0.0611],\n            [-0.0196, -0.0070, -0.0336,  ...,  0.0538,  0.0011, -0.0526]],\n  \n           [[-0.0292,  0.0362,  0.0731,  ..., -0.0373,  0.0153, -0.0147],\n            [ 0.0778,  0.0337,  0.0536,  ..., -0.0249,  0.0662, -0.0642],\n            [-0.0745,  0.0247, -0.0448,  ..., -0.0683, -0.0328,  0.0017],\n            ...,\n            [-0.0715,  0.0746,  0.0362,  ...,  0.0639, -0.0037, -0.0731],\n            [ 0.0450,  0.0637,  0.0439,  ..., -0.0595, -0.0225,  0.0423],\n            [ 0.0001, -0.0543,  0.0522,  ..., -0.0322,  0.0614, -0.0370]]],\n  \n  \n          [[[-0.0474, -0.0258, -0.0808,  ..., -0.0174, -0.0528,  0.0014],\n            [ 0.0245,  0.0302, -0.0102,  ..., -0.0430,  0.0605,  0.0408],\n            [-0.0687,  0.0047, -0.0120,  ..., -0.0609,  0.0781, -0.0438],\n            ...,\n            [ 0.0762, -0.0205, -0.0633,  ..., -0.0078, -0.0437,  0.0301],\n            [ 0.0439,  0.0715,  0.0218,  ..., -0.0366, -0.0655, -0.0312],\n            [ 0.0673,  0.0339,  0.0176,  ...,  0.0543,  0.0302, -0.0410]],\n  \n           [[-0.0761,  0.0347, -0.0359,  ...,  0.0157, -0.0319,  0.0136],\n            [ 0.0821,  0.0119, -0.0046,  ...,  0.0391, -0.0180, -0.0803],\n            [ 0.0332, -0.0023, -0.0606,  ..., -0.0680, -0.0592, -0.0271],\n            ...,\n            [-0.0300, -0.0821,  0.0725,  ..., -0.0444,  0.0227, -0.0015],\n            [ 0.0712, -0.0242, -0.0112,  ...,  0.0331,  0.0290,  0.0350],\n            [-0.0747,  0.0685, -0.0390,  ..., -0.0420,  0.0216, -0.0643]],\n  \n           [[ 0.0691,  0.0082, -0.0434,  ...,  0.0265, -0.0187,  0.0239],\n            [ 0.0659,  0.0340,  0.0809,  ..., -0.0591, -0.0544,  0.0585],\n            [-0.0040, -0.0801, -0.0017,  ...,  0.0030,  0.0664,  0.0572],\n            ...,\n            [-0.0302,  0.0610,  0.0065,  ...,  0.0261,  0.0419,  0.0040],\n            [ 0.0439, -0.0491,  0.0005,  ..., -0.0329, -0.0162, -0.0079],\n            [-0.0476, -0.0806, -0.0315,  ...,  0.0083,  0.0377,  0.0493]]],\n  \n  \n          [[[ 0.0724, -0.0777,  0.0651,  ...,  0.0404,  0.0192,  0.0609],\n            [-0.0593, -0.0568, -0.0516,  ..., -0.0107,  0.0274,  0.0052],\n            [-0.0220,  0.0656,  0.0246,  ..., -0.0394,  0.0082, -0.0590],\n            ...,\n            [-0.0750,  0.0539,  0.0486,  ..., -0.0103,  0.0424, -0.0347],\n            [-0.0085,  0.0806,  0.0427,  ..., -0.0792, -0.0110,  0.0290],\n            [-0.0629, -0.0615,  0.0491,  ...,  0.0382,  0.0804, -0.0390]],\n  \n           [[ 0.0795, -0.0511,  0.0493,  ..., -0.0546, -0.0529,  0.0821],\n            [ 0.0333, -0.0821, -0.0721,  ...,  0.0089,  0.0658,  0.0573],\n            [ 0.0098, -0.0397,  0.0695,  ..., -0.0783, -0.0289,  0.0690],\n            ...,\n            [-0.0619, -0.0459, -0.0070,  ...,  0.0720,  0.0608,  0.0607],\n            [ 0.0823, -0.0523,  0.0072,  ..., -0.0044,  0.0562,  0.0019],\n            [-0.0549, -0.0493,  0.0348,  ...,  0.0760,  0.0146, -0.0145]],\n  \n           [[ 0.0527,  0.0097,  0.0161,  ...,  0.0107, -0.0031,  0.0028],\n            [ 0.0071,  0.0663,  0.0657,  ...,  0.0428,  0.0051,  0.0045],\n            [ 0.0186, -0.0219, -0.0480,  ..., -0.0818, -0.0524, -0.0713],\n            ...,\n            [ 0.0748,  0.0650,  0.0607,  ..., -0.0266, -0.0374,  0.0074],\n            [ 0.0448,  0.0276, -0.0163,  ..., -0.0512, -0.0353,  0.0358],\n            [-0.0777,  0.0326, -0.0811,  ...,  0.0366,  0.0412, -0.0599]]]],\n         requires_grad=True)\n)", "parameters": [["weight", [64, 3, 7, 7]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [9408]}, {"name": "bn1", "id": 140584624435008, "class_name": "BatchNorm2d(\n  self.momentum=0.1, self.weight=Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 4.1000e-03, -2.6711e-03, -2.4313e-03, -2.5202e-03, -1.2327e-03,\n          -1.7882e-03, -2.0766e-03, -1.1351e-03,  2.3414e-04, -2.0826e-03,\n          -1.4961e-03, -5.7945e-03,  2.0529e-03,  1.1799e-03, -1.7222e-03,\n          -4.1126e-03, -3.9412e-03, -3.7171e-04, -2.9594e-03,  2.1248e-03,\n          -3.1597e-03,  4.8849e-05,  4.8458e-03, -8.6762e-04,  2.9588e-03,\n          -1.3571e-03,  3.6748e-03,  2.4475e-03,  1.0263e-03, -9.4546e-04,\n          -3.5448e-03,  3.6492e-03,  3.8036e-04, -2.0280e-04, -3.0946e-03,\n          -1.4832e-03, -5.3620e-04, -8.1365e-04,  2.2059e-03, -9.6156e-04,\n          -2.4871e-03,  3.0327e-04,  1.6459e-03,  2.5287e-03,  1.8803e-04,\n           2.6426e-03, -3.8609e-03,  2.7525e-03, -4.2547e-05, -1.1043e-03,\n          -3.4698e-03, -4.0871e-05,  1.7379e-03, -1.4818e-03, -2.7819e-03,\n           2.7920e-03,  3.4923e-03,  9.7986e-04,  2.9914e-03,  3.3834e-03,\n          -2.7001e-03, -2.1437e-03,  1.6235e-03,  1.7636e-03],\n         grad_fn=<AddBackward0>), self.running_var=tensor([0.9998, 0.9559, 0.9867, 0.9601, 0.9832, 0.9392, 0.9395, 0.9174, 0.9137,\n          0.9626, 0.9733, 1.0466, 0.9362, 0.9495, 0.9442, 1.0219, 0.9600, 0.9458,\n          0.9490, 0.9470, 0.9542, 0.9205, 1.0352, 0.9266, 0.9650, 0.9374, 0.9741,\n          0.9660, 0.9507, 0.9320, 1.0046, 0.9633, 0.9132, 0.9149, 0.9659, 0.9216,\n          0.9546, 0.9443, 0.9791, 0.9208, 0.9560, 0.9198, 0.9296, 0.9764, 0.9212,\n          0.9422, 0.9973, 0.9816, 0.9123, 0.9222, 0.9865, 0.9384, 0.9491, 0.9168,\n          0.9572, 0.9715, 0.9793, 0.9461, 0.9471, 0.9618, 0.9461, 0.9510, 0.9157,\n          0.9307], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n)", "parameters": [["weight", [64]], ["bias", [64]]], "output_shape": [[512, 64, 16, 16]], "num_parameters": [64, 64]}, {"name": "relu", "id": 140584624434816, "class_name": "ReLU()", "parameters": [], "output_shape": [[512, 64, 16, 16]], "num_parameters": []}, {"name": "pool", "id": 140584624434864, "class_name": "MaxPool2d(self.kernel_size=(3, 3), self.stride=(3, 3), self.padding=1)", "parameters": [], "output_shape": [[512, 64, 6, 6]], "num_parameters": []}, {"name": "layer1", "id": 140584624433856, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-0.0323,  0.0079, -0.0338],\n                [-0.0166, -0.0264, -0.0258],\n                [-0.0313,  0.0024,  0.0081]],\n      \n               [[ 0.0125, -0.0292, -0.0239],\n                [ 0.0329,  0.0315, -0.0100],\n                [ 0.0251,  0.0136, -0.0272]],\n      \n               [[ 0.0203,  0.0256, -0.0166],\n                [ 0.0216,  0.0413,  0.0371],\n                [-0.0070,  0.0043,  0.0025]],\n      \n               ...,\n      \n               [[-0.0394, -0.0260, -0.0213],\n                [-0.0282,  0.0138, -0.0262],\n                [ 0.0074, -0.0264, -0.0003]],\n      \n               [[ 0.0033,  0.0311, -0.0288],\n                [-0.0095, -0.0144, -0.0370],\n                [-0.0359,  0.0122,  0.0285]],\n      \n               [[-0.0412, -0.0362, -0.0302],\n                [ 0.0153, -0.0288,  0.0079],\n                [-0.0154, -0.0193,  0.0036]]],\n      \n      \n              [[[ 0.0379,  0.0255,  0.0049],\n                [ 0.0167, -0.0368, -0.0369],\n                [ 0.0031, -0.0318, -0.0168]],\n      \n               [[-0.0131,  0.0355,  0.0113],\n                [-0.0393, -0.0005,  0.0349],\n                [ 0.0211, -0.0394, -0.0315]],\n      \n               [[ 0.0388, -0.0166, -0.0127],\n                [ 0.0220,  0.0125, -0.0111],\n                [-0.0301, -0.0259,  0.0248]],\n      \n               ...,\n      \n               [[ 0.0134, -0.0253, -0.0324],\n                [ 0.0230,  0.0350, -0.0172],\n                [ 0.0056, -0.0259,  0.0183]],\n      \n               [[-0.0201, -0.0044, -0.0394],\n                [ 0.0022,  0.0414, -0.0020],\n                [-0.0050,  0.0221,  0.0167]],\n      \n               [[ 0.0039,  0.0121,  0.0400],\n                [ 0.0139,  0.0095, -0.0317],\n                [ 0.0361,  0.0102,  0.0280]]],\n      \n      \n              [[[ 0.0005,  0.0117,  0.0286],\n                [-0.0051, -0.0013, -0.0345],\n                [ 0.0270, -0.0363,  0.0364]],\n      \n               [[-0.0152,  0.0194,  0.0244],\n                [-0.0125,  0.0338, -0.0402],\n                [ 0.0268,  0.0138,  0.0271]],\n      \n               [[-0.0141,  0.0153, -0.0341],\n                [-0.0321,  0.0014, -0.0398],\n                [-0.0031, -0.0118, -0.0138]],\n      \n               ...,\n      \n               [[ 0.0157, -0.0323, -0.0245],\n                [ 0.0357,  0.0228, -0.0349],\n                [ 0.0145, -0.0406,  0.0212]],\n      \n               [[-0.0292,  0.0347, -0.0326],\n                [ 0.0004,  0.0199,  0.0336],\n                [-0.0309,  0.0176, -0.0270]],\n      \n               [[ 0.0392, -0.0348,  0.0143],\n                [ 0.0016, -0.0409,  0.0368],\n                [ 0.0003, -0.0267,  0.0296]]],\n      \n      \n              ...,\n      \n      \n              [[[ 0.0160,  0.0249, -0.0345],\n                [ 0.0213, -0.0093,  0.0074],\n                [-0.0307,  0.0359, -0.0263]],\n      \n               [[ 0.0174,  0.0296,  0.0385],\n                [-0.0076,  0.0233, -0.0177],\n                [-0.0125, -0.0406,  0.0365]],\n      \n               [[-0.0147, -0.0310, -0.0372],\n                [ 0.0094, -0.0363,  0.0138],\n                [-0.0234,  0.0399,  0.0243]],\n      \n               ...,\n      \n               [[ 0.0102,  0.0337,  0.0147],\n                [ 0.0210, -0.0100,  0.0316],\n                [ 0.0253,  0.0283, -0.0009]],\n      \n               [[ 0.0249,  0.0396, -0.0180],\n                [ 0.0250, -0.0351,  0.0239],\n                [ 0.0079, -0.0013, -0.0036]],\n      \n               [[-0.0259,  0.0392, -0.0056],\n                [-0.0003, -0.0050, -0.0115],\n                [-0.0308, -0.0147, -0.0013]]],\n      \n      \n              [[[-0.0086,  0.0208, -0.0357],\n                [ 0.0353,  0.0383,  0.0331],\n                [ 0.0280,  0.0188, -0.0224]],\n      \n               [[-0.0257, -0.0121,  0.0083],\n                [-0.0228, -0.0407, -0.0380],\n                [-0.0147, -0.0052, -0.0368]],\n      \n               [[-0.0090, -0.0391, -0.0326],\n                [ 0.0105, -0.0205, -0.0238],\n                [ 0.0406, -0.0364, -0.0224]],\n      \n               ...,\n      \n               [[ 0.0025,  0.0354, -0.0315],\n                [-0.0161, -0.0218, -0.0360],\n                [ 0.0319, -0.0116, -0.0354]],\n      \n               [[-0.0229,  0.0044, -0.0318],\n                [-0.0349, -0.0409,  0.0310],\n                [-0.0118,  0.0244,  0.0137]],\n      \n               [[ 0.0363,  0.0253, -0.0077],\n                [ 0.0049, -0.0072,  0.0338],\n                [-0.0280, -0.0215, -0.0346]]],\n      \n      \n              [[[-0.0399, -0.0345, -0.0076],\n                [-0.0260,  0.0410,  0.0342],\n                [ 0.0305,  0.0108,  0.0020]],\n      \n               [[ 0.0052, -0.0069,  0.0363],\n                [-0.0231,  0.0257, -0.0229],\n                [ 0.0194, -0.0342, -0.0401]],\n      \n               [[ 0.0040,  0.0156, -0.0246],\n                [-0.0119,  0.0201,  0.0149],\n                [ 0.0005, -0.0411, -0.0072]],\n      \n               ...,\n      \n               [[ 0.0309,  0.0213,  0.0081],\n                [-0.0309,  0.0333,  0.0171],\n                [ 0.0268,  0.0200, -0.0093]],\n      \n               [[-0.0167, -0.0033, -0.0140],\n                [ 0.0266, -0.0097,  0.0170],\n                [ 0.0062,  0.0212, -0.0110]],\n      \n               [[ 0.0268,  0.0293,  0.0399],\n                [ 0.0033, -0.0358,  0.0191],\n                [-0.0345, -0.0233,  0.0171]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0302,  0.0548, -0.0253,  0.0360,  0.0073, -0.0666, -0.0349,  0.0144,\n               0.0071,  0.0274, -0.0137, -0.0193,  0.0249,  0.0391, -0.0277, -0.0221,\n              -0.0008, -0.0023,  0.0316,  0.0174, -0.0314,  0.0247, -0.0442, -0.0238,\n              -0.0367, -0.0480, -0.0311,  0.0513,  0.0295, -0.0032,  0.0044, -0.0300,\n               0.0166,  0.0327,  0.0100,  0.0166, -0.0832, -0.0461,  0.0083, -0.0466,\n              -0.0170, -0.0428, -0.0625, -0.0532, -0.0553, -0.0026,  0.0252, -0.0211,\n               0.0659,  0.0150, -0.0333,  0.0038, -0.0059, -0.0143,  0.0161,  0.0118,\n               0.0599, -0.0211,  0.0178, -0.0540, -0.0224,  0.0312, -0.0845,  0.0446],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9165, 0.9217, 0.9279, 0.9139, 0.9094, 0.9151, 0.9287, 0.9094, 0.9200,\n              0.9180, 0.9403, 0.9147, 0.9219, 0.9114, 0.9214, 0.9282, 0.9120, 0.9149,\n              0.9347, 0.9138, 0.9194, 0.9195, 0.9133, 0.9207, 0.9149, 0.9320, 0.9210,\n              0.9234, 0.9164, 0.9134, 0.9126, 0.9141, 0.9213, 0.9261, 0.9135, 0.9384,\n              0.9381, 0.9348, 0.9178, 0.9207, 0.9132, 0.9212, 0.9192, 0.9168, 0.9198,\n              0.9241, 0.9144, 0.9141, 0.9187, 0.9163, 0.9178, 0.9158, 0.9138, 0.9165,\n              0.9164, 0.9171, 0.9199, 0.9118, 0.9342, 0.9186, 0.9157, 0.9166, 0.9460,\n              0.9325], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-0.0253, -0.0044, -0.0374],\n                [ 0.0275, -0.0042, -0.0148],\n                [ 0.0070, -0.0235, -0.0353]],\n      \n               [[ 0.0270,  0.0052,  0.0249],\n                [-0.0023, -0.0294, -0.0327],\n                [ 0.0175, -0.0026,  0.0391]],\n      \n               [[ 0.0005, -0.0147,  0.0354],\n                [-0.0378, -0.0312,  0.0085],\n                [-0.0010, -0.0355,  0.0242]],\n      \n               ...,\n      \n               [[-0.0058, -0.0263, -0.0161],\n                [ 0.0005,  0.0358,  0.0267],\n                [-0.0020,  0.0035,  0.0152]],\n      \n               [[ 0.0166,  0.0245,  0.0258],\n                [ 0.0392,  0.0135, -0.0037],\n                [-0.0079, -0.0283, -0.0126]],\n      \n               [[ 0.0195,  0.0270, -0.0096],\n                [-0.0248,  0.0295,  0.0280],\n                [-0.0267, -0.0202, -0.0019]]],\n      \n      \n              [[[-0.0033,  0.0330,  0.0310],\n                [-0.0397,  0.0150,  0.0151],\n                [-0.0165, -0.0121,  0.0256]],\n      \n               [[ 0.0294,  0.0026,  0.0003],\n                [ 0.0349,  0.0022, -0.0127],\n                [ 0.0083,  0.0097, -0.0092]],\n      \n               [[-0.0008,  0.0189,  0.0135],\n                [-0.0073, -0.0086, -0.0405],\n                [-0.0125, -0.0361,  0.0402]],\n      \n               ...,\n      \n               [[ 0.0126, -0.0006,  0.0088],\n                [ 0.0079,  0.0037, -0.0288],\n                [-0.0015, -0.0364,  0.0300]],\n      \n               [[ 0.0036,  0.0414, -0.0400],\n                [-0.0145, -0.0302,  0.0391],\n                [ 0.0092, -0.0136,  0.0198]],\n      \n               [[-0.0127, -0.0352, -0.0254],\n                [-0.0081, -0.0325, -0.0251],\n                [ 0.0168,  0.0228, -0.0300]]],\n      \n      \n              [[[-0.0222, -0.0221, -0.0348],\n                [ 0.0396, -0.0304,  0.0137],\n                [ 0.0239,  0.0037, -0.0164]],\n      \n               [[-0.0154, -0.0284, -0.0377],\n                [ 0.0233, -0.0069, -0.0083],\n                [ 0.0156, -0.0128,  0.0118]],\n      \n               [[ 0.0105, -0.0268,  0.0392],\n                [ 0.0256, -0.0107, -0.0407],\n                [ 0.0211,  0.0020, -0.0186]],\n      \n               ...,\n      \n               [[-0.0258, -0.0220, -0.0286],\n                [-0.0291,  0.0122,  0.0255],\n                [-0.0210, -0.0067,  0.0136]],\n      \n               [[ 0.0047, -0.0345, -0.0087],\n                [-0.0017,  0.0037,  0.0005],\n                [-0.0189,  0.0165,  0.0174]],\n      \n               [[ 0.0144, -0.0185,  0.0416],\n                [ 0.0329, -0.0321, -0.0201],\n                [ 0.0270,  0.0059,  0.0038]]],\n      \n      \n              ...,\n      \n      \n              [[[-0.0147, -0.0071,  0.0189],\n                [-0.0006,  0.0052, -0.0376],\n                [ 0.0357,  0.0333, -0.0268]],\n      \n               [[-0.0414, -0.0345, -0.0311],\n                [-0.0183, -0.0134, -0.0260],\n                [-0.0298, -0.0067, -0.0323]],\n      \n               [[-0.0029, -0.0198, -0.0032],\n                [-0.0044, -0.0313, -0.0111],\n                [ 0.0265, -0.0099,  0.0390]],\n      \n               ...,\n      \n               [[ 0.0028,  0.0299,  0.0178],\n                [ 0.0026,  0.0392, -0.0062],\n                [ 0.0061, -0.0055, -0.0157]],\n      \n               [[ 0.0409, -0.0387, -0.0358],\n                [-0.0219,  0.0389, -0.0357],\n                [-0.0089,  0.0115,  0.0041]],\n      \n               [[-0.0258, -0.0415, -0.0099],\n                [-0.0276, -0.0014, -0.0008],\n                [ 0.0248, -0.0167, -0.0360]]],\n      \n      \n              [[[ 0.0154, -0.0063, -0.0412],\n                [-0.0173,  0.0102,  0.0337],\n                [-0.0377, -0.0030, -0.0414]],\n      \n               [[ 0.0145,  0.0360, -0.0343],\n                [ 0.0090,  0.0012, -0.0116],\n                [ 0.0007,  0.0242, -0.0239]],\n      \n               [[-0.0093, -0.0023, -0.0350],\n                [-0.0359,  0.0115,  0.0126],\n                [-0.0274, -0.0404, -0.0289]],\n      \n               ...,\n      \n               [[ 0.0160, -0.0355,  0.0029],\n                [-0.0390, -0.0038, -0.0072],\n                [-0.0290,  0.0367, -0.0143]],\n      \n               [[ 0.0284,  0.0352, -0.0239],\n                [-0.0174, -0.0166,  0.0214],\n                [ 0.0257,  0.0138,  0.0294]],\n      \n               [[ 0.0019, -0.0397,  0.0027],\n                [-0.0290,  0.0341,  0.0143],\n                [-0.0036, -0.0249, -0.0149]]],\n      \n      \n              [[[ 0.0395,  0.0265, -0.0070],\n                [-0.0066, -0.0180,  0.0277],\n                [-0.0052, -0.0232, -0.0142]],\n      \n               [[ 0.0155,  0.0025,  0.0286],\n                [ 0.0350, -0.0200,  0.0105],\n                [-0.0366,  0.0290,  0.0370]],\n      \n               [[-0.0196, -0.0263, -0.0318],\n                [-0.0156,  0.0208, -0.0043],\n                [ 0.0352, -0.0066, -0.0395]],\n      \n               ...,\n      \n               [[ 0.0348, -0.0129,  0.0409],\n                [-0.0189, -0.0346, -0.0362],\n                [-0.0306, -0.0224,  0.0163]],\n      \n               [[-0.0395, -0.0049,  0.0301],\n                [ 0.0261, -0.0138,  0.0126],\n                [-0.0201,  0.0039, -0.0385]],\n      \n               [[-0.0242, -0.0094,  0.0049],\n                [-0.0210, -0.0372, -0.0411],\n                [ 0.0119, -0.0402, -0.0160]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-1.4311e-02,  2.0253e-02,  2.3139e-02,  3.7693e-02,  2.4516e-02,\n              -5.1692e-03, -1.3622e-02, -2.8578e-02,  1.7609e-02, -2.9865e-03,\n               1.3582e-02,  1.3706e-02,  2.5260e-02,  2.8073e-02, -8.0295e-03,\n               2.7624e-03,  3.7762e-02, -1.9659e-02,  3.0297e-03, -2.1411e-02,\n              -1.2110e-02, -2.0081e-02,  1.4226e-02,  1.6175e-02, -4.6329e-03,\n               2.3159e-02,  1.2677e-02,  2.0888e-02,  1.8505e-03,  1.9203e-02,\n               4.9847e-03, -3.6010e-03, -6.3643e-03, -4.2637e-03, -1.8290e-02,\n              -9.6331e-03, -2.9999e-02,  2.9325e-02, -1.1360e-02,  5.1674e-05,\n               1.1297e-02, -2.4845e-02,  4.3876e-03, -7.8493e-03,  1.0099e-02,\n              -1.6961e-02,  8.2802e-03, -1.0006e-02,  1.5091e-02, -1.5636e-02,\n               1.3877e-02, -4.2801e-03,  1.0550e-02, -1.4861e-02,  1.1905e-02,\n              -4.3370e-03, -4.2037e-03,  2.0458e-02,  1.4644e-02, -2.0842e-02,\n              -4.7621e-02, -2.6072e-02, -3.7805e-02,  1.4664e-02],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9062, 0.9123, 0.9171, 0.9118, 0.9073, 0.9069, 0.9082, 0.9089, 0.9076,\n              0.9089, 0.9118, 0.9092, 0.9083, 0.9110, 0.9067, 0.9096, 0.9131, 0.9182,\n              0.9089, 0.9082, 0.9138, 0.9123, 0.9109, 0.9111, 0.9112, 0.9109, 0.9105,\n              0.9079, 0.9103, 0.9102, 0.9098, 0.9136, 0.9099, 0.9128, 0.9079, 0.9058,\n              0.9096, 0.9124, 0.9094, 0.9102, 0.9080, 0.9084, 0.9096, 0.9093, 0.9119,\n              0.9088, 0.9078, 0.9073, 0.9083, 0.9088, 0.9104, 0.9129, 0.9063, 0.9101,\n              0.9077, 0.9089, 0.9094, 0.9148, 0.9081, 0.9104, 0.9211, 0.9104, 0.9131,\n              0.9078], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n  )\n)", "parameters": [["0.conv1.weight", [64, 64, 3, 3]], ["0.bn1.weight", [64]], ["0.bn1.bias", [64]], ["0.conv2.weight", [64, 64, 3, 3]], ["0.bn2.weight", [64]], ["0.bn2.bias", [64]]], "output_shape": [[512, 64, 6, 6]], "num_parameters": [36864, 64, 64, 36864, 64, 64]}, {"name": "layer2", "id": 140584624432656, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 0.0282, -0.0174,  0.0273],\n                [ 0.0017, -0.0069,  0.0408],\n                [ 0.0141,  0.0361, -0.0188]],\n      \n               [[ 0.0059,  0.0246, -0.0119],\n                [ 0.0013,  0.0297,  0.0296],\n                [-0.0189, -0.0021,  0.0260]],\n      \n               [[ 0.0009,  0.0136,  0.0121],\n                [ 0.0031, -0.0031,  0.0096],\n                [ 0.0335,  0.0034, -0.0405]],\n      \n               ...,\n      \n               [[-0.0011, -0.0186,  0.0088],\n                [-0.0411,  0.0161,  0.0116],\n                [-0.0222, -0.0172, -0.0207]],\n      \n               [[-0.0205, -0.0320,  0.0346],\n                [ 0.0253,  0.0069,  0.0218],\n                [-0.0157,  0.0370, -0.0152]],\n      \n               [[-0.0281, -0.0010, -0.0134],\n                [-0.0268, -0.0208,  0.0316],\n                [-0.0182,  0.0115, -0.0336]]],\n      \n      \n              [[[ 0.0147, -0.0216, -0.0303],\n                [ 0.0190, -0.0362,  0.0225],\n                [ 0.0321, -0.0179,  0.0285]],\n      \n               [[-0.0199,  0.0042, -0.0277],\n                [ 0.0151, -0.0313,  0.0082],\n                [-0.0114,  0.0289,  0.0312]],\n      \n               [[-0.0014, -0.0299, -0.0211],\n                [ 0.0078, -0.0380,  0.0309],\n                [ 0.0008, -0.0369, -0.0008]],\n      \n               ...,\n      \n               [[-0.0035,  0.0197, -0.0046],\n                [-0.0356,  0.0133, -0.0240],\n                [ 0.0369,  0.0328,  0.0143]],\n      \n               [[ 0.0345,  0.0376, -0.0409],\n                [-0.0352,  0.0141,  0.0042],\n                [-0.0013, -0.0260,  0.0377]],\n      \n               [[ 0.0272, -0.0326,  0.0183],\n                [-0.0414, -0.0080, -0.0301],\n                [ 0.0379,  0.0193,  0.0251]]],\n      \n      \n              [[[ 0.0169, -0.0401,  0.0311],\n                [ 0.0076, -0.0049, -0.0407],\n                [-0.0072, -0.0125,  0.0262]],\n      \n               [[-0.0112, -0.0203, -0.0014],\n                [-0.0261,  0.0166, -0.0306],\n                [ 0.0323,  0.0130,  0.0126]],\n      \n               [[-0.0357, -0.0259, -0.0073],\n                [-0.0346, -0.0405, -0.0003],\n                [-0.0145,  0.0124, -0.0169]],\n      \n               ...,\n      \n               [[-0.0311,  0.0233, -0.0218],\n                [ 0.0305, -0.0321,  0.0107],\n                [ 0.0196,  0.0038, -0.0344]],\n      \n               [[ 0.0048,  0.0266, -0.0016],\n                [-0.0292, -0.0331, -0.0319],\n                [-0.0144, -0.0334, -0.0357]],\n      \n               [[ 0.0115,  0.0139, -0.0154],\n                [ 0.0156, -0.0112,  0.0341],\n                [ 0.0330,  0.0411, -0.0017]]],\n      \n      \n              ...,\n      \n      \n              [[[ 0.0116,  0.0196, -0.0346],\n                [-0.0331,  0.0112,  0.0042],\n                [ 0.0074,  0.0334,  0.0161]],\n      \n               [[ 0.0335,  0.0001, -0.0370],\n                [-0.0352, -0.0218,  0.0245],\n                [-0.0189, -0.0100,  0.0315]],\n      \n               [[ 0.0167,  0.0104, -0.0298],\n                [-0.0034,  0.0366, -0.0397],\n                [-0.0141, -0.0395, -0.0335]],\n      \n               ...,\n      \n               [[ 0.0287, -0.0268,  0.0042],\n                [ 0.0266,  0.0055, -0.0168],\n                [ 0.0019, -0.0386, -0.0060]],\n      \n               [[ 0.0360,  0.0415, -0.0086],\n                [ 0.0320,  0.0138, -0.0078],\n                [ 0.0347,  0.0299,  0.0397]],\n      \n               [[ 0.0341, -0.0240,  0.0056],\n                [-0.0328,  0.0043, -0.0010],\n                [-0.0294, -0.0417,  0.0066]]],\n      \n      \n              [[[-0.0326, -0.0121, -0.0111],\n                [-0.0364, -0.0133, -0.0335],\n                [ 0.0243,  0.0360,  0.0354]],\n      \n               [[-0.0393, -0.0046,  0.0322],\n                [-0.0239, -0.0282, -0.0100],\n                [ 0.0002,  0.0227, -0.0090]],\n      \n               [[-0.0350,  0.0158, -0.0005],\n                [-0.0175, -0.0407,  0.0227],\n                [ 0.0277,  0.0026, -0.0080]],\n      \n               ...,\n      \n               [[-0.0130, -0.0259, -0.0032],\n                [-0.0016,  0.0132,  0.0397],\n                [ 0.0313,  0.0411, -0.0310]],\n      \n               [[ 0.0098, -0.0191,  0.0313],\n                [ 0.0251,  0.0228,  0.0028],\n                [ 0.0086,  0.0084,  0.0175]],\n      \n               [[ 0.0295, -0.0069, -0.0407],\n                [-0.0401, -0.0181,  0.0045],\n                [-0.0141, -0.0384, -0.0145]]],\n      \n      \n              [[[ 0.0115,  0.0234, -0.0348],\n                [-0.0170,  0.0178,  0.0268],\n                [ 0.0039, -0.0221, -0.0236]],\n      \n               [[-0.0027,  0.0142,  0.0003],\n                [ 0.0350,  0.0145, -0.0299],\n                [ 0.0267, -0.0139, -0.0086]],\n      \n               [[ 0.0083,  0.0208, -0.0130],\n                [ 0.0235,  0.0012, -0.0198],\n                [ 0.0220,  0.0388,  0.0364]],\n      \n               ...,\n      \n               [[-0.0007,  0.0046,  0.0291],\n                [-0.0390, -0.0089,  0.0078],\n                [ 0.0194,  0.0364,  0.0382]],\n      \n               [[-0.0093, -0.0368,  0.0385],\n                [-0.0304, -0.0358,  0.0130],\n                [ 0.0231,  0.0197,  0.0258]],\n      \n               [[-0.0011, -0.0288,  0.0159],\n                [ 0.0087, -0.0116,  0.0206],\n                [-0.0096, -0.0033, -0.0153]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0785,  0.0075, -0.0876, -0.0501,  0.0523,  0.0219,  0.0410,  0.0053,\n              -0.0298, -0.0204,  0.0590, -0.0345,  0.1295, -0.0202,  0.0509, -0.0114,\n              -0.0379, -0.0145,  0.0449,  0.0177, -0.0760,  0.0075, -0.0416,  0.0792,\n               0.0363, -0.0169,  0.0037, -0.0089, -0.0928, -0.0063,  0.0052,  0.0566,\n               0.0219,  0.0992, -0.0031, -0.0065,  0.0157, -0.0191,  0.0201,  0.0318,\n              -0.0128,  0.0304,  0.0143,  0.0267,  0.0234, -0.0066,  0.1181,  0.0207,\n              -0.0023,  0.0473,  0.0438,  0.0445, -0.0367,  0.0156, -0.0091, -0.0132,\n               0.0443, -0.0687, -0.0444,  0.0318, -0.0020,  0.0457, -0.0274,  0.0123,\n              -0.0784, -0.0655,  0.0360, -0.0487,  0.0023,  0.0473,  0.1026,  0.0193,\n               0.0278,  0.0194,  0.0400,  0.0417, -0.0406, -0.0769, -0.0247,  0.0239,\n               0.0099,  0.0337, -0.0074,  0.0279,  0.0317,  0.0170,  0.0181,  0.0706,\n              -0.0492, -0.0798, -0.0195,  0.0558, -0.0196, -0.0446, -0.0320,  0.0037,\n               0.0191,  0.0116,  0.0427, -0.0783, -0.0280, -0.0257,  0.1122,  0.0109,\n              -0.0651,  0.0297, -0.0235,  0.0544,  0.0233, -0.0378, -0.0772,  0.0518,\n              -0.0055, -0.0863, -0.0436,  0.0499, -0.0323, -0.0243, -0.0492,  0.0174,\n              -0.0052, -0.0568, -0.0597,  0.0381, -0.0069, -0.0264,  0.0180, -0.0141],\n             grad_fn=<AddBackward0>), self.running_var=tensor([0.9543, 0.9232, 0.9341, 0.9291, 0.9316, 0.9258, 0.9248, 0.9173, 0.9509,\n              0.9305, 0.9285, 0.9265, 0.9524, 0.9516, 0.9425, 0.9229, 0.9442, 0.9289,\n              0.9530, 0.9324, 0.9392, 0.9255, 0.9361, 0.9467, 0.9396, 0.9621, 0.9327,\n              0.9296, 0.9446, 0.9393, 0.9247, 0.9468, 0.9259, 0.9539, 0.9183, 0.9185,\n              0.9213, 0.9216, 0.9382, 0.9263, 0.9268, 0.9268, 0.9329, 0.9495, 0.9203,\n              0.9193, 0.9428, 0.9292, 0.9280, 0.9238, 0.9383, 0.9349, 0.9440, 0.9290,\n              0.9210, 0.9302, 0.9296, 0.9381, 0.9305, 0.9344, 0.9291, 0.9265, 0.9319,\n              0.9725, 0.9336, 0.9353, 0.9250, 0.9239, 0.9228, 0.9362, 0.9626, 0.9473,\n              0.9263, 0.9394, 0.9435, 0.9366, 0.9290, 0.9263, 0.9218, 0.9356, 0.9273,\n              0.9384, 0.9414, 0.9234, 0.9233, 0.9309, 0.9180, 0.9412, 0.9438, 0.9314,\n              0.9265, 0.9393, 0.9216, 0.9383, 0.9414, 0.9500, 0.9487, 0.9304, 0.9302,\n              0.9420, 0.9272, 0.9241, 0.9382, 0.9265, 0.9347, 0.9538, 0.9355, 0.9259,\n              0.9296, 0.9313, 0.9391, 0.9213, 0.9258, 0.9869, 0.9250, 0.9451, 0.9189,\n              0.9353, 0.9231, 0.9456, 0.9243, 0.9223, 0.9346, 0.9383, 0.9176, 0.9348,\n              0.9278, 0.9211], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-2.5600e-02,  2.5244e-02, -2.6351e-02],\n                [-2.7247e-02,  2.6553e-02,  3.0009e-03],\n                [ 2.4213e-02,  1.1522e-02,  2.5205e-03]],\n      \n               [[-5.5720e-03,  4.4935e-03, -1.5919e-03],\n                [ 2.6515e-02, -3.3507e-03,  3.9764e-03],\n                [-2.5141e-02, -2.4800e-02,  4.5031e-03]],\n      \n               [[ 1.4680e-02, -2.5411e-02, -8.0127e-03],\n                [-8.7515e-03, -7.5212e-03, -8.1898e-03],\n                [-1.3347e-03, -2.3529e-02, -1.5933e-02]],\n      \n               ...,\n      \n               [[-2.9316e-02, -7.7149e-03, -2.4252e-02],\n                [ 1.1930e-02,  1.2676e-02, -2.7153e-02],\n                [ 1.7636e-02,  2.5292e-02, -9.3731e-03]],\n      \n               [[-7.9332e-03, -1.2330e-03,  3.8290e-04],\n                [-2.1723e-04,  1.2643e-02,  1.4949e-03],\n                [-2.6116e-02, -1.7219e-02,  2.9044e-02]],\n      \n               [[ 1.4914e-02,  4.3665e-03, -2.0967e-02],\n                [ 9.5014e-03,  4.4956e-03,  2.8697e-02],\n                [ 2.9044e-02,  1.9223e-02,  1.8902e-03]]],\n      \n      \n              [[[-1.4147e-02,  1.2745e-03,  1.0343e-02],\n                [-2.6725e-02, -1.1532e-02,  1.4016e-02],\n                [ 1.5060e-02, -4.7544e-03, -2.4037e-02]],\n      \n               [[ 2.1907e-02, -2.4840e-02, -6.6339e-03],\n                [ 7.2060e-03, -1.5074e-02,  1.3773e-02],\n                [-1.3522e-02, -1.3018e-02, -1.8657e-02]],\n      \n               [[ 1.6402e-02,  1.8948e-02,  2.1795e-02],\n                [-7.8058e-03, -1.3960e-02,  9.9993e-03],\n                [ 1.5783e-02,  2.7374e-02,  1.5511e-03]],\n      \n               ...,\n      \n               [[-1.0081e-02,  7.0175e-03,  2.5275e-02],\n                [ 1.0629e-02, -1.0458e-02,  1.2460e-02],\n                [ 3.9821e-03, -1.7578e-02,  2.4675e-02]],\n      \n               [[ 1.4925e-02, -1.1658e-02, -9.8024e-03],\n                [ 2.1275e-02, -2.4127e-02, -2.7002e-02],\n                [-2.2735e-02,  1.5369e-02, -2.8788e-02]],\n      \n               [[ 3.2776e-03, -5.3499e-03,  1.3498e-02],\n                [ 2.6729e-02,  1.7487e-02,  2.3578e-02],\n                [-2.7782e-02, -2.7775e-02,  1.5613e-03]]],\n      \n      \n              [[[-1.4655e-02,  1.0056e-02, -1.9553e-02],\n                [-6.9841e-03,  2.5728e-02,  4.7668e-03],\n                [ 9.5949e-03, -2.6332e-02, -2.0956e-02]],\n      \n               [[-1.8600e-02, -7.0972e-03,  1.1809e-02],\n                [ 5.7806e-03, -1.1355e-03, -3.9158e-03],\n                [-8.8407e-03,  2.4170e-02,  2.8314e-02]],\n      \n               [[ 1.5833e-02, -1.9746e-02, -1.7591e-02],\n                [-3.8627e-03, -2.1322e-02,  3.8100e-03],\n                [ 9.0678e-03, -1.9578e-02,  1.9240e-02]],\n      \n               ...,\n      \n               [[-1.7628e-02, -2.5189e-02, -1.6329e-02],\n                [-2.6364e-02,  1.3943e-02, -1.0778e-02],\n                [ 1.8240e-02,  2.8924e-03, -1.8780e-02]],\n      \n               [[-1.5709e-02, -6.2788e-03, -7.6449e-03],\n                [ 1.2866e-02,  6.4925e-03, -2.5338e-02],\n                [-5.6679e-03, -5.3008e-03,  9.6181e-03]],\n      \n               [[ 3.7121e-03,  2.4113e-02, -2.0538e-02],\n                [ 1.8475e-02,  2.8045e-02, -7.9943e-03],\n                [ 2.7152e-02, -1.4770e-02,  5.4111e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[ 1.4284e-02,  8.3989e-03,  3.3056e-03],\n                [ 1.9738e-02,  2.8650e-02,  8.6706e-03],\n                [ 2.6078e-02,  4.3493e-03, -2.0567e-02]],\n      \n               [[ 2.2486e-02, -2.7628e-02, -1.6326e-03],\n                [ 2.8849e-02,  2.7024e-02,  1.0946e-02],\n                [ 2.2152e-02, -1.8588e-02,  1.6916e-02]],\n      \n               [[-7.4034e-03,  1.2736e-02,  1.4884e-02],\n                [-1.8183e-02, -3.2979e-03,  2.9413e-02],\n                [ 1.8391e-02, -2.1583e-02, -2.7601e-02]],\n      \n               ...,\n      \n               [[ 2.1282e-02, -8.7727e-03,  6.6979e-03],\n                [-1.5728e-02,  8.8239e-03,  2.4667e-02],\n                [-1.6939e-02,  1.3510e-02, -2.7113e-02]],\n      \n               [[ 1.1525e-02,  2.2997e-02, -3.9535e-04],\n                [-2.4826e-02,  1.9887e-02, -1.5364e-02],\n                [-1.8416e-02,  1.5261e-02,  2.8689e-02]],\n      \n               [[ 2.2470e-02, -7.6961e-03, -1.3636e-02],\n                [ 2.2517e-02,  1.9337e-02,  1.7461e-02],\n                [ 1.4285e-02, -8.2199e-04,  9.2880e-03]]],\n      \n      \n              [[[ 2.5425e-05, -2.8710e-02,  1.6435e-02],\n                [-1.8144e-02,  6.0900e-03,  2.8795e-02],\n                [ 4.8321e-03, -1.2768e-02,  2.3828e-02]],\n      \n               [[ 2.2831e-02, -1.8811e-02,  9.5717e-03],\n                [-1.8886e-02,  2.2124e-02,  2.7313e-03],\n                [ 1.6834e-02,  5.9257e-03, -1.6458e-02]],\n      \n               [[ 3.2164e-03,  2.9330e-02, -5.4145e-03],\n                [-9.6016e-04, -1.3564e-02,  2.5709e-02],\n                [ 1.1651e-02,  8.6060e-03, -1.1225e-02]],\n      \n               ...,\n      \n               [[-1.9311e-02,  2.7127e-04,  2.3396e-02],\n                [-9.1507e-03,  2.0045e-02,  3.8540e-03],\n                [-8.7368e-03, -2.0719e-02,  2.3893e-02]],\n      \n               [[-2.2555e-02,  2.7607e-02, -1.3549e-02],\n                [-2.0829e-02,  5.0205e-03,  2.5469e-02],\n                [ 5.8421e-03,  1.0741e-02,  7.2482e-03]],\n      \n               [[ 1.4614e-02,  7.3741e-03, -2.4766e-02],\n                [-2.0437e-03,  1.4804e-02, -1.9208e-02],\n                [ 1.7912e-02, -1.9316e-02,  2.3343e-02]]],\n      \n      \n              [[[ 2.2263e-02,  1.5254e-02,  2.1462e-02],\n                [-2.8021e-02,  1.7317e-02, -4.5392e-04],\n                [ 2.2169e-03, -2.8522e-02, -2.1416e-02]],\n      \n               [[-1.8518e-02,  1.9326e-02, -2.2705e-02],\n                [ 1.3036e-02, -7.7661e-03,  5.2281e-03],\n                [-7.8837e-03, -2.2047e-02,  2.5244e-02]],\n      \n               [[ 2.7877e-02, -1.4493e-02,  6.7188e-03],\n                [-6.8305e-03,  2.8887e-02,  2.2340e-02],\n                [-1.4285e-02,  2.1399e-02,  7.7281e-03]],\n      \n               ...,\n      \n               [[-2.7949e-02,  8.6155e-03,  6.4633e-03],\n                [ 2.1642e-02, -8.4766e-03,  6.9103e-03],\n                [ 2.2641e-02, -2.0864e-02, -1.8653e-02]],\n      \n               [[-2.9476e-03,  2.1372e-02, -7.7312e-03],\n                [-8.5642e-03,  4.6645e-03,  2.2027e-02],\n                [-2.1806e-02,  1.4822e-02,  2.3619e-03]],\n      \n               [[ 1.0284e-04, -2.3172e-02,  1.2366e-02],\n                [ 2.5048e-02,  5.8368e-04,  5.0567e-04],\n                [-2.7320e-02, -1.9393e-02,  2.6116e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-7.0385e-03,  6.0183e-03, -5.7651e-04,  1.4898e-03,  1.5767e-03,\n              -1.3700e-04,  1.4796e-02, -8.9150e-03,  2.1925e-02, -2.4484e-02,\n              -5.0911e-03,  4.2231e-03, -1.4205e-02, -1.3430e-02, -4.5627e-03,\n               1.1083e-02, -1.0759e-02, -1.2855e-02, -1.7879e-02, -1.1965e-02,\n              -1.4295e-02,  8.4510e-03, -6.8014e-03, -1.1107e-02,  1.1973e-02,\n              -5.1181e-03,  1.5411e-02,  1.2878e-02, -2.3438e-03, -4.9969e-03,\n               5.9119e-03, -1.6808e-02,  6.2155e-03,  8.9033e-03,  4.5228e-03,\n              -3.4844e-03, -1.4009e-02,  8.9451e-03, -2.0852e-02,  5.4614e-03,\n              -1.1196e-02, -1.0351e-02,  1.2667e-02, -5.7182e-03,  1.8739e-02,\n              -3.1816e-03, -4.5756e-04, -3.1381e-03,  1.1949e-02,  1.1198e-02,\n               2.3327e-03,  1.4371e-02,  4.6759e-03, -1.0144e-02, -6.9441e-03,\n              -1.5996e-02, -1.3783e-02,  4.9142e-03,  8.6976e-03, -2.5615e-03,\n              -9.5261e-03, -1.2622e-02, -1.8126e-03,  1.0530e-02, -4.0350e-04,\n               2.8519e-04, -1.9477e-02, -1.2973e-02,  1.0489e-02,  8.1695e-03,\n               5.6373e-03,  2.1726e-02,  2.6920e-02,  1.1275e-02,  2.8311e-02,\n               3.4407e-02,  2.6524e-03,  3.2080e-02, -8.1399e-03,  8.7888e-03,\n              -9.0651e-05, -8.6815e-04,  1.6917e-02, -6.5504e-03,  8.8699e-04,\n               9.1903e-03, -2.8190e-02,  1.0099e-02, -7.5167e-03, -1.5890e-02,\n              -1.1272e-02,  2.9217e-02,  1.9333e-03,  2.2174e-02,  2.2131e-02,\n               1.3237e-02,  2.6685e-02,  7.5403e-03, -1.5514e-02, -4.6317e-03,\n              -1.4063e-02, -1.4504e-02, -1.5355e-02, -8.8261e-03,  1.1213e-02,\n               1.9955e-02,  2.4450e-03,  1.3404e-02, -1.3498e-02,  2.1222e-03,\n               9.2325e-03,  2.6383e-02,  3.0009e-03, -7.4788e-03, -1.5851e-02,\n              -3.9673e-03, -3.7600e-03,  2.6345e-03, -3.3590e-02,  1.4657e-02,\n               1.9285e-02,  9.5783e-03, -1.1832e-02, -6.9240e-03, -1.9274e-03,\n              -8.1906e-03, -1.4385e-02,  3.0531e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9060, 0.9072, 0.9068, 0.9070, 0.9091, 0.9082, 0.9078, 0.9064, 0.9097,\n              0.9098, 0.9117, 0.9090, 0.9084, 0.9091, 0.9089, 0.9089, 0.9088, 0.9095,\n              0.9090, 0.9087, 0.9058, 0.9062, 0.9115, 0.9081, 0.9090, 0.9065, 0.9074,\n              0.9096, 0.9063, 0.9060, 0.9071, 0.9086, 0.9093, 0.9061, 0.9079, 0.9100,\n              0.9087, 0.9085, 0.9115, 0.9076, 0.9107, 0.9075, 0.9109, 0.9123, 0.9100,\n              0.9067, 0.9104, 0.9084, 0.9086, 0.9094, 0.9057, 0.9096, 0.9082, 0.9089,\n              0.9078, 0.9057, 0.9064, 0.9094, 0.9093, 0.9129, 0.9080, 0.9093, 0.9105,\n              0.9089, 0.9069, 0.9095, 0.9087, 0.9100, 0.9098, 0.9093, 0.9132, 0.9089,\n              0.9087, 0.9098, 0.9090, 0.9080, 0.9066, 0.9069, 0.9074, 0.9075, 0.9071,\n              0.9075, 0.9076, 0.9073, 0.9077, 0.9085, 0.9069, 0.9065, 0.9071, 0.9102,\n              0.9068, 0.9075, 0.9099, 0.9091, 0.9074, 0.9069, 0.9117, 0.9072, 0.9070,\n              0.9059, 0.9074, 0.9076, 0.9082, 0.9098, 0.9067, 0.9082, 0.9112, 0.9099,\n              0.9076, 0.9069, 0.9072, 0.9098, 0.9105, 0.9072, 0.9092, 0.9062, 0.9079,\n              0.9064, 0.9115, 0.9088, 0.9062, 0.9094, 0.9081, 0.9077, 0.9081, 0.9135,\n              0.9066, 0.9074], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[-0.0936]],\n        \n                 [[-0.0375]],\n        \n                 [[-0.1174]],\n        \n                 ...,\n        \n                 [[ 0.0913]],\n        \n                 [[-0.0361]],\n        \n                 [[ 0.0796]]],\n        \n        \n                [[[-0.0723]],\n        \n                 [[ 0.0165]],\n        \n                 [[ 0.0404]],\n        \n                 ...,\n        \n                 [[-0.1048]],\n        \n                 [[-0.0411]],\n        \n                 [[-0.0387]]],\n        \n        \n                [[[-0.0671]],\n        \n                 [[-0.0502]],\n        \n                 [[ 0.1136]],\n        \n                 ...,\n        \n                 [[-0.1019]],\n        \n                 [[-0.0658]],\n        \n                 [[ 0.0814]]],\n        \n        \n                ...,\n        \n        \n                [[[-0.0669]],\n        \n                 [[-0.1175]],\n        \n                 [[-0.0816]],\n        \n                 ...,\n        \n                 [[-0.0969]],\n        \n                 [[ 0.0299]],\n        \n                 [[ 0.0567]]],\n        \n        \n                [[[ 0.0255]],\n        \n                 [[ 0.0207]],\n        \n                 [[ 0.0601]],\n        \n                 ...,\n        \n                 [[ 0.0907]],\n        \n                 [[ 0.0067]],\n        \n                 [[-0.0363]]],\n        \n        \n                [[[-0.0441]],\n        \n                 [[ 0.0891]],\n        \n                 [[-0.0231]],\n        \n                 ...,\n        \n                 [[ 0.0425]],\n        \n                 [[-0.0651]],\n        \n                 [[ 0.0552]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 8.5573e-03, -2.4295e-03, -5.2716e-02, -2.8312e-03,  4.6616e-02,\n                 3.5859e-02, -7.5235e-02, -1.0025e-01, -7.9094e-03, -6.3492e-03,\n                -4.1360e-02, -6.1476e-02,  3.8170e-02, -3.8464e-03, -3.5772e-02,\n                 4.5850e-02,  2.1975e-02, -1.0252e-02, -1.0771e-02, -1.3622e-02,\n                -4.9695e-02, -8.4250e-02, -1.7574e-02,  4.6825e-02, -9.2053e-02,\n                -3.8314e-02, -7.0791e-03,  3.8060e-02,  6.0129e-02,  1.0992e-01,\n                -8.8923e-02,  8.9183e-03,  1.2069e-02,  1.0839e-01,  7.8202e-02,\n                -7.8615e-03, -8.7809e-02,  9.8474e-02, -1.4693e-02,  4.5118e-02,\n                 7.4844e-03,  1.1135e-01,  5.2441e-03,  4.4182e-02,  8.7875e-02,\n                -1.1445e-01, -3.7092e-02, -7.6928e-02,  2.0762e-02, -6.8745e-02,\n                 1.8071e-02, -3.8986e-02,  4.4795e-02, -8.9590e-02, -6.8953e-02,\n                -1.1330e-01, -2.3656e-02,  1.8364e-02, -8.8189e-02, -6.8565e-02,\n                 1.8988e-02,  7.7412e-02,  4.0159e-03,  4.4287e-02,  1.2714e-02,\n                 2.5001e-02,  3.5845e-02, -1.0674e-01, -7.3027e-03, -4.3786e-02,\n                 4.0891e-02,  5.5350e-02,  1.8798e-02, -2.3240e-02, -1.3364e-02,\n                 1.4647e-02,  4.9289e-02,  9.6137e-02,  2.0232e-03, -1.0171e-01,\n                 4.6939e-02,  3.4853e-02,  2.8347e-02, -5.3637e-02,  5.6235e-02,\n                 6.0806e-02, -7.1840e-02, -6.9915e-02,  1.1626e-01,  5.1570e-02,\n                 5.4393e-02, -1.1282e-02,  4.0278e-02,  7.9358e-02,  3.0915e-02,\n                -3.3032e-02,  4.8038e-02,  5.4670e-02,  2.6440e-02,  5.2815e-02,\n                 7.4537e-02,  9.3837e-03, -3.5127e-02,  5.8349e-02, -1.2077e-04,\n                -1.0130e-02, -5.3048e-02, -7.1810e-03,  2.6062e-02, -1.6922e-02,\n                -1.8199e-03,  4.4853e-02,  1.1054e-02, -5.5309e-03,  3.4767e-02,\n                 1.5483e-02,  6.1983e-02, -1.5379e-02, -6.2260e-03,  8.1303e-02,\n                -4.1874e-03,  5.0231e-02, -1.9430e-02,  3.6418e-05, -6.6006e-02,\n                -1.4806e-02, -1.4188e-02,  3.7047e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9256, 0.9387, 0.9246, 0.9301, 0.9485, 0.9262, 0.9438, 0.9643, 0.9209,\n                0.9714, 0.9279, 0.9335, 0.9344, 0.9316, 0.9335, 0.9310, 0.9349, 0.9310,\n                0.9440, 0.9181, 0.9227, 0.9510, 0.9291, 0.9350, 0.9354, 0.9321, 0.9211,\n                0.9373, 0.9294, 0.9454, 0.9617, 0.9221, 0.9337, 0.9370, 0.9213, 0.9195,\n                0.9277, 0.9255, 0.9616, 0.9530, 0.9258, 0.9508, 0.9272, 0.9316, 0.9408,\n                0.9692, 0.9260, 0.9391, 0.9560, 0.9387, 0.9228, 0.9448, 0.9284, 0.9237,\n                0.9390, 0.9349, 0.9193, 0.9220, 0.9415, 0.9283, 0.9322, 0.9269, 0.9333,\n                0.9315, 0.9237, 0.9330, 0.9233, 0.9230, 0.9212, 0.9161, 0.9230, 0.9343,\n                0.9276, 0.9374, 0.9284, 0.9432, 0.9449, 0.9696, 0.9354, 0.9580, 0.9533,\n                0.9388, 0.9250, 0.9266, 0.9309, 0.9431, 0.9225, 0.9353, 0.9368, 0.9217,\n                0.9224, 0.9414, 0.9442, 0.9262, 0.9175, 0.9149, 0.9373, 0.9441, 0.9327,\n                0.9205, 0.9300, 0.9243, 0.9372, 0.9353, 0.9234, 0.9249, 0.9610, 0.9348,\n                0.9231, 0.9203, 1.0208, 0.9190, 0.9455, 0.9552, 0.9415, 0.9480, 0.9591,\n                0.9202, 0.9338, 0.9511, 0.9220, 0.9184, 0.9237, 0.9471, 0.9237, 0.9275,\n                0.9433, 0.9335], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [128, 64, 3, 3]], ["0.bn1.weight", [128]], ["0.bn1.bias", [128]], ["0.conv2.weight", [128, 128, 3, 3]], ["0.bn2.weight", [128]], ["0.bn2.bias", [128]], ["0.downsample.0.weight", [128, 64, 1, 1]], ["0.downsample.1.weight", [128]], ["0.downsample.1.bias", [128]]], "output_shape": [[512, 128, 3, 3]], "num_parameters": [73728, 128, 128, 147456, 128, 128, 8192, 128, 128]}, {"name": "layer3", "id": 140584624433280, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 1.3138e-02, -9.7435e-03,  1.8583e-02],\n                [ 4.4787e-03,  1.6834e-02, -2.3040e-02],\n                [-6.7300e-03, -2.6906e-02, -2.9307e-02]],\n      \n               [[-4.0313e-03, -8.6458e-04,  1.4083e-02],\n                [-8.9454e-04,  2.1397e-02,  9.3426e-06],\n                [ 8.0766e-03, -2.5078e-02, -1.4779e-02]],\n      \n               [[ 3.5377e-03,  1.4545e-02,  3.3451e-03],\n                [-2.0007e-02, -1.5267e-02, -2.1194e-02],\n                [-1.7069e-02, -1.7195e-02,  4.1072e-03]],\n      \n               ...,\n      \n               [[ 5.8881e-03, -2.5999e-02, -2.6794e-02],\n                [ 9.0688e-03, -2.0150e-02, -1.3618e-02],\n                [ 2.4555e-02, -2.7890e-02, -2.7524e-02]],\n      \n               [[ 9.3052e-04, -2.2202e-02,  2.0891e-02],\n                [-2.9377e-02,  2.1911e-02,  2.0945e-02],\n                [ 2.5027e-02,  2.5360e-02,  2.2168e-02]],\n      \n               [[-1.7273e-02,  2.6293e-02, -4.8518e-04],\n                [ 7.0420e-03, -7.8173e-03,  2.1130e-02],\n                [ 7.6962e-04, -3.9233e-04,  1.4758e-02]]],\n      \n      \n              [[[ 1.7917e-02, -2.4119e-02, -4.0511e-03],\n                [ 2.8314e-02,  9.1011e-03, -1.3399e-02],\n                [ 2.3674e-02, -1.0940e-02, -2.8738e-02]],\n      \n               [[ 2.0390e-02,  2.9130e-02,  2.4962e-02],\n                [-2.0996e-02,  2.0148e-02, -6.2430e-03],\n                [-1.0925e-03, -1.4643e-02, -5.6695e-03]],\n      \n               [[ 1.5333e-02,  1.8557e-02,  1.4278e-02],\n                [-3.1945e-03,  1.1018e-02,  2.8446e-02],\n                [ 2.3581e-02,  1.1900e-02, -4.1232e-03]],\n      \n               ...,\n      \n               [[ 2.2218e-02, -5.5576e-03,  2.0362e-02],\n                [ 1.8627e-02, -9.6944e-03,  2.1724e-02],\n                [-2.3258e-02,  7.5860e-03,  2.7823e-02]],\n      \n               [[ 2.2559e-02,  6.0839e-03, -1.9683e-02],\n                [ 1.7705e-02, -7.9236e-03, -1.9871e-02],\n                [-6.4022e-03,  8.0343e-03,  1.5632e-02]],\n      \n               [[-6.7504e-03,  1.1893e-02, -2.7683e-02],\n                [ 7.9950e-03, -1.0707e-02, -2.7129e-02],\n                [-1.4682e-02, -1.0616e-02, -2.1142e-02]]],\n      \n      \n              [[[ 9.8951e-03, -4.4690e-03,  6.7779e-03],\n                [-2.3608e-02,  5.4625e-03, -2.9447e-02],\n                [-9.6645e-03, -1.0367e-02, -2.4778e-02]],\n      \n               [[ 2.8157e-02, -1.1586e-02, -2.7958e-02],\n                [-4.5807e-03, -1.5452e-03, -1.2691e-02],\n                [ 1.1582e-02, -1.1323e-02,  4.9794e-04]],\n      \n               [[ 2.3008e-02, -2.7096e-02, -1.3287e-02],\n                [ 1.1257e-02,  2.7415e-02, -1.5346e-02],\n                [-2.3650e-02, -7.7052e-03, -1.7947e-02]],\n      \n               ...,\n      \n               [[-4.9685e-03,  3.3347e-04,  2.5964e-02],\n                [-2.2633e-02, -1.5058e-02,  1.6412e-02],\n                [-1.9153e-02, -9.9577e-03, -2.4741e-02]],\n      \n               [[ 2.3883e-02, -7.9677e-03, -7.4520e-03],\n                [ 2.8212e-02, -1.1007e-02, -7.4798e-03],\n                [ 2.2571e-02, -7.6998e-03, -1.7194e-02]],\n      \n               [[-2.1267e-03, -1.0467e-02, -1.2959e-02],\n                [ 2.7820e-02, -1.0370e-02,  2.4549e-02],\n                [ 1.9846e-02,  2.4248e-02,  2.7654e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[-1.8437e-02,  1.2596e-02, -1.5962e-02],\n                [ 1.2786e-02,  1.5096e-02,  2.6953e-02],\n                [ 3.4446e-03,  2.2632e-02,  2.5532e-02]],\n      \n               [[ 2.3869e-02, -2.8872e-02,  2.6368e-02],\n                [ 6.8934e-03,  2.2600e-02,  5.1572e-03],\n                [ 4.1016e-03, -7.6255e-03, -1.5278e-02]],\n      \n               [[ 2.2234e-02,  2.4026e-02,  1.3703e-02],\n                [ 1.1314e-02, -1.6846e-02, -1.1868e-02],\n                [-2.9012e-02,  1.4382e-02,  2.7439e-02]],\n      \n               ...,\n      \n               [[-1.2290e-02,  1.8249e-02,  1.5882e-02],\n                [-8.1558e-03, -4.3683e-03,  1.6799e-02],\n                [-2.4762e-02,  1.8102e-02, -1.8698e-02]],\n      \n               [[ 4.4659e-03, -2.3784e-02, -2.5094e-02],\n                [ 5.5255e-03,  7.4340e-03,  1.8577e-02],\n                [-2.6435e-02, -4.3896e-03,  2.4891e-02]],\n      \n               [[ 4.0476e-03,  2.3317e-03,  1.2058e-02],\n                [-7.4126e-03,  1.9024e-03, -7.6235e-03],\n                [ 1.6546e-02, -1.3183e-02, -1.1704e-02]]],\n      \n      \n              [[[ 1.2517e-02,  2.5579e-02, -1.8007e-02],\n                [-2.5768e-02,  7.6004e-03, -2.4977e-02],\n                [ 1.5350e-02,  1.1974e-02, -1.2333e-02]],\n      \n               [[-1.9971e-02,  1.1772e-02,  1.7968e-02],\n                [ 4.6129e-03,  1.1589e-04, -1.0756e-02],\n                [-1.8381e-02, -8.4197e-03,  2.3026e-02]],\n      \n               [[-1.7439e-03, -6.6339e-03, -9.4240e-03],\n                [ 7.2481e-03, -9.2135e-03, -9.4097e-03],\n                [-1.3765e-02,  1.6586e-02,  2.0286e-02]],\n      \n               ...,\n      \n               [[-8.7051e-03,  2.3657e-02,  1.9596e-03],\n                [-4.3985e-03, -2.8985e-02,  9.7407e-03],\n                [-7.0887e-03,  1.8645e-02,  1.6572e-02]],\n      \n               [[ 1.0976e-02,  1.1368e-02,  1.3854e-02],\n                [ 2.3548e-02,  2.4586e-02, -2.2279e-02],\n                [-6.5783e-03,  9.4706e-03,  2.2279e-02]],\n      \n               [[ 1.0540e-02, -1.5165e-02,  7.7608e-03],\n                [-1.6353e-02,  2.5401e-02,  1.5268e-02],\n                [-1.3822e-02, -2.2832e-02,  2.1563e-02]]],\n      \n      \n              [[[ 4.6617e-03,  1.8359e-02, -1.4369e-02],\n                [ 2.1194e-02,  2.9186e-02, -2.8847e-02],\n                [ 1.0820e-02,  1.1054e-02, -2.5664e-02]],\n      \n               [[-7.2897e-03,  2.5317e-02, -9.7990e-03],\n                [-2.7151e-02, -1.8406e-02, -2.6402e-02],\n                [ 2.6649e-04,  1.6623e-03, -1.3184e-02]],\n      \n               [[ 1.2375e-02, -7.4139e-03,  2.5477e-02],\n                [-2.8640e-03,  2.7645e-02,  1.6944e-02],\n                [ 4.3109e-03, -2.1037e-02, -2.8509e-02]],\n      \n               ...,\n      \n               [[-6.3582e-04,  2.7747e-07,  2.3397e-02],\n                [-1.4801e-02, -8.2053e-03, -2.9338e-02],\n                [-3.8082e-03,  1.0972e-02, -1.0998e-02]],\n      \n               [[ 2.1272e-02,  1.6327e-02,  1.7292e-02],\n                [ 1.9414e-02, -2.2706e-02,  2.1439e-02],\n                [ 1.6340e-02,  2.0757e-02, -2.6712e-02]],\n      \n               [[ 1.4102e-02, -2.9338e-02, -4.8591e-04],\n                [ 2.5667e-02,  1.6259e-02,  1.7639e-02],\n                [ 2.0697e-02, -9.7526e-03, -1.3491e-02]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-9.8707e-03, -2.5877e-02,  2.3721e-03, -7.5201e-03, -7.6153e-04,\n              -9.6972e-03, -1.1721e-02,  6.5584e-03, -1.1520e-02,  1.4810e-02,\n              -3.2727e-02,  3.4064e-02,  1.2635e-02,  1.8563e-02, -1.1720e-02,\n               6.3974e-03,  2.5412e-05,  1.2091e-03, -2.4186e-02, -1.3983e-02,\n               7.7200e-03,  1.5571e-02, -6.9694e-03,  2.0735e-03, -2.5274e-02,\n               4.4953e-03, -1.8267e-02, -1.4145e-02, -5.7894e-03,  7.1828e-03,\n              -3.4569e-03,  2.7226e-02,  5.4980e-03, -9.8672e-03,  8.0669e-03,\n              -1.4148e-02,  2.1983e-02, -9.5625e-03, -2.0089e-02,  1.0563e-02,\n               9.7944e-03, -1.6713e-02,  1.1769e-03, -2.0472e-02, -3.1890e-02,\n               9.9745e-03, -1.1203e-02,  2.3430e-02, -7.3077e-03,  4.8199e-03,\n              -1.2776e-03, -1.9764e-02, -1.6217e-02, -1.6183e-02, -1.0897e-04,\n               1.0306e-02,  9.2858e-03,  1.5488e-02, -5.3164e-03,  2.2652e-02,\n               2.1931e-02,  1.6744e-03, -9.6372e-03, -3.8700e-03,  5.5209e-04,\n               4.5851e-03, -1.1021e-02,  1.0820e-02, -3.4738e-03,  5.0582e-03,\n              -4.4240e-03, -1.1334e-02, -8.7011e-03,  2.5308e-02,  3.0676e-03,\n               2.4973e-02,  1.6769e-03, -1.1953e-02,  2.0154e-02,  2.3615e-02,\n              -1.5772e-02,  6.7208e-06, -1.6986e-02, -3.7049e-04, -2.9062e-03,\n               8.7869e-04, -1.5946e-02,  1.7916e-03, -1.7843e-02,  3.9396e-02,\n              -4.4941e-03, -2.4184e-03, -1.1958e-02, -1.9216e-03,  3.9932e-02,\n               1.9315e-04, -5.6066e-03, -2.6598e-03,  5.8255e-03,  2.4524e-02,\n              -1.0116e-02,  1.3402e-02, -5.3495e-03,  4.3754e-03, -1.1426e-02,\n               4.4404e-03,  2.3908e-02, -4.6822e-04,  4.5943e-03,  4.2083e-03,\n              -8.0541e-05,  2.7246e-02, -1.6914e-02,  2.7702e-02,  3.8377e-03,\n              -2.1551e-03, -5.0661e-03, -2.5288e-02, -1.9444e-02,  4.0149e-02,\n              -2.0087e-02,  7.1539e-04,  3.3449e-03, -1.0094e-02, -4.5215e-05,\n               2.7322e-04,  5.7518e-03, -1.1365e-02,  3.0098e-03,  5.2502e-03,\n               1.0029e-02, -2.4866e-02,  1.2604e-02, -4.0129e-03, -1.0427e-02,\n               9.5317e-03,  1.9636e-02, -1.1051e-02, -8.8138e-03, -5.3400e-03,\n              -6.9673e-03, -1.0994e-02, -5.1487e-03, -7.4788e-03, -1.1433e-02,\n              -3.3579e-02, -3.6772e-04, -4.1864e-03,  8.3639e-05, -5.2108e-03,\n              -5.0385e-04, -1.8582e-03, -1.8737e-03, -2.3820e-02,  3.2324e-02,\n               2.8685e-02,  6.7513e-03,  2.3065e-02, -9.0792e-03,  4.3284e-02,\n               8.8526e-03, -1.2821e-02,  3.5397e-02,  1.3668e-02, -2.0167e-02,\n               2.4321e-02, -2.1301e-02, -2.5433e-02,  1.1744e-03, -2.9363e-02,\n               1.1381e-02,  1.3618e-03, -1.0108e-02,  5.7813e-03, -1.1773e-02,\n               6.9388e-03, -4.5379e-04, -1.9486e-02, -6.6694e-03, -4.3225e-03,\n              -2.1988e-02,  3.0676e-02,  3.9990e-03, -8.0866e-03, -8.5969e-04,\n               2.4594e-02, -1.0828e-03,  1.0313e-02,  2.3348e-02,  2.9541e-02,\n              -2.4122e-02, -1.4954e-02,  8.6910e-03,  2.9658e-02, -3.1422e-02,\n               1.5099e-02,  9.2899e-03,  5.6782e-04, -5.0299e-03, -3.9972e-02,\n               6.7514e-03,  7.3840e-03, -1.0672e-02, -1.3765e-02, -1.1705e-02,\n               9.3672e-03, -1.2169e-04,  2.8876e-02,  9.1521e-03, -2.8323e-02,\n               2.4024e-02, -5.1268e-03, -1.9358e-02,  1.3196e-03,  2.2896e-02,\n               2.1027e-02,  4.4906e-02,  8.5674e-03, -2.7173e-02,  6.5476e-04,\n              -5.9701e-03,  7.6607e-03, -3.0118e-03,  8.8834e-03,  1.8188e-02,\n              -2.4079e-03,  1.5254e-02,  1.0728e-02, -8.4714e-03, -5.5152e-04,\n               3.7393e-03,  3.8382e-03,  1.4632e-02, -6.9167e-03,  2.2399e-02,\n              -2.1156e-03, -2.7805e-02,  1.0286e-02, -5.8827e-03, -2.6329e-02,\n              -2.8657e-02, -3.2080e-02,  1.2897e-02, -5.0396e-04,  1.1543e-02,\n               1.4396e-02, -1.3595e-02,  1.2177e-02,  1.0028e-02, -6.7259e-03,\n               1.1261e-02, -1.1036e-02,  1.4474e-03,  7.9496e-03, -2.3946e-02,\n              -1.4945e-02], grad_fn=<AddBackward0>), self.running_var=tensor([0.9091, 0.9094, 0.9141, 0.9098, 0.9121, 0.9097, 0.9215, 0.9105, 0.9092,\n              0.9151, 0.9177, 0.9141, 0.9123, 0.9116, 0.9088, 0.9169, 0.9092, 0.9118,\n              0.9151, 0.9141, 0.9118, 0.9122, 0.9101, 0.9106, 0.9135, 0.9137, 0.9164,\n              0.9090, 0.9173, 0.9104, 0.9108, 0.9183, 0.9185, 0.9127, 0.9089, 0.9119,\n              0.9108, 0.9081, 0.9128, 0.9166, 0.9114, 0.9151, 0.9129, 0.9154, 0.9201,\n              0.9083, 0.9099, 0.9164, 0.9117, 0.9149, 0.9171, 0.9111, 0.9141, 0.9170,\n              0.9157, 0.9117, 0.9153, 0.9206, 0.9118, 0.9090, 0.9117, 0.9115, 0.9109,\n              0.9131, 0.9106, 0.9154, 0.9152, 0.9127, 0.9090, 0.9118, 0.9122, 0.9128,\n              0.9123, 0.9257, 0.9164, 0.9131, 0.9100, 0.9224, 0.9113, 0.9250, 0.9099,\n              0.9180, 0.9170, 0.9142, 0.9115, 0.9120, 0.9111, 0.9099, 0.9119, 0.9167,\n              0.9106, 0.9236, 0.9151, 0.9109, 0.9155, 0.9148, 0.9124, 0.9084, 0.9139,\n              0.9104, 0.9127, 0.9086, 0.9164, 0.9165, 0.9126, 0.9113, 0.9134, 0.9138,\n              0.9154, 0.9122, 0.9117, 0.9113, 0.9147, 0.9124, 0.9159, 0.9086, 0.9144,\n              0.9121, 0.9112, 0.9101, 0.9118, 0.9147, 0.9084, 0.9102, 0.9133, 0.9118,\n              0.9149, 0.9113, 0.9111, 0.9175, 0.9092, 0.9124, 0.9170, 0.9078, 0.9090,\n              0.9102, 0.9149, 0.9132, 0.9099, 0.9085, 0.9120, 0.9083, 0.9143, 0.9168,\n              0.9115, 0.9145, 0.9104, 0.9153, 0.9138, 0.9125, 0.9109, 0.9076, 0.9169,\n              0.9197, 0.9172, 0.9204, 0.9119, 0.9111, 0.9098, 0.9097, 0.9112, 0.9146,\n              0.9138, 0.9182, 0.9092, 0.9179, 0.9088, 0.9113, 0.9105, 0.9174, 0.9084,\n              0.9106, 0.9154, 0.9131, 0.9154, 0.9127, 0.9105, 0.9127, 0.9112, 0.9134,\n              0.9196, 0.9121, 0.9114, 0.9179, 0.9179, 0.9139, 0.9194, 0.9141, 0.9113,\n              0.9136, 0.9155, 0.9185, 0.9129, 0.9258, 0.9140, 0.9088, 0.9118, 0.9160,\n              0.9116, 0.9196, 0.9165, 0.9112, 0.9127, 0.9159, 0.9138, 0.9148, 0.9144,\n              0.9125, 0.9145, 0.9150, 0.9150, 0.9105, 0.9115, 0.9237, 0.9089, 0.9126,\n              0.9149, 0.9232, 0.9093, 0.9124, 0.9159, 0.9156, 0.9108, 0.9107, 0.9178,\n              0.9104, 0.9112, 0.9155, 0.9120, 0.9110, 0.9100, 0.9126, 0.9141, 0.9116,\n              0.9149, 0.9103, 0.9240, 0.9090, 0.9133, 0.9103, 0.9099, 0.9185, 0.9127,\n              0.9116, 0.9139, 0.9176, 0.9122, 0.9146, 0.9170, 0.9085, 0.9164, 0.9161,\n              0.9147, 0.9094, 0.9132, 0.9093], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 1.9462e-02, -8.4213e-03, -6.2029e-03],\n                [ 5.5704e-03,  7.7963e-03,  1.5108e-02],\n                [-1.5347e-03,  1.9503e-02, -4.5004e-03]],\n      \n               [[ 7.8319e-03, -1.1752e-04,  2.2467e-03],\n                [-5.2420e-03,  5.7619e-03, -5.5450e-03],\n                [-6.2578e-03,  1.5024e-02, -7.5310e-03]],\n      \n               [[ 3.3592e-03, -6.5217e-03,  1.2046e-03],\n                [-1.9710e-02,  6.3546e-03,  1.2060e-02],\n                [-2.1354e-04, -9.8517e-03, -1.6268e-02]],\n      \n               ...,\n      \n               [[-8.0341e-03, -5.4890e-03, -1.4373e-02],\n                [-1.5310e-02,  1.1085e-02,  8.2796e-04],\n                [ 2.0324e-02, -1.6088e-02, -1.0980e-02]],\n      \n               [[-1.2709e-02, -1.4123e-02,  1.2658e-02],\n                [-3.5859e-03,  6.0905e-03, -1.9573e-03],\n                [ 1.4335e-02, -3.1969e-03,  1.4708e-02]],\n      \n               [[ 1.9883e-02,  1.6476e-03,  1.4431e-02],\n                [-1.4377e-02, -1.9071e-02, -8.8299e-05],\n                [ 1.7985e-02, -1.1025e-02, -4.8867e-03]]],\n      \n      \n              [[[ 1.4617e-02, -1.0802e-03, -1.4110e-04],\n                [-1.9581e-02, -1.7268e-02, -1.6593e-02],\n                [ 6.3263e-03, -1.1954e-02, -1.6451e-03]],\n      \n               [[ 1.1239e-02,  1.1822e-02,  1.2130e-03],\n                [-1.0454e-02,  1.4085e-02, -1.2905e-02],\n                [-1.3434e-02,  5.2506e-03,  1.4340e-02]],\n      \n               [[-1.9440e-02, -1.1383e-02,  9.0347e-03],\n                [-2.4465e-05, -1.3303e-02, -4.0653e-03],\n                [ 1.7023e-02,  1.1866e-02, -1.7882e-02]],\n      \n               ...,\n      \n               [[ 5.6298e-03,  2.0892e-03, -1.4868e-02],\n                [-1.3042e-02, -1.9282e-02,  5.5489e-03],\n                [-1.7082e-02,  1.5197e-02,  7.0833e-03]],\n      \n               [[ 1.1672e-02,  1.4953e-02, -3.6553e-03],\n                [-3.5550e-03,  4.4269e-03, -1.4648e-02],\n                [ 1.0354e-02,  5.0858e-03,  1.8008e-02]],\n      \n               [[ 9.5912e-03,  1.8436e-02,  8.0096e-03],\n                [ 1.5184e-02,  3.8433e-03, -4.2931e-03],\n                [-7.2596e-03, -1.4179e-02,  1.4461e-02]]],\n      \n      \n              [[[-2.0506e-02,  1.0932e-02, -6.8330e-03],\n                [ 9.4968e-03, -6.2838e-03, -5.3211e-03],\n                [-8.5245e-03, -1.1625e-02,  1.9392e-02]],\n      \n               [[-1.2070e-02,  2.2758e-03,  3.5684e-03],\n                [-3.4339e-03, -1.0911e-02,  1.7932e-02],\n                [ 8.6560e-04, -1.6355e-02,  8.6710e-03]],\n      \n               [[ 3.0375e-03, -5.4796e-03, -2.0732e-03],\n                [-1.9048e-02, -5.1527e-03,  5.7569e-03],\n                [ 1.5838e-02,  2.2424e-03, -1.6419e-02]],\n      \n               ...,\n      \n               [[ 6.3878e-03, -1.9611e-02,  1.0850e-03],\n                [ 4.3493e-03, -5.1541e-05, -8.0211e-03],\n                [-6.6055e-03,  1.2503e-02,  7.6594e-03]],\n      \n               [[ 1.9257e-03, -3.9312e-03, -8.4684e-03],\n                [-2.3278e-03, -2.0601e-02,  1.4001e-02],\n                [ 8.7506e-03,  2.0492e-02,  8.9075e-04]],\n      \n               [[ 9.9193e-03, -1.2472e-02,  5.3823e-03],\n                [ 1.8994e-03, -4.5439e-03, -1.3905e-02],\n                [ 1.7069e-02,  3.8383e-03, -7.7398e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[-2.7331e-03,  1.3516e-02,  1.1992e-02],\n                [ 1.1980e-02, -1.9109e-02,  1.2573e-03],\n                [-1.9677e-02, -2.4728e-03,  1.9100e-02]],\n      \n               [[-1.1722e-03, -7.8378e-03, -9.4922e-03],\n                [-1.0600e-02,  2.0528e-02,  1.8987e-02],\n                [-1.4704e-02, -1.1616e-02,  3.9919e-03]],\n      \n               [[ 1.3353e-02,  6.3310e-03,  9.7872e-03],\n                [ 4.9117e-03, -1.9613e-02, -1.3920e-02],\n                [-5.3096e-03, -2.2377e-03, -1.7148e-02]],\n      \n               ...,\n      \n               [[-1.1976e-02,  6.1166e-03, -5.7639e-03],\n                [-1.0706e-02,  1.2023e-02,  3.4268e-03],\n                [-1.7448e-02,  5.4648e-03,  9.6276e-03]],\n      \n               [[ 5.9670e-04,  8.0883e-03,  1.3264e-02],\n                [ 1.4126e-03, -1.8026e-02, -1.3447e-02],\n                [-5.6694e-03, -1.3839e-02, -1.3750e-02]],\n      \n               [[-1.0607e-03,  1.4224e-02,  6.2046e-03],\n                [ 9.8440e-03,  3.3625e-03,  1.0620e-03],\n                [-9.4101e-03, -1.7724e-02,  9.3794e-03]]],\n      \n      \n              [[[-9.1451e-03, -5.0454e-03, -1.6757e-02],\n                [-2.0525e-02,  9.7228e-03, -4.5839e-03],\n                [ 1.9529e-02,  4.0566e-03,  6.4905e-03]],\n      \n               [[-8.7166e-03, -3.1395e-03,  1.8189e-02],\n                [-5.8298e-03, -3.3634e-03, -6.2668e-03],\n                [-1.1877e-02, -1.7546e-02, -2.0095e-02]],\n      \n               [[-2.8355e-03,  4.1408e-03,  6.6673e-03],\n                [ 1.8305e-02,  6.4857e-03,  2.7055e-03],\n                [ 1.6723e-02, -4.7616e-03, -6.3638e-03]],\n      \n               ...,\n      \n               [[-6.9430e-03, -1.9769e-02,  1.4602e-02],\n                [ 9.9365e-03, -3.2957e-03,  6.6118e-03],\n                [-1.2137e-02, -1.7553e-02, -2.6069e-03]],\n      \n               [[ 1.5981e-03, -1.9164e-03, -1.3754e-02],\n                [ 3.3153e-03, -1.7705e-02,  5.6220e-03],\n                [ 1.9806e-02,  1.1002e-02,  2.9218e-03]],\n      \n               [[-5.4087e-03, -1.7395e-02, -7.9037e-03],\n                [-7.8394e-03, -1.1870e-02,  1.6599e-02],\n                [ 1.7905e-02, -1.0261e-02,  9.2751e-03]]],\n      \n      \n              [[[ 1.8349e-02,  1.3086e-02,  3.2562e-03],\n                [-1.3104e-03, -5.4428e-03, -2.2331e-03],\n                [ 1.5909e-02,  5.7828e-03,  1.5627e-04]],\n      \n               [[ 1.4395e-02,  1.7680e-02, -1.7625e-02],\n                [-9.9346e-03,  1.1791e-02, -2.0391e-02],\n                [-1.0465e-02, -1.6171e-02, -1.2241e-02]],\n      \n               [[-1.6862e-02, -3.2236e-03, -1.8695e-02],\n                [-6.1936e-03,  1.2604e-02, -2.5645e-03],\n                [ 7.8904e-03,  5.3030e-03,  8.3481e-03]],\n      \n               ...,\n      \n               [[ 9.8827e-03,  2.2602e-04,  4.4849e-03],\n                [ 1.3890e-02,  9.0169e-04, -7.1914e-03],\n                [-7.5059e-03, -1.1371e-02, -1.3856e-03]],\n      \n               [[ 5.7476e-03, -2.0729e-02,  1.9315e-02],\n                [ 1.7555e-02, -1.9900e-02,  5.2504e-03],\n                [-1.1440e-02, -1.4896e-02, -1.9943e-03]],\n      \n               [[-1.4705e-02, -1.0491e-02,  1.5617e-02],\n                [-1.5233e-02, -1.9325e-02,  8.1498e-03],\n                [ 2.9480e-03,  1.1090e-02, -1.9816e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n             requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 1.2914e-02, -7.6852e-03,  7.6233e-03,  1.2736e-02,  4.8652e-03,\n               1.1509e-02,  6.0191e-03, -1.7141e-02,  5.4017e-03, -1.4170e-02,\n              -2.8953e-03,  2.4436e-02, -8.4522e-04, -4.9434e-03, -8.4309e-03,\n              -8.9396e-03, -4.4493e-03, -1.1524e-03,  1.0494e-02, -6.6322e-03,\n              -6.8711e-03, -3.4405e-02,  8.5309e-03, -1.0030e-02,  1.9834e-02,\n               1.8643e-02,  1.0602e-02, -1.0029e-04,  1.0404e-02, -5.0851e-03,\n               8.0962e-03, -1.1747e-02, -7.4695e-03, -1.5120e-02, -1.0429e-02,\n               2.0950e-02, -2.5757e-03, -9.0801e-03, -7.1530e-03, -1.1853e-02,\n               1.2570e-02,  4.5534e-04,  3.2161e-04, -1.2382e-02, -7.4646e-03,\n              -3.1561e-02, -4.7916e-03,  1.1636e-02, -2.4596e-03,  1.6551e-02,\n              -7.9922e-03, -1.2315e-02,  1.3846e-03, -9.5675e-03, -3.4056e-04,\n               2.3438e-02, -2.7525e-02, -7.4691e-03,  9.3266e-03, -1.3274e-02,\n              -1.6492e-03, -1.8534e-03,  2.8668e-03,  3.4744e-03,  7.7467e-03,\n               2.5200e-03,  4.2378e-03, -1.0149e-02, -1.0449e-02, -1.6984e-02,\n               1.3155e-02, -1.0779e-02, -2.1736e-02, -1.0708e-02,  1.0495e-02,\n              -1.6286e-04, -1.3468e-02,  5.7397e-04, -1.7076e-03, -4.7493e-03,\n              -2.7490e-03,  2.2677e-02,  1.2155e-02, -6.9575e-03, -1.0499e-03,\n               1.9804e-02, -1.5559e-02, -1.5191e-02,  1.5812e-02,  6.1843e-03,\n              -1.0114e-02,  3.5521e-03, -6.8331e-03,  1.8729e-03,  1.5257e-02,\n              -1.3938e-02,  1.6383e-02,  1.1211e-02, -4.6256e-03, -6.5685e-03,\n               6.8149e-03,  9.2462e-03, -1.8700e-02,  1.5432e-02,  1.4199e-02,\n               1.0166e-02,  5.3965e-03,  1.0389e-02, -7.0891e-03, -2.9666e-04,\n              -1.2070e-02, -1.0307e-02,  7.4147e-03,  3.3331e-02, -4.8264e-03,\n               1.8491e-02,  1.0274e-02, -9.9159e-03, -8.5081e-03,  4.3822e-03,\n              -4.9115e-03,  9.6398e-03, -4.2830e-03,  1.4142e-03,  1.7421e-02,\n               3.8381e-03,  1.1217e-02,  4.3492e-03, -1.6791e-02,  3.7351e-03,\n              -1.3733e-02, -9.4138e-03,  7.3810e-03, -2.2651e-03,  2.0868e-02,\n               1.3955e-02,  5.3897e-03,  3.4217e-04, -2.8036e-02, -1.5526e-02,\n              -1.6463e-02,  1.1583e-02,  5.0321e-03,  4.7904e-03,  1.9066e-02,\n              -1.4249e-02,  2.1906e-02,  6.1925e-03, -1.3762e-02,  1.9047e-03,\n               1.1907e-02,  2.3641e-03, -1.3075e-02, -1.2624e-02, -1.8539e-02,\n               6.1929e-03, -5.3833e-03, -6.1977e-04,  7.0130e-03, -4.1460e-03,\n               1.3781e-02, -9.9870e-03, -1.4052e-03,  2.4162e-03,  8.3165e-03,\n               1.0261e-02, -1.8615e-03,  9.8300e-03, -8.6889e-03,  3.2074e-03,\n              -3.8772e-03, -2.1395e-02, -4.9793e-04, -3.8239e-03, -2.5668e-03,\n               1.5479e-02,  1.1575e-02, -7.8405e-03, -1.0264e-02,  1.0747e-02,\n              -1.3333e-02,  2.6070e-02, -8.1161e-03,  1.5616e-03,  7.6627e-03,\n              -3.8934e-03, -5.3176e-03, -1.2049e-02,  5.4769e-03, -3.0629e-03,\n              -3.8265e-03,  7.5621e-04, -5.6209e-04,  1.2451e-03, -3.1201e-04,\n              -7.5426e-03,  1.3240e-02,  9.2530e-03, -1.8162e-02,  1.9876e-03,\n              -1.8325e-02,  2.5808e-03,  9.1373e-05, -1.5355e-02,  3.1434e-03,\n              -5.8149e-03, -5.7001e-03, -1.4007e-02,  1.0815e-03, -4.4717e-03,\n               1.1203e-02,  9.6563e-03,  1.0581e-02,  2.1731e-03, -1.2318e-02,\n              -1.1167e-02,  1.7921e-02, -4.3605e-03,  1.1267e-03,  7.8489e-04,\n               3.2517e-03, -1.1452e-02, -1.6022e-02, -1.3077e-02,  1.1655e-02,\n              -1.4029e-02, -2.1463e-02, -3.3053e-02,  2.0178e-02,  6.8563e-04,\n              -1.5276e-03,  5.4747e-03, -1.7643e-02, -2.2100e-03, -1.1827e-02,\n               6.2742e-03,  1.4691e-02, -1.1868e-02,  7.8345e-03,  1.6822e-02,\n              -5.2792e-03,  1.3012e-02, -1.4428e-02, -1.5389e-02, -2.0170e-03,\n               4.5598e-03,  4.2942e-03, -9.9249e-03,  5.3594e-04,  1.5665e-02,\n               2.0509e-02,  7.1555e-03, -2.2759e-02, -2.4558e-03, -1.3116e-02,\n              -9.2329e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9045, 0.9048, 0.9040, 0.9061, 0.9040, 0.9042, 0.9056, 0.9046, 0.9067,\n              0.9047, 0.9044, 0.9048, 0.9044, 0.9051, 0.9073, 0.9038, 0.9048, 0.9064,\n              0.9056, 0.9053, 0.9049, 0.9062, 0.9077, 0.9042, 0.9076, 0.9053, 0.9055,\n              0.9059, 0.9073, 0.9050, 0.9043, 0.9052, 0.9048, 0.9050, 0.9054, 0.9058,\n              0.9050, 0.9047, 0.9116, 0.9042, 0.9156, 0.9058, 0.9063, 0.9085, 0.9060,\n              0.9049, 0.9097, 0.9086, 0.9068, 0.9119, 0.9072, 0.9061, 0.9066, 0.9039,\n              0.9051, 0.9067, 0.9047, 0.9051, 0.9056, 0.9048, 0.9074, 0.9049, 0.9095,\n              0.9076, 0.9064, 0.9041, 0.9054, 0.9060, 0.9047, 0.9066, 0.9065, 0.9055,\n              0.9064, 0.9054, 0.9072, 0.9051, 0.9052, 0.9043, 0.9042, 0.9075, 0.9053,\n              0.9060, 0.9063, 0.9049, 0.9057, 0.9078, 0.9066, 0.9075, 0.9047, 0.9064,\n              0.9057, 0.9046, 0.9074, 0.9041, 0.9041, 0.9048, 0.9077, 0.9057, 0.9042,\n              0.9054, 0.9046, 0.9108, 0.9077, 0.9082, 0.9045, 0.9058, 0.9067, 0.9056,\n              0.9058, 0.9044, 0.9064, 0.9055, 0.9045, 0.9072, 0.9061, 0.9044, 0.9049,\n              0.9049, 0.9042, 0.9054, 0.9064, 0.9053, 0.9048, 0.9052, 0.9067, 0.9074,\n              0.9052, 0.9061, 0.9055, 0.9087, 0.9041, 0.9097, 0.9046, 0.9078, 0.9097,\n              0.9042, 0.9042, 0.9060, 0.9068, 0.9037, 0.9049, 0.9053, 0.9041, 0.9050,\n              0.9063, 0.9056, 0.9057, 0.9045, 0.9086, 0.9069, 0.9054, 0.9044, 0.9068,\n              0.9070, 0.9046, 0.9048, 0.9045, 0.9058, 0.9061, 0.9058, 0.9039, 0.9105,\n              0.9041, 0.9070, 0.9046, 0.9050, 0.9043, 0.9046, 0.9066, 0.9054, 0.9072,\n              0.9073, 0.9040, 0.9059, 0.9043, 0.9056, 0.9098, 0.9064, 0.9074, 0.9066,\n              0.9059, 0.9050, 0.9048, 0.9054, 0.9051, 0.9054, 0.9043, 0.9079, 0.9043,\n              0.9056, 0.9114, 0.9054, 0.9059, 0.9041, 0.9060, 0.9043, 0.9060, 0.9065,\n              0.9044, 0.9048, 0.9060, 0.9054, 0.9046, 0.9050, 0.9046, 0.9046, 0.9047,\n              0.9045, 0.9036, 0.9060, 0.9050, 0.9063, 0.9064, 0.9077, 0.9043, 0.9059,\n              0.9057, 0.9089, 0.9047, 0.9057, 0.9077, 0.9054, 0.9057, 0.9052, 0.9054,\n              0.9044, 0.9050, 0.9096, 0.9098, 0.9045, 0.9044, 0.9051, 0.9045, 0.9062,\n              0.9040, 0.9064, 0.9074, 0.9059, 0.9054, 0.9040, 0.9056, 0.9105, 0.9055,\n              0.9101, 0.9079, 0.9045, 0.9065, 0.9056, 0.9050, 0.9059, 0.9049, 0.9047,\n              0.9051, 0.9053, 0.9052, 0.9052], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[-0.0822]],\n        \n                 [[ 0.0744]],\n        \n                 [[-0.0829]],\n        \n                 ...,\n        \n                 [[-0.0605]],\n        \n                 [[-0.0704]],\n        \n                 [[ 0.0468]]],\n        \n        \n                [[[-0.0259]],\n        \n                 [[ 0.0775]],\n        \n                 [[-0.0725]],\n        \n                 ...,\n        \n                 [[-0.0595]],\n        \n                 [[-0.0870]],\n        \n                 [[-0.0478]]],\n        \n        \n                [[[-0.0192]],\n        \n                 [[ 0.0761]],\n        \n                 [[ 0.0584]],\n        \n                 ...,\n        \n                 [[-0.0238]],\n        \n                 [[ 0.0798]],\n        \n                 [[ 0.0198]]],\n        \n        \n                ...,\n        \n        \n                [[[-0.0589]],\n        \n                 [[ 0.0059]],\n        \n                 [[ 0.0473]],\n        \n                 ...,\n        \n                 [[ 0.0013]],\n        \n                 [[ 0.0176]],\n        \n                 [[ 0.0115]]],\n        \n        \n                [[[ 0.0766]],\n        \n                 [[ 0.0691]],\n        \n                 [[-0.0232]],\n        \n                 ...,\n        \n                 [[ 0.0759]],\n        \n                 [[ 0.0305]],\n        \n                 [[-0.0062]]],\n        \n        \n                [[[-0.0356]],\n        \n                 [[ 0.0343]],\n        \n                 [[ 0.0613]],\n        \n                 ...,\n        \n                 [[ 0.0755]],\n        \n                 [[-0.0332]],\n        \n                 [[-0.0854]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n               requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-0.0153,  0.0206,  0.0376,  0.0407, -0.0226,  0.0100,  0.0048, -0.0042,\n                 0.0255,  0.0375,  0.0336, -0.0048, -0.0733,  0.0245, -0.0019, -0.0208,\n                 0.0145, -0.0389, -0.0072,  0.0001,  0.0233,  0.0615,  0.0671,  0.0034,\n                -0.0247,  0.0129,  0.0595, -0.0090,  0.0398,  0.0155,  0.0169,  0.0351,\n                 0.0155, -0.0355, -0.0220,  0.0136,  0.0208,  0.0086,  0.0247, -0.0245,\n                -0.0452,  0.0225,  0.0167,  0.0468,  0.0218, -0.0152,  0.0267, -0.0239,\n                -0.0382, -0.0070,  0.0285,  0.0035, -0.0250, -0.0239,  0.0061,  0.0213,\n                -0.0282,  0.0213,  0.0233, -0.0005,  0.0600, -0.0145, -0.0256,  0.0573,\n                 0.0066, -0.0109,  0.0189,  0.0430,  0.0555, -0.0388, -0.0546, -0.0089,\n                 0.0090, -0.0202, -0.0386,  0.0346, -0.0308,  0.0841, -0.0052,  0.0648,\n                -0.0032, -0.0078, -0.0058,  0.0625, -0.0395,  0.0474,  0.0007, -0.0239,\n                 0.0266, -0.0100, -0.0301,  0.0188,  0.0139, -0.0179,  0.0368, -0.0329,\n                 0.0288, -0.0450, -0.0520, -0.0057, -0.0123, -0.0039, -0.0161, -0.0367,\n                -0.0119, -0.0113,  0.0089,  0.0270,  0.0094, -0.0129, -0.0384, -0.0036,\n                -0.0270,  0.0249, -0.0498, -0.0329, -0.0235,  0.0124,  0.0332,  0.0389,\n                 0.0456,  0.0322, -0.0060,  0.0083, -0.0463, -0.0131, -0.0527, -0.0343,\n                 0.0496, -0.0114,  0.0412,  0.0169, -0.0008,  0.0370, -0.0112,  0.0013,\n                 0.0312,  0.0367,  0.0165,  0.0137,  0.0168, -0.0187,  0.0173,  0.0085,\n                 0.0030, -0.0088, -0.0016, -0.0228, -0.0129, -0.0048,  0.0373,  0.0207,\n                -0.0200, -0.0541,  0.0122, -0.0328, -0.0035, -0.0200, -0.0156,  0.0339,\n                 0.0118, -0.0471,  0.0421,  0.0448,  0.0378, -0.0126, -0.0544, -0.0130,\n                 0.0162, -0.0507,  0.0281,  0.0502, -0.0030, -0.0263, -0.0194,  0.0163,\n                 0.0178,  0.0032,  0.0323, -0.0312,  0.0029, -0.0024, -0.0418, -0.0085,\n                 0.0340,  0.0034, -0.0434,  0.0863, -0.0216, -0.0040, -0.0027, -0.0251,\n                 0.0282,  0.0010, -0.0232, -0.0334,  0.0224, -0.0283, -0.0256,  0.0163,\n                 0.0726, -0.0183,  0.0346,  0.0140, -0.0232, -0.0372, -0.0142, -0.0610,\n                 0.0056,  0.0176,  0.0180, -0.0331, -0.0328, -0.0514,  0.0486,  0.0169,\n                -0.0015, -0.0237,  0.0276,  0.0338, -0.0368, -0.0089, -0.0347, -0.0344,\n                -0.0071,  0.0275, -0.0615,  0.0313,  0.0400, -0.0180,  0.0087,  0.0541,\n                 0.0306,  0.0350,  0.0069,  0.0427, -0.0237, -0.0012,  0.0192,  0.0343,\n                 0.0077, -0.0343, -0.0016, -0.0283,  0.0131, -0.0193,  0.0095,  0.0360,\n                 0.0027, -0.0345,  0.0148, -0.0382,  0.0009,  0.0293,  0.0343,  0.0065],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9199, 0.9145, 0.9197, 0.9148, 0.9173, 0.9223, 0.9202, 0.9166, 0.9180,\n                0.9210, 0.9177, 0.9163, 0.9248, 0.9162, 0.9167, 0.9167, 0.9257, 0.9253,\n                0.9180, 0.9218, 0.9166, 0.9199, 0.9213, 0.9169, 0.9171, 0.9186, 0.9126,\n                0.9247, 0.9216, 0.9216, 0.9186, 0.9161, 0.9215, 0.9208, 0.9296, 0.9192,\n                0.9151, 0.9220, 0.9142, 0.9191, 0.9285, 0.9216, 0.9378, 0.9205, 0.9151,\n                0.9187, 0.9179, 0.9192, 0.9217, 0.9328, 0.9169, 0.9122, 0.9117, 0.9156,\n                0.9158, 0.9213, 0.9239, 0.9150, 0.9158, 0.9185, 0.9155, 0.9193, 0.9164,\n                0.9248, 0.9225, 0.9249, 0.9227, 0.9228, 0.9252, 0.9189, 0.9368, 0.9161,\n                0.9214, 0.9246, 0.9233, 0.9309, 0.9274, 0.9272, 0.9213, 0.9197, 0.9185,\n                0.9250, 0.9253, 0.9249, 0.9166, 0.9244, 0.9234, 0.9238, 0.9162, 0.9247,\n                0.9161, 0.9152, 0.9240, 0.9225, 0.9251, 0.9441, 0.9192, 0.9173, 0.9187,\n                0.9233, 0.9172, 0.9233, 0.9200, 0.9236, 0.9173, 0.9168, 0.9126, 0.9191,\n                0.9171, 0.9315, 0.9228, 0.9240, 0.9206, 0.9169, 0.9176, 0.9254, 0.9204,\n                0.9215, 0.9172, 0.9148, 0.9247, 0.9232, 0.9215, 0.9123, 0.9172, 0.9253,\n                0.9124, 0.9204, 0.9244, 0.9144, 0.9119, 0.9106, 0.9147, 0.9196, 0.9215,\n                0.9174, 0.9162, 0.9239, 0.9212, 0.9219, 0.9268, 0.9190, 0.9206, 0.9170,\n                0.9149, 0.9141, 0.9224, 0.9183, 0.9184, 0.9197, 0.9184, 0.9155, 0.9159,\n                0.9210, 0.9270, 0.9172, 0.9155, 0.9151, 0.9198, 0.9139, 0.9202, 0.9243,\n                0.9138, 0.9160, 0.9142, 0.9139, 0.9181, 0.9180, 0.9121, 0.9272, 0.9143,\n                0.9269, 0.9209, 0.9191, 0.9162, 0.9186, 0.9210, 0.9177, 0.9149, 0.9117,\n                0.9149, 0.9216, 0.9173, 0.9151, 0.9140, 0.9137, 0.9236, 0.9189, 0.9181,\n                0.9209, 0.9178, 0.9118, 0.9111, 0.9149, 0.9161, 0.9168, 0.9136, 0.9192,\n                0.9164, 0.9236, 0.9220, 0.9189, 0.9232, 0.9204, 0.9171, 0.9336, 0.9246,\n                0.9157, 0.9238, 0.9193, 0.9197, 0.9171, 0.9321, 0.9230, 0.9286, 0.9221,\n                0.9178, 0.9179, 0.9313, 0.9195, 0.9192, 0.9162, 0.9157, 0.9242, 0.9255,\n                0.9184, 0.9319, 0.9181, 0.9134, 0.9161, 0.9179, 0.9262, 0.9175, 0.9215,\n                0.9155, 0.9260, 0.9127, 0.9163, 0.9269, 0.9182, 0.9275, 0.9252, 0.9170,\n                0.9188, 0.9152, 0.9206, 0.9191, 0.9248, 0.9214, 0.9228, 0.9161, 0.9297,\n                0.9160, 0.9162, 0.9187, 0.9133], grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [256, 128, 3, 3]], ["0.bn1.weight", [256]], ["0.bn1.bias", [256]], ["0.conv2.weight", [256, 256, 3, 3]], ["0.bn2.weight", [256]], ["0.bn2.bias", [256]], ["0.downsample.0.weight", [256, 128, 1, 1]], ["0.downsample.1.weight", [256]], ["0.downsample.1.bias", [256]]], "output_shape": [[512, 256, 2, 2]], "num_parameters": [294912, 256, 256, 589824, 256, 256, 32768, 256, 256]}, {"name": "layer4", "id": 140584624432848, "class_name": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(\n      self.stride=2, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[-3.4193e-03, -1.9959e-02,  1.6271e-02],\n                [ 4.6256e-03, -1.8649e-02,  1.1434e-02],\n                [ 1.7800e-02, -1.8404e-02, -9.2159e-03]],\n      \n               [[-1.2232e-02,  2.4538e-03, -1.5974e-02],\n                [ 1.7801e-03, -1.8495e-02,  1.0005e-02],\n                [ 1.7002e-02, -1.6602e-02, -6.6377e-03]],\n      \n               [[ 7.7735e-03,  1.4969e-02,  9.8363e-03],\n                [-1.5368e-02,  5.0689e-03,  1.8480e-02],\n                [ 6.5988e-03, -1.8238e-02,  1.4206e-02]],\n      \n               ...,\n      \n               [[ 1.7387e-02,  1.5024e-02,  9.2879e-03],\n                [ 7.9440e-03,  1.6513e-02, -1.4102e-02],\n                [-2.0192e-02, -1.4270e-02, -1.4086e-03]],\n      \n               [[-9.4816e-03,  1.9433e-02, -7.8743e-03],\n                [-8.8050e-03, -2.0785e-02, -6.6559e-03],\n                [ 4.9161e-03,  6.6729e-03,  1.5509e-02]],\n      \n               [[-1.7284e-02, -4.6515e-03,  1.5063e-02],\n                [ 1.7699e-02,  1.0122e-02, -1.6277e-02],\n                [ 1.4573e-02,  1.6023e-02, -1.4457e-02]]],\n      \n      \n              [[[ 1.6492e-02, -4.7978e-04,  1.8760e-02],\n                [ 1.9193e-02, -7.2055e-03,  1.3402e-02],\n                [ 8.1131e-03,  5.3162e-03, -5.2044e-03]],\n      \n               [[-8.5494e-04, -2.9363e-03, -1.2840e-02],\n                [ 9.2966e-05, -2.0756e-02, -2.0188e-02],\n                [-4.9820e-03, -3.1473e-04,  9.9013e-03]],\n      \n               [[-1.1980e-03, -1.8937e-02, -3.4097e-03],\n                [-7.1710e-03, -8.2014e-03, -1.0084e-02],\n                [-2.3927e-03, -6.1835e-03,  1.1310e-02]],\n      \n               ...,\n      \n               [[-1.5881e-02,  1.7514e-02, -1.1016e-02],\n                [-4.2417e-03,  4.3609e-03, -1.3281e-02],\n                [ 1.0578e-02, -1.7175e-02, -1.1868e-02]],\n      \n               [[ 1.5495e-02,  1.6718e-02,  1.4640e-02],\n                [ 9.7102e-03, -9.7401e-03,  9.1438e-03],\n                [-1.7674e-02, -1.2307e-02,  7.6521e-03]],\n      \n               [[ 1.1731e-02,  1.6143e-03,  1.9790e-02],\n                [ 1.3461e-02, -1.6059e-02,  3.0156e-03],\n                [-1.9783e-02,  1.6179e-02, -3.4204e-03]]],\n      \n      \n              [[[ 3.5445e-04, -4.6740e-03,  1.0159e-02],\n                [ 1.3151e-02, -1.5639e-02, -1.7258e-02],\n                [ 1.2213e-02, -1.8635e-02,  5.8679e-03]],\n      \n               [[ 1.1261e-02,  1.7081e-02,  6.6590e-03],\n                [-1.5657e-02,  7.4673e-03,  5.4273e-03],\n                [ 1.7725e-02,  5.4497e-03, -4.5101e-03]],\n      \n               [[ 1.4429e-02, -1.8587e-02, -4.7218e-03],\n                [ 7.1745e-03,  7.4585e-03,  1.1750e-02],\n                [-7.3857e-03,  2.6116e-04,  1.2197e-02]],\n      \n               ...,\n      \n               [[-3.1165e-03, -8.0431e-03, -1.9723e-02],\n                [ 1.1129e-02,  1.2208e-02, -2.2056e-03],\n                [-1.5413e-02, -6.4032e-03, -2.9322e-03]],\n      \n               [[-1.9846e-02, -1.3067e-02, -1.8860e-02],\n                [ 1.7429e-02,  1.0956e-02, -1.3094e-02],\n                [-1.6888e-02, -6.7490e-03, -1.4222e-02]],\n      \n               [[-1.5130e-02,  7.2759e-03,  3.9626e-03],\n                [ 2.3181e-03, -1.7210e-02, -2.0288e-02],\n                [-1.0319e-02, -9.8263e-03, -1.3161e-02]]],\n      \n      \n              ...,\n      \n      \n              [[[-6.1575e-04, -7.3020e-03, -4.7067e-03],\n                [-1.3080e-02,  7.6441e-03, -1.4157e-02],\n                [ 1.7734e-02, -3.3896e-03, -1.6091e-02]],\n      \n               [[ 9.4596e-03, -2.0794e-02,  7.6569e-03],\n                [ 1.0633e-02, -1.6425e-02,  1.4036e-02],\n                [-1.5204e-02,  1.9593e-02, -1.5982e-03]],\n      \n               [[-1.3456e-02,  1.2910e-02, -2.5627e-03],\n                [ 7.1104e-03,  4.7000e-03,  1.5347e-02],\n                [ 1.2250e-02,  5.9186e-03,  5.4586e-03]],\n      \n               ...,\n      \n               [[-1.0386e-02, -8.0540e-03, -1.1735e-02],\n                [-1.8086e-02, -5.4586e-03,  1.3598e-02],\n                [-1.3827e-02, -1.1660e-02,  4.4578e-03]],\n      \n               [[-2.7419e-03,  6.1785e-03,  1.6712e-02],\n                [ 2.2059e-03, -1.6073e-02,  1.0248e-02],\n                [ 6.0425e-03, -1.0101e-02,  1.0307e-02]],\n      \n               [[ 1.8784e-02, -1.5434e-03, -6.4869e-03],\n                [-1.9194e-02, -1.8047e-02, -9.2751e-03],\n                [-1.2995e-02,  1.9490e-02, -6.5794e-03]]],\n      \n      \n              [[[ 5.0372e-04, -8.6271e-03,  1.0006e-02],\n                [ 3.4135e-03,  1.5167e-02,  5.8248e-03],\n                [-1.0910e-02,  1.3427e-02, -1.3862e-02]],\n      \n               [[-5.8365e-03, -1.0430e-02, -7.6966e-03],\n                [-1.7716e-02, -9.2622e-03, -2.5507e-03],\n                [-1.6571e-02,  1.9592e-02, -4.0430e-03]],\n      \n               [[-1.1210e-02,  8.5709e-03,  2.1015e-03],\n                [-5.9349e-03, -1.0947e-02,  2.4249e-03],\n                [-7.9166e-03,  1.3137e-02, -2.1673e-03]],\n      \n               ...,\n      \n               [[-4.7681e-03, -1.3923e-02,  1.5923e-02],\n                [-1.4422e-02,  1.7665e-02, -7.7554e-04],\n                [-3.1083e-03, -1.3903e-02, -9.7266e-03]],\n      \n               [[ 1.0262e-02,  1.5193e-02,  2.1640e-03],\n                [-1.8771e-02,  8.0294e-03, -7.8629e-03],\n                [-1.9121e-02, -6.8685e-03,  4.6865e-03]],\n      \n               [[ 2.6417e-03, -4.6393e-03,  1.9182e-03],\n                [-5.1050e-03,  8.5453e-03, -1.9847e-02],\n                [ 1.1548e-02,  6.4032e-03,  6.7177e-03]]],\n      \n      \n              [[[-1.2669e-02,  2.0572e-02, -1.1325e-02],\n                [ 1.9155e-02,  1.4610e-03, -1.2793e-02],\n                [-9.5103e-03,  1.7256e-02,  3.3774e-03]],\n      \n               [[-2.5392e-03, -8.0457e-03, -3.9132e-03],\n                [-2.8781e-03,  6.9147e-03, -7.6408e-03],\n                [-5.3736e-03,  1.1267e-02, -7.6118e-03]],\n      \n               [[ 8.1662e-03,  2.0775e-02,  2.2655e-03],\n                [-9.1947e-03, -1.9474e-02,  9.5952e-03],\n                [ 6.4101e-03,  1.1485e-02, -1.5912e-03]],\n      \n               ...,\n      \n               [[ 1.7742e-02,  1.8192e-03,  1.5437e-02],\n                [ 1.7205e-03,  1.7005e-03,  1.5712e-02],\n                [-5.3777e-03, -1.3170e-02, -5.7820e-03]],\n      \n               [[ 9.3420e-03, -1.6117e-02, -4.1522e-05],\n                [ 9.9403e-04, -1.8504e-02, -1.7951e-02],\n                [-9.6282e-03, -1.1728e-02, -8.9316e-03]],\n      \n               [[ 1.0781e-02,  9.0107e-03,  1.2392e-03],\n                [ 4.9600e-04, -1.5531e-02, -3.0335e-03],\n                [ 7.1474e-03, -1.8215e-02, -1.5078e-02]]]], requires_grad=True)\n    )\n    (bn1): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([-4.4789e-02, -2.1056e-02, -1.4945e-02,  5.2947e-02,  1.2388e-03,\n              -1.7095e-02,  3.6001e-02,  3.9907e-03,  4.1101e-04, -1.3483e-02,\n              -5.2115e-03,  2.2758e-02, -1.1962e-02,  1.0722e-02,  2.9760e-02,\n               2.1768e-02, -2.1115e-02, -9.2488e-03,  3.9955e-02, -2.9434e-02,\n              -2.9041e-02, -2.9502e-03, -3.5795e-02,  1.7833e-02, -1.5465e-02,\n              -3.8277e-02, -6.0491e-03, -5.1259e-02,  2.0413e-02, -2.4725e-02,\n              -3.9170e-02,  2.6169e-02,  4.5499e-03, -4.9660e-02,  4.8102e-02,\n               8.2191e-03,  2.6229e-02,  7.6105e-03,  2.8636e-02,  1.3744e-02,\n              -1.5909e-02,  1.7036e-02,  3.6237e-02, -1.9736e-02, -1.3698e-02,\n               2.3757e-02, -3.4822e-02,  2.3341e-02,  9.3171e-03, -1.4856e-02,\n               1.2128e-02, -1.8650e-02, -1.2602e-02,  1.0870e-02, -1.2497e-02,\n               3.6992e-02, -2.8356e-04,  9.2733e-03,  2.2204e-02,  1.6363e-02,\n               6.9581e-03, -1.4860e-02,  2.2037e-02,  1.4904e-02,  4.2250e-02,\n              -3.1464e-02, -3.0383e-02, -1.3933e-02,  2.8603e-02,  3.0491e-02,\n              -2.6460e-03,  1.5651e-02, -9.7938e-03,  4.8029e-02, -1.0962e-02,\n              -5.6217e-02,  6.6158e-03, -2.2570e-02, -7.0636e-03,  6.4340e-03,\n               8.7018e-03,  4.2435e-02, -2.4133e-02, -9.4371e-03, -2.8889e-02,\n              -1.7988e-02, -1.0635e-02, -3.5117e-02,  2.3835e-02,  4.1335e-02,\n               7.6724e-03, -4.3901e-02,  1.7069e-02, -4.5431e-02,  7.2284e-03,\n               2.1446e-02,  7.5382e-04,  2.8173e-04,  1.4734e-02, -2.1140e-02,\n               2.6391e-03, -2.2724e-03,  1.3107e-02, -2.5282e-03, -7.4263e-03,\n              -1.3139e-02,  2.6227e-02,  1.2926e-02,  1.8103e-02, -1.1747e-02,\n               6.1793e-03,  3.2050e-03,  3.6255e-02,  4.6549e-02, -3.5188e-02,\n               4.3026e-02, -7.4900e-03, -1.2185e-02, -4.0548e-02,  2.0019e-03,\n              -4.7012e-02, -4.5047e-05,  3.3214e-02,  2.7750e-02,  2.0245e-02,\n              -2.6950e-02, -1.1610e-02,  2.6726e-02,  4.8567e-03,  7.3135e-03,\n              -8.4896e-03, -1.8869e-02,  4.7917e-03, -6.0136e-03, -1.4136e-02,\n               1.2475e-02, -4.3154e-02, -4.1703e-02, -5.1795e-02, -2.2864e-02,\n               4.1200e-02,  8.9519e-03,  1.7954e-02,  2.6540e-02, -6.1045e-02,\n               1.1307e-02,  3.7118e-02,  3.3304e-02,  6.5504e-03, -3.6492e-02,\n              -2.6289e-02, -1.5731e-02,  1.4053e-02, -1.0237e-02,  3.3221e-02,\n              -2.0668e-02, -6.2515e-03, -3.1927e-02, -9.8809e-03,  4.7793e-03,\n               5.3548e-03,  4.3266e-02,  8.7973e-03,  1.3011e-02,  1.2052e-02,\n               4.1951e-02,  2.6735e-02, -8.5230e-03,  3.3820e-02, -4.6238e-03,\n              -1.8727e-02,  2.9801e-02,  7.5179e-03,  4.6045e-02, -3.6257e-02,\n               5.8493e-03,  5.9438e-03, -1.6897e-02,  2.9115e-02,  7.3985e-03,\n               6.8753e-03, -6.0791e-03, -6.2528e-02, -2.9574e-02, -5.9794e-02,\n              -1.1225e-02, -1.4542e-02,  7.2690e-03,  1.8648e-02,  8.3868e-03,\n               1.0397e-02, -3.1843e-02, -5.1808e-02, -2.5480e-02,  3.6817e-02,\n               2.1978e-02,  3.6780e-02, -4.0722e-02, -1.0512e-02, -5.1463e-02,\n              -6.1894e-02, -7.7880e-04, -5.2634e-02,  4.7633e-04,  7.4569e-03,\n               1.3758e-02, -1.7540e-02,  1.9308e-03,  4.2199e-02, -1.6599e-02,\n               5.8403e-03, -3.4997e-02,  4.5967e-03,  3.1989e-02,  2.3055e-02,\n               2.2342e-02,  2.1070e-03,  5.2857e-03, -2.2543e-03,  3.1321e-02,\n              -6.7034e-03, -3.0783e-02, -2.2518e-02, -2.0750e-02,  3.7481e-03,\n              -2.2036e-02,  7.4058e-03,  3.3263e-03, -2.7653e-02,  4.3334e-02,\n               2.5596e-02, -5.5671e-03,  1.0631e-03,  1.8665e-02,  8.1631e-04,\n              -1.7226e-02,  2.6158e-02, -2.4361e-02,  2.9661e-02,  2.4311e-02,\n               4.8173e-03,  1.0439e-02,  9.4750e-03,  1.4837e-02,  9.9004e-04,\n               2.9775e-02, -7.0654e-03, -1.1018e-02, -2.0575e-02, -5.2760e-03,\n               3.0160e-02, -1.2055e-02,  2.1124e-02,  2.0485e-03, -1.2894e-02,\n               8.5490e-03,  5.6528e-02, -8.8002e-03, -2.6806e-03,  3.4730e-02,\n               4.0319e-02, -3.5088e-03, -3.3451e-02, -2.2365e-02,  1.3790e-02,\n               4.6671e-02, -1.1842e-02,  4.3678e-03, -2.9017e-02, -5.9967e-03,\n              -3.7237e-03, -2.4376e-04,  1.9116e-02,  2.8475e-02, -1.9456e-03,\n               7.7845e-03,  2.7725e-02,  8.4274e-03, -3.3462e-02,  1.6623e-02,\n               1.8192e-02, -3.2266e-02,  5.4337e-03, -1.6269e-02, -2.4809e-02,\n               1.5393e-02,  1.2890e-02,  1.4913e-02, -8.1992e-03, -1.5895e-02,\n               1.6275e-02,  1.0631e-02, -3.5348e-02,  5.1448e-02,  2.4001e-02,\n               1.2318e-02,  3.5738e-02,  7.9323e-03, -3.9945e-03,  1.8320e-02,\n              -4.5261e-02, -3.3579e-02,  1.7470e-02,  7.6742e-04,  1.5118e-02,\n              -1.3933e-03,  5.5841e-02, -5.5910e-03, -7.0992e-03,  4.3472e-02,\n              -1.5800e-02, -1.7451e-02,  1.9910e-02,  1.2594e-02, -7.5821e-04,\n              -2.6380e-02, -2.3249e-02,  4.1436e-02, -1.4412e-02,  1.9497e-02,\n              -2.3755e-02,  3.6843e-02,  1.2676e-03,  8.7736e-03, -1.0082e-02,\n               1.0220e-02,  2.0135e-02, -1.1346e-02, -3.9800e-02,  1.5377e-02,\n              -2.9620e-02,  1.5783e-02,  7.2604e-04,  2.3005e-02, -5.1069e-02,\n              -2.1876e-04,  2.3817e-02, -3.8032e-02,  1.9734e-02, -2.0223e-02,\n              -3.5096e-02, -6.5387e-03,  4.6172e-02,  3.2890e-02,  2.9012e-03,\n              -4.0832e-02, -1.2583e-02, -2.6417e-03,  1.7249e-02, -1.3018e-02,\n              -3.5436e-03,  3.0522e-02,  3.8905e-02,  1.1474e-03, -4.9061e-03,\n              -4.7782e-03, -3.7833e-03, -2.2172e-02,  9.9383e-04,  2.1003e-02,\n               4.5382e-02,  1.0590e-02,  3.6566e-02, -2.9301e-02, -3.4150e-03,\n               1.0838e-02, -4.4323e-03, -5.5273e-03, -1.4021e-02,  2.7015e-02,\n              -2.7112e-02,  2.5479e-02,  3.0312e-02, -1.8505e-02, -3.3453e-02,\n              -2.1005e-02,  2.1618e-02, -3.0532e-02,  5.0557e-03,  8.7994e-04,\n               8.7587e-03,  5.0182e-02,  1.9779e-02, -3.6263e-02, -3.3076e-02,\n              -1.0091e-02, -5.1127e-03, -3.9425e-04,  5.2323e-02,  6.7965e-03,\n               1.4198e-02,  3.3532e-02,  3.2899e-02,  1.9263e-02,  1.3118e-02,\n               3.5363e-02,  3.9111e-02, -1.5563e-02, -1.6520e-02, -1.3402e-02,\n              -2.5936e-02, -2.4089e-02, -7.7025e-03, -2.7063e-02, -2.2746e-02,\n               1.1930e-02,  1.3306e-02,  2.7815e-02,  9.7426e-03,  1.3316e-02,\n               1.7949e-02, -8.8595e-03,  4.6241e-03,  8.5529e-03,  6.8710e-02,\n              -3.2393e-03,  3.8151e-02, -2.1084e-03, -1.3885e-03, -1.5150e-02,\n               2.8329e-02,  1.4991e-02,  1.0992e-03,  1.6199e-02,  8.3779e-03,\n              -2.0186e-03,  3.2709e-02, -3.4380e-03,  8.3779e-03, -5.7914e-03,\n              -1.5779e-03,  1.8109e-02,  1.2971e-02, -5.0324e-02, -2.2806e-02,\n               2.0076e-02,  6.9309e-03,  3.8632e-02, -5.4102e-02,  1.0490e-02,\n               4.3415e-02, -5.6509e-03, -4.4865e-03,  3.9230e-02, -9.6737e-04,\n               2.5472e-02,  1.0862e-02,  3.7056e-02,  5.4230e-03,  4.7424e-02,\n              -3.6853e-02,  3.8999e-02,  2.0202e-03,  1.7166e-02, -2.6452e-02,\n              -1.9580e-02,  1.9533e-02, -1.1818e-02, -8.2440e-03, -1.8010e-02,\n              -3.2452e-02, -3.8009e-03, -1.1762e-03, -1.6964e-03, -2.3502e-02,\n              -4.0492e-03,  3.8438e-02,  1.3428e-02, -2.2756e-02, -1.3703e-02,\n              -3.6113e-02, -2.6646e-02, -1.5683e-03, -1.2970e-02, -7.0262e-04,\n              -3.9330e-03,  1.1690e-04,  9.5027e-04, -2.1910e-02, -2.8198e-02,\n              -2.6363e-02,  5.1454e-02,  1.4135e-02, -5.6188e-02,  4.1224e-02,\n               5.3970e-02, -2.6225e-02, -2.5279e-03,  7.6758e-03,  2.3086e-02,\n               6.1348e-02,  1.3235e-02,  8.4143e-03,  5.2645e-04,  1.6426e-03,\n              -2.9903e-02,  1.7451e-02, -3.9872e-02,  2.3480e-02, -6.1908e-03,\n              -5.6011e-03, -4.5957e-03, -3.2401e-02, -2.5126e-02, -3.9429e-02,\n               4.9571e-02,  1.9295e-02,  7.4294e-03,  2.3935e-02,  1.3644e-02,\n               9.8903e-03, -4.2169e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9108, 0.9082, 0.9093, 0.9103, 0.9106, 0.9104, 0.9082, 0.9063, 0.9085,\n              0.9100, 0.9081, 0.9097, 0.9066, 0.9114, 0.9077, 0.9078, 0.9073, 0.9060,\n              0.9093, 0.9085, 0.9089, 0.9071, 0.9065, 0.9120, 0.9084, 0.9076, 0.9092,\n              0.9145, 0.9073, 0.9068, 0.9071, 0.9095, 0.9071, 0.9099, 0.9096, 0.9066,\n              0.9081, 0.9082, 0.9075, 0.9069, 0.9133, 0.9080, 0.9088, 0.9088, 0.9094,\n              0.9073, 0.9098, 0.9079, 0.9066, 0.9090, 0.9087, 0.9086, 0.9107, 0.9097,\n              0.9085, 0.9099, 0.9089, 0.9072, 0.9075, 0.9082, 0.9087, 0.9123, 0.9162,\n              0.9088, 0.9073, 0.9084, 0.9102, 0.9065, 0.9089, 0.9083, 0.9123, 0.9098,\n              0.9113, 0.9124, 0.9075, 0.9097, 0.9102, 0.9121, 0.9084, 0.9078, 0.9109,\n              0.9089, 0.9107, 0.9062, 0.9072, 0.9114, 0.9079, 0.9115, 0.9093, 0.9079,\n              0.9083, 0.9082, 0.9072, 0.9073, 0.9084, 0.9097, 0.9114, 0.9072, 0.9109,\n              0.9151, 0.9091, 0.9093, 0.9085, 0.9099, 0.9078, 0.9093, 0.9081, 0.9087,\n              0.9073, 0.9091, 0.9070, 0.9083, 0.9076, 0.9110, 0.9130, 0.9074, 0.9077,\n              0.9100, 0.9086, 0.9075, 0.9089, 0.9104, 0.9097, 0.9115, 0.9098, 0.9081,\n              0.9077, 0.9096, 0.9068, 0.9072, 0.9088, 0.9101, 0.9068, 0.9070, 0.9075,\n              0.9056, 0.9089, 0.9079, 0.9071, 0.9111, 0.9099, 0.9100, 0.9077, 0.9076,\n              0.9087, 0.9120, 0.9074, 0.9073, 0.9076, 0.9083, 0.9081, 0.9083, 0.9076,\n              0.9092, 0.9114, 0.9083, 0.9070, 0.9092, 0.9082, 0.9081, 0.9068, 0.9076,\n              0.9089, 0.9079, 0.9078, 0.9096, 0.9081, 0.9101, 0.9141, 0.9074, 0.9089,\n              0.9081, 0.9086, 0.9071, 0.9089, 0.9078, 0.9077, 0.9063, 0.9065, 0.9084,\n              0.9073, 0.9068, 0.9122, 0.9070, 0.9071, 0.9070, 0.9098, 0.9081, 0.9098,\n              0.9092, 0.9120, 0.9081, 0.9094, 0.9083, 0.9108, 0.9091, 0.9101, 0.9114,\n              0.9102, 0.9090, 0.9087, 0.9073, 0.9077, 0.9089, 0.9098, 0.9085, 0.9067,\n              0.9063, 0.9092, 0.9063, 0.9078, 0.9079, 0.9074, 0.9099, 0.9069, 0.9087,\n              0.9071, 0.9072, 0.9072, 0.9069, 0.9101, 0.9117, 0.9108, 0.9077, 0.9062,\n              0.9076, 0.9160, 0.9095, 0.9070, 0.9090, 0.9067, 0.9104, 0.9067, 0.9073,\n              0.9065, 0.9135, 0.9067, 0.9091, 0.9132, 0.9079, 0.9113, 0.9075, 0.9084,\n              0.9079, 0.9075, 0.9081, 0.9073, 0.9084, 0.9072, 0.9087, 0.9065, 0.9084,\n              0.9077, 0.9095, 0.9083, 0.9102, 0.9085, 0.9061, 0.9081, 0.9057, 0.9071,\n              0.9098, 0.9077, 0.9073, 0.9084, 0.9094, 0.9104, 0.9161, 0.9075, 0.9076,\n              0.9106, 0.9099, 0.9098, 0.9143, 0.9092, 0.9075, 0.9100, 0.9112, 0.9088,\n              0.9095, 0.9083, 0.9070, 0.9078, 0.9086, 0.9105, 0.9084, 0.9112, 0.9075,\n              0.9101, 0.9095, 0.9080, 0.9080, 0.9090, 0.9098, 0.9076, 0.9073, 0.9085,\n              0.9085, 0.9076, 0.9096, 0.9066, 0.9160, 0.9082, 0.9093, 0.9110, 0.9080,\n              0.9083, 0.9084, 0.9100, 0.9085, 0.9081, 0.9099, 0.9099, 0.9079, 0.9067,\n              0.9089, 0.9107, 0.9082, 0.9074, 0.9054, 0.9087, 0.9097, 0.9065, 0.9088,\n              0.9101, 0.9066, 0.9072, 0.9102, 0.9072, 0.9087, 0.9086, 0.9074, 0.9068,\n              0.9089, 0.9134, 0.9069, 0.9093, 0.9091, 0.9085, 0.9080, 0.9085, 0.9092,\n              0.9085, 0.9083, 0.9063, 0.9096, 0.9080, 0.9087, 0.9073, 0.9110, 0.9058,\n              0.9065, 0.9127, 0.9080, 0.9105, 0.9071, 0.9075, 0.9100, 0.9073, 0.9073,\n              0.9155, 0.9099, 0.9104, 0.9084, 0.9098, 0.9071, 0.9077, 0.9105, 0.9057,\n              0.9087, 0.9079, 0.9104, 0.9079, 0.9069, 0.9112, 0.9093, 0.9094, 0.9133,\n              0.9069, 0.9095, 0.9069, 0.9083, 0.9072, 0.9081, 0.9097, 0.9075, 0.9081,\n              0.9082, 0.9087, 0.9075, 0.9132, 0.9077, 0.9100, 0.9075, 0.9088, 0.9061,\n              0.9083, 0.9073, 0.9084, 0.9082, 0.9060, 0.9076, 0.9078, 0.9075, 0.9079,\n              0.9074, 0.9085, 0.9099, 0.9109, 0.9100, 0.9082, 0.9096, 0.9100, 0.9067,\n              0.9144, 0.9074, 0.9079, 0.9083, 0.9073, 0.9078, 0.9076, 0.9065, 0.9075,\n              0.9060, 0.9073, 0.9074, 0.9091, 0.9088, 0.9075, 0.9083, 0.9116, 0.9082,\n              0.9081, 0.9125, 0.9072, 0.9073, 0.9072, 0.9111, 0.9105, 0.9067, 0.9081,\n              0.9063, 0.9080, 0.9073, 0.9067, 0.9071, 0.9075, 0.9077, 0.9086, 0.9140,\n              0.9097, 0.9062, 0.9092, 0.9094, 0.9100, 0.9061, 0.9067, 0.9102, 0.9064,\n              0.9119, 0.9138, 0.9059, 0.9100, 0.9069, 0.9097, 0.9099, 0.9069, 0.9072,\n              0.9080, 0.9057, 0.9092, 0.9093, 0.9079, 0.9084, 0.9082, 0.9057, 0.9079,\n              0.9093, 0.9069, 0.9116, 0.9087, 0.9140, 0.9112, 0.9102, 0.9098, 0.9087,\n              0.9068, 0.9068, 0.9107, 0.9079, 0.9077, 0.9073, 0.9080, 0.9086, 0.9062,\n              0.9145, 0.9062, 0.9076, 0.9064, 0.9094, 0.9081, 0.9093, 0.9072, 0.9070,\n              0.9142, 0.9064, 0.9072, 0.9110, 0.9095, 0.9066, 0.9080, 0.9085],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (conv2): Conv2d(\n      self.stride=1, self.padding=(1, 1), self.weight=Parameter containing:\n      tensor([[[[ 7.0713e-03, -3.2920e-03,  1.0447e-02],\n                [-1.4654e-02, -1.2171e-02,  4.9875e-03],\n                [ 1.3364e-02, -4.5170e-03, -2.6299e-03]],\n      \n               [[-1.0111e-02,  1.4470e-03,  8.3608e-03],\n                [-9.8386e-03,  5.8603e-03,  5.3375e-03],\n                [ 1.3582e-02, -1.1575e-02,  1.0044e-03]],\n      \n               [[ 6.5463e-03,  1.1208e-02, -1.1922e-02],\n                [-8.2007e-03,  1.1898e-02, -1.0155e-02],\n                [-7.5282e-03,  1.3713e-02,  1.3034e-02]],\n      \n               ...,\n      \n               [[-3.7455e-03,  1.4203e-02, -4.7352e-03],\n                [-3.2614e-03, -3.5699e-04, -7.3458e-03],\n                [-5.2282e-03, -1.4066e-02,  7.4221e-03]],\n      \n               [[-6.5884e-03,  6.6623e-03, -9.8764e-03],\n                [-1.4373e-03, -1.3615e-02, -8.2970e-03],\n                [-1.1727e-02, -1.2306e-02, -1.2668e-02]],\n      \n               [[ 2.8250e-03, -1.4457e-02,  1.1001e-02],\n                [-4.1617e-03, -1.0711e-02, -9.3469e-03],\n                [ 4.3049e-03, -7.9340e-03,  8.4053e-03]]],\n      \n      \n              [[[-7.7739e-03, -1.9621e-03, -9.8135e-04],\n                [ 6.1110e-03,  1.2579e-03,  7.0518e-03],\n                [ 7.7696e-03, -5.3429e-03, -2.1007e-03]],\n      \n               [[ 9.5775e-03, -5.7754e-03,  9.7852e-03],\n                [ 1.2085e-02, -8.1239e-03, -4.4724e-03],\n                [ 5.8853e-03, -1.0717e-02, -1.3844e-02]],\n      \n               [[ 3.4424e-03,  9.3029e-03,  1.4574e-02],\n                [ 1.1712e-03,  1.3389e-02, -8.6091e-03],\n                [-8.7763e-03,  1.1324e-02,  6.3431e-03]],\n      \n               ...,\n      \n               [[-9.0063e-03,  5.7952e-03,  5.9543e-03],\n                [ 1.2521e-02,  1.4654e-02, -3.4027e-03],\n                [-7.9313e-03, -8.2988e-03,  3.3417e-03]],\n      \n               [[ 5.1353e-03, -1.3434e-02, -1.3608e-02],\n                [ 4.7611e-03,  1.3747e-02,  1.3169e-02],\n                [-4.4886e-03,  1.1272e-02, -2.9103e-03]],\n      \n               [[ 6.0364e-03, -1.5401e-03, -5.1181e-03],\n                [-6.7156e-03,  3.5119e-03,  8.3375e-03],\n                [ 9.5208e-03, -1.1309e-02,  2.3291e-03]]],\n      \n      \n              [[[-1.4346e-02,  7.7788e-03, -4.7244e-03],\n                [-1.0401e-02,  1.5612e-03, -3.6299e-03],\n                [-1.4334e-02, -1.2262e-02, -8.1352e-03]],\n      \n               [[ 1.4851e-03,  1.4524e-02, -9.3184e-03],\n                [-1.8766e-03,  3.0339e-03,  1.4712e-02],\n                [-8.7736e-03,  1.4111e-02, -9.6848e-03]],\n      \n               [[ 1.4653e-02,  4.0846e-03,  4.2331e-03],\n                [ 1.1483e-02, -7.1336e-03, -5.0772e-03],\n                [ 4.4636e-03,  1.4093e-02, -1.3839e-02]],\n      \n               ...,\n      \n               [[-1.0724e-02, -1.0500e-02,  1.4854e-03],\n                [ 8.1113e-03, -1.3805e-02, -1.3489e-02],\n                [ 3.5164e-03,  4.2966e-03,  8.8508e-03]],\n      \n               [[-4.1040e-03, -1.0234e-02, -1.2134e-02],\n                [ 4.9530e-03, -9.0202e-03, -3.2892e-03],\n                [ 1.1880e-02, -4.9991e-04,  3.1295e-03]],\n      \n               [[-1.3718e-02, -4.6022e-03, -1.0991e-02],\n                [-5.6592e-03,  1.3277e-02, -1.1039e-02],\n                [ 8.4715e-03, -2.0946e-03, -3.6605e-03]]],\n      \n      \n              ...,\n      \n      \n              [[[-3.4203e-03, -8.9031e-03, -4.3739e-03],\n                [-1.9683e-03,  3.2298e-03,  7.8864e-03],\n                [ 9.9251e-03, -9.9262e-03,  7.8674e-03]],\n      \n               [[ 4.5345e-03,  4.4433e-03, -6.1860e-03],\n                [ 4.3509e-03,  8.8859e-03,  3.6858e-03],\n                [-5.7189e-03, -1.3226e-02, -9.2443e-03]],\n      \n               [[ 1.4575e-02, -9.7981e-03,  1.3875e-02],\n                [-9.3784e-04,  1.3129e-02,  6.6537e-03],\n                [ 1.3642e-02,  1.3218e-02, -6.6855e-03]],\n      \n               ...,\n      \n               [[ 1.1885e-02,  8.4020e-03,  7.5456e-03],\n                [ 4.2505e-03,  9.6383e-03,  4.8911e-05],\n                [-1.4510e-02,  1.3159e-02,  4.2734e-03]],\n      \n               [[ 3.8279e-03, -5.7880e-03,  9.2834e-03],\n                [-3.8896e-03,  1.4494e-02, -7.6106e-03],\n                [-3.0089e-03,  3.2835e-03,  1.0546e-02]],\n      \n               [[-1.7947e-03, -1.2492e-03, -4.7328e-03],\n                [-1.0965e-02,  1.4106e-02,  1.2346e-03],\n                [-1.0316e-02, -9.5004e-03, -4.1754e-03]]],\n      \n      \n              [[[-1.2913e-02, -1.1893e-02, -9.8049e-03],\n                [-9.5277e-03,  5.7639e-03,  2.2479e-03],\n                [ 2.9676e-03, -1.2694e-02,  1.0853e-02]],\n      \n               [[-1.2976e-05,  8.9669e-03,  8.9508e-03],\n                [-1.2403e-02, -4.2248e-03, -9.8009e-03],\n                [ 1.0970e-02,  4.1394e-03, -4.7395e-03]],\n      \n               [[ 1.2465e-02,  1.1580e-05, -1.2282e-02],\n                [ 1.2986e-02,  1.1009e-02,  9.4547e-03],\n                [-1.2455e-02, -2.5444e-03, -3.1725e-03]],\n      \n               ...,\n      \n               [[ 7.5299e-04, -4.5216e-03,  2.5045e-03],\n                [-1.4398e-02, -3.4163e-03,  1.2843e-02],\n                [-1.0438e-03, -8.9786e-03,  1.0931e-02]],\n      \n               [[-1.0878e-03, -9.6602e-03, -1.1507e-02],\n                [ 9.1381e-03,  1.2618e-02,  4.8557e-03],\n                [-1.4041e-03,  4.7521e-03,  3.9449e-03]],\n      \n               [[ 1.4430e-02, -3.5699e-04, -6.8119e-03],\n                [ 1.3306e-02, -4.5083e-03,  2.7349e-03],\n                [ 3.3034e-03,  6.2830e-03,  6.2297e-03]]],\n      \n      \n              [[[ 1.0127e-02, -2.2397e-03, -8.1296e-03],\n                [ 5.5462e-03, -1.3909e-03, -5.8124e-04],\n                [-8.8612e-03, -8.5221e-03,  9.3776e-03]],\n      \n               [[ 1.2451e-02,  3.5359e-03,  1.4668e-02],\n                [-6.4091e-03,  1.1861e-02, -1.1035e-02],\n                [-5.2225e-03, -3.2297e-03,  6.3400e-03]],\n      \n               [[-1.5606e-03,  1.0249e-03,  1.2966e-02],\n                [-4.9489e-03, -7.9487e-03, -3.4327e-03],\n                [ 3.4581e-04, -1.2758e-02, -4.9391e-03]],\n      \n               ...,\n      \n               [[ 1.1185e-02, -7.7479e-03, -2.4527e-03],\n                [-5.4837e-04, -3.6783e-04,  1.4237e-03],\n                [ 9.6851e-04, -1.1696e-02, -3.1552e-03]],\n      \n               [[ 1.8307e-03,  1.1578e-02, -7.4672e-03],\n                [-1.0209e-02,  6.5986e-03,  8.5700e-03],\n                [-1.2304e-02,  1.9765e-03,  1.0818e-02]],\n      \n               [[ 1.2080e-02, -1.0336e-02,  1.2714e-02],\n                [ 4.3818e-03,  1.2681e-03, -9.6825e-03],\n                [-3.2150e-03,  3.9504e-03,  1.0180e-02]]]], requires_grad=True)\n    )\n    (bn2): BatchNorm2d(\n      self.momentum=0.1, self.weight=Parameter containing:\n      tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n              1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n      tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n              0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 9.7260e-03,  4.4709e-03,  1.9530e-02,  1.0657e-02,  5.7080e-04,\n               1.5498e-02,  1.2902e-03, -1.8457e-03, -1.6602e-02, -1.1359e-02,\n              -8.0433e-03, -7.1038e-04, -1.0687e-02,  2.2703e-04, -1.0426e-02,\n               2.3936e-03,  5.8917e-04, -1.1832e-02, -4.3634e-03,  8.2884e-03,\n               8.4581e-03,  3.5326e-03, -3.3877e-04,  2.5142e-03,  1.0097e-02,\n              -7.1396e-03,  1.3453e-02,  6.1933e-03, -1.1790e-03,  8.6360e-03,\n              -7.1744e-03, -6.5477e-03, -3.0232e-03,  7.4612e-03, -1.3750e-02,\n              -2.8676e-03, -5.8559e-03,  1.3651e-02,  2.7943e-03, -1.2915e-02,\n              -3.4294e-03, -1.1450e-02,  3.3298e-03, -4.1072e-03,  6.7737e-03,\n              -3.9050e-05,  1.2488e-02, -1.6832e-03,  5.9994e-03, -8.2133e-04,\n              -2.1022e-02,  1.8883e-04,  4.2444e-03, -3.3275e-04,  8.7710e-03,\n               2.1098e-03, -5.8867e-03, -7.7460e-04, -8.4232e-03, -6.1099e-03,\n               4.8897e-04,  1.0630e-02,  1.9685e-04,  3.4699e-03, -2.2453e-03,\n               6.5591e-03, -8.3525e-03,  3.9388e-03,  1.5222e-03, -2.2234e-03,\n               6.4303e-03, -3.1793e-03, -6.4041e-03, -1.1652e-02,  3.2607e-03,\n               8.2539e-03, -1.9577e-02,  2.3515e-03,  3.4220e-03, -8.2723e-03,\n               8.4005e-03, -6.8366e-03, -1.3770e-02, -3.4866e-03,  1.4529e-02,\n               1.7187e-03,  1.1554e-03,  7.0109e-03, -1.0370e-02,  1.9998e-03,\n               4.5918e-04, -1.4589e-02, -5.5100e-03,  1.1356e-03, -6.8572e-03,\n              -1.1503e-02, -2.7382e-03, -2.5924e-03, -1.6911e-03,  2.4997e-03,\n              -8.0235e-04,  1.1039e-02,  1.2975e-02, -3.5780e-03, -1.6027e-03,\n               1.5060e-02,  2.5950e-03,  5.9378e-03,  1.4439e-04, -5.4955e-03,\n               9.7480e-03, -8.8344e-03, -1.2920e-02, -7.1102e-03, -1.7436e-03,\n              -4.8982e-03, -1.5597e-03,  1.3979e-03,  4.1943e-04,  8.3629e-03,\n               2.3215e-04,  3.9781e-03, -1.4610e-02,  8.4984e-03,  1.5186e-02,\n              -1.3213e-02,  3.6670e-04, -2.2404e-03, -8.8734e-04, -1.2966e-02,\n               1.2462e-02, -4.2342e-03, -6.9969e-03,  5.8559e-03,  5.6479e-03,\n               6.0763e-04,  8.5432e-03, -7.2897e-03, -3.6307e-04,  1.1925e-02,\n              -5.7583e-04, -7.3370e-04,  4.5130e-03,  7.9526e-03,  7.8446e-04,\n              -1.0767e-02, -1.5182e-03, -6.2494e-04, -6.7891e-03, -7.0360e-03,\n               1.1224e-02,  1.7057e-03,  3.4628e-03,  1.4113e-03,  2.3201e-03,\n               3.8508e-03, -3.5098e-04, -8.5141e-03, -5.7745e-03,  9.8633e-03,\n              -2.3786e-03, -1.0368e-02,  1.0231e-03, -4.4314e-03, -1.0905e-04,\n               1.3250e-02, -5.6088e-03, -9.8390e-03, -1.0656e-02,  9.8263e-03,\n               6.4523e-03,  1.2910e-02, -1.4602e-02,  3.5777e-03, -1.1031e-02,\n               7.6468e-03,  1.9280e-02,  6.0253e-04,  4.7834e-03, -9.6638e-03,\n              -1.1236e-02, -8.3752e-03, -5.0167e-03, -6.3180e-03, -4.9217e-03,\n              -1.1525e-02,  6.3175e-03, -8.9822e-03,  4.7164e-03,  8.7178e-04,\n               1.7024e-03,  5.5180e-03,  7.8399e-03, -2.6134e-03, -1.6344e-03,\n              -8.3515e-03,  8.7622e-03,  3.3704e-03,  1.2074e-03, -3.2997e-03,\n              -9.0666e-03, -5.8721e-03, -3.2851e-03,  1.5709e-02, -6.4309e-03,\n               1.6246e-03, -3.6360e-03,  9.9657e-03,  6.2962e-03,  1.0729e-03,\n               4.0143e-03, -9.6130e-04,  9.0832e-03,  1.0230e-03, -1.7765e-03,\n              -1.0832e-02,  5.5038e-03,  1.2157e-02,  7.2719e-03, -5.4740e-03,\n              -1.0529e-02,  3.9875e-03,  1.0458e-02,  1.4475e-02,  5.8962e-03,\n              -1.5795e-03,  3.0422e-04,  1.1694e-03, -3.4501e-03,  3.9255e-03,\n              -9.8299e-03, -2.3531e-03,  2.7725e-03, -2.6556e-03,  1.1134e-02,\n              -5.3787e-03, -1.0171e-02,  7.5351e-03, -3.8660e-03,  9.9357e-03,\n              -1.1591e-02, -2.3691e-03,  4.4021e-03,  7.0900e-03,  1.0873e-02,\n               6.5552e-03, -5.3513e-04,  3.4176e-03,  6.8253e-03, -1.6742e-02,\n               1.4858e-02,  1.4509e-03, -3.6755e-03, -1.9804e-03, -1.4496e-02,\n               3.2452e-03, -1.3512e-02, -7.8332e-03,  1.6427e-02, -4.8768e-03,\n              -3.0869e-03, -6.9169e-04, -7.0451e-03, -2.0382e-03, -1.4382e-03,\n               1.0268e-02,  3.4098e-03, -2.0470e-03, -1.4099e-03,  1.0652e-02,\n               1.1132e-02, -1.4421e-04,  6.4976e-03, -5.6062e-03,  1.3147e-03,\n               1.0294e-02, -9.3742e-03,  1.2423e-02,  1.1802e-02,  4.0683e-03,\n               3.5255e-03, -1.6689e-03, -3.6710e-04, -7.7810e-04, -1.2363e-02,\n              -1.1615e-02, -5.4253e-03,  2.1131e-03, -5.2295e-04,  4.9693e-05,\n               8.6395e-03,  1.2883e-03,  1.8129e-03, -7.3317e-04,  1.1773e-03,\n              -7.8409e-03, -1.5022e-02,  1.6415e-03, -8.5513e-03, -3.1253e-03,\n               1.6083e-03, -7.0237e-03,  5.9203e-03, -1.0008e-03,  4.3533e-03,\n               2.9510e-03,  1.7451e-03,  1.2064e-02, -2.0233e-03,  6.6778e-03,\n               7.2317e-03, -1.6512e-02,  6.8492e-03,  4.3790e-03,  4.2499e-03,\n              -1.2833e-02,  7.4196e-03,  1.2405e-02, -5.9107e-03,  2.3130e-03,\n               2.1594e-03,  6.9920e-03, -7.2882e-03, -1.4435e-02,  1.5387e-02,\n              -1.3662e-03, -7.7761e-03,  5.3097e-03,  7.4181e-03,  7.9634e-04,\n               7.7393e-03,  1.0238e-02,  1.2443e-02,  6.5233e-03, -5.1309e-03,\n              -1.2463e-02,  7.6513e-03,  7.8060e-03,  2.0804e-03,  8.6385e-03,\n               7.1221e-03, -2.8268e-03, -6.8907e-03,  3.5784e-03, -3.4643e-03,\n               4.5031e-05, -1.9325e-02, -2.6668e-03, -1.1658e-02, -1.4185e-03,\n               6.4511e-03,  8.2358e-03,  1.6079e-02,  1.7693e-02, -2.4581e-02,\n               6.8130e-03,  1.0419e-02,  6.0142e-04, -1.0477e-02, -1.3197e-02,\n              -2.6857e-03,  5.7821e-03, -1.2692e-03, -6.0941e-03,  4.7854e-03,\n               1.0340e-02,  4.7610e-03,  5.2462e-03,  6.4901e-05,  3.8905e-03,\n              -8.0123e-03, -3.6134e-03,  1.2543e-02, -6.4292e-03, -2.4238e-03,\n               1.1631e-03, -6.3977e-03, -6.7494e-03, -1.2903e-02, -1.5174e-02,\n              -6.2118e-03,  2.5514e-04,  5.1900e-03, -6.8417e-03,  1.1400e-02,\n               5.2030e-03, -2.9519e-03, -1.0122e-02, -8.6169e-03,  8.3614e-03,\n               2.8791e-03, -1.5343e-02,  4.8989e-03,  4.9542e-03, -2.6082e-03,\n               6.3407e-03, -1.7430e-03, -4.0451e-03,  5.5138e-03, -2.3494e-04,\n               1.8448e-02, -7.5122e-03, -9.4332e-04,  1.7703e-02,  5.6289e-03,\n              -5.0285e-03, -4.2417e-03, -9.3049e-04,  4.9890e-03,  9.2236e-03,\n              -1.7446e-03,  7.4938e-03, -1.2877e-03, -4.4805e-03,  6.1710e-03,\n              -1.1280e-02, -3.3702e-03,  8.1059e-03,  1.7711e-02, -3.5983e-03,\n               1.7789e-02, -1.7080e-02,  8.3383e-03,  1.5136e-02,  2.0254e-02,\n               8.6350e-03, -4.9525e-03,  7.3331e-04,  6.3584e-03, -6.7336e-03,\n               7.6263e-03,  4.7391e-03, -5.2598e-03,  1.7263e-03,  1.1581e-02,\n               6.0956e-03,  4.8775e-04, -1.4674e-03,  3.2978e-03, -1.7906e-02,\n              -1.0818e-04,  1.8581e-02,  8.3462e-03,  1.3991e-02,  1.8379e-02,\n               9.5289e-04,  8.9414e-04, -1.3297e-02, -1.0896e-02, -5.5194e-03,\n               2.6813e-03,  4.3203e-03,  5.3200e-03, -6.9274e-03,  2.9893e-03,\n               2.6003e-03, -8.4684e-03,  1.8878e-03,  8.7866e-04, -9.1304e-03,\n               2.5621e-04, -4.0690e-03, -1.2251e-02, -3.3884e-03, -4.0339e-03,\n              -1.3811e-03,  5.4818e-03, -4.4799e-04, -1.1825e-02, -6.8505e-03,\n              -2.1359e-03, -1.0869e-03, -7.2048e-03,  3.5688e-03, -2.7867e-03,\n               4.9473e-03,  2.5579e-03,  3.0669e-03, -1.5172e-04,  1.1027e-03,\n              -2.0115e-03,  1.1856e-02,  4.7324e-03, -1.4119e-05, -5.4192e-03,\n               1.5957e-03,  2.1425e-03, -7.7463e-03,  4.4239e-03,  1.9235e-03,\n              -6.9629e-03,  3.9352e-03, -1.2022e-02,  1.0292e-03,  1.6523e-02,\n               3.6779e-03,  3.9287e-03, -1.0412e-02, -1.6661e-02, -3.6993e-03,\n              -5.1778e-03, -6.2370e-03, -4.9712e-03, -1.0907e-02,  5.5067e-03,\n               6.1760e-03,  1.1396e-02, -3.4099e-03,  9.5951e-03, -1.1136e-03,\n              -2.6466e-03,  1.3615e-03], grad_fn=<AddBackward0>), self.running_var=tensor([0.9015, 0.9011, 0.9017, 0.9016, 0.9012, 0.9018, 0.9021, 0.9013, 0.9015,\n              0.9011, 0.9011, 0.9013, 0.9019, 0.9011, 0.9014, 0.9015, 0.9013, 0.9012,\n              0.9012, 0.9013, 0.9015, 0.9015, 0.9012, 0.9012, 0.9014, 0.9016, 0.9016,\n              0.9013, 0.9016, 0.9017, 0.9013, 0.9010, 0.9011, 0.9010, 0.9014, 0.9012,\n              0.9012, 0.9015, 0.9014, 0.9011, 0.9013, 0.9015, 0.9015, 0.9015, 0.9010,\n              0.9014, 0.9021, 0.9013, 0.9016, 0.9015, 0.9019, 0.9016, 0.9013, 0.9012,\n              0.9016, 0.9014, 0.9015, 0.9012, 0.9014, 0.9012, 0.9011, 0.9013, 0.9013,\n              0.9009, 0.9012, 0.9010, 0.9013, 0.9014, 0.9012, 0.9017, 0.9017, 0.9012,\n              0.9013, 0.9012, 0.9012, 0.9016, 0.9013, 0.9010, 0.9011, 0.9010, 0.9012,\n              0.9016, 0.9013, 0.9014, 0.9012, 0.9013, 0.9010, 0.9012, 0.9012, 0.9013,\n              0.9009, 0.9016, 0.9011, 0.9014, 0.9015, 0.9012, 0.9011, 0.9015, 0.9014,\n              0.9011, 0.9014, 0.9010, 0.9015, 0.9012, 0.9010, 0.9012, 0.9012, 0.9014,\n              0.9011, 0.9013, 0.9010, 0.9011, 0.9011, 0.9011, 0.9012, 0.9010, 0.9015,\n              0.9011, 0.9011, 0.9011, 0.9012, 0.9013, 0.9017, 0.9013, 0.9020, 0.9012,\n              0.9010, 0.9012, 0.9017, 0.9011, 0.9011, 0.9011, 0.9012, 0.9010, 0.9012,\n              0.9013, 0.9009, 0.9013, 0.9011, 0.9014, 0.9012, 0.9011, 0.9017, 0.9012,\n              0.9011, 0.9019, 0.9012, 0.9012, 0.9011, 0.9014, 0.9019, 0.9020, 0.9012,\n              0.9013, 0.9015, 0.9013, 0.9012, 0.9014, 0.9011, 0.9009, 0.9013, 0.9010,\n              0.9013, 0.9013, 0.9013, 0.9012, 0.9011, 0.9011, 0.9012, 0.9014, 0.9011,\n              0.9014, 0.9015, 0.9013, 0.9011, 0.9012, 0.9016, 0.9014, 0.9012, 0.9014,\n              0.9013, 0.9012, 0.9013, 0.9011, 0.9014, 0.9017, 0.9011, 0.9013, 0.9014,\n              0.9013, 0.9012, 0.9016, 0.9011, 0.9013, 0.9011, 0.9014, 0.9018, 0.9010,\n              0.9013, 0.9012, 0.9012, 0.9013, 0.9013, 0.9012, 0.9013, 0.9012, 0.9012,\n              0.9011, 0.9011, 0.9012, 0.9011, 0.9011, 0.9010, 0.9013, 0.9016, 0.9013,\n              0.9012, 0.9010, 0.9013, 0.9013, 0.9013, 0.9012, 0.9013, 0.9013, 0.9013,\n              0.9014, 0.9012, 0.9012, 0.9014, 0.9014, 0.9016, 0.9017, 0.9011, 0.9012,\n              0.9017, 0.9014, 0.9010, 0.9012, 0.9013, 0.9012, 0.9010, 0.9011, 0.9014,\n              0.9015, 0.9013, 0.9014, 0.9012, 0.9013, 0.9010, 0.9014, 0.9019, 0.9014,\n              0.9011, 0.9011, 0.9012, 0.9011, 0.9014, 0.9011, 0.9017, 0.9011, 0.9015,\n              0.9013, 0.9022, 0.9012, 0.9010, 0.9014, 0.9011, 0.9013, 0.9015, 0.9011,\n              0.9023, 0.9012, 0.9014, 0.9011, 0.9017, 0.9013, 0.9013, 0.9017, 0.9010,\n              0.9013, 0.9012, 0.9015, 0.9014, 0.9013, 0.9013, 0.9009, 0.9012, 0.9014,\n              0.9014, 0.9016, 0.9012, 0.9012, 0.9013, 0.9013, 0.9013, 0.9013, 0.9014,\n              0.9013, 0.9012, 0.9013, 0.9013, 0.9014, 0.9019, 0.9015, 0.9013, 0.9012,\n              0.9013, 0.9016, 0.9017, 0.9011, 0.9011, 0.9012, 0.9012, 0.9011, 0.9015,\n              0.9014, 0.9012, 0.9016, 0.9012, 0.9011, 0.9011, 0.9017, 0.9012, 0.9014,\n              0.9012, 0.9030, 0.9014, 0.9016, 0.9013, 0.9014, 0.9016, 0.9013, 0.9014,\n              0.9012, 0.9017, 0.9014, 0.9020, 0.9014, 0.9015, 0.9012, 0.9012, 0.9014,\n              0.9013, 0.9017, 0.9012, 0.9009, 0.9013, 0.9017, 0.9010, 0.9010, 0.9011,\n              0.9012, 0.9020, 0.9015, 0.9018, 0.9011, 0.9013, 0.9010, 0.9011, 0.9016,\n              0.9017, 0.9014, 0.9013, 0.9011, 0.9013, 0.9012, 0.9011, 0.9014, 0.9014,\n              0.9011, 0.9012, 0.9012, 0.9021, 0.9013, 0.9014, 0.9010, 0.9015, 0.9014,\n              0.9011, 0.9015, 0.9016, 0.9011, 0.9012, 0.9012, 0.9014, 0.9014, 0.9011,\n              0.9013, 0.9011, 0.9011, 0.9012, 0.9021, 0.9010, 0.9016, 0.9013, 0.9019,\n              0.9014, 0.9015, 0.9013, 0.9013, 0.9019, 0.9012, 0.9011, 0.9013, 0.9012,\n              0.9012, 0.9012, 0.9012, 0.9013, 0.9011, 0.9012, 0.9013, 0.9011, 0.9013,\n              0.9017, 0.9013, 0.9011, 0.9013, 0.9012, 0.9010, 0.9025, 0.9013, 0.9014,\n              0.9016, 0.9014, 0.9011, 0.9011, 0.9014, 0.9015, 0.9014, 0.9012, 0.9011,\n              0.9015, 0.9013, 0.9015, 0.9017, 0.9011, 0.9015, 0.9012, 0.9017, 0.9010,\n              0.9017, 0.9011, 0.9012, 0.9013, 0.9011, 0.9013, 0.9014, 0.9012, 0.9012,\n              0.9012, 0.9010, 0.9014, 0.9012, 0.9011, 0.9012, 0.9017, 0.9013, 0.9012,\n              0.9010, 0.9012, 0.9009, 0.9018, 0.9012, 0.9015, 0.9015, 0.9012, 0.9012,\n              0.9015, 0.9012, 0.9010, 0.9013, 0.9011, 0.9011, 0.9013, 0.9011, 0.9010,\n              0.9011, 0.9011, 0.9013, 0.9010, 0.9013, 0.9012, 0.9012, 0.9010, 0.9010,\n              0.9012, 0.9013, 0.9011, 0.9015, 0.9012, 0.9010, 0.9012, 0.9011, 0.9015,\n              0.9014, 0.9011, 0.9014, 0.9011, 0.9012, 0.9011, 0.9013, 0.9012, 0.9012,\n              0.9013, 0.9011, 0.9012, 0.9015, 0.9018, 0.9012, 0.9009, 0.9011],\n             grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n    )\n    (downsample): Sequential(\n      (0): Conv2d(\n        self.stride=2, self.padding=0, self.weight=Parameter containing:\n        tensor([[[[ 0.0078]],\n        \n                 [[-0.0584]],\n        \n                 [[-0.0316]],\n        \n                 ...,\n        \n                 [[-0.0525]],\n        \n                 [[-0.0295]],\n        \n                 [[ 0.0165]]],\n        \n        \n                [[[-0.0399]],\n        \n                 [[ 0.0062]],\n        \n                 [[ 0.0371]],\n        \n                 ...,\n        \n                 [[-0.0231]],\n        \n                 [[-0.0380]],\n        \n                 [[-0.0259]]],\n        \n        \n                [[[-0.0284]],\n        \n                 [[-0.0573]],\n        \n                 [[ 0.0344]],\n        \n                 ...,\n        \n                 [[ 0.0065]],\n        \n                 [[-0.0085]],\n        \n                 [[ 0.0486]]],\n        \n        \n                ...,\n        \n        \n                [[[-0.0503]],\n        \n                 [[-0.0259]],\n        \n                 [[-0.0541]],\n        \n                 ...,\n        \n                 [[ 0.0567]],\n        \n                 [[ 0.0408]],\n        \n                 [[ 0.0562]]],\n        \n        \n                [[[-0.0565]],\n        \n                 [[ 0.0012]],\n        \n                 [[ 0.0038]],\n        \n                 ...,\n        \n                 [[-0.0500]],\n        \n                 [[-0.0195]],\n        \n                 [[-0.0299]]],\n        \n        \n                [[[ 0.0448]],\n        \n                 [[ 0.0369]],\n        \n                 [[-0.0108]],\n        \n                 ...,\n        \n                 [[-0.0409]],\n        \n                 [[ 0.0625]],\n        \n                 [[ 0.0508]]]], requires_grad=True)\n      )\n      (1): BatchNorm2d(\n        self.momentum=0.1, self.weight=Parameter containing:\n        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), self.bias=Parameter containing:\n        tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), self.eps=1e-05, self.running_mean=tensor([ 0.0002,  0.0012,  0.0159, -0.0143, -0.0074,  0.0355,  0.0042, -0.0377,\n                 0.0195,  0.0349,  0.0218,  0.0512,  0.0292, -0.0462,  0.0318,  0.0306,\n                -0.0036, -0.0352, -0.0447,  0.0599, -0.0831,  0.0364, -0.0061,  0.0117,\n                 0.0035, -0.0043,  0.0593, -0.0164,  0.0167, -0.0558,  0.0317, -0.0491,\n                 0.0462,  0.0637,  0.0428,  0.0149,  0.0547, -0.0190,  0.0064,  0.0125,\n                 0.0104,  0.0296,  0.0604,  0.0518, -0.0545, -0.0608, -0.0345, -0.0256,\n                 0.0092, -0.0358,  0.0068,  0.0124,  0.0555, -0.0252,  0.0110, -0.0440,\n                -0.0078,  0.0386, -0.0115, -0.0460,  0.0031,  0.0103, -0.0100, -0.0140,\n                 0.0350,  0.0155, -0.0076,  0.0253, -0.0200, -0.0294, -0.0137, -0.0007,\n                -0.0325, -0.0061,  0.0132,  0.0060, -0.0498,  0.0680, -0.0052, -0.0021,\n                 0.0102,  0.0143,  0.0148,  0.0447,  0.0096, -0.0469, -0.0230,  0.0022,\n                -0.0107, -0.0445,  0.0216, -0.0189, -0.0012, -0.0209, -0.0626,  0.0037,\n                 0.0195,  0.0360,  0.0180,  0.0214, -0.0816, -0.0554,  0.0365, -0.0031,\n                -0.0471,  0.0284,  0.0126, -0.0192,  0.0029,  0.0535,  0.0149,  0.0027,\n                 0.0132, -0.0009, -0.0084, -0.0378,  0.0363, -0.0057,  0.0048, -0.0279,\n                -0.0754, -0.0129, -0.0281,  0.0522, -0.0487, -0.0514, -0.0149, -0.0172,\n                -0.0602, -0.0067, -0.0166, -0.0088,  0.0329, -0.0394,  0.0197,  0.0118,\n                -0.0607, -0.0009, -0.0289,  0.0499, -0.0219, -0.0055, -0.0319,  0.0453,\n                -0.0399,  0.0264,  0.0204,  0.0207, -0.0801,  0.0643, -0.0078, -0.0347,\n                -0.0155,  0.0631, -0.0298, -0.0092, -0.0193,  0.0056, -0.0002,  0.0614,\n                -0.0182,  0.0170,  0.0138, -0.0420, -0.0261,  0.0184,  0.0608, -0.0149,\n                -0.0588, -0.0231,  0.0792, -0.0445, -0.0301,  0.0044, -0.0453, -0.0228,\n                 0.0243,  0.0073, -0.0021,  0.0262,  0.0259,  0.0307,  0.0189, -0.0274,\n                 0.0020, -0.0729, -0.0178, -0.0408, -0.0381,  0.0579,  0.0278,  0.0485,\n                 0.0831,  0.0877, -0.0276,  0.0192,  0.0183, -0.0330, -0.0220, -0.0086,\n                 0.0322, -0.0019, -0.0091,  0.0192, -0.0266, -0.0796, -0.0453,  0.0123,\n                -0.0640, -0.0543,  0.0346, -0.0012,  0.0169, -0.0295, -0.0638,  0.0630,\n                -0.0138,  0.0192, -0.0495, -0.0324, -0.0009, -0.0083, -0.0123, -0.0089,\n                -0.0634,  0.0015, -0.0557, -0.0097,  0.0196,  0.0582, -0.0530, -0.0144,\n                -0.0007,  0.0297, -0.0653, -0.0553,  0.0062, -0.0400, -0.0339,  0.0113,\n                -0.0051, -0.0634, -0.0328, -0.0210,  0.0363, -0.0325,  0.0289, -0.0349,\n                -0.0438, -0.0164,  0.0418, -0.0259, -0.0069, -0.0355, -0.0372, -0.0061,\n                -0.0021,  0.0229, -0.0192, -0.0056, -0.0282, -0.0590,  0.0348, -0.0499,\n                 0.0057, -0.0259, -0.0128,  0.0098, -0.0044,  0.0237, -0.0131,  0.0705,\n                 0.0410, -0.0213,  0.0114, -0.0225, -0.0424, -0.0050,  0.0209,  0.0549,\n                -0.0284, -0.0323,  0.0044, -0.0272,  0.0190, -0.0097,  0.0056,  0.0038,\n                 0.0164, -0.0386, -0.0184, -0.0844, -0.0077, -0.0578, -0.0654, -0.0549,\n                -0.0105,  0.0375,  0.0599,  0.0498, -0.0336, -0.0216,  0.0750,  0.0149,\n                 0.0613,  0.0386,  0.0602, -0.0121,  0.0129,  0.0731, -0.0219, -0.0459,\n                -0.0109,  0.0144,  0.0111, -0.0325,  0.0191,  0.0503,  0.0068, -0.0288,\n                -0.0123, -0.0071, -0.0281, -0.0351,  0.0371,  0.0342, -0.0314,  0.0431,\n                -0.0103,  0.0145, -0.0754,  0.0649,  0.0204,  0.0669, -0.0517,  0.0172,\n                -0.0396, -0.0322,  0.1129, -0.0223,  0.0100, -0.0072,  0.0173,  0.0479,\n                -0.0167,  0.0307, -0.0236,  0.0530,  0.0151,  0.0031,  0.0200, -0.0372,\n                 0.0481, -0.0210,  0.0354, -0.0642, -0.0075,  0.0048,  0.0189, -0.0078,\n                -0.0202, -0.0187,  0.0222, -0.0144, -0.0249,  0.0457,  0.0364,  0.0560,\n                -0.0171,  0.0628, -0.0152,  0.0133, -0.0513,  0.0117, -0.0473,  0.0056,\n                -0.0167,  0.0063,  0.0286, -0.0308,  0.0335, -0.0219, -0.0291, -0.0562,\n                 0.0069, -0.0348, -0.0184, -0.0231,  0.0950, -0.0356,  0.0787, -0.0262,\n                 0.0231,  0.0475,  0.0237,  0.0494,  0.0435, -0.0218,  0.0148,  0.0020,\n                -0.0512,  0.0293,  0.0305, -0.0542, -0.0371, -0.0282, -0.0147, -0.0405,\n                 0.0235, -0.0078, -0.0552,  0.0020, -0.0470,  0.0070,  0.0067,  0.0687,\n                -0.0459, -0.0351, -0.0404, -0.0814,  0.0428,  0.0478, -0.0083, -0.0075,\n                 0.0329, -0.0167,  0.0164,  0.0413, -0.0596, -0.0351,  0.0009,  0.0057,\n                 0.0005, -0.0203, -0.0364, -0.0083,  0.0422, -0.0080, -0.0583,  0.0369,\n                -0.0097,  0.0072, -0.0006, -0.0110,  0.0558, -0.0503,  0.0190,  0.0290,\n                -0.0586, -0.0141, -0.0348,  0.0012,  0.0132,  0.0468,  0.0459,  0.0353,\n                 0.0930, -0.0108, -0.0195, -0.0749, -0.0343,  0.0211,  0.0296, -0.0208,\n                 0.0330,  0.0290, -0.0218,  0.0137,  0.0277,  0.0068,  0.0261, -0.0258,\n                 0.0106, -0.0151, -0.0903,  0.0290,  0.0574, -0.0587,  0.0046,  0.0338,\n                -0.0220,  0.0116, -0.0244, -0.0911, -0.0114, -0.0299,  0.0435,  0.0519,\n                 0.0751, -0.0156, -0.0317,  0.0554,  0.0246, -0.0453, -0.0241, -0.0557,\n                 0.0113, -0.0004, -0.0351, -0.0379,  0.0385,  0.0363,  0.0054,  0.0048,\n                -0.0301,  0.0306,  0.0184, -0.0926,  0.0138, -0.0516, -0.0238,  0.0259],\n               grad_fn=<AddBackward0>), self.running_var=tensor([0.9228, 0.9154, 0.9116, 0.9140, 0.9141, 0.9201, 0.9160, 0.9201, 0.9124,\n                0.9162, 0.9136, 0.9192, 0.9171, 0.9290, 0.9147, 0.9152, 0.9231, 0.9105,\n                0.9183, 0.9176, 0.9119, 0.9186, 0.9171, 0.9192, 0.9157, 0.9139, 0.9158,\n                0.9161, 0.9214, 0.9169, 0.9131, 0.9203, 0.9146, 0.9148, 0.9156, 0.9133,\n                0.9173, 0.9249, 0.9144, 0.9163, 0.9160, 0.9236, 0.9165, 0.9194, 0.9192,\n                0.9162, 0.9163, 0.9161, 0.9201, 0.9112, 0.9141, 0.9120, 0.9158, 0.9135,\n                0.9131, 0.9147, 0.9139, 0.9226, 0.9172, 0.9199, 0.9143, 0.9180, 0.9152,\n                0.9165, 0.9137, 0.9139, 0.9152, 0.9160, 0.9178, 0.9200, 0.9117, 0.9137,\n                0.9144, 0.9175, 0.9176, 0.9127, 0.9180, 0.9300, 0.9129, 0.9138, 0.9220,\n                0.9244, 0.9128, 0.9135, 0.9141, 0.9151, 0.9191, 0.9140, 0.9199, 0.9151,\n                0.9121, 0.9122, 0.9126, 0.9112, 0.9193, 0.9143, 0.9138, 0.9163, 0.9205,\n                0.9096, 0.9184, 0.9149, 0.9161, 0.9169, 0.9165, 0.9138, 0.9201, 0.9252,\n                0.9272, 0.9138, 0.9172, 0.9146, 0.9152, 0.9156, 0.9150, 0.9129, 0.9186,\n                0.9197, 0.9137, 0.9186, 0.9261, 0.9199, 0.9193, 0.9186, 0.9238, 0.9189,\n                0.9158, 0.9145, 0.9311, 0.9158, 0.9173, 0.9215, 0.9124, 0.9196, 0.9122,\n                0.9169, 0.9133, 0.9195, 0.9141, 0.9181, 0.9172, 0.9160, 0.9120, 0.9123,\n                0.9191, 0.9158, 0.9178, 0.9143, 0.9211, 0.9122, 0.9262, 0.9139, 0.9168,\n                0.9140, 0.9136, 0.9218, 0.9140, 0.9214, 0.9189, 0.9167, 0.9198, 0.9172,\n                0.9143, 0.9136, 0.9194, 0.9134, 0.9207, 0.9114, 0.9121, 0.9151, 0.9156,\n                0.9156, 0.9179, 0.9162, 0.9159, 0.9254, 0.9197, 0.9161, 0.9273, 0.9243,\n                0.9162, 0.9126, 0.9217, 0.9214, 0.9191, 0.9267, 0.9215, 0.9221, 0.9163,\n                0.9185, 0.9158, 0.9198, 0.9251, 0.9161, 0.9139, 0.9143, 0.9182, 0.9146,\n                0.9228, 0.9113, 0.9143, 0.9148, 0.9141, 0.9208, 0.9185, 0.9224, 0.9182,\n                0.9170, 0.9138, 0.9150, 0.9150, 0.9149, 0.9207, 0.9259, 0.9131, 0.9271,\n                0.9152, 0.9166, 0.9111, 0.9129, 0.9184, 0.9211, 0.9137, 0.9130, 0.9244,\n                0.9156, 0.9166, 0.9288, 0.9168, 0.9208, 0.9136, 0.9336, 0.9243, 0.9141,\n                0.9303, 0.9189, 0.9161, 0.9137, 0.9210, 0.9192, 0.9185, 0.9190, 0.9143,\n                0.9166, 0.9264, 0.9222, 0.9145, 0.9153, 0.9214, 0.9157, 0.9144, 0.9138,\n                0.9159, 0.9247, 0.9180, 0.9136, 0.9177, 0.9121, 0.9193, 0.9222, 0.9144,\n                0.9169, 0.9198, 0.9168, 0.9129, 0.9196, 0.9174, 0.9165, 0.9178, 0.9123,\n                0.9119, 0.9197, 0.9150, 0.9122, 0.9153, 0.9117, 0.9328, 0.9130, 0.9143,\n                0.9159, 0.9158, 0.9171, 0.9169, 0.9136, 0.9172, 0.9163, 0.9190, 0.9437,\n                0.9170, 0.9233, 0.9158, 0.9188, 0.9152, 0.9165, 0.9180, 0.9166, 0.9138,\n                0.9120, 0.9126, 0.9326, 0.9205, 0.9139, 0.9249, 0.9180, 0.9206, 0.9205,\n                0.9203, 0.9157, 0.9136, 0.9123, 0.9155, 0.9187, 0.9168, 0.9153, 0.9190,\n                0.9150, 0.9161, 0.9156, 0.9150, 0.9192, 0.9122, 0.9292, 0.9120, 0.9152,\n                0.9206, 0.9131, 0.9138, 0.9168, 0.9204, 0.9161, 0.9176, 0.9165, 0.9127,\n                0.9258, 0.9121, 0.9163, 0.9200, 0.9141, 0.9199, 0.9172, 0.9172, 0.9144,\n                0.9192, 0.9187, 0.9159, 0.9160, 0.9199, 0.9152, 0.9114, 0.9215, 0.9161,\n                0.9126, 0.9191, 0.9185, 0.9141, 0.9159, 0.9143, 0.9136, 0.9168, 0.9130,\n                0.9225, 0.9126, 0.9166, 0.9220, 0.9119, 0.9410, 0.9188, 0.9149, 0.9127,\n                0.9388, 0.9126, 0.9178, 0.9136, 0.9134, 0.9157, 0.9236, 0.9146, 0.9166,\n                0.9164, 0.9209, 0.9198, 0.9124, 0.9162, 0.9168, 0.9192, 0.9125, 0.9154,\n                0.9118, 0.9172, 0.9164, 0.9295, 0.9144, 0.9132, 0.9161, 0.9169, 0.9153,\n                0.9188, 0.9157, 0.9214, 0.9228, 0.9137, 0.9227, 0.9108, 0.9156, 0.9265,\n                0.9174, 0.9147, 0.9167, 0.9292, 0.9151, 0.9192, 0.9200, 0.9163, 0.9144,\n                0.9190, 0.9146, 0.9182, 0.9310, 0.9175, 0.9229, 0.9134, 0.9124, 0.9207,\n                0.9188, 0.9152, 0.9162, 0.9174, 0.9213, 0.9192, 0.9140, 0.9164, 0.9133,\n                0.9161, 0.9156, 0.9170, 0.9168, 0.9123, 0.9134, 0.9154, 0.9088, 0.9144,\n                0.9166, 0.9153, 0.9190, 0.9167, 0.9148, 0.9137, 0.9147, 0.9181, 0.9162,\n                0.9131, 0.9167, 0.9255, 0.9137, 0.9128, 0.9155, 0.9212, 0.9113, 0.9195,\n                0.9174, 0.9132, 0.9149, 0.9118, 0.9179, 0.9209, 0.9172, 0.9131, 0.9146,\n                0.9188, 0.9210, 0.9123, 0.9148, 0.9139, 0.9121, 0.9166, 0.9121, 0.9115,\n                0.9140, 0.9188, 0.9191, 0.9133, 0.9239, 0.9161, 0.9189, 0.9147, 0.9153,\n                0.9153, 0.9143, 0.9245, 0.9133, 0.9111, 0.9334, 0.9166, 0.9133, 0.9181,\n                0.9184, 0.9145, 0.9175, 0.9157, 0.9197, 0.9241, 0.9130, 0.9200, 0.9119,\n                0.9182, 0.9123, 0.9360, 0.9308, 0.9153, 0.9236, 0.9164, 0.9243],\n               grad_fn=<AddBackward0>), self.num_batches_tracked=tensor(1.)\n      )\n    )\n  )\n)", "parameters": [["0.conv1.weight", [512, 256, 3, 3]], ["0.bn1.weight", [512]], ["0.bn1.bias", [512]], ["0.conv2.weight", [512, 512, 3, 3]], ["0.bn2.weight", [512]], ["0.bn2.bias", [512]], ["0.downsample.0.weight", [512, 256, 1, 1]], ["0.downsample.1.weight", [512]], ["0.downsample.1.bias", [512]]], "output_shape": [[512, 512, 1, 1]], "num_parameters": [1179648, 512, 512, 2359296, 512, 512, 131072, 512, 512]}, {"name": "avgpool", "id": 140584624434768, "class_name": "AveragePool()", "parameters": [], "output_shape": [[512, 512]], "num_parameters": []}, {"name": "fc", "id": 140584624434720, "class_name": "Linear(in_features=512, out_features=10, bias=True)", "parameters": [["weight", [10, 512]], ["bias", [10]]], "output_shape": [[512, 10]], "num_parameters": [5120, 10]}], "edges": []}